`/`, `:`, `\`, `|`, `<`, `>`, `"`, `?` and `*` are already always stripped.
As said in #500, here we should pull ` `and`.`. (My fault)
You have some unmerged lines here
I'm not sure, setup.py may need to be standalone. I'm looking into it.
There is a solution on stack overflow actually : http://stackoverflow.com/questions/2058802/how-can-i-get-the-version-defined-in-setup-py-setuptools-in-my-package
not necessary, `self.YOUTUBE_URL` or better `self.URL` should be sufficient.
We should really provide a better interface to test against, something along the lines of `download(url)`.
Capitalize S and add a anding dot. (Yes, I'm a pedant perfectionist ^^)
So, what's the behavior on `opts.retries == None`? At least it is not clear...
A ``` python if count == retries: ``` is needed here, otherwise on the last retry the program will say **Retrying...** and then exit.
This should be ``` python if count == retries: ``` since you changed the while condition to `count < retries` and since this will never be reached if the retries are infinite.
Sure! Thanks for pointing out the issue :smile:
This is missing a `cwd=` spec at the latest. If we need a git revision number, we should really think about releasing more often instead.
A definite **-1** on including codenames here. Since the Windows build, we don't release that often anymore, but as soon as that gets going again, weekly releases shouldn't be too uncommon.
Please use `print()` syntax, as we support also Python 3(.3+)
These should be moved below, into the code. `_VALID_URL` is also used by suitable, that's the only reason it's up here.
We should use the various `compat_*` down below and delete these imports, so that the code also runs on Python 3.
The argument will already be a character string, no need to decode it.
These methods should be deleted. If we'd decide to show metadata by default, we should do so for all IEs.
You can simply use `webpage = self._download_webpage(url, video_id)` here.
I believe their generic name is `YouPorn`, so you can just delete this line.
> Umh... this is a breaking change. @phihag should we add a hidden backwards compatibility `--write-srt`, or just specify the change in release notes? > Please stay backwards-compatible.
Here you should simply `trouble()` and `return` out, as it seems to be a fatal error.
I think GenericIE should be the last in the list.
You must enclose in parenthesis : `(commandLineConf if not arguments else arguments)` See the difference in this example: ``` python >>> 1 + 2 + 3 if False else 10 # == (1 + 2 + 3) if False else 10 10 >>> 1 + 2 + (3 if False else 10) 13 ```
Ok, I thougth you wanted to override only the command line arguments, but it sounds sensible to override all the arguments. It that's what you want, then enclose it in parenthesis: `(systemConf + userConf + commandLineConf) if not arguments else arguments`, or write an if/else clause, it's more explicit.
This doc should be updated.
It's better to use `self._download_webpage(url, video_id)`
`split` returns a list, video_id must be a string.
This should be a character string (prefixed with `u`)
We keep in InfoExtractors.py the gen_extractor function with the list and we start moving things to `youtube_dl/extractors/*`. That way once we have moved all we can implement the import tricks if we want.
One question: the name of the new module should be `extractors` or `extractor`?, I personally prefer the plural form.
I think that's a good idea.
Can we save the trickery for later? In the first version, I'd expect ``` python from youtube_dl.extractors.youtube import YoutubeIE, YoutubePlaylistIE from youtube_dl.extractors.escapist import EscapistIE .... ``` That also makes it rather easy to move the extractors off one by one - just keep `InfoExtractors.py` as it is and import the newly moved classes.
I think you don't need `locals()` here.
I'd prefer to use a dictionary `IE_dict` in the form `{'YoutubeIE': <Youtube IE class>}`, it's easy to implement and would simplify this line.
But it would need to modify `gen_extractors`, it's not a great deal.
It may be interesting to allow to add new items just as strings, for example : ``` IEs = [ ('Youtube', ['YoutubePlaylistIE', 'YoutubeChannelIE', 'YoutubeUserIE', 'YoutubeSearchIE', 'YoutubeIE']), 'Generic', ] ``` It would make easier to add simple IEs.
Yes, it may confused, but writing the same thing twice doesn't make much sense. It's also that I don't like to much using tuples for this, I would prefer dictionaries, but they would break the order, so that's the only solution.
It's in the patch if gave to you. Of course your code works, but this can be handled by other functions to simplify things.
In python 3.X print is a function, for printing to screen use `self.to_screen`. But if it's a fatal error then `raise ExtractorError`
Don't do this manually use `_download_webpage_handle`
Try to use `compat_urllib_parse`, it fails in python 3 : https://travis-ci.org/rg3/youtube-dl/jobs/7816570 Before making a new commit, please make sure it runs both in python 2 and python3
Remove that line, this module is not available in python 3 and it's not used in the script.
I'm pretty sure PEP8 mandates the `break` to be in a new line, but that's not that important.
Can we `_html_search_regex`? It looks like virtually all our regexes want to extract HTML.
I'd rather see this part for all extractor errors.
This is in the test suite, and in the test suite, warnings are an error of us, aren't they? So why isn't the implementation ``` raise Exception(message) ```
We don't need this line at all if we use a new helper method that also cleans the HTML from the regex.
Well, the helper method can be smart about when to call `cleanHTML`
`None`, to indicate we didn't get it, and not that we did get it and it was empty.
This can be removed now.
Can we resolve these IDs? There may also be a time encoded in there.
Do we need it? It's not used anywhere else.
This line is not useful, you can just remove it.
This will fail in Python 3, because we'd be posting a string. Instead, we can just pass in a dictionary.
Not necessary, just leave these two lines out.
It's not used inside FileDownloader.py so you probably don't need it.
They are actually different :`MIGcBg` vs. `MIGmBg`
Unless it supports videos from other sites, the IE name already says for what it is.
Instead of `'%s' % video_url` is enough to use `video_url`
It also failed in your previous commit: https://travis-ci.org/rg3/youtube-dl/jobs/8459585. b64encode returns bytes, you must decode to get a string : `base64.b64decode(googleString).decode('ascii')`, I'm not sure since I have little experience with base64.
`title = self._search_regex('\<meta name\="description" content="(.+?)" \/\>',webpage, 'video title')`
For simple regex searchs like those here, you can use `self._search_regex`, it shows nicer error messages when it fails
Aren't you comparing strings here? In any case, I'd rather remove the `format_limit` option, or automatically construct a `-f` string from it for backwards compatibility.
`reduce` is not a built-in in python3 http://docs.python.org/3.0/whatsnew/3.0.html#builtins
Oh, nevermind, we need to set the `downloader` property on the extractor.
This license is not compatible with youtube-dl's Unlicense
You have to use `filepath`, because the destination path can be anywhere in the filesystem, if you use `title` it will end in the current directory.
You can use `is` here, it's idiomatic with `None` anyways.
We have a very similar function already in utils.
You have underindented here with just a tab instead of two. We uses spaces.
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
The formats code should be ordered by format quality: prefering mp4 over webm, for the video it would be: 138 137 248 136 247 ...
This can be written more succinctly as `args.get('adaptive_fmts', u'')`
This option is also available in ffmpeg, `['-ar', self._preferredquality]` shoudl be enough.
`lxml` is not part of the standard library, you'll need to use regular expresions.
Sorry, I meant that YoutubeIE and DailymotionIE should directly inherit from SubtitlesIE or NoAutoSubtilesIE, without an intermediate class.
I think it's better to just make DailymotionIE and YoutubeIE a sublcass of the subtitles IE class
This will fail if `play_path` is not an entry in the dictionary.
Couldn't we simply look at the extension (below) to get the format map? Then we don't have to update this list.
Alright, I suppose it's a good idea to not bother users with the `DASH` variants now.
Nevermind, I have reread that and noticed that there are two loops.
Well, then better write that out - a [google search for AHLS](https://www.google.com/search?q=AHLS) doesn't return anything useful.
Can we discuss the renaming of video dimensions in another PR? In any case, I'm not sure they are all progressive, and that all these formats are standardized.
I didn't realise the normal webpage also had the video urls, thanks!
That's all present in the lines above and below this one, so I'd remove this as well.
This is a tuple, that can't be right. Also, a test is only really useful with some things to test against, like title and md5sum of the image. You can run it with `python tests/test_download TestDownload.test_DefenseGouvFr`.
Side note: Wow, these guys are military, but don't support https? Oh my...
To get regex highlighting, this string should be prefixed with `r`. That's just convention, but plain neat.
Not strictly necessary here, so I'd rather remove it.
Since you don't use the part after the id using `/.*` is enough.
We should download the embed page for videos with age gate and extract the player url.
This should go into `YoutubeDL.py`
I think we should allow this to be either None or False, when using YoutubeDL from a python script it would feel more natural.
`_decrypt_signature_age_gate` can be removed now.
These are not necessarily specific to the fd - I can very well imagine that we'll add progress hooks from the control logic as well (for example _playback has started_ or so)
Here I am for duplicating to support different tools. I think the overhead is tolerable.
Yeah, all the referrer logic here should be automatic. ... Is there a reason we don't have the original url stored in the info_dict, @phihag? Like in `info_dict['webpage_url']` or `info_dict['page_url']` (that we use for RTMP support, and could then avoid to set)
Why not set an empty title? I think you want to check `if value is None` here
Can we calculate that beforehand? Also `os.environ.get` could make this much more readable.
For backwards compatibility reasons, we must also read the old file.
In python3 it's in `urllib.parse`, you can use `compat_urlparse`.
This docstring isn't really saying anything not present in the class name, so feel free to remove it.
Note for us: Before merging, switch this to the new #980 structure, i.e. `formats` entry consisting only of url+format+ext. All further listformats or selection business can be removed as well.
You can simply md5 the description and set the content to `u'md5:_md5_goes_here'`.
There's an additional parenthesis: (u'%(title)s **)**
I would chek here for `self.params.get('test', False))` instead of passing it as an argument.
This can be shortened as ``` py url = 'https://www.youtube.com/annotations_invideo?features=1&legacy=1&video_id=%s' % video_id return self._download_webpage(url, video_id, note=u'Searching for annotations', errnote=u'Unable to download video annotations') ```
This would extract the annotations on every single invocation, wouldn't it? I'd rather only do that when annotations have been requested.
You can import `try_rm` from helper
Nitpick: please remove the space before the `:`
160 is a video format.
and 139 is a audio format
Are the tc_url and the page_url needed, it seems that the test videos can be downloaded without them.
`_search_regex` raises `RegexNotFoundError` (a child of `ExtractorError`) if the regex don't match. so you have to catch it.
This should actually be just `self.url_result(embedded_url)`.
At least for met (and [travis](https://travis-ci.org/rg3/youtube-dl/jobs/13163574#L284)) , the webpage doesn't contain the rating info. If there're only adult videos you can directly set the age_limit to 18.
It seems you aren't using the module, remove this line.
When the parameter is set and quiet is false, it is silently ignored. That is surprising for users. Instead, we should throw an error.
`dynamic` is not a very descriptive name. It looks like what you want is to pass the output both to a file and to stdout. In general, this problem has already been solved by `tee`.
This fails on python 3 and doesn't look too good.
The name `tuppl` is misleading, these are match objects. Also, I'd very much prefer to use the named groups. Also, this doesn't need to be a list, since it is only ever consumed once.
Upps, that's wrong, the results are indeed tuples.
Yes, you're right. This is indeed overly fancy, but works.
The new group is non-capturing and should be marked as such. I'm not sure what the other group (`(["\']])`) was ever for, so we can mark it non-capturing as well
This line can just be removed.
Can you name an example URL? That would make testing on our part much simpler.
Since these regexps are only used by `_real_extract`, they can be moved there. Only `_VALID_URL` must be associated to the class.
This should be just `return info`
By removing the `not` and turning the conditions around, this code can be simplified.
We may use proper XML parsing here and simply call `self._download_xml`
`if mediatype == u'video':` is idiomatic Python.
We now have a fully-fledged format system which can be used.
Same as above, we should create a `formats` array here.
This and the following line are not necessary.
This is superfluous, the extension can be extracted automatically.
Why do we need this translation? The original names seem to be quite fine.
No, ADS is the equivalent of xattr. I'd rather have both in one option.
This line is not needed.
franceinter is also available without the `www` domain. Additionally, dots match everything, so this should be ``` _VALID_URL=r'http://(?:www\.)?franceinter\.fr/player/reecouter\?play=(?P<id>[0-9]{6})' ```
What if the webpage doesn't contain the specified string? This should be far easier and robust by calling `self._search_regex`
What if the title does not start exactly 30 characters after this? Also, what if this string is not found.
According to [PEP8](http://www.python.org/dev/peps/pep-0008/), there should be a space after the comma. But again, better just move the contents of this function to the bottom.
This is missing a test, which we require for all new extractors. Otherwise, we can't be sure that the extractor still works after code changes.
You only ever call these functions once, so you can write them directly.
This line is unnecessary, webpage is never used.
This is not correct in general. I think you only want the URL here.
They write themselves as Dropbox
Tests should always contain an `info_dict` field. You can take the output you get as a starting point
You don't want to match `.*`, but `[^?#]*`. Also, the dot in `dropbox.com` should be escaped.
This helper function is unnecessary, it's easier to just specify the dictionary. In general, I want to remove the other helper functions as well.
`video_id` and all other properties will be the same for all results. That's most likely not a good idea. Instead, construct it as below.
This should not be necessary. Instead, a fitting picture out of `thumbnails` should automatically selected. If it's not, we'll fix that.
By the way, the pythonic way is to just evaluate `thumb_list`
He meant to evaluate if `thumb_list` is true in a boolean check, not to run `eval` with its content. ``` python if thumb_list: ... ``` Is equivalent to: ``` python if len(thumb_list) > 0: ... ```
This parses JSON, we should use a [proper parser](http://docs.python.org/dev/library/json.html).
This parsing looks quite messy. For example, when doesn't this condition hit? In any case, you don't need the parentheses here, but spaces around `==` are quite idiomatic.
This should be video_id.ext (@phihag @jaimeMF why do we ask to specify this instead of the two fields in `info_dict`?), so `home-alone-games-jontron.mp4`
Sorry, I wasn't aware of this. Will change every IE I touch to the new style and deprecate `file`.
Good catch, thanks!
Using the search and replace feature of your editor has converted `"owner_u"` to `"owner_"` ;).
This will fail in Python 3.x. We should use the normal output methods, like `to_screen`.
Sure, that would be fine.
Don't use `;` unless you need to write more than 1 statement per line (personally, I would also avoid doing that)
I meant a _webpage that uses embed.ly to embed some non-video content and also contains a video that is detected by code after this_. I concur that the best action would be to wait for a test case, and then decide how we can exclude it or improve some other code.
There is no group needed here. I've fixed that in master.
There should be an `'id'` and an `ext` key here. You can run the tests with `python test/test_download.py TestDownload.test_OCWMIT` (and `test_OCWMIT_1`). Due to an oversight, they were just skipped. I've fixed this.
If you merge in the newest master, all those silly `u` prefixes become unnecessary. (`r` is, on the other hand, still usefull)
All files should now start with `from __future__ import unicode_literals`. Also, I'd like to follow the conventoin of separating stdlib and our imports.
file is being deprecated. New extractors should specify `id` and `ext` in the info_dict.
This line does not help and is not necessary.
There is no more need for the url group. (And most definitly, it's not needed here). Simply take the URL you're getting.
The info_dict is missing here. I'm considering to upgrade the "missing keys" output to an error.
The second parameter to webpage should not generally be `None` or a generic ID. Instead, pass in an ID of the video.
PEP8 mandates two empty lines here.
Great! The whole point of this is that you can skip all those silly `u` prefixes in your file though ;)
The brackets are not necessary here, returning a plain list is being deprecated.
Since we don't do any network requests after this line, it's not really needed, and does not contribute to the user's experience. Nowadays, our helper methods will already output status reports when it's needed. Formally deprecating or removing all of this is on my TODO list.
You don't need to quote double quotes here.
There are three matching groups in here, one of which can evaluate to an empty string. That's unlikely to be correct.
This matches too much, doesn't it? For example, what if the content is ``` <a href="http://streamingX13Xcl.com/foo/bar.flv"> File name: baz.mp4 ```
You don't need all the `u` prefixes since your file starts with `from __future__ import unicode_literals`.
This line is superfluous. IE_NAME only needs to be specified if you want something other than the class name.
This does not match an ID. Virtually all extractors should have a group called `id` here, even if it just serves as a preliminary ID.
There should be a `_TEST` entry here. Refer to our [developer instructions](https://github.com/rg3/youtube-dl#developer-instructions) for a short introduction.
It is blocking, but that's not the problem - `parser.error` is guaranteed to terminate the program, so this line is indeed superfluous
I'd lose the space here, between `%s:` and `%s`, so that we get precisely what is sent.
By the way, there are trailing spaces in this and the preceeding line. In `git show` (or `git diff` while you're developing) they show up in red, and pep8 will (rightly) fault them.
This looks as if it would be better to split the code into two extractors. You can easily pass from one into the other by returning a `url_result`.
If the output should include the URL, we would do that generally. Instead, pass something useful (like the user name or so) into the second parameter.
According to pep8, there's a missing space after `+` in here.
`xrange` has been removed in Python 3. Simply use `range` instead.
Don't use floating point math for calculations that depend on precise results! Instead, you can simply calculate `((videos_count + self.PAGINATED - 1) // self.PAGINATED) + 1`
This line is superfluous, the youtube-dl core can guess the extension better than that already.
Sorry, we actually request 10KiB, and `K` does stand for Kilobyte in head.
`K` in head stands for `Kibibyte`, the test uses Kilobytes.
Nowadays, you can require the protocol here. URLs without protocol are handled by the generic IE.
Why is the indexing 1-based? That doesn't seem correct. In any case, this code traverses the entire list even for `--playlist-items 1`, which doesn't seem to correct.
Please pick descriptive names. I'd rather call the string representation `playlistitems_str`.
The `list` call is superfluous here and can be safely removed.
`Specify` doesn't explain much. The help should mention that theses are indices, and what base they are. Also, one example is probably enough.
This check is not necessary, the code will simply fail later. Even if it were, you wouldn't need `bool`
The current `playliststart` and `playlistend` options feel redundant with `playlistitems`. First of all, they are ignored when playlistitems is given, and even if not, the code is unnecessarily complex when we could simply set playlistitems to `playliststart-playlistend`
For example, `playlistitems` may come from a graphical selection where the user selects the items of a playlist. It would be highly suprising to download all videos if none are selected.
Remove this field and add the id and extension to the info_dict: ``` 'info_dict': { 'id': '45734260', 'ext': 'flv', ... } ``` You can sea [an example in the README](https://github.com/rg3/youtube-dl/blob/master/README.md#adding-support-for-a-new-site). ``` ```
The test in the generic extractor should be enough, you can complete the information and remove it from here.
Why all the `u`s? They are not necessary when the file starts with `from __future__ import unicode_literals`.
This function is not necessary anymore and deprecated. Basically, all I/O functions now output something, so we don't need to [inform the user about things that should work instantly](http://blogs.msdn.com/b/oldnewthing/archive/2004/04/26/120193.aspx).
The indentation is messed up here, it should be 4 instead of 2 spaces. You may want to get a better editor - a modern editor should take care of indentation automatically.
What was the intention of this change? It does not look intentional.
The indentation is messed up here, it should be 4 instead of 3 spaces.
Then the method should be called `get_pause_status` or be a [property](https://docs.python.org/dev/library/functions.html#property) `pause_status`.
The variable name `shand` is non-descriptive
The variable name `phand` is non-descriptive
data_len is a string so it's wrong to compare it with an integer, and it can't be done on python 3.x
This is never reached if Content-length is not set.
No objections on that, you are absolutely correct. Looks like 32fd27ec982d3ba43bd4abe1cfe0dce9568f17b6 covers all the cases.
Regexes should use raw strings (`r'file=(.+\.mp4)'`), so that the backslash is preserved.
I believe this can be achieved already by setting `extract_flat`.
This value looks an awful lot like a `display_id`
TODO: do ;)
The format ID looks like the bitrate. Is it? If so, we should add a `tbr` or `vbr` entry here.
This line will fail if `int_or_none` returns `None`.
Using `for index, url in enumerate(s['_stream'][::-1]):` would simplify the code
I've tried http://mediadownloads.mlb.com/mlbam/2014/07/12/ in a browser and it takes more than 50 seconds to download. We should use a faster method, for example looking into: http://m.mlb.com/gen/multimedia/detail/6/6/3/34496663.xml
Why is this method needed? All it does is call the superclass
This only works on Python 3! On Python 2, `type('')` is unicode here! We should use `compat_str`
`(byte_counter / rate_limit) - elapsed` sometimes takes negative values (tested on python2). Negative delay to `sleep` results in `IOError` and failed download. There should be at least a check for that.
This should only be printed when sleep interval is provided. It's kind of misleading and unrelevant to see when you didn't want any sleep interval.
We already have that function, it's called `parse_duration` :)
We already have that function as well, it's called `str_to_int`.
the `p` is not optional, the s in `https` is.
This means that if the first line fails, `view_count` is not a declared variable and therefore the following will fail - exactly the opposite of what you want.
`re.search` is almost always incorrect in an extractor. Use `self._search_regex` instead, because the latter fails appropriately when the regexp isn't found.
Oh, you're right. Sorry, I confused that.
Won't video_thumbnails be undefined? I'd suggest returning just one thumbnail, since this is a guess anyways.
You don't need to call `report_extraction` here and above, since the extraction is over already.
`(?:http://)` does not make any sense. I think you want just `http://`. And I'm pretty sure the id should not include a fragment (starting with `#`) if present.
Ok, it shouldn't be needed unless the extractor accepts different types of urls (like the youtube extractor).
No, the scheme (`http://`) should be in the regexp, just not in parentheses. Adding a `#` in the negative character class in the last group should be sufficient to correctly match `http://behindkink.com/1970/01/01/foo#bar`.
Oh, alright, leave it in. I was suprised to find a `\x` instead of a `\u`, but specifically for the non-breaking space, that should be fine.
This fails on Python 2. (duh!) Instead, we should fix SSL support in general.
Ewww, `++a` doesn't do what you think it does. It's the same as `a` or `+a` or `+++++a`, it just does nothing in this case. There is no increment operator in Python, you have to use `a += 1`. Thus this loop may be infinite because you never increment `download_tries`
You'll also need to build the manifest url with the `href` attribute.
This is intenational. The ID is not human-readable, but allows one to find the definitive source of the video (so it should be unique for each video). The default output format includes both title and id, so it's not useful to let them be the same value.
nitpick: `url not in api_response` is somewhat nicer to read.
This should not raise a generic Exception, but an `ExtractorError`.
This looks like a really complicated way of writing `time.time()`
Where did you see anyone mention `file`? We'll remove this then. Newer extractors should just set `id` and `ext`, and `file` will be calculated automatically.
Don't go spelunking in other extractor's internals! Instead, return `{'_type': 'url', 'url': real_url, 'ie_key': 'Soundcloud'}`
This should be a self-contained description, so start with `vk.com`
Why is the `m?` group in here? If it's optional anyways, you can just leave it out ;)
This looks like `orderedSet(m.group(1) for m in re.finditer(r'href="/video([0-9_]+)"')`. Also, since it only gets called once, feel free to move it in the main function.
Where did you get this construction from? It's quite outdated, and we'd be happy to update it ;)
The whole code until here can be simplified to `page_id = self._match_id(url)`
This does not look correct. If the code doesn't do anything, just remove it.
On python 2.6, you must specify the position of the arguments: `url = 'http://x-minus.org/dwlf/{0}/{1}.mp3'.format(video_id, token)`
Could you post an example url that needs this (for example https://myspace.com/fiveminutestothestage/video/little-big-town/109594919 works)
Ok, nice (in my case is 80 vs 180 s). Mention it in the commit.
That should work. Open a new PR with two commits: - `[myspace] Use play_path for faster download` - `[myspace] Add extractor for albums` (it could be done in this PR but you would need to use `git rebase` and `git push --force`)
Yes, but the tests fail as soon as anything fails, even if it's just a warning for the user.
Python interpreters cannot generally interpret tail-recursive functions with constant overhead. Therefore, I'd rather prefer a a loop instead of recursion on error.
The description is always optional, so there should be a `fatal=False` in here.
This is not the right place for this method. Instead, it should be written generically and moved into `common.py` (or just be an option to the `_download_webpage`)
No offense, but did you actually run these tests? Looking at the changes, I'd be quite surprised if they still worked.
This code is not required anymore (it used to - where did you still find it) - `_real_extract` only gets called when th eURL is suitable, and being suitable is defined as `re.match(self._VALID_URL, url)`.
This change is unrelated, you must open a new PR for it. That's one of the reasons why it's a good idea to use a new branch for each PR.
Why have you changed the quotes here? It's not that important, but we strive to use `'` when possible, and have when possible a consistent quote character in a file.
Indenting is messed up here.
`'%s'` is very unlikely to be a helpful error message.
Please prefix this parameter with `rtmp_` so it's clear it's for RTMP only, unless it makes sense to return a result with `real_time` that does not use RTMP.
This does not work in python 2.6.
We should indicate that this is only a guess - the value may be smaller or larger than the actual size.
Do or remove otherwise.
There are more video formats available that should be extracted as well.
Why not just use ``` video_id = self._match_id(url) player_page = self._download_webpage(url, video_id) ```
This should be removed. `player_url` is used for RTMP.
It looks like you want to make the extension optional. If so, please move the dot into the group of extensions - otherwise, it's not obvious what is getting matched here.
Alright, isn't it possible to just catch this case a little bit nicer? I.e. look at `formats` at an opportune moment, and give out a better error message.
`{}` doesn't work in python 2.6.
Yes, that will work.
`{}` does not work in python 2.6
FYI: formats with larger preference value is more preferable.
Why a string `'-1'` instead of integer `-1` here? It causes GfycatIE broken in Python 3.
You are right, it's fine.
Since `bestvideo+bestaudio/best` will be the default, maybe it's better to use `to_screen`.
This will fail if neither mplayer nor mpv is available.
I get `2505a456-9506-4078-bf89-9a3b7d797679`
You still hardcode the path schema. If it changes to, say, `/vrmedia2/<id>-clean.<ext>` the extraction is broken then. You need to capture the whole path: ``` for ext, path in re.findall(r"data-([^=]+)='([^']+)'", webpage): format_url = https://voicerepublic.com + path ... ```
Use `self._html_search_meta` instead.
There is no such field, must be `uploader`.
You may want to output this meaningful error with `ExtractorError` if this is for sure.
Format URLs should be extracted from the page itself (`data-m4a='/vrmedia/2296-clean.m4a'` and so on) as it won't break if storage path schema changes sometime.
The warning should be printed in `check_version`, otherwise it's printed when we check it in `YoutubeDL`. In that case, it may make more sense to rename `check_outdated` to something like `is_updated` or `is_outdated` (you'll have to negate it at least in one place so it's up to you to choose the name).
Ok, but we should use a different message, because > `Your copy of avconv is outdated, update avconv to version 10-0 or newer if you encounter any errors.` doesn't tell the user that it can't be used for `bestvideo+bestaudio` and that it won't get the best quality.
Probably 2.1 and newer work (I guess that was the version I was using for #2089). Apparently 2.2 [is the one that includes libav 10.6](https://ffmpeg.org/download.html#release_2.2).
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
Filenames should be `encodeFilename`'ed, otherwise non-ASCII will make subprocess fail on python2 @ Windows.
I think this should be renamed to channel or container (for what `c` in id stands for).
on python 2.6 the namespace is not allowed, you have to manually especify it or use [`xpath_with_ns`](https://github.com/rg3/youtube-dl/blob/c1c924abfeda45f29b991bb74f315f0e79dcf126/youtube_dl/utils.py#L162).
You should preserve the old test as well. If there are no differences in extraction process apart from dotted URL new test should be set [`only_matching`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/xhamster.py#L43-L46). You may probably want to patch `EMPFlixIE` counterpart extractor the same way.
Is it really required for a single video URL? All tests you provided end up with empty `data` array and construct `playlist` it from [the original `url`](https://github.com/hlintala/youtube-dl/blob/yle/youtube_dl/extractor/yle.py#L109).
For lives you should build `title` with `self._live_title` and set `is_live` to `True`.
this field must be a number.
``` for i, video_url in enumerate(video_urls): ```
I think [`_extract_m3u8_formats`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L847) does the work better. If the existing method does not fit the need, feel free to modify it.
`_search_regex` raises a `RegexNotFoundError` if the given pattern is not found. As a result, this checking does not work.
It's not necessary because `_search_regex` already raises exceptions in failure.
It's better to put all Python 2/3 compatibility codes to [compat.py](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/compat.py)
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
What's the purpose of http://example.com/v.flv here? It always gives a 404 error and I think it's unrelated to iQiyi
Extractors should extract information of all possible formats, not the specified format only. Instead, `YoutubeDL` class handles format selection.
What's the error indicated by `raw_data['code'] != 'A000000'`? It's better to give an informational message.
Ok, keep them.
I see little sense testing extraction for all the different domains since it's all the same. One direct and one rtmp would be enough.
Use `self._search_regex` and `utils.unified_strdate` instead.
For rtmp it's always flv despite of the extension.
`{}` doen't work in python 2.6.
I would prefer mimic the original nova.cz player's behavior instead of letting rtmpdump to parse the URL as it may not always work.
Name an example URL where `og:description` has HTML tags.
Use `utils.remove_end` instead.
The import order convention we try to follow is: 1. `__future__` imports 2. standard library imports 3. internal imports (aka: all relative imports)
This triggers `FutureWarning` on python 2: ``` youtube_dl\extractor\youtube.py:819: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. if segment_list: ```
It's quite weird, while in the youtube webpage it says it was uploaded `May 10, 2015`, the first element where we try to get the date says: ``` <meta itemprop="datePublished" content="2015-05-01"> ``` I will change it to `20150501` for now.
mkv is even more generic but already mentioned.
I think it's safe to use `avi` here since we use extensions for this option. Moreover, it will simplify [this code](https://github.com/aurium/youtube-dl/blob/master/youtube_dl/postprocessor/ffmpeg.py#L297-L301) a bit.
If URL schema changes `format_id` extraction will fail and break the extractor. This should not be fatal.
I think that this line would produce unexpected results with multiple urls. The first one would use the generic extractor and the rest would use the normal extractors. (I may have misunderstood it)
If we are always setting it to `False` I think we can remove the parameter from process_ie_result until we actually need it.
Could you provide more info on that scenario? I was unable to find any example URL that wouldn't be covered by previous patterns.
Though not thoroughly tested, I think [`int_or_none`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py#L1278-1284) has similar functionality.
This line causes SyntaxError in Python 2.6: ``` $ python2.6 test/test_download.py TestDownload.test_Neteasemusic Traceback (most recent call last): File "test/test_download.py", line 11, in <module> from test.helper import ( File "/home/yen/tmp/youtube-dl-ping-neteasemusic/test/helper.py", line 12, in <module> import youtube_dl.extractor File "/home/yen/tmp/youtube-dl-ping-neteasemusic/youtube_dl/__init__.py", line 43, in <module> from .extractor import gen_extractors, list_extractors File "/home/yen/tmp/youtube-dl-ping-neteasemusic/youtube_dl/extractor/__init__.py", line 342, in <module> from .neteasemusic import ( File "/home/yen/tmp/youtube-dl-ping-neteasemusic/youtube_dl/extractor/neteasemusic.py", line 113 time_stamp: text for time_stamp, text in re.findall(lyrics_expr, translated) ^ SyntaxError: invalid syntax ```
`xrange` is not defined in Python 3.x
It might be better to use `base64.b64encode()` here as that function does not append an extra `\n`
[Total bitrate is already included as a criterion in format sorting](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L756). If there's no special need, leaving this field blank is fine.
Fields other than `url` are optional. Use things like `details.get('XXX')` to prevent exceptions if any of them is unavailable.
Seems keys in `info['brs']` are respective heights of each format. If so, using the field `height` is more comprehensive. And there's no need to keep the `preference` field anymore.
In Python 3.x `bytearray` accepts only bytes-like objects, and strings are not.
This is [clearly a copyrighted video](https://en.wikipedia.org/wiki/Fairy_Tail#Anime), and I suspect that they don't have the rights for hosting it. Please use a video uploaded with author permission.
Missing fields should be replaced with `NA` similarly to [output template](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/YoutubeDL.py#L568).
It's a meta M3U8. It's better to extract formats in it via `_extract_m3u8_formats` function.
Usually variables named `page` are for storing downloaded data (webpage, API page, etc.). This variable is usually named as `video_id`, `media_id`, etc. Also, it's better to use `_match_id` function here.
There's no need to use `u` prefix given `unicode_literals` is declared.
Item 6 of [this instruction](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site).
It does not matter that it's not an actual id. The code where you extract the actual `video_id` remains the same.
You don't need to specify any symbolic name for a group since you have only one.
It should be robust in case of some missing fields.
Uh oh. Just use `self._match_id(url)` and change the symbolic name of regex group to `id`.
The keys of the `dash_formats` dict are the `format_id` fields themselves, so you could directly `set(dash_formats)` or `set(dash_formats.keys())` to make it clearer.
Hm, this XML layout reminds me [TNAFlix](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/tnaflix.py) alot.
You don't get it. `xpath_text(item, 'res', 'resolution', True)` => `<height>p`. Extract this height, put into format entry along with `url` and `resolution` as `height`. Stuff all formats into `info['formats']`. Run `_sort_formats` on it, it [sorts on height](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L759). In case of flv you obviously should not sort anything. However for simplicity you can handle flv-case the same way - putting it into `info['formats']` (single format entry is ok).
One more thing, avoid long strings if possible.
Here you generally have a height that you can put into `height` field and `_sort_formats` will sort according to it.
Everything apart from `url` is optional.
That's just for your info. With this you could build info dict right on return without constructing it by pieces that will result in (imho) more simple and clean code.
Use `utils.xpath_text` instead, again with `fatal=False`.
Use `compat.compat_str` instead.
All the fields except `title` should be passed `fatal=False` in order not to break extraction if webpage layout for any of these field changes.
They are in ascending order today. Tomorrow that may change and break the extractor logic. Use generic sorting mechanism: `self._sort_formats`.
No, default is ok. I've just emphasized it should not be fatal in order not to break extraction.
format in terms of youtube-dl is a media pointed by URL that has [some metadata that describes it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L64-L120). `_sort_formats` performs [sorting according this metadata](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L753-L767).
And `audio_selector` as well to catch `-f 'bestvideo+'`.
I would add an additional check here to catch syntax errors like `-f 'bestvideo,,best'`. ``` python if not current_selector: raise syntax_error('Expected a selector', start) ```
Ok, It's up to you.
We could break out of the loop early and do not try to filter if all formats has been already filtered out.
`formats` can be filtered out completely. ``` python -m youtube_dl nVlEpd92MJo -f "best[height<40]" [youtube] nVlEpd92MJo: Downloading webpage [youtube] nVlEpd92MJo: Downloading video info webpage [youtube] nVlEpd92MJo: Extracting video information [youtube] nVlEpd92MJo: Downloading DASH manifest [youtube] nVlEpd92MJo: Downloading DASH manifest Traceback (most recent call last): File "c:\Python\Python279\lib\runpy.py", line 162, in _run_module_as_main "__main__", fname, loader, pkg_name) File "c:\Python\Python279\lib\runpy.py", line 72, in _run_code exec code in run_globals File "C:\Dev\git\youtube-dl\master\youtube_dl\__main__.py", line 19, in <modul e> youtube_dl.main() File "youtube_dl\__init__.py", line 406, in main _real_main(argv) File "youtube_dl\__init__.py", line 396, in _real_main retcode = ydl.download(all_urls) File "youtube_dl\YoutubeDL.py", line 1614, in download url, force_generic_extractor=self.params.get('force_generic_extractor', Fals e)) File "youtube_dl\YoutubeDL.py", line 667, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "youtube_dl\YoutubeDL.py", line 713, in process_ie_result return self.process_video_result(ie_result, download=download) File "youtube_dl\YoutubeDL.py", line 1273, in process_video_result formats_to_download = list(format_selector(formats)) File "youtube_dl\YoutubeDL.py", line 991, in selector_function for format in f(formats): File "youtube_dl\YoutubeDL.py", line 1022, in selector_function yield formats[format_idx] IndexError: list index out of range ```
This won't work in case of live HLS WebVTT streams because you constantly get new subtitle segments at the same playlist URL. It's a shame X-TIMESTAMP-MAP support patch hasn't been merged to ffmpeg yet after 3 years, but in my use-case (vlive.tv) it's not required, so dumping HLS WebVTT via ffmpeg works quite good.
Weird. Yesterday it was working: ``` python -m youtube_dl http://mobile-ondemand.wdr.de/CMS2010/mdb/ondemand/weltweit/fsk0/75/752868/752868_8108527.mp4 [download] Destination: 8108527-752868.mp4 [download] 1.4% of 291.34MiB at 8.53MiB/s ETA 00:33 ERROR: Interrupted by user ``` But not now.
The semantics of tests should be kept. This test should test `*-videoplayer_size-[LMS].html` URL. Same for others: `*-videoplayer.html`, `*-audioplayer.html` (on two different domains).
Extraction from JSON may result in duplicate download links like in your audio test URL.
That's very brittle.
This should be processed with `self._extract_m3u8_formats` instead.
No. With `*-videoplayer_size-[LMS].html` a single media file URL is tested, otherwise - a playlist. Generally, this extractor should be split in two: single media extrator and playlist extractor.
[Use `utils.qualities` instead](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/npo.py#L123).
You don't need this. This is done behind the scenes if `_VALID_URL` requires URL scheme name.
Here you expect `mp4` or `flv` but [`ext` is always `mp4`](https://github.com/Hazmo/youtube-dl/blob/efukt/youtube_dl/extractor/efukt.py#L48).
As already mentioned `id` and `display_id` should be extracted.
Use quotes consistently - only a single or only a double. Use different kind of quotes only if there is no way to use same kind of quotes.
`id` and `display_id` should be tested as well.
Here should be `md5` of the file's first 10k.
This wasn't used previously, is it really needed? If the answer is yes, you should use a `set` instead of a dictionary for keeping track of them.
Since this extractor by itself can't provide too much info, maybe it would be better to remove the _real_extract and _VALID_URL, don't import it in `__init__`and do something similar to`MTVServicesInfoExtractor`, which is the base class for the extractors that need it.
This can be placed in its own file.
If 'title' is not in `js`, there should be a fallback title.
`'only_matching': True` is enough.
That's wrong by the way. It matches zero on more `\d+-` parts. Also do not capture group if you don't use it.
avconv does not support `-cookies`, use `-headers` instead. You should also pass all headers.
m3u8 can't be extracted now since `formats` will be overwritten by the code below.
`duration` must be numeric.
Raise `ExtractorError` instead.
Everything except title and the actual video should be optional.
This URL is now subscriber only. I've managed to find free one: ``` https://shahid.mbc.net/ar/episode/90574/%D8%A7%D9%84%D9%85%D9%84%D9%83-%D8%B9%D8%A8%D8%AF%D8%A7%D9%84%D9%84%D9%87-%D8%A7%D9%84%D8%A5%D9%86%D8%B3%D8%A7%D9%86-%D8%A7%D9%84%D9%85%D9%88%D8%B3%D9%85-1-%D9%83%D9%84%D9%8A%D8%A8-3.html ```
This is not matched by `_VALID_URL`.
It does. It does not supposed to fail completely if some of the optional fields are missing.
Then just keep this code and > create default fallbacks if extraction of these fails
Upper case is idiomatic for constants.
UPPER_CASE is idiomatically used for constants in python. Therefore `_API_VARS` should not change it's state since initially defined. Here you modify it: ``` python self._API_VARS[key] = player_info[key] ```
This is warning, not an exception. Tests fail on warnings if you not tell them otherwise. If description can really be missing and this is expected behavior you should pass `default=None` to `self._html_search_regex`.
There is no need to provide full test for every URL schema if they don't test some special extraction scenario. [`only_matching`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/rds.py#L31-L32) is enough.
`ExtractorError` is not raised when `fatal=False`.
Use more python idiomatic `.get('title')` and `if alt_title:` instead.
Using `self._download_xml` is better here. And by the way, it's uncommon to use prefixes or suffixes in the second field.
Inherit from `InfoExtractor` is enough.
You may want `self._html_search_regex` here.
[`determine_ext`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py#L901) is more robust. However, usually you don't need to specify `ext` in formats dictionary.
Use `self._html_search_meta()` instead.
I would prefer `tags` to be similar to `categories` i.e. a list instead of string.
There is no `getheader` in python 2.
`os.pathconf` and `os.pathconf_names`. You should also test whether `os.pathconf_names` contains `PC_NAME_MAX`. And probably catch an `OSError`.
> os.pathconf and os.pathconf_names both available on unix Not necessarily, [they can be disabled](https://github.com/python/cpython/blob/9586a26986ab6fe8baac15d6db29b5e19c09ba65/Modules/posixmodule.c#L10483-L10489): ``` Python 2.7.2 (default, Aug 3 2015, 13:02:32) [GCC 5.2.0] on linux4 ... >>> import os >>> dir(os.pathconf) Traceback (most recent call last): File "<stdin>", line 1, in <module> AttributeError: module object has no attribute 'pathconf' ```
Use `utils.get_filesystem_encoding` instead.
This check is not necessary now as you test pathconf anyway.
Sorry: ``` python while True: try: info_dict['title'] = ... break except ...: trun_len -= 1 ```
This is already done by `utils.get_filesystem_encoding`.
I would prefer testing with `hasattr` instead to avoid [possible breakages](https://github.com/rg3/youtube-dl/commit/22603348aa0b3e02c520589dea092507a04ab06a) I've learned about the hard way.
> it can work on other systems if it is available That's the main point.
``` Traceback (most recent call last): File "youtube_dl/__main__.py", line 19, in <module> youtube_dl.main() File "C:\Dev\git\youtube-dl\master\youtube_dl\__init__.py", line 410, in main _real_main(argv) File "C:\Dev\git\youtube-dl\master\youtube_dl\__init__.py", line 400, in _real_main retcode = ydl.download(all_urls) File "C:\Dev\git\youtube-dl\master\youtube_dl\YoutubeDL.py", line 1649, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "C:\Dev\git\youtube-dl\master\youtube_dl\YoutubeDL.py", line 669, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "C:\Dev\git\youtube-dl\master\youtube_dl\YoutubeDL.py", line 715, in process_ie_result return self.process_video_result(ie_result, download=download) File "C:\Dev\git\youtube-dl\master\youtube_dl\YoutubeDL.py", line 1315, in process_video_result self.process_info(new_info) File "C:\Dev\git\youtube-dl\master\youtube_dl\YoutubeDL.py", line 1558, in process_info if self.params.get('merge_output_format') is None and not compatible_formats(requested_formats): File "C:\Dev\git\youtube-dl\master\youtube_dl\YoutubeDL.py", line 1543, in compatible_formats set('mp3', 'mp4', 'm4a', 'm4p', 'm4b', 'm4r', 'm4v'), TypeError: set expected at most 1 arguments, got 7 ```
Do not capture group if you don't use it. `id` must match at least one character.
This is not supported in python 2.6.
It's probably better to test for `type` tag to be equal to `video`.
For lives `self._live_title` should be use.
This is also available as JSON if `&type=js` is appended to the query. With JSON extra webm format is served, e.g. http://servix.idnes.cz/media/video.aspx?idvideo=V150730_150323_hodinovy-manzel_kuko&reklama=1&idrubriky=hodinovy-manzel&idostrova=play&idclanku=A150730_150323_hodinovy-manzel_kuko&type=js&element=video1439148852492. This is used by web player itself.
URLs must contain scheme name. There is no need to thoroughly test all the thumbnails.
There is no need to test the exact match for URL since it may change and break the test.
Every field apart from `title` and `url` should be treated optional and shouldn't break extraction if missing. Change every other field to `video_info.get('description')`
Use `utils.int_or_none` instead.
I think we don't end messages with `...` in any other extractor, so for consistency I would just use `'Checking user profile'`.
You should directly do: ``` res = { '_type': 'url_transparent', 'url': 'https://youtube.com/watch?v=' + meta['youtubeKey'], 'id': video_id, } ```
If it can be downloaded with native hls without any problem it should be forced in `_extract_m3u8_formats` and removed from test.
If you want to force this just pass `entry_protocol='m3u8_native'` to `_extract_m3u8_formats`.
Here you should consider the case that `playinfo` does not have the `'message'` field, too.
`<span[^>]+class="name">...` is better.
Is it possible to get the secret from the website rather using an hardcoded one? The key may be changed in the future and the extractor is broken before the new key is committed.
This is not always the case. Here is the `en` lyrics http://y.qq.com/#type=song&mid=001JyApY11tIp6.
It's not an [`.lrc` format](https://en.wikipedia.org/wiki/LRC_%28file_format%29) but just a plain text.
The code should match the content. If it's not possible to figure out the lang code there is no point placing it in subtitles. Moreover it's not time bound so can't even barely be treated as subtitles.
It's not always an .lrc since it does not always follow the format and not bound to time tags.
Just update the dictionary with `subtitles` key when `.lrc` detected. There is no need in code duplication. Also replace `zh-CN` with something more generic like `origin`.
`.lrc` may contain another tags apart from time tags (ID tags) that are completely filtered out now.
This should be relaxed since in most cases there is no need in exact match of dicts or dicts' keys. Moreover some internal, compatibility or autocalculated fields may be placed in original dict. So, `same_keys` feature should be removed completely.
Originally, it was a description. I don't see much point keeping duplicate data. So `origin` and `zh-CN` should be enough.
This dict wrapping trick is only requred when both `i` and `j` are not dicts. This approach will fail with misleading assertion (`invalid value for field _`) when corresponding list items are of different type.
Prefer consistent using of single quotes when possible.
I expected an example URL for > more time in one line
Combining could be moved to a postprocessor and exposed as a new generic cli option (I would prefer it to be in different PR if any).
Combining is a task of a player or any other consumer, youtube-dl should not do that. Otherwise in general we would need to generate all `n! / (n - 2)!` partial permutations of unique subtitles for consistency.
What's the point combining original and translated lyrics to a single file? There should be 2 separated entries in `subtitles` instead.
This should be renamed to `--write-md5`.
It `md5` flag should not go here as an argument. Instead it should be extracted in particular downloader from `self.params`.
I guess no, duplicated options would be misleading for end-user.
Calculate it in chunks after the final file is on disk. As said it makes no sense to calculate it immediately since the final file may be modified by postprocessor.
`sous_titre` may be [an empty string](http://www.francetvinfo.fr/replay-jt/france-3/soir-3/jt-grand-soir-3-lundi-26-aout-2013_393427.html).
Just append `?password=%s` to the URL when `videopassword` is present instead of duplicating calls to `retrieve_data`.
`_` is idiomatic way to denote unused variables.
I've yet cherry-picked your first commit in 12439dd5ec46ccac1ef13db7280473ecef44a096, thanks. As of second lets wait to hear other opinions.
I guess it's ok, I wouldn't use it but I don't mind.
As you have pointed, it could match unwanted urls so I would avoid adding it. If you are only interested in ComCarrCoff, you could try to add a custom extractor because the method may not be used in too many sites..
As a compromise we could add an `--aggressive-extraction` option that will try aggressive strategies like this one.
Yes, argumentless options handling should be moved in separate method as well: ``` python def _argless_option(self, command_option, param, expected_value=True): ... return [command_option] if param == expected_value else [] ```
It may be useful if default option's value is `None` (that I overlooked for `nocheckcertificate`), but I guess it's better to remove it and require usage of argless option method for such cases.
Also `_valueless_option` is probably a better name for it.
Second check should be removed.
This syntax is not supported on Python 2.6.
Some not covered example URLs: https://www.nowness.com/category/art-and-design/antony-donaldson-robert-graham-soft-orange https://cn.nowness.com/category/èºæ¯ä¸è®¾è®¡/å¯¼æ¼åªè¾ä¸åº-antony-donaldson
I see. By the way, `cinematique` branch makes no sense since there is no such extractor.
Some tests for these embeds could be useful, at least for reference.
Request wrapping code can be moved to the base class.
`http://api.nowness.com/api/` part can also be moved to `api_request`.
`id` should also be relaxed to allow hieroglyphs.
Avoid shadowing built-in names, `id` in this case.
python's `json` use escape sequences, but both are supported. I prefer to directly use the character (imaging having to use escape sequences for titles in chines, arabic ...).
I don't see any reason to use unicode escape sequences here, you can directly use `'Ã­'`.
`compat_str`, on python 2.x `str` is the same as `bytes`.
dict comprehensions don't work in python 2.6.
dict comprehensions unsupported in python 2.6.
I don't know if there's an actual performance gain, but it's common to use a list: ``` ret = [] for x in ...: ret.append(newstring) return ''".join(ret) ```
`.*` on both ends of regex make no sense. Also when capturing at least one character is required - there is no sense capturing empty video id.
...and output it here instead of const string `video_id` here that makes no sense.
You should capture a part of URL that represents a video in unique way...
Do not use static strings instead of actual video id.
Just return the playlist all the time.
There is no need in `fatal` when `default` is provided.
Don't capture groups if you are not going to use them.
This code looks similar to `sd` format and can be extracted to a function.
This will fail if `vidwidth` is missing.
`not source_url` implies `source_url == ''`.
Should be `r'var\s+video...`.
You should add a coding cookie on top of the file instead.
This does not prevent from using `self._search_regex` in any way.
Please add `expected=True` here as it's not a youtube-dl's fault.
According to the [W3C HTML5 syntax spec](http://www.w3.org/TR/html5/syntax.html), using `'\s'` to match whitespaces is better here.
Use `for index, (item_got, item_expected) in enumerate(zip(got, expected)):` instead. Either use descriptive names for type variables or don't extract them as variables at all.
This code duplication may be eliminated.
As well as this.
I've already suggested using single quotes.
Use `self._sleep` instead.
No, entries without title should not be ignored. On this stage we only require video id/URL.
Even if all videos have a title the regex may stop to detect the title someday and I prefer getting all the videos in the playlist instead of missing some or all of them. Also, I haven't tested but titles could be empty or have only spaces (I don't know if YouTube checks that before allowing you to submit the video).
It does not matter whether it's permitted or not. Relying on mandatory title where it's technically not required will more likely result in broken extraction if layout changes.
Title part should be optional.
Passing `default=None` to `_search_regex` is cleaner.
Unless I'm missing something, this field is not being checked, it should go inside `'info_dict`' instead.
This may not be a good check. `format_id` like 'abcp' passes the check, too. On the other hand, we usually leave the resolution field `None` if it's unknown.
Duration **must** be in seconds, it's in milliseconds now.
Sorry I didn't check it, it should look into `<div class='more_info'>`
Unfortunately the webpage contains two different descriptions, one in the `<meta name='description' ..>` and the other in the `<p class='initial'>`. I would choose the second because it seems to be more complete and it's what the user sees when browsing the webpage.
For utility functions, either put it in an `InfoExtractor` subclass as `staticmethod` or `classmethod` if it's a site-specific one, or put it in [`youtube_dl/utils.py`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py) if it's a generic one.
These dependencies are unacceptable. Moreover you don't use them.
Either **do** or remove.
`postInfo` contains far more metadata that should also be extracted. Consider [this](https://github.com/taichatha/youtube-dl/blob/master/youtube_dl/extractor/common.py#L63-L198) for a set of relevant fields.
Generally `fatal` and `default` are not used simultaneously. Also the default should be `None` to indicate this value is unavailable. Empty string may be confusing.
There's no need if only one group.
Use `sanitize_open` or at least `encodeFilename`
Consistently use single quotes.
Just `if last_downloaded_segment:` is enough.
Relying on segment's URL is a bad idea since it may contain a dynamic part.
This breaks streaming to stdout.
Since you now skip already downloaded segments the total fragment message displays wrong value after restarting: `[hlsnative] Total fragments: ...`. As a result - wrong download progress data.
Instead of adding new parameters put them into `ctx`.
This could be better placed into a dict.
This field is not necessary - it does not provide more information than what `format_id` and `ext`. Also, height values should be placed in the field `height`.
Use `self._og_search_*` functions here.
Usually we use `md5:(md5 of the long text)` for such cases.
In this case use snippets like: (no testsed) ``` mobj = re.match(self._VALID_URL, url) video_id = mobj.group('id') year = mobj.group('y') month = mobj.group('m') day = mobj.group('d') ```
Use a for loop to eliminate duplicated codes.
Usually this field does use full URLs. Instead `'re:^https?://.*\.jpg$'` as described in https://github.com/rg3/youtube-dl#adding-support-for-a-new-site.
Do not capture a group if you don't use it.
I think this two lines are better using compat_urllib_parse_urlparse and the `_replace` method: - It's not immediately clear clear why you use `video_url[:10]`, it's better if you explicitly change the path - `rendition_url.rindex('.')` won't works as expected if the query contains a dot, and normal `index` will match the dot from the domain.
If they switch to strings this will break formats' sorting. Same should be done for width & height.
This can now be omitted.
Make it non fatal, provide correct `ext`, consider forcing to native hls by default, and set same `preference` as for direct links since for some videos hd is only available via hls (e.g. https://player.vimeo.com/video/98044508).
Wrap in `int_or_none`.
Change to `config_files.get('hls', {}).get('all')`.
This also should not break extraction if it's missing.
This one is not restricted http://www.canvas.be/video/de-afspraak/najaar-2015/de-afspraak-veilt-voor-de-warmste-week.
Duration must be in seconds.
This video is georestricted.
`format_id` should be meaningful and not the same for every format.
No groups apart from `id` should be captured.
Dependency on `requests` is unacceptable.
Use `utils.int_or_none` instead. Use `data.get('duration')` instead of testing key manually.
`ext` here makes no sense.
Missing `type` and `url` in `target` should be tolerated without extractor breakage.
Matched data-video should not be empty.
`self._search_regex` is enough here.
Correct field name is `format_id`.
If first target happens to fail no formats will be extracted at all.
Just copy paste the whole line I've posted.
Change to `r'data-video=(["\'])(?P<id>.+?)\1', webpage, 'data-video', group='id')`.
`r'data-video="(.+)"'` is greedy and therefore will match everything from the first to the last quote. If webpage layout will change to something like `data-video="mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true"` your regex will match `mz-ast-5e5f90b6-2d72-4c40-82c2-e134f884e93e" data-preroll="true`.
Use `self._match_id` is better.
Also pass `m3u8_id='hls'`.
Missed `'` after \1.
Every field apart from title should be treated as optional and extraction should not fail when it's missing.
The idiomatic way to extract `id` is to use a group in `_VALID_URL`.
Instead of try-except you should pass `default=None` for first `config URL` and try alternative way when `not config_url`.
Usually we use syntaxes like 'soundcloud said: ' but not brackets in error messages.
`{}` doesn't work in python 2.6.
> The number of items in the collection returned is limited to 50 by default with a maximum value of 200 Collection is a single page thus 200 is the limit for a single page. So, `_RESULTS_PER_PAGE` should be set to 200 and `_MAX_RESULTS` to `float('inf')`.
Don't use `u`.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
This is already imported and (in general) you should only use `import`s at the top level.
This makes no sense since it's redirected to https anyway.
It shouldn't fail if `user` or `username` is missing.
All this metadata is available and should be extracted.
This extracts mess instead of download URL: ``` >youtube-dl http://www.youporn.com/watch/505835/sex-ed-is-it-safe-to-masturbate-daily/ -f 720p-1500k-cdn2b-0 -g http://cdn2b.download.youporn.phncdn.com/201012/17/505835/720p_1500k_505835/YouPorn%20-%20Sex%20Ed%20Is%20It%20Safe%20To%20Masturbate%20Daily.mp4?rs=200&ri=2500 &s=1445177200&e=1445350800&h=178da8509b9b869e9ad12809e016f078' title='Download Video'>MP4 HD - For Windows 7/8, Mac and iPad</a> <span class='downloadsize'>(37 MB)</span> </div> <div class='downloadOption'> <i class='icon icon-download to-text-green'></i> <a href='http://cdn2b.download.youporn.phncdn.com/201012/17/505835/480p_370k_505835/YouPorn%20-%20Sex%20Ed%20Is%20It%20Safe%20To%20Masturbate%20Daily.mp4?rs=150 &ri=1000&s=1445177200&e=1445350800&h=e5b5291f1ade3a2ea66af312daf37a04' title='Download Video'>MP4 - For Windows 7/8, Mac and iPad</a> <span class='downloadsize'>(9 MB)</span> </div> <div class='downloadOption'> <i class='icon icon-download to-text-green'></i> <a href='http://cdn2b.download.youporn.phncdn.com/201012/17/505835/240p_240k_505835/YouPorn%20-%20Sex%20Ed%20Is%20It%20Safe%20To%20Masturbate%20Daily.mp4?rs=150 &ri=1000&s=1445177200&e=1445350800&h=7b1ceb68fd3affe8f2308a6501cbb218' title='Download Video'>MP4 - For iPhone/iPod</a> <span class='downloadsize'>(6 MB)</span> </div> <div class='downloadOption'> <i class='icon icon-download to-text-green'></i> <a href='http://cdn2b.download.youporn.phncdn.com/201012/17/505835/180p_150k_505835/YouPorn%20-%20Sex%20Ed%20Is%20It%20Safe%20To%20Masturbate%20Daily.3gp?rs=150 &ri=1000&s=1445177200&e=1445350800&h=c8ce8aa7f320c8c9253a1a3a6817539c ```
Use `\s*` instead.
All these fields should be `fatal=False`.
There is no such field. Should be `upload_date` in `YYYYMMDD`.
These are also available.
Just remove the `try:` and `except:` lines.
`self._html_search_meta` is better here.
`_html_search_regex` or `_html_search_meta` does not raise `KeyError` in case of not found.
duration must be `int`.
This syntax is not supported in python 2.6. You should use `xpath_*` from `utils`.
Avoid shadowing built-in names.
you can get json output by appending `&format=json` to the api request url.
Since you use regex anyway you can just capture both numbers and use plain `int`. Also consistently use single quotes.
This will fail if `width_x_height` will not contain `x`.
Extraction should not fail due to changes in XML layout. If `key` is removed the whole extraction will be broken.
You may not specify this since you provide `thumbnails`. It will be autocalculated.
Same here. And use `utils.int_or_none` instead.
The method we follow here is to use the `_VALID_URL`, which you would define as: ``` python _VALID_URL = r'https?://(?:www\.)?stitcher\.com/podcast/[\/a-z\-]+(?P<id>\d+)\?.+' ``` And then you only need to do: ``` python audio_id = self._match_id(url) ```
As said duration must be int and in seconds.
Should be `fatal=False`. Use `utils.int_or_none`. It must be in seconds.
`[\s\S]*` on both sides are noop due to `*`.
Should be `fatal=False`.
i didn't mean to check the urls, i mean to extract all `mpath` urls by putting them in a `formats` array and return them with id and title.
The previous `print` looks like a debug statement. Please remove it. And, `ExtractorError` (with `expected=True`) is better than `ValueError` here.
it's better to check if `data['result'] == 0` if not than raise an ExtractorError with `data['msg']`.
it better to extract all the urls in the `mpath` array.
You could just do: ``` python is_video = mobj.group('type') == 'Video' formats = [{ 'url': url_info['url'], 'vcodec': url_info.get('codec') if is_video else 'none', 'width': int_or_none(url_info.get('width')), 'height': int_or_none(url_info.get('height')), 'tbr': int_or_none(url_info.get('bitrate')), 'filesize': int_or_none(url_info.get('filesize')), } for url_info in urls_info] ```
Looks like old videos with 5 digit length video id are still available via xstream in much better quality than vgtv.
Then make it compatible? It's not about using in another extractor but about using as shortcuts by user in the first place. And it obviously should be preserved for backward compatibility.
I've already suggested how to cover both scenarios without getting any error. > Looks like old videos with **5 digit length video id** are still available via xstream in much better quality than vgtv. New video ids are 6 digit length.
I'm aware of that.
This field appears twice.
I would always return a `multi_video` result.
I don't have a strong preference, but you can achieve the same with: ``` python for format_id, fmt in playlist.items(): if format_id.isdigit(): formats.append({ 'url': fmt['src'], 'format_id': '%s-%s' % (fmt['quality'], fmt['type'].split('/')[-1]), 'quality': quality(fmt['quality']), }) ```
That's not needed, it's automatically detected.
One more to `compat_str`.
Actually I mean to process `videos` always when `videos` present and also try playlist when either no `videos` present or `type` is `evt` (or not `vpr`).
Maybe, who knows. At least we can try playlist if `videos` is empty and if type is `evt` (or not `vpr`).
This branch should probably take place always when `cfg['type']` equals `"evt"` regardless of the `cfg['videos']` content - potentially `videos` of `evt` item may contain some promo videos that will "shadow" the actual videos of this event with current approach.
This is unused.
This will fail if `cfg` turns out to contain inner dictionary.
And this as well.
I also saw `cfg:{.+?},ga:function(`, so let's just add both: ``` python [r'cfg\s*:\s*({.+?}),[\da-zA-Z_]+:\(?function', r'cfg\s*:\s*({[^}]+})'] ```
Directly access the opener, after all we are only using it internally.
It's unused now.
Please remove debugging lines.
This is too broad. It must not capture plain text URLs.
Do not break PEP 8 guidelines.
It does since there may be no postprocessing at all.
This does not stylistically matches [opening message](https://github.com/hedii/youtube-dl/blob/output-message-when-playlist-downloaded/youtube_dl/YoutubeDL.py#L747), should be same prefix. Also it does not mention the playlist name.
Opening message is already applied for `multi_video`, I don't see any reason for closing message not to be applied for it as well.
Relying on `userMayViewClip` is probably a [bad idea](https://github.com/rg3/youtube-dl/commit/2b6bda1ed86e1b64242b33c032286dc315d541ae#diff-0b85b33765d2939b773726fda5a55b06).
Neither do I. But afair when I had one working, `userMayViewClip` caused some problems by not always correlating with videos that can't actually be downloaded.
This change breaks MTVServicesEmbeddedIE
When there is no `topicTitle`, `categories` will be `[None]` that does not make any sense.
This field is added automatically no need to add it by hand.
`InfoExtractor._check_formats` should be used for validating download URLs.
Instead of this loop use [anitube's approach](https://github.com/nexAkari/youtube-dl/blob/6722ebd43720e836af8217fa078fa1a604b98229/youtube_dl/extractor/anitube.py#L35-L51). For text fields use `utils.xpath_text`.
Use single quotes consistently.
hd should be always tried to be extracted whether it's present or not. There is no need in this flag.
Also there is no need in `info` here - you can return info dict from here and add more fields at the call place once it's returned.
Usually we place each imported name in individual lines. For example: ``` from ..utils import ( sanitized_Request, ExtractorError, urlencode_postdata, ) ```
In this case `quality` is determined from other attributes, like the bitrate or the resolution. There's no need to specify `quality` values explicitly.
Usually we don't use the URL as `video_id`. If `video_id` is unknown yet, it's OK to use part of the URL, for example "a-housecat-a-stray-god-and-a-tail".
This makes no sense - you already have it in `url`.
Second argument should be `video_id`.
You should consult some git manual.
This line is unnecessary.
You should not silence all another exceptions but re-raise.
This line is unnecessary.
All fields that may potentially contain non-ASCII must be [utf-8 encoded](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/vimeo.py#L43).
`medium`, `high` and `highest` result in a webpage downloaded instead of media: ``` [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'http://www.funimation.com/shows/air/videos/official/breeze', u'-f', u'highest', u'-v', u'--proxy', u'159.203.253.115:8080'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.05 [debug] Git HEAD: 7b99b5f [debug] Python version 2.6.6 - Windows-2003Server-5.2.3790-SP2 [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4 [debug] Proxy map: {u'http': u'159.203.253.115:8080', u'https': u'159.203.253.115:8080'} [Funimation] http://www.funimation.com/shows/air/videos/official/breeze: Downloading webpage [Funimation] http://www.funimation.com/shows/air/videos/official/breeze: Extracting information [debug] Invoking downloader on u'http://wpc.8c48.edgecastcdn.net/008C48/SV/480/AIRENG0001/AIRENG0001-480-4000K.mp4?RGJ2KdXHZ87YZHjym4dN6OYXCAJDiGuI4QWB4gyktVzSPnQmAbKYudcnkn0mya80YoaX5BYPLd2LZeLymJGe9zsjAuZy7bZWL3o-Wx-fj1lLOTipN9wMF6aA4YaNWiIC-DXAGmKINCMYJ-3rc9x6_jEvBw' [download] Destination: Air - Breeze-AIRENG0001.mp4 [download] 100% of 10.29KiB in 00:00 ```
Put `raise` after `if` section at the same indent.
This can be easily detected from m3u8 that's served in `sdUrl` for default youtube-dl UA.
There's no need to create a base class if there's only one derived class.
There should be an `id` group.
In this case video extensions can be correctly determined from URLs. There's no need to specify here.
You don't check whether login succeeded or not.
You try logging in even when no username/password is specified. This makes no sense.
I guess it a typo? now -> not
Making logging in with credentials mandatory prevents ability to authenticate with cookies.
Such a format is not available in Python 2.6. Use `'{0}-{1}K.mp4{2}'` instead.
There's no need to `return` after `raise`.
There should be a fallback title if `_og_search_title()` returns `None`.
It's not allowed in Python to add strings and integers directly. The following syntax should work: ``` '%s-%d.mp4' % (video_base, video_bitrate) ```
An example is the last few lines of [facebook.py](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/facebook.py).
Just assign the correct value to `video_id` later.
``` python try: webpage = self._download_webpage(request, url) except ExtractorError as e: if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: raise ExtractorError('%s is not available in your region.' % self.IE_NAME, expected=True) raise ```
Don't try logging in when `username` is `None` obviously.
This is used to give lower priority to f4m manifests (for particular formats the extension is set to `flv`), I would keep it.
Yes, but we should use `self._extract_f4m_formats` instead, which already sets the `ext` field
Code at lines 37-42 is similar to code @ 51-56. It should be moved to separate routine.
Pass `default` to `_og_search_title` instead.
Login should be in `_real_initialize`.
Please remove them. We don't keep unused codes in the codebase.
I know that. If Youku merges the two API calls, we should do the same thing. EDIT: Sorry I didn't see the new commits below.
`data1` and `data2` use the same URL. Please merge their usages.
If I got it right, resourse unpacking happens every time `tr` is called. Have you measured the overhead imposed by this approach? Probably it would be better to unpack it once to temp dir on start and cleanup on exit.
It's probably more idiomatic to receive a list of path segments without separators here and pass them to `os.path.join` again as list of segments.
When running as `youtube-dl.exe` build with python 2 from path containing non-ASCII `find_file_in_root` ends up returning `None`. When `localedir=None` is passed to `gettext.translation` it picks up `_default_localedir` constructed using `os.path.join` with mixture of byte strings and unicode strings that results in similar problem: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "gettext.pyo", line 468, in translation File "gettext.pyo", line 451, in find File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` We can workaround be skipping it completely when we found no locale dir: ``` python locale_dir = find_file_in_root('share/locale/') if locale_dir: try: ... ``` Or we can mimic what `gettext` do by appending extra path in `get_root_dirs`: ``` python ret.append(os.path.join(decodeFilename(sys.prefix), 'share', 'locale')) ```
On python 2 `root` is byte string and `file_path` is unicode. When `root` contains non-ASCII `os.path.join` fails when trying to decode with default ascii encoding: ``` C:\temp\ÑÐµÑÑ\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {'http': 'http://127.0.0.1:8888', 'https': 'https://127.0.0.1:8888'} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "youtube_dl\utils.pyo", line 2604, in find_file_in_root File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` `root` should be decoded with proper encoding: ``` python full_path = os.path.join(decodeFilename(root), file_path) ```
This condition is not needed, t is always None here.
All these import are unused, check your code with flake8.
There must be [`from __future__ import unicode_literals` on top](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site).
Use `self._download_json` instead.
Use `self._parse_json` instead.
`ch_userid` is mandatory, without it 404 is returned, e.g. http://channel.pandora.tv/channel/video.ptv?prgid=53294230&ref=main&lot=cate_01_2 shouldn't be matched. `http://(.*?\.)?` should be `http://(?:.+?\.)?`. `(?P<prgid>.*?)` should be `(?P<prgid>.+?)`. `.` representing dots should be `\.`.
We don't use underscore in extractor file names.
You've already extracted `prgid` and `ch_userid` from URL, you should not additionally rely on these fields in `video_set`.
Second argument should be `video_id`.
[Correct way](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/pluralsight.py#L95-L100) to implement order independent argument matching.
Also always use single quotes whenever possible.
Part after `\?` should be removed since it's not used anymore.
Use `js_to_json` and `parse_json`.
You don't need list here. Just return it directly.
Outer parentheses are not idiomatic in python.
> return it directly ``` python return self.url_result('http://imgur.com/%s' % album_id) ```
there is a new extractor for the jwplatform(you should rebase your work on a newer version). it can extract all the formats and info.
it's better to use self._match_id(url).
I guess `p7jnfw5hw9_187060b6fd` or `p7jnfw5hw9-187060b6fd` is better than `p7jnfw5hw9187060b6fd` for readability.
FYI: `_sort_formats` uses multiple criteria for sorting, and `format_id` is the last one. https://github.com/rg3/youtube-dl/blob/bd3f9ecabeb5e189f16580c61c56d93148079363/youtube_dl/extractor/common.py#L810-L826
It's OK to have no `id` group for complicated cases, such as query parameters. [Here's another example](https://github.com/rg3/youtube-dl/blob/bd3f9ecabeb5e189f16580c61c56d93148079363/youtube_dl/extractor/youtube.py#L1940). However, usually it's better to have some named groups.
This will fail in Python 3.2 as old `base64.b64decode` requires bytestrings. You need: ``` base64.b64decode(v['play_url']['main_url'].encode('utf-8')).decode('utf-8') ```
You need [`url_transparent`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L232-L237). For example: ``` return { '_type': 'url_transparent', 'ie_key': 'LetvCloud', 'description': description, ... } ```
The group `id` is not for IDs but URL parameters. This can be misleading, and I suggest a different name.
Some fields (for example width, height, format_note, etc.) are optional. Invalid or missing values should be gated. For example: ``` int_or_none(v.get('play_url', {}).get('vheight')) ```
Here using list comprehension simplifies the code: ``` urls = [{ 'url': ... ... } for v in play_json['data']['video_info']['media'].values()] ``` And by the way, there should be a meaningful name instead of `v`.
Information extractor description should be a more meaningful name. You can even use non-English names, for example `ä¹è§äº`.
Don't forget to call `self._sort_formats()`, or the formats are not sorted: ``` $ youtube-dl -F test:letvcloud [TestURL] Test URL: http://yuntv.letv.com/bcloud.html?uu=p7jnfw5hw9&vu=467623dedf [LetvCloud] p7jnfw5hw9467623dedf: Downloading playJson data [info] Available formats for p7jnfw5hw9467623dedf: format code extension resolution note 52 mp4 1920x1072 1080p 13 mp4 960x544 é«æ¸ 51 mp4 1280x720 720P 28 mp4 1280x720 åç» 22 mp4 1280x720 è¶ æ¸ 21 mp4 640x352 æ æ¸ (best) ```
Not tested, but `var\s+video\s*=\s*([^;]+);` may be better.
It's better to use `_search_regex` for JavaScript codes instead of `_html_search_regex` as the latter is designed for HTML markups.
We use single quotes as much as possible.
In `_html_search_regex` (as well as `_search_regex`), the default value for default is `None`, so there's no need to specify it explicitly.
If there are two APIs, it's better to include both APIs and let users to choose from flv and mp4.
If such information is available, it's better to include it in the extractor.
I guess format codes are some denotions not related to the quality. `_sort_formats()` will sort the formats by resolutions, bitrates, etc., so it would not be a problem. If formats are not sorted, users may download videos with worse qualities. For example in the case I've mentioned previously, 640x352 is downloaded instead the 1920x1072 one.
Better to use ```self._search_regex``` or ```self._html_search_regex``` instead of ```re.search```. The first two handles errors better.
1. Dots should be escaped 2. Better to use ```\s*``` or ```\s+``` instead of spaces in regular expressions for robustness
Should be ```sources\.pdl``` here
Better to use ```r're:^https?://.*\.jpg$'``` here. Note that the ```r``` prefix is necessary or the code will fail on Python 3.6.
Any test case using this approach? I can't find it.
Better to have some URLs in _TESTS with ```'only_matching': True``` for changes to _VALID_URL
Use `_extract_info_from_player`, without the `s`. Or just `_extract_info` or `_extract_from_player`, they are common in other extractors.
You forgot to change this.
The `return` is not needed anymore.
use instead ``` pytho raise ExtractorError('no player : no sound in this page.', expected=True) ```
Use `_search_regex`, it reports an error message if the regex doesn't match.
Note that the `formats` field is optional, the `url` can be set in the main dict. For example the appleconnect test URL (https://itunes.apple.com/us/post/idsa.4ab17a39-2720-11e5-96c5-a5b38f6c42d3) fails.
`title` must be mandatory I've already told about this.
I mean it should not be `http?`. `display_id` should be extracted from the `url` if present or may be extracted from further downloaded pages when does not present in the `url`.
No need to escape forward slash. Should be `https?`. Part after `(?P<id>\d+)` should be optional since `id` is only used for extraction.
`json_output.get('synopsis')`. You can find best practices in any recently modified extractor.
All fields apart from `title` and `formats` should be treated optional and extraction must not fail when any of these fields are missing.
Groups apart from `id` are not used, no need to capture them. For multiline string use triple quotes.
Everything apart from `title` should be optional.
hls and rtmp are available as well.
What's the point of lines 104-108? `ext` is already flv.
Can't be. `_extract_f4m_formats` always [returns flv](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L941).
Everything after `id` may be skipped and not matched since http://www.rte.ie/radio/utils/radioplayer/rteradioweb.html#!rii=16:10507902 works just fine.
There is no need in `[]` around `#`.
I guess it's better to put it on single line since it's much shorter now.
Extractors should have unique names. Common naming schema is `rte`, `rte:radio` etc.
No need to be escaped either.
this part is not handled by ZDFIE, i will move it to extract_from_xml_url in zdf.py.
You are returning a tuple of `(name, value)` item instead of a list of values as you do later.
I think it's more explicit to use a list comprehension: ``` python default_criteria_order = [name for name, _ in default_key] ```
This check should take place earlier and should be more general `if not video_url:`. Same should be done for `video_play_path`. Also these fields better extracted with `xpath_text`.
These changes (alphabetic reordering) are not related to qq extractor thus should not be present in this PR.
If any of the formats fails the whole extraction is broken.
Code duplication in these three methods. Again they are only used once, so there is really no need in them.
I don't see much point extracting it in separate class. Base class inherited from `InfoExtractor` is preferred.
I don't see much point in there constants since each is only used once.
Avoid shadowing built-in names.
Today it provides, tomorrow it doesn't. But extractor is broken across all the distros some of which are tend not to update fast enough.
Video that does not provide any sources sounds like unexpected outcome therefore it should not be muted. If user wants to ignore this she have to pass `-i`.
Extration should be tolerate to any missing optional data. Anything apart from title and data required for formats extraction considered optional. So if `description_root` is missing if should not break the extraction.
Use single quotes consistently whenever possible. If this is expected error pass `expected=True`.
This defeats the whole point of `ExtractorError`. It should be propagated and handled outside the extractor not muted in extractor.
Code duplication in 70-102 and 104-137.
Code duplication in 142-145 and 146-149.
Code duplication in 155-163 and 165-172.
This results in `http://videa.hu/oembed/?url=url=http%3A%2F%2Fvidea.hu%2Fvideok%2Fkreativ%2Figy-lehet-nekimenni-a-hosegnek-ballon-hoseg-CZqIVSVYfJKf9bHC&format=json&format=json` that is obviously not what was intended. Second argument must be `video_id`.
Extraction must be tolerate to missing fields everything apart from title and formats is optional.
No newline at the end of file.
All video formats must be extracted.
This should be extracted from `_VALID_URL` with `self._match_id`.
No need to use `sanitized_Request` request here, pass url directly to download method.
1. `video_url` must be unicode string. 2. On python 3.2 `b64decode` accepts only bytes.
It should be `decode('utf-8')` not encode. Also use lowercase for encoding name for consistency.
Use `compat_urllib_parse_unquote` instead.
I guess there's no need to download the key to the dist and read it from the disk. Just `urlopen` should be fine.
Looks similar to https://github.com/rg3/youtube-dl/blob/77b8b4e/youtube_dl/extractor/common.py#L1152-L1153. Maybe it's better to some helper function in `utils.py` for parsing M3U8 manifests
This breaks the old player.
As of [RFC1378](https://www.ietf.org/rfc/rfc1738.txt) file URL scheme _does not specify an Internet protocol or access method for such files_. So I suppose it'd be technically more correct to refer to it as `file:// URL scheme` or simply `file:// scheme` (this is how it's referred in urllib). Note two slashes since generic form is `file://<host>/<path>`.
Default part should be removed since there is no default.
This change is unrelated and breaks updating.
audio volume is a tautology, there is no such thing like video volume. `metavar` should be `VOLUME`.
Valid ISO 3166-1 code for Sweden is `se`.
`&#39;` should be unescaped to single quotes to match the title shown on the webpage.
`itertools.count` does not accept keyword arguments until Python 2.7.
Both quoted and quoteless should be tried. Looking at their JSON they can switch back any moment.
Still too restrictive for http://future.arte.tv/fr/la-science-est-elle-responsable.
These files are actually [TTML](https://www.w3.org/TR/ttml1/) subtitles.
Can you properly format the code? Check with flake8.
Missing `mpxRefId` should not break the extraction.
This can be simplified to just `if len(m3u8_formats) == 1:`.
Please don't create strange identations.
m3u8 downloads with ffmpeg should be [skipped](https://github.com/rg3/youtube-dl/commit/60ad3eb9706861d4182ab44ee19d64350ca2e36e).
This is too broad and detects the same video twice.
This is usually called `display_id` and included in info dict as well.
3rd argument should be the name of field you search for, i.e. `'video id'`.
Use `self._search_regex` instead. Now it can be simplified to just `data-mid="([^"]+)"`.
From what I've seen there is always only one video on the page thus no need in playlist.
If both tests test the same scenario leave only one.
No need to inherit from `NPOIE`.
`id` should not be optional. No need in trailing `/`.
It would be better to have a test case.
I guess this function is too long to be a nested function.
ISO/IEC 23009-1:2014 declares the `lang` attribute uses rules defined in RFC5646, which then uses language codes from ISO639-2. `mul`, `und`, `zxx` and `mis` are special values. I guess they should be handled specially.
Any reason for not using helper functions like `_add_ns`? Codes look lengthy without that.
These options should be kept and probably marked deprecated in order to avoid breakage of user configurations and code that uses them. `youtube_include_dash_manifest` logic should be expressed via `skip_protocols`.
Technically these are not protocols. I'm aware we are using some of them in `protocol` metafield in info dict but still I'd prefer diffrerent wording. Moreover I'd prefer something even more generic like `--skip-extraction-steps` with comma separated value list of possible values: `m3u8`, `f4m`, `mpd`, `smil`, `metadata` etc. (possibly even extractor-specific ones). This would allow to fine-tune the extraction not to download unnecessary data and minimize network I/O.
This should be clarified that it's not guaranteed to actually exclude processing of mentioned things since extractor may use own extraction routines.
This will require them to remove these options from configurations/code anyway.
Debug code must be removed.
Debug code must be removed.
`_search_regex` is enough here.
`_search_regex` is enough here.
Field name is supposed to be `key` not `long_video_id`.
No need for this check, this is already checked in `_sort_formats`.
This will fail if `encodingOption` is missing. Also wrap in `int_or_none` from `utils`.
You've already did this with `videos` and `captions`: `int_or_none(vid.get('encodingOption', {}).get('height'))`
`fatal` should be `False` for this field.
Either implement or remove.
No need to specify encoding. `fatal` should be `True` (default) since you are not providing any alternative extraction scenario.
This routine is not used.
Why did you remove this test? It does not work with your changes.
Use `compat_urlparse.urljoin` instead.
Title should be mandatory.
Do not capture groups if you are not going to use them.
This should be extracted right from `_VALID_URL`.
DPlay_2 fails as well.
Code duplication should be eliminated.
Use single quotes consistently.
Just take the last element`domain.split('.')[-1]`.
If both test the same functionality leave only one or make the other `'only_matching': True`.
This check makes no sense.
If will fail anyway if any of these fields is missing and it's better to fail early explicitly in extractor than in some generic code.
This is not pythonic to place it on a single line. Check code with flake8 and fix all the suggested issues.
Instead of `video_url` there should be `formats` extracted from hds manifest with `self._extract_f4m_formats`.
This will result in reference to unassigned variable when time fields are missing.
When requested with proper headers/cookies it returns JSON: ``` json {"status":"error","message":"Please log in to get access to this content!","data":{"stream-access":["https :\/\/streamaccess.unas.tv\/hdflash2\/vod\/22\/471029.xml?streamid=471029&partnerid=22&label=laola1tv &area=ice_hockey_del&ident=2796309820160209163815&klub=0&unikey=0&timestamp=20160209163815&auth=f7a094f27a0946900ad049524a9e45f3" ,"https:\/\/streamaccess.unasmedia.tv\/hdflash2\/vod\/22\/471029.xml?streamid=471029&partnerid=22&label =laola1tv&area=ice_hockey_del&ident=2796309820160209163815&klub=0&unikey=0&timestamp=20160209163815&auth =f7a094f27a0946900ad049524a9e45f3"]},"code":5} ```
See firebug/developer tools.
Use `compat_urlparse.urljoin` instead.
This will fail if any of time fields is missing.
Use `re.sub` instead.
Instead of try-except use `fatal=False` passed to `_download_json`.
`iq` will never match.
This can be more PEP8 with a '\n': ``` Python class YahooSearchIE(LazyLoadExtractor): _VALID_URL = None _module = 'youtube_dl.extractor.yahoo' @classmethod def suitable(cls, url): return re.match(cls._make_valid_url(), url) is not None @classmethod def _make_valid_url(cls): return 'yvsearch(?P<prefix>|[1-9][0-9]*|all):(?P<query>[\\s\\S]+)' ```
Currently IEs are randomly sorted. I guess sorted IE names make `lazy_extractors.py` look better.
Must be list, not a string.
`id` must match at least one character.
`id` should be extracted via `_VALID_URL` when present.
Use `_parse_json` instead.
Extraction should be tolerate to missing fields. Everything apart from title and formats considered optional and should not break the extraction if missing.
I guess this was deleted by a mistake.
Pattern should include `<iframe` part.
Extraction should be tolerate to missing fields.
Initial `video_id` or `display_id` should be extracted from input URL.
Today it's there - tomorrow it's not. Extractors must be as robust as possible and tolerate to layout changes. Subtitles is not a critical metadata to extract thus when `subtitleUrls` disappears (if ever) if must not break the main extraction routine.
If there is no `url` you'll end up with invalid entry in `subtitles`.
Extraction must tolerate missing `subtitleUrls`.
It should match at least one character.
This should be `fatal=False`.
extraction must be tolerate to missing fields.
Use `compat_urllib_parse_unquote_plus` instead.
Use `height` attribute is better here. `_sort_formats` can sort formats by heights.
Unnumbered placeholders are not supported in Python 2.6.
`duration` is optional. This line shouldn't fail if doc does not contain a `DURATION` element.
This regex should also be fixed.
Just use `replace` or `re.sub`.
you can use `float_or_none`.
``` content_el = itemdoc.find(self._xpath_ns('.//content', 'http://search.yahoo.com/mrss/')) duration = float_or_none(content_el.attrib.get('duration')) if content_el is not None else None ``` or ``` content_el = find_xpath_attr(itemdoc, self._xpath_ns('.//content', 'http://search.yahoo.com/mrss/'), 'duration') duration = float_or_none(content_el.attrib['duration']) if content_el is not None else None ```
Technically, explicit 0 is also valid value for `start_time` but filtered with this check.
`start_time` may be `None` at this point.
This will prevent downloading http URLs with time pointers without ffmpeg in `PATH`. ffmpeg availability should also be checked here.
`end_time` could be < than `start_time` producing negative duration.
I suppose it's better to wrap into list here and store bare option string in `available_opt`. This is also more idiomatic to type it in uppercase.
I've still suggest at least keeping the code of mplayer downloader.
Just keep the old `RtspFD` now and replace it with generalized external fd in further PR in order to have clear vision of changes: what was removed and what was the replacement in a single commit/PR.
Some convenience method like `can_download(info_dict)` checking for `available() and supports(info_dict)` would be handy (this is used twice in the code).
You've forgot to pass `info_dict` to `supports()`.
I guess single commit would be fine. There are not that much changes.
This should go after the `external_downloader` check. Now when `--external-downloader` is explicitly specified and there is a [start; end] range ffmpeg is used anyway.
I don't think so. If one explicitly specify an `--external-downloader` he does this intentionally and expresses own will to use exactly this downloader thus all the consequences (that the whole video will be downloaded instead of a fragment) are his responsibility.
`and not info_dict.get('requested_formats')` part should be removed here since it blocks ffmpeg from choosing as downloader via `--external-downloader` when single video/audio media file is requested.
When we have an m3u8 download and no ffmpeg installed we'll get unhandled exception and crash with a traceback. Previously there were an additional check before this line that has been reporting an error and returning quietly so that `check_version()` were never reached thus no unhandled exception. For now lets just restore it and later think how to move these checks outside actual downloading routines to phase when we select downloader.
Alternatively you can just restore it after this PR is merged.
Of course when it appears inside `script`.
This should be simplified to something like `r'<a[^>]+id="video-%s"[^>]+data-kaltura="([^"]+)' % video_id`
This can be simplified to just something like `<script[^>]+src=["\'].+?partner_id/(\d+)`. Also for better robustness additional pattern for `"partner_id":1529571` can be added.
Passing title here makes no sense for non playlists since it will be overridden by delegated extractor. If you want to pass metadata that won't be overridden you should return info dict of [`url_transparent` type](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/makertv.py#L28-L31).
It should be `https?` not `http?`. Change `\S+` to `[^#]+`.
Extraction should be delegated to [kaltura extractor](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/ellentv.py#L81-L83).
If both test the same extraction scenario leave only one.
Unnamed placeholders are not supported in python 2.6.
Keep both variants.
Never use bare except.
Use `utils.parse_duration` instead.
This looks similar to `utils.decode_packed_codes`.
Instead of `resolution` and `preference` it should be extracted as `height`.
This will break the entire extraction if some format's page fails to download. `_download_webpage` should be receive `fatal=False`.
No need to escape forward slash.
Use single quotes consistently.
Do not capture groups if you are not going to use them.
This will break the entire extraction if there is no match for some format.
It's not about on what to be called. It's about on what it will be called. Malformed webpage is very possible case and even when one single format webpage is malformed the **whole** extraction will be broken. The error should not be hidden rather a warning printed (this is what you'll get with `fatal=False`) but it should not break everything.
The general strategy is not to fail if you have enough data to extract at least one format and title. You don't know for sure until you've successfully downloaded the format webpage and ensured it's well formed. If you break on any minor stuff like this you'll get the whole extraction broken in all the distros and packages where youtube-dl is present some of which are non-rolling release and never updated. Thus you'll get broken extractor in such distros although could easily prevent this by correctly handling minor breakages.
This will break the entire extraction if url is not found for some format.
`duration` should not appear in format dict.
Because it's a [field of info dict](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L180), not format dict.
`errnote` of `_download_webpage` should be used instead.
I've already suggested using `Downloading` as idiomatic wording.
It would be useful to mention version in `'url'` to know which has failed. Version can be passed outside to `_get_format`.
This can be simplified to just `if 'abc' in webpage`.
Use `self._og_search_property` instead. Duration extraction should not be `fatal`.
No need to escape forward slash.
This matches just any plain text that meet this pattern. Instead it should extract it via pattern that has embed semantics. This also won't work if actual media is stored under different URI.
`full_url` does not exist on Python 2.x
This can even go before `if` block at 1237 if we remove references to `thumbnail` (1905-1908) in `list_thumbnails`. Since `thumbnails` will always be present at that point when at least one thumbnail is extracted.
There is a playlist title that can be extracted. Defaulting to `- BBC News` in order to remove `- BBC News` then looks clumsy.
Add `default=playlist_title` in order not to fail when `<title>` does not contain `- BBC`. Regex should match at least one characted and title should be greedy `r'(.+)\s*-\s*BBC'`.
`_og_search_title` should be preserved as well.
Now `- BBC ...` part of regex here makes no sense since it's handled below.
Place right here in the following order: 1. If no title extract with `_og_search_title`. 2. If still no title extract from `<title>`. 3. Extract `playlist_description`.
Should be `default=None`.
`default` implies non fatal.
`playlist_description` is now not extracted on this branch.
Should be `default=None`. Suffix part is not cropped: ``` [download] Downloading playlist: School which breaks down barriers in Jerusalem - BBC School Report ```
In http://www1.wdr.de/mediathek/video/sendungen/lokalzeit-bergisches-land/video-lokalzeit-bergisches-land-vom--236.html (from #9517), the `alt` URL is a M3U8 manifest.
Use `_download_json` with `strip_jsonp`. Need to change `strip_jsonp` so that the dollar sign is recognized.
I suggest extracting from `mediaLink` element instead of matching the JS URL as it may change in the future. For example: ``` Python media_link_obj = self._parse_json(self._html_search_regex( r'class="mediaLink\b[^"]*"[^>]+data-extension="([^"]+)"', webpage, 'media link'), display_id, transform_source=js_to_json) js_url = media_link_obj['mediaObj']['url'] ```
This calculation is wrong. `str_to_int` just removes dots resulting in 11000 instead of 1100.
A typo? `pariod`
Pass tuple of regexes to `_search_regex` instead.
``` python course_id = self._search_regex( (r'data-course-id=["\'](\d+)', r'&quot;id&quot;: (\d+)'), webpage, 'course id') ```
I guess something like `base_url` is a better name than `url`.
I guess using `content_type` instead of `ct` improves readability.
`r'(?s)(<(?P<tag>video|audio)[^>]*>)(.*?)</(?P=tag)>'` is better as numbers may change in the future.
`(?s)` has no effects in regular expressions without `.`
Some more: avc2, avc3, avc4. These would be enough.
`media_attributes` is somewhat misleading. It's the whole tag but not just attributes.
This should be called `parse` in order to match the naming convention of the other parse/extract methods since it does not download the webpage but just parses it.
Such cases should be handled, too. I guess a possible approach is creating a table with common video and audio codecs. If given codecs are not on the table, fallback to `video,audio` order.
Should be `'\n'`. Removing newlines is the work of individual extractors.
Leave only one test if all test the same extraction scenario. For different URL schemas use `only_matching` test.
We usually use default fallbacks instead for shorter code: `info.get('cover', {}).get('maxres', {}).get('url')`
There are multiple thumbnails available that can all be extracted.
`utils.int_or_none` should be used instead.
`age_limit` should be `None` when `adult` is missing.
`utils.parse_iso8601` should be used instead and extracted as `timestamp`.
Move it right after `title = info['title']`.
Video URL should be extracted the very first right after the title.
It does not check the other path since you neither download it here nor test the `url` not to contain `mp4:`. Thus you can't be sure whether this test involved the other path or not.
Simplify to `rtmp://camwithher.tv/clipshare/%s' % ('mp4:%s.mp4' if int(video_id) > 2010 else video_id)`.
rtmp's container is **always** flv.
Generally, this is not that much important scenario so `only_matching` is well enough.
`.*` at the end makes no sense. http://camwithher.tv/view_video.php?page=&viewkey=6e9a24e2c0e842e1f177&viewtype=&category= does not match.
Idiomatic name is `video_id`.
Second argument should be an id extracted from `_VALID_URL`.
This should match only integers.
Use `compat.compat_urllib_parse_unquote` instead.
I would prefer indent similar to the former code.
1 is ok.
I guess it's ok to always return stable float here.
This can be simplified to `[None] * 5` or `(None, ) * 5`.
This regex should be split into multiple lines for bettercode perception.
All integral numeric metafields should be wrapped in `int_or_none`.
Title is mandatory.
According to `status` in format dict particular files may also fail. Extraction should be tolerate to such files and missing `url`.
Something like: `Also leave '-' intact in order not to break streaming to stdout`.
I guess this should better go right to `_ffmpeg_filename_argument`. It does also work with just `-`.
Use `self.playlist_result` instead.
How are missing songs related to using proxy? https://myspace.com/mekowilliamssho/music/songs have no songs but suggests using proxy. This check should be removed completely. youtube-dl will handle this on it's own.
If both tests do test the same scenario leave only one.
This should be removed.
Parsing should be the way. It's better than hard-coded values in general. An approach is modifying [`_extract_m3u8_formats`](https://github.com/rg3/youtube-dl/blob/d671237/youtube_dl/extractor/common.py#L1061-L1179) so that it can handle subtitles, too.
> Though there might be several streams without NAME too. according to the standard `NAME` attribute is REQUIRED.
Not all live videos have subtitles. For example http://www.vlive.tv/video/7871/1996-%EC%95%88%EC%8A%B9%EC%A4%80-%EC%9C%A4%EC%A0%95%EC%9E%AC-%EB%9D%BC%EC%9D%B4%EB%B8%8C does not have a subtitle when I just checked. By the way, I guess `_get_live_sub_url` is a better function name.
If `air_start` is `None`, the message should be `Coming soon!` instead of `Coming soon! None`.
Merge them: `from __future__ import division, unicode_literals`
Use ... instead of â¦ here. Non-ascii outputs may break random things.
Should extract chunklists via `self._extract_m3u8_formats` here.
> An approach is modifying _extract_m3u8_formats so that it can handle subtitles, too. i already proposed this in #8820 as a first step to handle WEBVTT m3u8 subtitles.
This is media file extension not a playlist extension thus must be reverted.
What's the point of such verbosity? It can be safely written on a single line skipping arg names - they are not keyworded anyway.
`WrzutaPlaylistIE` should be place in `wrzuta.py`.
`if playlist_size:` is enough.
Instead of this playlist extractor should be separated into `WrzutaPlaylistIE`.
`if playlist_size > len(entries):` is enough.
When there is only one capture group just use unnamed one.
What's the point extracting one line routine in separate method that is used only once? This just complicates readability.
Does not work on python 3: ``` [debug] System config: [] [debug] User config: ['-o', '%(title)s-%(id)s-f%(format_id)s.%(ext)s'] [debug] Command-line args: ['http://heroesf70.wrzuta.pl/playlista/6Nj3wQHx756/lipiec_-_lato_2015_muzyka_swiata', '-F', '-v'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2016.06.03 [debug] Git HEAD: c173b4e [debug] Python version 3.5.1 - Windows-10-10.0.10240-SP0 [debug] exe versions: ffmpeg 3.0, ffprobe 3.0, rtmpdump 2.4 [debug] Proxy map: {} [wrzuta.pl:playlist] 6Nj3wQHx756: Downloading webpage Traceback (most recent call last): File ".\youtube_dl\__main__.py", line 19, in <module> youtube_dl.main() File "C:\Dev\youtube-dl\master\youtube_dl\__init__.py", line 421, in main _real_main(argv) File "C:\Dev\youtube-dl\master\youtube_dl\__init__.py", line 411, in _real_main retcode = ydl.download(all_urls) File "C:\Dev\youtube-dl\master\youtube_dl\YoutubeDL.py", line 1736, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "C:\Dev\youtube-dl\master\youtube_dl\YoutubeDL.py", line 676, in extract_info ie_result = ie.extract(url) File "C:\Dev\youtube-dl\master\youtube_dl\extractor\common.py", line 341, in extract return self._real_extract(url) File "C:\Dev\youtube-dl\master\youtube_dl\extractor\wrzuta.py", line 149, in _real_extract playlist_id=extracted_id, File "C:\Dev\youtube-dl\master\youtube_dl\extractor\wrzuta.py", line 173, in _extracted_playlist_content number_of_extracted_entries_still_lower_than_list_size = playlist_size > len(entries) TypeError: object of type 'map' has no len() ```
This method is unused. Either use somewhere or remove.
Whats the point reconstructing the URL? You already have it in `url`.
Use `self._match_id` instead.
You should actually keep this line and change https://github.com/costypetrisor/youtube-dl/blob/autonumber_start/youtube_dl/YoutubeDL.py#L296, otherwise it won't be incremented.
It would be confusing, eg: when using `--max-downloads 5 --autonumber-start 3` I guess the expected behaviour is to download 5 videos, not 3. So it should probably be also modified.
You'll have to provide a default value for `autonumber_start` (like 1), because when using the youtube_dl module from python it will usually be missing.
Redundant or not it should be preserved either by providing manually or by copying from `artist` automatically when not present (that is not implemented) since it can be used by someone already.
`creator` should not necessarily contain artist in terms of a person or a band. It can be a studio/channel/whatever. The meaning of `creator` has expanded from the initial description wording. Just keep the old format for `creator`.
`creator` should be preserved for backward compatibility.
Selection by attribute does not work in python 2.6, use `find_xpath_attr` instead.
Sorry for the late response. This check may give a false alert if someday afreeca.tv decides to use a different name than `./track/flag`. `./track/video/file` is more reliable. The overall extraction workflow should be: 1. Check `./track/video/file` 2. Raise an error if no entries 3. Check other fields
Should not fail if `upload_date` does not exist.
Should be after `.aenetworks`.
Use `xpath_text` for all `track.find('XXX').text` occurrences. This function provides more information for debugging.
If there's only one format, just use `'url'`.
I think it would be more consistent to use seconds, as with `start_time`, `end_time` and `duration` in the info dict.
Coding cookie is only required for non-ASCII sources.
Check code with flake8 and fix all issues.
Correct approach is to mimic web browser: 1. Use https://media.loc.gov/services/v1/media?callback=jQuery18205494330186516222_1463503121265&id=E6AB0B2585930180E0438C93F0280180&context=jsonp&_=1463503124082 2. Mimic web player url construction: ``` js // generate hls live streaming url over https url = url.replace("rtmp", "http"); // some urls come in with no extension and we need it on there for access if(url.indexOf('.mp4') == -1 && url.indexOf('.mp3') == -1){ url += (is_video ? '.mp4' : '.mp3'); } if(url.indexOf('vod/mp4:') > -1 ){ url = url.replace("vod/mp4:", "hls-vod/media/")+".m3u8"; } else if ( url.indexOf('vod/mp3:') > -1 ){ url = url.replace("vod/mp3:", ""); } ```
Carry long lines.
This is superfluous since you provide `formats`.
Use `self._search_regex` or `self._html_search_regex`. And an JPEG URL only does not indicate it's the thumbnail. A stricter filter needed.
`.get()` idiom is used for optional fields only.
Better to use `determine_ext` instead of `.endswith`
There's no need to include an `IE_NAME` which is the same as the extractor name.
For these cases, use `self._og_search_thumbnail` or `self._html_search_meta`.
This function call is already (indirectly) included in `_download_webpage`.
`formats` is always a list of dictionaries.
There should be spaces between `%`.
Conversion between different date formats is redundant. Just return Unix timestamps.
Should not use `ext` here as `ext` is already specified in `formats`.
`duration` should be numbers.
Wrong fallback value in `thumbnail`, `description` and `creator`.
`unescapeHTML` + `_search_regex` = `_html_search_regex`.
Use a info dict with `'_type': 'url_transparent'` to delegate video URL extraction to `FavourMeEmbedIE`.
If the title is not available from the webpage, it can be omitted. People who want a title should use the non-embed URL.
Use more relaxed regex.
The third parameter of `_html_search_regex` is name but not ID.
Incorrect fallback value.
That's incorrect. `id` may be arbitrary length https://www.vidio.com/watch/77949-south-korea-test-fires-missile-that-can-strike-all-of-the-north.
This regex does not look like generic embed. Provide several examples that use this embedding.
Generic extractor is for generic embed templates used across multiple sites.
Then use `enumerate()` instead.
See `unified_strdate()` in `utils.py`.
Use `idx, html_entry`.
The may match something unexpected. `r'var\s+__desc_popup_d_\d+\s*=\s*({[^><]+});'` is better.
An error should be reported if neither `audio-preview-url` nor `video-preview-url` is present. `dict_get()` may be useful here.
`'id'` is required.
`get_element_by_attribute()` may be useful.
It's better to sort imported items alphabetically.
There's no need to name a group if not used.
It should be the following as the (valid) URL variations of https://itunes.apple.com/us/itunes-u/uc-davis-symphony-orchestra/id403834767 don't match otherwise ``` python _VALID_URL = r'https?://itunes\.apple\.com/[a-z]{2}?/?[a-z0-9-]+/?(?P<display_id>[a-z0-9-]+)?/(?:id)?(?P<id>[0-9]+)' ``` * https://itunes.apple.com/itunes-u/id403834767 * https://itunes.apple.com/us/itunes-u/id403834767 * https://itunes.apple.com/itunes-u/uc-davis-symphony-orchestra/id403834767
No longer used.
Direct URLs should also be extracted.
These formats should not be removed.
Extract human readable title from the `webpage`.
Use `self._match_id` instead.
`default=None` implies `fatal=False`.
~~[`_og_search_video_url`](https://github.com/rg3/youtube-dl/blob/7b2fcbf/youtube_dl/extractor/common.py#L740-L744) may be even better.~~ Sorry `_og_search_video_url` can't be applied for `re.findall`, while `_og_regexes` is still useful.
This check does not make any sense. If there are no formats extraction should stop immediately.
`display_id` will always be empty.
Use more relaxed version, e.g. `[^/]+` to match segments and `[^/?#&]+` to match id.
Use `utils.remove_start` instead.
Better to use `unified_strdate()`.
`parse_duration()` should work.
_ is not a special character in Python's regular expressions. There's no need to escape it.
With this you drop 720p.
It should not be included in `formats` at all rather than filtering.
not all video has HD quality and even some of them doesn't have the same url format so it better to follow the logic in the browser: downloading the embed page which contain video urls: `http://www.comingsoon.it/videoplayer/embed/?idv=<id>` extract `vLwRes` and `vHiRes`, if both urls are the same then only sd quality is available else both sd and hd format is available. also don't hardcode `ext` it can be detected automaticaly from the url(some video in the web site served as flv files).
Should be better with ```isinstanceof(filename, BytesIO)```
Or ```if not hasattr(filename, 'write')``` or ```if isinstance(filename, compat_str)```? Checking for an exact type makes the codes less flexible.
Extract `sources` and `videoTitle` separately.
Remove unused codes.
Use `self._sort_formats(formats)` instead.
Remove debugging codes.
Also extract title from `<title>(.+)</title>` in case current extraction fails
Either way is fine. In general `<title>` is better than extracting from embedded data as the former is less likely to change. If there's an example that `<title>` or `settings['title']` is missing, a fallback should be provided, otherwise the extraction should be fatal.
The `title` _should_ be mandatory
Change double quotes to single quotes
You can just use 'playlist_count'
OK. How about creating one base class, define the outside functions there, and make derivative classes for playlists that return `playlist_result()`. This way you'll reduce the boilerplate code
Use the `'md5:xxx'` syntax if the value is too long.
Something like: `'description': 'md5:6a9622b911565794c11f25f81d6a97d2'`
I meant other headers. like Accept\* ones. Usually they are not necessary.
Remove this line.
It's added in Python 2.6, so it's OK to use it as youtube-dl supports Python 2.6+ only.
and the function that normally used to encode postdata is `urlencode_postdata`.
i think that `flv_data` should be used with `data` param and `Content-Type` dict used with `headers` param of `_download_webpage`.
`.format()` is not supported for some Python versions
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
`_og_search_title` has already defaulted to `fatal=True`
Enclose in parentheses
Don't use raw urllib2 APIs. Instead, use `self._request_webpage()`, which is more robust.
Use `self._download_json` instead.
This shouldn't be fatal; use `.get()` instead
This shouldn't be fatal; use `.get()` instead
Shouldn't be fatal
Any reason to have three full tests? If not, you could set two of them to `'only_matching': True`
AFAIR there are no extractors that use personal API keys since there is almost always a way to workaround mimicking browser's behavior without it. It should not use env at all. Any integration should use generic interface but not a zoo of system wide env variables.
Yes. Though you may provide fallback sources for it (if any). But overall title should be fatal. `unknown` hardcode may be used if absence of title is allowed by source provider or in case there is no title expected at all (in this case video id is usually used).
youtube-dl is wide targeted. Absence of any authentication is preferred since not everybody may have/want to bother to obtain an API key.
1. Explicit `encoding` may be added to `_parse_json` and forwarded there from `_download_json`. With this one will be able to specify encoding manually, `utf-8-sig` in this case. 2. BOM may be dropped in `_parse_json` leaving webpage intact.
This website uses JWPlayer. Use `JWPlatformBaseIE._extract_jwplayer_data` instead. Usually it's not a good practice to re-invent the wheel.
`u` prefix is not necessary as `from __future__ import unicode_literals` has the same effect, and such a syntax is not available in Python 3.2.
`js_to_json` in `utils.py` should do the work.
Just cut it with `utils.remove_end`.
For tv/se `og:title` contains unnecessary suffix.
Don't capture groups you don't use.
What I actually meant is to put them in a tuple or a list.
Just put both regexes under the same `_search_regex`.
@phihag first wrote codes for `ENGLISH_MONTH_NAMES` in caefb1de877e163fa3ece44757cb2fae6adf47e4. I guess it's because lists are usually used to handle homogeneous data and tuples for heterogeneous ones.
This should be generalized in the first place. Now it simply a code duplication.
`skip_fragment` sounds more logical in this context. Also `append_url_to_file` may be renamed to something better reflecting it's actual content.
@yan12125 maybe just `fatal`? Also what I meant about renaming `append_url_to_file` is something like `download_fragment` or `process_fragment` or so.
> Also append_url_to_file may be renamed to something better reflecting it's actual content. the function can be removed completely and move the code directly to the for loop.
These are not used.
Use `self._parse_html5_media_entries` instead.
`https?://` is better in this case. openload supports both HTTP and HTTPS.
`nhk.or.jp` does not even resolves.
Use `headers` parameter of `_download_json` instead. Also use `query` instead of stuffing it into the URL.
Use `_request_webpage` instead.
This restricts password to the only single one while it should be on per extractor basis. As a result this breaks credentials input mechanism completely, one are unable to have different credentials per extractor anymore.
Don't use bare `except:`
Use `js_to_json` instead. EDIT: I didn't see your previous concern. In this case, fix the problem of function calls first and then pass it to `js_to_json`.
Are you sure it isn't Javascript code instead of JSON? If so, you can use `js_to_json`
It's better to fail instead of fallback.
Config files specified via the command line sounds like a user configuration for me.
Just `File to read configuration from`.
`{}` won't work in python 2.6.
It's absolutely pointless to clarify paths here. youtube-dl may be running on Window host where these paths make no sense at all.
metavar should be `FILE`. `type` is already string by default.
`parser.error` is a function, not an exception class.
`parser.error` calls `sys.exit`, so simply `parser.error('....')` (without `raise`) should work.
Should be `--config-file` or `--config`.
This change does not make much sense.
That's incorrect. `\1` must be a number of capture group.
I guess you misunderstand this completely. Simply speaking `(["\'])(?P<url>(?:(?!\1).)+)\1` regular expression means: 1. Match either `"` or `'` and remember it as `\1`. 2. Then match any character but `\1` until `\1` is found and remember it as `url`. Thus it will match everything between quotes regardless of quotes' type (double or single) found, e.g. it will capture `123` as `url` group of out `data-video="123"` as well as `data-video='123'` strings. This is applied in order to be future-proof in case of quotes' type change. So it's does not make any sense here and in most of other places in this PR.
Actually, it's an opposite. It's a check for successful login.
`headers` now can be passed right in `_download_webpage`. Also there is already the same dict in `login_request` that may be extracted and reused.
This breaks the extraction of prochan embeds. `[^"]+` is only applied for youtube group.
Don't capture groups you don't use.
This should not break extraction if it fails.
There is no such licence `all-rights-reserved` exist. Lack of information should be expressed as `None`.
This should not be touched.
In both extraction functions you extract these fields from `webpage` (as fallbacks in the second case) but you do it differently (split on `|` here and as is in the second case). What's the rationale? `_search_regex` is fatal by default and will break extraction if it's not found for optional fields.
`if not formats` is enough.
Use `_sort_formats` instead.
Eliminate code duplication.
Eliminate code duplication.
No need for that check it's already checked in `parse_iso8601`.
With `default` you say that it's an expected scenario when field is missing. According to tests - it's not thus `fatal=False` should be used instead. Or provide test these fields.
`_html_search_regex` already calls `unescapeHTML`.
Sorry I didn't notice this line. It's mandatory, too.
m3u8 formats available using this template `http://pl-hls%s.live.panda.tv/live_panda/%s%s.m3u8`.
> My guess is : > SD -> Super Definition(not Standard Definition) -> è¶ (means super)æ¸ (1080P) -> '' > HD -> High Definition -> é«(means high)æ¸ (720P) -> _mid > OD -> Ordinary Definition -> æ æ¸ (540P) -> _small i can confirm this from the AS code. - the player assume that all qualities are available when `stream_addr` is not present. - the first part of `plflag` is also used to determine the suffix and the second part change for the value `21`. ``` python plflag0, plflag1 = video_info['plflag'].split('_') plflag0 = int(plflag0) - 1 if plflag1 == '21': plflag0 = 10 plflag1 = '4' ``` if `pflag0 < 1` than `live_panda` will be added to the begining of the suffix.
yes, this is correct.
Use `utils.qualities` to produce value for `quality` fields instead of `preference`. Also eliminate code duplication.
You should make extraction tolerate to these fields missing not remove them.
You still duplicate the URL and unnecessary `if/elif` branches.
lower quality is available with the `_small` suffix.
This will break extraction if there is no `hostinfo` key in `data`. Fix all other optional fields as well.
This variable is pointless, `video_id` can be used or renamed to `room_id`.
Likely, `stream_addr.items()` fails if 'stream_addr' is not in `video_info` Update: stream_addr is necessary for generating URLs, so should be `video_info['stream_addr']`. It's the same for `plflag` and `room_key`.
I've already pointed out: use `quality`.
`self._live_title` must be used.
Title is mandatory field thus it should be `data['roominfo']['name']`.
If 'plflag' is not in `video_info`, this line crashes. This should work: (not tested) `.get('plflag')` => `.get('plflag', '')`
You don't need to check languages. Read the whole post.
Outer parentheses are superfluous. Don't capture groups you don't use. No need to escape forward slash. `[a-z]`s to be replaced with `[^/]+`.
`keys()` is not a list under python 3. There is also no guarantee `chash` is the only in `fields` dict.
Add: ``` 'params': { 'skip_download': True, } ```
Replace with `md5:...`.
This as well.
It does not look like it's possible by design to embed several videos on a video page at vk. So just use `url_result` on the first URL.
Web player does not send this query.
Network connections in your browser.
Either `text` or `html` is mandatory.
Looks good. Thanks.
Why not just ``` video_url = ... width = ... height = ... ... if not video_url: video_url = ... ... formats = [{ 'url': video_url, 'width': width, 'height': height, }] ```
Nothing really changed. You construct the same structure two times.
Dimensions are uninitialized when `media` is not available.
The latter is simpler. The `fatal` parameter can be added to `get_element_by_id` later if there are more similar use cases.
Some extractors use the domain name as `IE_DESC`, so I guess it's OK.
`title` is mandatory while `get_element_by_id` fails silently if nothing is found.
This should be done in generic way similar to `_search_regex`.
Each PR should be granular enough and solve some particular problem. For example, contain either only homogeneous changes (e.g. `_match_id` refactoring) across whole code base or heterogeneous changes for a single extractor.
This change is not related to PR's purpose. Don't mix unrelated changes in single PR.
Put it in this one is OK.
It's better to fix thumbnail extraction instead of remove the test.
My concern was there are too many small PRs. Moving them out of this one is of course cleaner.
Use `remove_end` instead in this case.
Use `_html_search_meta` instead
Don't add list items if 'url' is None.
**Always** check code with flake8.
`re.sub` part can be put in `transform_source` parameter of `_parse_json`.
the second parameter for `_download_*` methods is `video_id`, use the `note` parameter for the message.
What's the point changing old time-proven regexes with another ones? That's not an improvement.
This statement does not conform to PEP8.
`<h1>` is more likely to change than `<title>`, so I guess extracting from `<title>` is better.
`_parse_jwplayer_data` should also be used. See `tvnoe.py` for an example.
Looks like another JWPlayer site. Use `_extract_jwplayer_data` instead.
You seem misunderstand the point completely. This piece of code will break extraction if failed and it can fail easily due to inner dict in JSON (since you don't allow `}`), due to parts `js_to_json` can't fix or due to some other reason that will eventually produce unparsable JSON.
I've already pointed out: replacing one time-proven working data extraction approach with another is not an improvement and I will reject all such further PRs. All additional scenarios should be added as fallbacks. Again, this code is not PEP 8 compliant.
This does not match anything.
This allows URLs like 2doc.nlabc/... that is incorrect.
Don't use the same variable name for different pages
This is incorrect and may break other websites. According to [1], an array _is_ a playlist. Codes for handling incorrect JWPlayer usage in IQM2 should be in `iqm2.py`. [1] https://support.jwplayer.com/customer/portal/articles/1481963-building-managing-playlists#fndtn-code
Use `(?i)` in regex itself if you want case insensitivity.
I guess `JWPlatform._extract_jwplayer_data` can be used here.
That's not a place for essays and repeating the code itself.
This code is for extracting IQM2 videos, not for IQM2 itself. Introduction texts to IQM2 are unnecessary.
Use `self.url_result(inner_url, 'Generic')` instead.
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
Well, basically that's the problem of these people not us. We don't care whether one can read regexp or not. Moreover most likely next time this code is read by someone is when extractor breaks due to layout change. Chances are this snippet is already irrelevant by that time.
Such tests should be included as [only matching](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/abc.py#L52-L53).
> Is there a good way to do that that I'm missing? Currently no. You can add it in another pull request. Don't be surprising that youtube-dl lacks such a basic utility. It's an old project from 2006.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
That's simulated. No such URLs is seen in the wild so far and no one will ever intentionally upper case some part of it.
These should be moved at places of their usage since they only used once.
Don't include irrelevant changes. Open a new pull request instead.
Thanks for finding out the bug. However, progress hooks should always be called; otherwise third party applications using youtube-dl's Python API will be broken. In this case `report_progress` should be fixed instead.
I guess `filename` can also be included.
Well, indeed, currently it will.
This will return non existent path if conversion fails.
This change is unrelated.
Missing `role` should not break the extraction.
For optional fields like `duration`, `feed['feed']['duration']` should be `feed.get('feed', {}).get('duration')`. All fields other than `id``,``title``and``formats`` are optional.
Python 3.2 doesn't like u-literals.
`if not description` is enough, same for other occurrences.
Optional fields should not break extraction if missing.
This looks like intentional quote or so.
Should not break if missing.
Unescaped dots. Also `)` may be contained in a string within JSON so matching `[^)]*` may fail.
Do not pass `default=None` to `_html_search_regex` instead.
Using `FormatData` four times.
Still a code duplication.
`display_id` should be used for console output.
Formats should have `quality`.
Code duplication should be eliminated.
I don't see any flac is the source code anyway.
API URLs should be used.
Also where did you get `https://mp3d.jamendo.com/download/track/%s/mp32` from? Browser uses different location.
There is no need to test URLs.
I've already pointed out: API URLs should be used.
I've already pointed it out numerous of times: it's not about variables in the first place but about the code flow unnecessary complexity. With variables one forced to jump back and forth between the place where it's defined and the place of actual usage in order to make sure group name matches and so on.
It should not. See the description of the field.
All these regexes are only used once thus make no sense as separate variables.
What's the point changing the original indent? With this style you just waste lots of space.
If `_search_regex` fails `None` will be passed to `_parse_json`.
This is no-go imo. `Game` string is localized and this regex will work only on English YouTube page. Also don't capture groups you don't use.
`{}` won't work in python 2.6.
All debug output should be removed. `{}` does not work in python 2.6.
`flags=re.DOTALL` does not make any sense since dot is not used in regex.
There should be a hardcoded fallback `player_id` value in case any of these two lines fail.
If `video` not in `media` empty formats will be returned that does not make any sense.
`for key, value in media.get('images', {}).items():`
Don't capture groups you don't use.
Read and follow code conventions. Check code with flake8.
`,` is fine here.
What's the point of this? `og:url` is always the same as the original `url`. If not provide a test for that.
`_search_json_ld` may be useful here
Just put original texts: æ§ããã«è¨ã£ã¦é å¼µã£ã¦ã
Use `query` param.
`_search_regex` with `default=None` instead.
No need to include script tag. It also won't work if JSON happen to contain inner dict.
`title` is mandatory field.
Looks like webm 600 and 2500 are only ever available.
This should be fixed with lookahead assertion instead.
Do not shadow built-in names.
This test should be `only_matching`.
Should be `only_matching`.
Don't capture groups you don't use.
I guess you misuse \\. For example: ``` >>> repr(re.match(r"""'(?:[^'\\\\]*(?:\\\\\\\\|\\\\['"nurtbfx/\\n]))*[^'\\\\]*'""", r"""'\'""")) 'None' ```
``` >>> re.match(r'/(?=[^*])[^/\n]*/[gimy]{0,4}', r'''/\/\/\//''') <_sre.SRE_Match object; span=(0, 3), match='/\\/'> ```
Don't remove tests for the original JSInterpreter. Before jsinterp2 replaces jsinterp, tests should be there to guarantee nothing gets broken by accident.
Function calls are complex. For example: ``` from youtube_dl.jsinterp import JSInterpreter jsi = JSInterpreter(''' function a(x) { return x; } function b(x) { return x; } function c() { return [a, b][0](0); } ''') print(jsi.call_function('c')) ```
Sorry but Javascript is not context-free. For examlpe: ``` code3 = ''' a = {'var': 3}; function c() { return a.var; } ''' jsi = JSInterpreter(code3) print(jsi.call_function('c')) ```
> They can't be multiline, can they? Yep. According to [ECMA 262 5.1](http://www.ecma-international.org/ecma-262/5.1/), CR (U+000D), LF (U+000A), LS (U+2028) and PS (U+2029) are not allowed in RegExp literals
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Duplicated keys/parameter names are another issue, which can be ignored in parsing and checked in semantic checking. In youtube-dl it's safe to assume all inputs are valid Javascript so there's no need to handle it.
`{}` won't work in python 2.6.
Instead `default` should be used in order to mute the warning.
`--rm-cache-dir` wipes the whole cache thus should never be suggested to use.
`--ap-mso` should not be touched since it's a separate stand-alone mechanism that has stable unique MSO identifiers for TV providers across all extractors while this options is not.
The file created in this particular case (assuming no `--hls-use-mpegts`) is mp4. Check it if you don't believe.
First, if extractor allows using authenticated cookies it should respect `--cookies`. Next it should process credentials. Usually youtube-dl does not cache sensitive data.
You cache cookies for authentication but have no mechanism for invalidation. If this cached cookie expires or becomes invalid for some reason you'll get stuck trying to authenticate with it over and over again. And due to it is preferred over credentials, authentication won't work at all. This should be detected and cached data should be cleared.
`ext` should be mp4.
This prevents from authenticating with `--cookies`.
I did not test it much but if it does not actually solve the problem then I don't see much point in it either.
I would strongly avoid adding per extractor specific options. Ideally, end user should not even be aware of the cache. Currently, there is no proper mechanism for caching sensitive data and usually when one needs to cache session he just uses `--cookies` on his own.
Actually, why not just do ```py video_params = self._parse_json(self._search_regex( r'\bvlive\.video\.init\(([^)]+)\);', webpage, 'video params'), video_id, transform_source=lambda s: '[' + s + ']') status, long_video_id, key = video_params[2], video_params[5], video_params[6] ``` It should be much more robust.
Use `self._download_json` instead.
Don't capture unused groups
Never ignore generic exceptions
This is equivalent to InfoExtractor._match_id
Use compat_urllib_parse_unquote instead. Also, there's no need to create a function with only one line.
Lots of codes in this method duplicates swfinterp.py. Re-use existing codes instead.
Use compat_HTTPError instead
filename should already be a unicode object. Does cgi.parse_header return bytes-like objects? If so a wrapper is necessary.
Use _request_webpage instead of plain urlopen. The former includes fixes and custom features missing from urllib.request
There are lots of Content-Type calls. Please merge them together
Just modify mimetype2ext rather than introducing hacks in individual extractors
Then don't use `get`.
`id` by no means should be `None`.
Will break if no `location`.
Don't capture unused groups.
No need to escape `#`. No need to capture groups you don't use.
No, it doesn't.
No `stats` - broken. No `raw` - broken.
There is no point in `or None` since `None` is already default.
Use regular string format syntax instead.
No need to escape `/`.
Here and so on.
Title is mandatory.
`'live'` is not an id.
There is no point in `get()` here. You will fail anyway if there is no corresponding keys in dict.
`int_or_none` and `float_or_none` for all numeric fields.
`_download_json`. `query` parameter for query.
Any related identifier.
The best way is modifying determine_ext and make it recognize whether it's audio or video. As of now you can just omit this line and let the ext be unknown_video.
It's better to use the original ```mp3download.action``` URL here as redirected URL may change in the future.
cookie => cookies. There are 3 items.
Use _hidden_inputs or _form_hidden_inputs - they're better than the long regex pattern.
An example is extract_attributes, which uses HTMLParser. The point here is that _hidden_inputs are much well tested. On the contrary, there are lots of tricks in HTMLParser on different Python versions.
In this case you can just omit this line as determine_ext will be automatically called under the hood.
Is 'http://res.infoq.com/downloads/mp3downloads/' hardcoded somewhere? If so it's better to point out where it is. (for example, xyz.html or abc.js)
`{}` won't work in python 2.6.
What's the point of this at all? It's already implemented in `FFmpegMetadataPP`.
As a rule of thumb, raw strings should be always used for regular expressions. But for now I guess it's enough to only fix ones with invalid escape sequences. At least this is what this PR's title stands for.
To be removed.
`field_preference` must be `list` or `tuple`. There is no need to touch this usually since default sorting works fine.
This will also apply when `online` key does not exist.
`scale` passed to `float_or_none` instead.
This should be extracted in the first place.
This should not be fatal.
This should be extracted in the first place.
MB stands for mega 10^6, not 2^20. Also this file size has nothing to do with resulting mp3 file. It's probably for original wav.
No point in separate variable.
`user_info` may be `None`.
`created_at_i` as `timestamp`.
```[^>]+?``` should be ```[^>]*```
Original extractor classes (`ie_key`s) should be preserved. Otherwise you break existing download archives.
1. There is no other way in this case. pluzz.francetv.fr does not exist anymore. 2. `ie_key`s in `--download-archive` file.
first_three_chars = int(float(ol_id[:3])) is cleaner
Should be [^"]+ instead of [^"]*
`self._parse_json` raises `ExtractorError` on its own.
You can't know whether it's an article with no video or video id pattern just changed. Original approach should be reverted.
SD quality is also available. Both should be extracted.
Seems this is exactly the video URL? Re-using extracted URLs is better
Usually display_id is used before the actual video_id is extracted.
Need fatal=False or default=None
```[\s]``` => ```\s```
```/tv/tags/[^/]*?``` => ```/tv/tags/[^/]+```
Better to use unified_strdate for parsing dates.
mobj would be None if nothing is matched
Seems you're trying to extract the first part after tv/ in the URL? Just put a named group in _VALID_URL.
Better to include ```categories``` and ```display_id``` in the test as well.
Use _parse_json and js_to_json here
Don't override built-in symbol ```format```. Also, just use {'url': ..., 'format_id': ..., ...}
Better to include colons in the regex to avoid false positives. For example ```r'movieName\s*:\s*\'...```
About ```default``` parameter: for required fields (title, ...) there should be no default, and for optional fields (thumbnail, ...) use either default=None or fatal=False. The former fails silently and the latter reports a warning.
Dots in regular expressions should be escaped.
In youtube-dl usually standard modules are imported with simply the syntax 'import xxx'
'quality' and 'resolution' are optional. There should not be KeyError if they are missing
resolution is defined as "Textual description of width and height". "medium" does not fit.
Use _sort_formats instead of sorted. The former is more robust.
This should be a ```list```.
``` r'itemprop\s*=\s*\"ratingValue\"\s*> ``` ``` r'itemprop\s*=\s*\"ratingValue\"[^>]*> ``` So that the code can live even if new attributes are added to this element Also, there's no need to escape double quotes in strings encapulated by single quotes.
What's the point? It's not alphabetic altogether anyway.
the list of MSOs that depend on watchTVeverywhere was automatically generated(3a5a18705f2a7faf64a4b69665511ef5f0c6084d) and keeping it grouped in the bottom of the list was intentional.
the default `username_field` and `password_field` has been set to `username` and `password` to prevent deuplication of the values for every MSO that depend on watchTVeverywhere, so it whould be better to keep the values it explicitly for DIRECTV as it's independant from watchTVeverywhere.
Should not break if there is no `resolution` key.
Use self._report_warning instead
Don't hardcode it. The default User-Agent can be found in ```std_headers``` in utils.py
Lack of data must be expressed by `None` not empty string.
Should be string. Use `self._match_id` instead.
Unite in single list comprehension.
All similar tests should be `only_matching`.
This will capture till the last quote.
`get_element_by_class` is already a string or `None`. `strip` on `None` will break.
This is pointless. If no formats can be extracted extraction should stop immediately.
What's the point of this? `canonical_url` is the same as `url`.
Everything apart from query is the same and this unnecessary webpage downloading should be removed.
It's a conditional expression that allows to skip formats extraction. As I've already said this is **pointless** since valid list of formats must be always present. `None` `formats` is **not allowed**.
Format it properly. This applies for all code.
Use `query` for query.
Use `query` for query.
Use `query` for query.
Should be `mp4`.
Should no break if some format has no location.
Sorting is incorrect. Best format is not really the best.
Breaks if no `rate` key in `stream`.
if not surl you should go to the next iteration immediately.
This is bitrate, not quality.
`surl.get(...)` or `try_get`.
`_live_title` should be used.
`[0]` must be in `getter`.
Must be numeric.
Should not break the extraction if missing.
This should be extracted first.
Must be numeric.
Sholdn not break the extraction if missing.
Use `default=None` to `_search_regex` instead. `k.startswith('video_vars[video_urls]')` check is redundant.
Nothing changed. Breaks in case regex does not match.
This field is height not quality.
`flashvars[k]` is `v`.
Append dict right away.
`for k, v in flashvars.items()`.
Invalid syntax at all.
Don't capture unnecessary groups.
Will break the extraction if there is no title attribute. This should be optional.
I did not ask to remove **all formats extraction** but to remove **code duplication**.
All formats should be extracted.
No need in another test.
I've already pointed out there is no need in another test since they are broken once room is offline.
Identifier may have a format tag.
If they are all the same extract any that is not null.
It should extract the album title field reported by the webpage. No more no less.
Must be int.
That's explained in code conventions you've ticked as read.
All these new fields must be optional.
It's not a track id.
Must be int.
It's artist not an album artist.
This extracts first title found not necessarily an album title. All captured groups must be at least one character length.
Use plain unicode.
`urljoin` already does these checks.
Generic embed is an embed that may be embedded anywhere. If you can't provide any example URL (apart from visir.is site itself) of embed with such code then it's not a generic embed.
It comes from http://www.visir.is/section/MEDIA?template=related_json&kat=5&subkat=73. `kat` and `subkat` come from a webpage.
This should not be hardcoded.
look at the embed page you will find the rules to construct urls for all content types provided by the website(there is a direct link for video and audio types).
This is not a generic embed.
This duplicates code from `MedialaanIE`.
170-172 - code duplication.
For now extract it in a new base IE class.
`vrtvideo` should be hardcoded.
Should be delegated via `url_result`.
178-187 - code duplication.
You should have only one single method for extracting info.
Consistently use flags inside regex.
This can be expressed in regex.
Plays fine without any authentication in browser.
This is not true at the moment.
`title` is mandatory. Move flags into regex itself.
All debug code should be removed.
Code duplication at 58-60 and 68-70.
Do not shadow `url` variable. `fatal` has no effect when `default` is provided.
If n URLs are downloaded with single command login will take place n times. You should store login flag or use `_real_initialize`.
You can leave note about video not requiring authentication but you should not repeat the code itself: >When no video_id is found >so just fall back to the generic extractor
Some `display_id` should be extracted from `url` and shown instead of `None`.
https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/bigflix.py#L23-L25 All tests that test same extraction scenario should be made `only_matching`.
@mrBliss OK, just to make sure everything is good :-) I'm really waiting for this one for a long time so... ;-)
[Correct extraction delegation](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/lcp.py#L76).
There are two scenarios: video without auth and video with auth. The rest are matching only.
These looks like mandatory fields.
Pass `m3u8_data` as `query` to download method.
Did you even read what I've posted? >The tests will then be named `TestDownload.test_YourExtractor`, `TestDownload.test_YourExtractor_1`, `TestDownload.test_YourExtractor_2`, etc.
`only_matching` is only matched against `_VALID_URL`, such tests are for reference purposes. There is no sense in several similar tests that cover one scenario.
Video is gone.
Breaks downloading of videos that does not require authentication.
With your code `video_id` is allowed to be `None`. This is broken in this case.
Use single quotes consistently.
1. `preference` is pointless. 2. `fatal=True` is the default. 3. Should be lowercase `hls`.
Yes. This is due to a playlist.
default=None vs. fatal=False: the former is silent while the latter issues a warning is matching fails. The first try should be silent and the second try should issue a warning. Anyway don't use default=None and fatal=False in a single call.
Original regexes should be tested first.
This line is unnecessary as there's already self._og_search_description
Better to use fatal=False instead of default=None here. For optional fields, if all extraction approaches failed, youtube-dl should issue a warning.
Use ```_og_search_description()``` as a fallback instead.
_og_search_description is a youtube-dl function
Don't remove old patterns.
Yes you're almost there
This is my idea: 1. get_element_by_id is not touched (by HTML standards IDs are unique) 2. get_element_by_attribute => get_elements_by_attribute 3. get_element_by_class => get_elements_by_class And replace all usages in extractors. It's better to open a new pull request for this change.
Javascript (HTML DOM API) has ```getElementById``` only. If a webpage has multiple elements with the same ID, website developers have to do parsing stuffs if they want to access all elements of that ID. It would be suprising to me if there's really such a site. Also, it's better to make function in utils match Javascript.
It would be better to extend ```utils.get_element_by_class``` and make it return multiple results
The most simple way is to use `utils.extract_attributes`.
`?` makes no sense here. Arbitrary attribute position should be supported in case this is dynamic.
All tests placed here must have URLs that match extractor's `_VALID_URL`. Tests for generic embeds should be placed into generic extractor. ``` PS C:\Dev\youtube-dl\master> py -3.5 .\test\test_all_urls.py ...F............. ====================================================================== FAIL: test_no_duplicates (__main__.TestAllURLsMatching) ---------------------------------------------------------------------- Traceback (most recent call last): File ".\test\test_all_urls.py", line 94, in test_no_duplicates self.assertTrue(ie.suitable(url), '%s should match URL %r' % (type(ie).__name__, url)) AssertionError: False is not true : PikselIE should match URL 'http://www.uscourts.gov/cameras-courts/state-washington-vs-donald-j-trump-et-al' ---------------------------------------------------------------------- Ran 17 tests in 8.066s FAILED (failures=1) ```
I'd prefer the latter. Generic extractor is already too big.
Title is mandatory.
Don't change extractor name.
As said **don't touch** extractor's name. We don't need unnecessary download archive breakages.
No, you can't. Title is mandatory meaning you must fail if you can't extract it. Read new extractor tutorial.
This is **changing of the name** because you've merged `IndavideoIE` into `IndavideoEmbedIE` (or simply dropped it) and then renamed it to `IndavideoIE` as a result you've **broken** all download archives made with previous versions of youtube-dl. Just do what is asked.
What's the purpose of this line? ```sorted``` does not change the target ``` >>> a = [1, 3, 2] >>> sorted(a) [1, 2, 3] >>> a [1, 3, 2] ```
The point is that ```urlcode``` is not modified. I change the code to: ``` print(urlcode) print(sorted(urlcode, key=lambda key: urlcode[key])) print(urlcode) ``` And it prints: ``` {30: '.', 19: '6', 39: 'a', 8: 'U', 41: 'e', 10: 'o', 32: '.', 35: 's', 5: 'c', 11: '~', 40: 'G', 23: '1', 34: '~', 42: 'F', 9: '9', 25: '0', 7: 's', 3: 'f', 27: '1', 37: '4', 20: '1', 14: '8', 1: 'U', 21: '9', 18: '3', 31: '0', 24: '4', 28: '1', 12: '1', 2: 'E', 16: '5', 4: 'G', 22: '~', 15: '6', 17: '6', 29: '2', 0: 'k', 13: '4', 38: 'h', 33: '0', 26: '.', 36: 'U', 6: 'l'} [30, 32, 26, 25, 31, 33, 23, 27, 20, 28, 12, 29, 18, 37, 24, 13, 16, 19, 15, 17, 14, 9, 21, 2, 42, 40, 4, 8, 1, 36, 39, 5, 41, 3, 38, 0, 6, 10, 35, 7, 11, 34, 22] {30: '.', 19: '6', 39: 'a', 8: 'U', 41: 'e', 10: 'o', 32: '.', 35: 's', 5: 'c', 11: '~', 40: 'G', 23: '1', 34: '~', 42: 'F', 9: '9', 25: '0', 7: 's', 3: 'f', 27: '1', 37: '4', 20: '1', 14: '8', 1: 'U', 21: '9', 18: '3', 31: '0', 24: '4', 28: '1', 12: '1', 2: 'E', 16: '5', 4: 'G', 22: '~', 15: '6', 17: '6', 29: '2', 0: 'k', 13: '4', 38: 'h', 33: '0', 26: '.', 36: 'U', 6: 'l'} ```
Probably this will work instead? `urllink = ''.join([str(urlcode[key]) for key in sorted(urlcode.keys())])`
It should always return a list.
This doc string does not match the function now.
It's a list of elements not the first element.
It worth adding a doc string.
This can be simplified to `return retval[0] if retval else None`.
```dict_get``` makes codes even shorter
Use ```extract_attributes``` instead. The order of video_id, account_id, ... may change.
Imported items should be sorted alphanumerically. Note that in this project symbols are case-insensitive during sorting. (I know there are many places not sorted well; it would be great if an automation tool or someone can fix them...)
The convention here is put 'md5' before 'info_dict'
As we're now in a dedicated extractor, just choose a better one from either the value of ```og:title``` or ```<title>``` contents. This can make codes simpler.
As this code is moved to a dedicated extractor, only 'data-brightcove-video-id' should be detected.
If a method is used by at least two extractors, it can be moved to ```common.py```
Sorry I may be inaccurate. Capitalized symbols are sorted together with non-capitalized symbols. For example, this is correct: ``` error_to_compat_str, ExtractorError, fix_xml_ampersands, float_or_none, GeoRestrictedError, ```
I was talking about the video on programme-tv.net. That's unrelated to bostonglobe.com. > That doesn't work for the bostonglobe.com case Of course it won't as these lines are for programme-tv.net > and then bostonglobe.py should import that Nope. bostonglobe.py requires special handling and is unrelated to _extract_urls() > As a side note, this pattern is quite similar to #12005 That's just a reminder for myself but not requiring you to implement both patterns in a single pull request. I'm sorry if that's misleading. And apparently it's time to seperate codes for bostonglobe.com and programme-tv.net into two pull requests. Otherwise there might be more confusions.
As already mentioned by @remitamine it's not a generic embed and should not be used in generic extractor.
You'll need ```BostonGlobeIE``` in ```bostonglobe.py```
Thanks. It turns out that data-brightcove-video-id is a bostonglobe-specific usage, so a dedicated extractor is better.
Besides providing another example, you can also prove that Brightcove's Javascript indeed references data-brightcove-video-id. For example, in https://github.com/rg3/youtube-dl/blob/732d116/youtube_dl/extractor/jwplatform.py#L36, there are lots of deprecated usages.
Why do you disallow `story` to be an id? Because you do not want it to be an id in your particular case that is clearly an ad hoc hack cause other extractors may use `_generic_id` and may allow `story` to be an id.
>also `data-brightcove-video-id` is not used by the standard embeds https://docs.brightcove.com/en/perform/brightcove-player/guides/in-page-embed-player-implementation.html. You should add separate extractor instead. Or should provide another example URLs across web to prove this pattern is used not only on this site.
There should not be any ad hoc hacks in generic methods.
`compat_urlparse.urlparse` to extract query from original url. `update_url_query` to pack into new URL. Do not pack if original URL is used.
That would break non-ASCII titles
> compat_input can't cope with non-ascii titles Any Python bug report for that? Maybe we need to implement our ```compat_input``` rather than wrapping existing functions.
> Also trying to copy&Paste this specific Unicode Char ends in two question marks. Some terminals (at least Konsole and QTerminal) can't handle non-BMP (i.e., Unicode characters > 0xFFFF). You may be another victim
The only criteria is don't drop non-ASCII characters. You can do whatever fix you want.
Remove it from here obviously.
`title` is not guaranteed to be present.
Uppercase is not honored.
I guess "yes" should be the default answer and should look like `(Y/n)`.
It should ask each time. Plain return should use the default answer.
`skipnextvid` is not initialized if `'title' not in entry`.
Most playlists don't have titles in their entries' at this time thus this option won't work for most of the cases.
```process_ie_result``` is called twice. It's a big function and may contain heavy network I/O. For example: ``` $ youtube-dl -v "http://www.kuwo.cn/album/502294/" --geo-verification-proxy https://secure.uku.im:8443/ --playlist-prompt [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', 'http://www.kuwo.cn/album/502294/', '--geo-verification-proxy', 'https://secure.uku.im:8443/', '--playlist-prompt'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.03.22 [debug] Git HEAD: 093dad9e2 [debug] Python version 3.6.0 - Linux-4.10.4-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.2.4, ffprobe 3.2.4 [debug] Proxy map: {} [kuwo:album] 502294: Download album info [download] Downloading playlist: MadeÂ SeriesÂ ãMã [kuwo:album] playlist MadeÂ SeriesÂ ãMã: Collected 1 video ids (downloading 1 of them) [kuwo:song] 6430200: Download song detail info [kuwo:song] 6430200: Download ape url info [kuwo:song] 6430200: Download mp3-320 url info [kuwo:song] 6430200: Download mp3-192 url info [kuwo:song] 6430200: Download mp3-128 url info [kuwo:song] 6430200: Download wma url info [kuwo:song] 6430200: Download aac url info [kuwo:song] 6430200: Download album detail info Do you want to download video 1/1: "Loser"? (Y/n) y [download] Downloading video 1 of 1 [kuwo:song] 6430200: Download song detail info [kuwo:song] 6430200: Download ape url info [kuwo:song] 6430200: Download mp3-320 url info [kuwo:song] 6430200: Download mp3-192 url info [kuwo:song] 6430200: Download mp3-128 url info [kuwo:song] 6430200: Download wma url info [kuwo:song] 6430200: Download aac url info [kuwo:song] 6430200: Download album detail info [debug] Invoking downloader on 'http://other.web.rf01.sycdn.kuwo.cn/60c92ace79968c55a3e53629a5bdec2d/58d2a746/resource/n1/71/58/2389040631.mp3' [download] Destination: Loser-6430200.mp3 [download] 100% of 8.37MiB in 00:00 [download] Finished downloading playlist: MadeÂ SeriesÂ ãMã ``` This is a geo-restricted album. In this example each "Download foobar url info" can take a few seconds if the proxy is slow. Sounds not good. By the way, process_ie_result does not always return a dictionary. For example: ``` youtube-dl -vF "http://www.kuwo.cn/album/502294/" --geo-verification-proxy https://secure.uku.im:8443/ --playlist-prompt [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-vF', 'http://www.kuwo.cn/album/502294/', '--geo-verification-proxy', 'https://secure.uku.im:8443/', '--playlist-prompt'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.03.22 [debug] Git HEAD: 093dad9e2 [debug] Python version 3.6.0 - Linux-4.10.4-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.2.4, ffprobe 3.2.4 [debug] Proxy map: {} [kuwo:album] 502294: Download album info [download] Downloading playlist: MadeÂ SeriesÂ ãMã [kuwo:album] playlist MadeÂ SeriesÂ ãMã: Collected 1 video ids (downloading 1 of them) [kuwo:song] 6430200: Download song detail info [kuwo:song] 6430200: Download ape url info [kuwo:song] 6430200: Download mp3-320 url info [kuwo:song] 6430200: Download mp3-192 url info [kuwo:song] 6430200: Download mp3-128 url info [kuwo:song] 6430200: Download wma url info [kuwo:song] 6430200: Download aac url info [kuwo:song] 6430200: Download album detail info [info] Available formats for 6430200: format code extension resolution note aac aac unknown audio@ 48k wma wma unknown mp3-128 mp3 unknown audio@128k mp3-192 mp3 unknown audio@192k mp3-320 mp3 unknown audio@320k (best) Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1910, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 773, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 950, in process_ie_result desc = entry_result.get('title', entry_result.get('id', '')) AttributeError: 'NoneType' object has no attribute 'get' ```
None of the optional metadata should break the extraction.
There is no need in this method.
There is no need in this method.
There is no need in this method.
Should not be fatal.
To be removed.
It's not removed: `(\d+|ref:[^"\']+)` captures number or string starting with `ref:`.
The other case was some legacy code. I don't see much sense in such messages since it's clear what extractor is delegated to since all messages from the final extractor are prefixed with `IE_NAME`.
Before these changes there were 4 references and extraction followed them strictly in order of reference declaration.
At least move iframe extraction after video tag extraction.
Another problem when match happens by video tag only is that there is actually no indication whether it's a brightcove embed or not at all. Only common attributes `data-video-id`, `data-account`, `data-player` and `data-embed` that technically may be used by others. Previously such indication was obtained by matching script tag with brightcove JS. There are some cases when script tag is located before video tag that were not detected previously. Do you have example URLs with video tags only and without brightcove indication? If not there should be added an additional check for presence of brightcove JS script on the page.
>So, do you consider any of the above indications sufficient to check for? No, I don't think any of non standard embed code should be considered for generic embeds. >Maybe the right answer is to move the search for `<video>` tags into `generic.py`, and then call a series of methods from extractors (like `._extract_urls()` but different) to determine if they want to try to retrieve (perhaps the Brightcove methods would return `Yes` in cases where the standard `<script ... players.brightcove.net>` was present, and `Maybe` in the other cases. Then `GenericIE` could call the `Yes` extractor(s) if there is one (any), or iterate over the `Maybe`s if not. Don't know if I get this right but one possible solution is to separate brightcove video tag only extraction (without brightcove indication) to another *ambiguous* method in `BrightcoveNewIE` and call it very last in generic extractor as a last resort. This will allow not to accidentally shadow another extractors' embeds but still detect such video only tag brigthcove embeds. Though this still may produce false positives. Another solution is to prohibit such generic embeds at all and add a separate extractor for each such case.
It's more confusing to see extraction to happen in different order.
`ref:` should not be removed from video id.
To be removed.
Do not capture groups you don't use.
There is no need in excessive verbosity.
Ids must stay intact.
Title is invalid.
That's completely different videos.
Cause http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 is a link to a segment not the whole episode.
i think that can be fixed by followning what's been done in the website player. for http://www.srf.ch/play/tv/10vor10/video/snowden-beantragt-asyl-in-russland?id=28e1a57d-5b76-4399-8ab3-9097f071e6c5 the player will request the m3u8 url with `start=532.255&end=646.082` appended to the url query while in http://www.srf.ch/play/tv/10vor10/video/newsflash?id=493b764e-355b-440a-9257-23260a48a217 it uses `start=1183.96&end=1228.52` so the manifest urls and duration are the ones that should be changed not the other video information(id, title...).
It will be great if they are sorted alphanumerically and wrapped.
The order of dictionaries is not deterministic before Python 3.6. For example: ``` $ PYTHONHASHSEED=0 python2 test/test_utils.py -v TestUtil.test_dfxp2srt test_dfxp2srt (__main__.TestUtil) ... /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2629: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. ele = dfxp.find(_x('.//ttml:' + p)) or dfxp.find('.//' + p) /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2630: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. if not ele: ok ---------------------------------------------------------------------- Ran 1 test in 0.004s OK $ PYTHONHASHSEED=1 python2 test/test_utils.py -v TestUtil.test_dfxp2srt test_dfxp2srt (__main__.TestUtil) ... /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2629: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. ele = dfxp.find(_x('.//ttml:' + p)) or dfxp.find('.//' + p) /home/yen/Projects/youtube-dl/youtube_dl/utils.py:2630: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead. if not ele: FAIL ====================================================================== FAIL: test_dfxp2srt (__main__.TestUtil) ---------------------------------------------------------------------- Traceback (most recent call last): File "test/test_utils.py", line 1093, in test_dfxp2srt self.assertEqual(dfxp2srt(dfxp_data_with_style), srt_data) AssertionError: u'1\n00:00:02,080 --> 00:00:05,839\n<font color="white" face="sansSerif" size="1 [truncated]... != u'1\n00:00:02,080 --> 00:00:05,839\n<font color="white" face="sansSerif" size="1 [truncated]... Diff is 663 characters long. Set self.maxDiff to None to see it. ---------------------------------------------------------------------- Ran 1 test in 0.005s FAILED (failures=1) ```
Use `self._parse_json` instead. This also looks very similar to `sources` parsing in `self._parse_jwplayer_data` that should be probably generalized.
There is no point in separate method that is only used once.
`flags` should be embedded in regex itself.
`\s*` make no sense at the end.
This is determined automatically.
No point checking this.
This matches https://vshare.io//0f64ce6 that is incorrect.
There is no point checking `url`.
This should be more relaxed.
Do not remove any code.
That's incorrect. https://www.stream.cz/pohadky/lego-staveni/10015386-dalsi-staveni-z-lego-city-starter-set is a video but processed as a playlist.
It should not match `h|`.
Since video ids seems unique across both sites I see no point separating extractors.
`<h3>` is intentional.
You did not test for that thus not catched.
Wrong. Extractor key is `Vier`.
No sense to use named group when it's the only one.
`lxml` is not allowed.
`'http:' + None = TypeError`
Second parameter must be video id.
Minor issue: there's no need to assign a variable if it's not used. UPDATE: Sorry, I didn't notice it's used below...
And it's better to place jwplayer-related tests together
If ```options[0]``` _is_ ```{```, options should be returned.
1. ```options``` should be escaped - ```re.escape``` 2. Nested variables won't be detected. For example: (from https://developer.jwplayer.com/jw-player/docs/developer-guide/customization/configuration-reference/) ``` var config = { "playlist": [{ "file": "/assets/sintel.mp4", "image": "/assets/sintel.jpg", "title": "Sintel Trailer", "mediaid": "ddra573" },{ "file": "/assets/bigbuckbunny.mp4", "image": "/assets/bigbuckbunny.jpg", "title": "Big Buck Bunny Trailer", "mediaid": "ddrx3v2" }] }; ```
Sorry for previous noises. I just actually checked the HTML source of your example and found that this PR doesn't work as expected. ("resolving the variable to the JSON string") Try to print the value of ```mobj``` after this line.
This change does not make any sense. Old regex matches http://www.spiegel.tv/filme/putins-trollfabriken/embed/?autoplay=true just fine.
Groups around `video` and `sptv/spiegeltv` are superfluous.
This may match across several `meta`s.
This should be handled in 3qsdn extractor.
Playlist extraction should only take place after no video formats found.
You should add support for this playlist-alike 3qsdn URLs in any non-breaking way.
This will break the extraction if webpage download fails. Also, `title` and `description` from the webpage does not bring any senseless names so there is no point extracting them. Same title may be extracted from JS code.
Instead of try except use `default`.
One if (1 line) vs try except + RegexNotFoundError import (4 lines).
End users do not read source codes thus will never find this advice.
This should be removed.
This is only used once.
No need in this message.
Remove this line.
Use `compat_str` instead. Also check code with flake8 and squash commits.
`update()` one dict with another instead.
This will fail if `media_data.get('descriptions')` is not a `list`.
Should be non fatal.
This should be obtained from http://www.szenik.eu/visionneuse/visio_v7_js.php?key=bw0Y9lgV47&width=670 and current code should be used as a fallback.
This fallback should be removed now.
None of the optional fields should break the extraction if missing. Read new extractor tutorial.
Has no effect on hls.
This fields are mandatory. It's pointless to allow `None`s for them.
Title is mandatory.
What's the point of this base class? It's only inherited once.
What's the point of this method? It's only used once.
Query should be passed as `query` parameter.
Will break if `picture_url` is `None`.
Will break if `episode_title` is `None`.
It's already VRT.
There is already an extractor with such `IE_NAME`.
Use consistent naming and not capitalized here and non capitalized in different places.
Fix code duplication, use single quotes and squash commits.
Use `self._html_search_meta` or `self._og_search_title` instead.
There is always only one video on the page thus you should take first entry and return as regular info dict.
Use `self._parse_html5_media_entries` instead.
>How could it work otherwise? `_call_downloader()` instantiates an instance of `FFmpegPostProcessor`, and only at that time is a decision made on the executable name. It should instantiate `FFmpegPostProcessor` on it's own like `available` do. It may accept `info_dict` as input parameter. >Meaning as an `@property`? OK. Meaning both this and implementation.
1. It breaks if called before `_call_downloader`. 2. It should not store state. 3. It should be implemented similar to `available`.
It's obvious from `'is_live': True`.
m3u8 is also available.
Use single quotes consistently.
What's the point of `# match self._live_title` here? Remove.
What's the point of this? Include into regex.
Do not capture groups you don't use.
No need to escape whitespace.
`_search_regex` is enough here.
There is no point inheriting from `WSJIE`.
What's the point extracting it here? Also use http://video-api.wsj.com/api-video/player/iframe.html?guid=... instead or add a `wsj:` shortcut.
Instead of this extraction should be delegated to `WSJIE` via `url_result`.
Also pass `video_id` since it's known beforehand.
Never ever use bare except.
No, no hardcodes here. You are implementing a generic mechanism thus there should be a way to control it on per extractor basis. Or it would be easier to just introduce an option describing playlist order instead of this hack and require user to specify it if he wants such optimization.
No `upload_date` is guaranteed to be present.
Did you even bother to run your code? `self.params.get('date_playlist_order')` will always be `None` thus `break` will never trigger since [you did not forward it to `params`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/__init__.py#L310-L429). Even if you did, it **won't work** (repeating this 3rd time already) if `daterange` is an **inner interval** inside `daterange`. In such case if the first/last video (depending on the order) does not belong to `daterange` extraction will stop on it and won't process further items that may belong to `daterange`.
Breaks on `-F`.
This breaks backward compatibility. Options' dests (`playlistreverse` and `playlistrandom`) should not be changed.
I've already pointed out this does not work.
There is already `--playlist-reverse`.
This won't work at all if starting playlist entry is not in `daterange`.
Option should be called something like `--date-playlist-order` and should accept `asc`, `desc` or `none`. Code should process playlist according to it.
Should be cut down to couple of sentences. Possible arguments' values should be written all lowercase without quotes. See `--fixup` for an example.
This is a bad idea since it breaks reusing of `YoutubeDL` instances. Even worse it stores string instead of just reference.
No, it's not the point. It **must not** skip items than belong to the range specified.
This is very brittle heuristic.
There are more pages. All should be extracted and extractor should be renamed to `NPOProgramIE`.
Both used only once, move to the place where used. Also relax both regexes.
`default=None` for the first.
Should not be fatal.
Extraction should not break if one of the formats is missing.
`https://streamango.com/f/clapasobsptpkdfe` is not matched by this regex.
This is not true in general.
`{}` does not work in python 2.6.
`mso_id` is always `None` at this point.
This introduces an ambiguity in case of several mso available. That's why it won't be accepted.
You should extract `partner_id` and `entry_id` and return `kaltura:...` shortcut.
This should include `iframe` part.
No, your approach is more fragile. You match just everything on kaltura.com domain and it may redirect to anywhere and do anything. This iframe embed is unambiguous, exact and explicit [official embed code](https://knowledge.kaltura.com/embedding-kaltura-media-players-your-site) that has well defined semantics and won't change in future.
Just leave a link to kaltura embedding page.
Do not remove the old domain.
Do not capture groups you don't use.
None of the optional metadata should break the extraction when missing.
All these regexes should be relaxed.
Parenthesis are superfluous.
Parenthesis are superfluous.
Must be int.
now, the `VideoId` is not included in the params, instead there is: ``` _cne.watchPage.embedPlayer(params, 'http://player-backend.cnevids.com/inline/video/58d1865bfd2e6126e2000015.js?'); ```
i think it would be better to add support for this embed urls and return a url result instead of inline handling of the embed url.
Do you have examples of such relaxed embeds? The only I've encountered is `http://www.washingtonpost.com/video/c/embed/<uuid>`.
`self._search_json_ld` should be improved instead.
No escape for `\` needed. `(\?[^/?]+)?` makes no sense at the end.
`display_id` is already string.
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'replace' Supposing that is fixed, the inner portion may still be `None`, leading to, > TypeError: int() argument must be a string or a number, not 'NoneType' use `int_or_none`
`get` could return `None` resulting in, > TypeError: cannot concatenate 'str' and 'NoneType' objects
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'split'
Note `video_data` may be `None`.
Note `dl_data` may be `None`.
`get` could return `None` resulting in, > AttributeError: 'NoneType' object has no attribute 'group'
[You should not be calling `__contains__` directly.](http://stackoverflow.com/a/1964949/1072626)
Code duplication. This is already implemented in `CeskaTelevizeIE`.
Does not match `var IDEC='`.
Superfluous. Move directly into the method call.
Read coding convention on optional fields and fix all issues.
`{}` won't on python 2.6.
Should be `var\s+`.
it whould be better to iterate once and extract the needed information.
this will fail if `type` is not present.
if the `ext` can't be detected than fallback to `vtt` as it's the `ext` that most likely to be.
`season` and `season_id` can also be extracted here.
as the `ext` is already present in the subtitle url it might be better to let youtube-dl extract it directly from the url(in case that start serving other types of subtitles) instead of the hardcoded value.
do you have an example with subtitles.
no need to create a method when it will be used once.
you can iterate here using `values` method and make it in a single line without checking if `Item` is present: ```python for metadata in video_data.get('__children', {}).get('Item', {}).values(): ```
I would prefer hiding all phantomjs related code in a separate wrapper class.
Temp file is not removed in this case.
The current working directory is not always writable.
Sorry - dismiss that
Yes, if it tests the same extraction scenario. There is only one extraction scenario in this code.
[No, you don't get it](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/acast.py#L80).
All duplicate tests should be set `only_matching`.
No. ``` for ext in ('mp3', 'opus') # or KNOWN_EXTENSIONS: if ext in d...: d... = ext break ```
No. Override `suitable`.
What's the point of this extractor? It's covered by album extractor. Remove.
What are you trying to do here? There should be a single regex.
Use consistent naming.
Will never happen.
All tests that test similar extraction scenario should be `only_matching`.
No need to escape `-`.
Use single quotes. This change breaks all tests. No purpose of this change described, no tests. Change does not match PRs title.
`if 'id="user-login"' in webpage:`.
Not all URLs contain video id.
Therehere could be a fallback to video/embed URL and extraction without metadata with a warning to use credentials for correct metadata printed.
`default` implies non `fatal`.
No exception here just warning.
Should be `display_id`.
There could be a fallback with id extracted from URL.
No, it's not. If video_id extraction from page fails whole extraction fails.
If makes sense to login in the first place when credentials are provided.
There are two unrelated flags `_logged_in` and `logged_in`.
This is not necessarily true. Login errors should be detected and output.
This is not true either. Login may be achieved via authorized cookies.
Standard message is `Unable to log in`.
What's not clear? `video_id` extraction from `webpage` does not participate in formats extraction thus should not be fatal. It does not matter whether you can find an example URL or not for which `video_id` extraction fails. It may not fail now but may start failing in future (e.g. due to regexes change) breaking the whole extraction. I don't have such example URL. Change regexes to invalid ones to test such scenario.
You should **capture** error message and **output** it.
This should not be removed.
Capturing empty string does not make any sense.
Don't mix unrelated changes in one PR.
Don't shadow built-in names.
This one is more precise.
Don't capture groups you don't use.
You should fix the reason not the consequence.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
Yes - keeping intermediate files is useful for debugging. People can run the same ffmpeg command again and see what's going on.
`-k` is supposed to work for explicit files not for debugging purposes.
What's the point of this? This is an implementation detail and it should not leave any trace if `-k` is used.
This must be applied only in case of chapters.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
This can be moved inside `if chapters:` condition.
`or []` is superfluous here.
OK. I'll open an issue for discussing this. For now you can remove this line.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
`esquire` is not a valid adobe pass requestor id, the fallback value should be `style`(the value that is actually used now), the same apply to adobe pass resource id.
Don't. Must be a separate article extractor.
Don't touch `_real_extract` signature.
Don't touch the ids.
Don't shadow built-ins.
Ok, it's fine to separate it in order not to calculate it every time in a loop over links, but it should be located near `spoken_subtitles`.
You should not touch extraction at all. It's already implemented in `NuevoBaseIE`.
See how browser calls it.
No need in explicit group name.
`video_id` is already a string.
No need in these variables.
No hardcodes. Extraction should be implemented in terms of `NuevoBaseIE` instead.
All formats should be extracted.
Should not be greedy.
151-165 code duplication.
Should be non `fatal`.
Separate variable is superfluous.
Make more relaxed and add title regex as fallback.
Move flags into regex.
It should not break extraction if `sts` key is missing.
Should be extracted from `ytplayer_config`.
This methods is only used when subtitles' extraction depends on additional expensive work, i.e. network I/O. In this case subtitles are already extracted along with formats and using this method does not make any sense.
This may not be available. Graceful message should be printed in this case.
`video_id` literal is not a video id.
Who cares about effectiveness when it's broken? With your comparator `'1080' < '720'` that is incorrect.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
`video_id` may be `None`.
Doesn't work in python 2.6.
Don't capture groups you don't use.
Don't capture groups you don't use.
No need to use `_search_regex` here. It's already guaranteed that regex is matched.
Regex should be relaxed. Dots should be escaped.
Remove all unused code.
There are multiple formats, all should be extracted.
No need to use named groups when there is only one group.
Title should contain title, not air date.
Remove all debug output.
Don't touch unrelated stuff.
This video has `og:title`.
Move to the place where it's first used.
3 is easily possible, say `Title` is renamed to `Song`. With proper fallback extractor continues to work. Without it - fails completely. Read coding conventions where the rationale clearly explained.
I'm **not talking about any particular test case**. I'm talking about a potential situation when this code: ``` <td class="key">Title:</td> <td class="value"></td> ``` **is not present on the webpage**. 46 will **fail** in this case because it has `fatal=True` and you will not reach a "fallback" in this case.
It's more rational to use `False` as default since it's used more often in this code.
There are more metadata to be extracted: `duration`, `abr`, `asr`, `filesize_approx`.
So what? It's still not reachable when 46 fails. Extracting empty strings does not make any sense.
Dots should be escaped. Query may contain more arguments that will be incorrectly captured by this regex as path.
This is never reachable.
And `_html_search_regex` will fail breaking the extraction completely.
This will break extraction if some field is missing. Optional fields should not break extraction.
Change to nested function. Capture `page` from outside code instead of passing as argument.
This is not reachable when title extraction fails.
You are not falling back. If regex does not match HTML you will fail immediately at 46. ``` <td class="key">Title:</td> <td class="value"></td> ``` may not be present in HTML code and you will never reach your "fallback" in this case.
This is already checked in `_sort_formats`.
Query should be passed as `query`.
55-57 what are you doing? `for video_url_key, video_url_value in video_data['files'].items():`. That's all.
What's the point of `.get('files', {})`? Use `for k,v in data.items()` instead.
`True` is default.
This is pointless, you don't have any fallback.
`fatal=False` is pointless here since you don't have any fallback.
56-71 code duplication.
Bother to read >[adding new extractor tutorial](https://github.com/rg3/youtube-dl#adding-support-for-a-new-site) and [youtube-dl coding conventions](https://github.com/rg3/youtube-dl#youtube-dl-coding-conventions) sections
Both formats should be extracted.
Extraction is broken if any these key is missing.
Default sorting is just fine. Remove `field_preference`.
JSON should be parsed as JSON.
`player_id` is not extracted in this fallback but used at 71.
No need to use named group when there is only one group.
Title is mandatory.
No `ExtractorError` is raised here, `except` will never trigger.
Extract display id.
Never use bare except.
It should capture video id. Remove parentheses.
Title is mandatory.
`_search_regex` per each field. Add fallback.
Extraction must be delegated to brightcove extractor.
Dots should be escaped. Regex should be relaxed. No `u''`.
It's not better in this way? ```python class RaiPlayLiveIE(RaiBaseIE): _VALID_URL = r'(?P<url>https?://(?:www\.)?raiplay\.it/dirette/(?P<id>\w*))' _TEST = { 'url': 'http://www.raiplay.it/dirette/rai3', 'only_matching': True, } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) url, channel = mobj.group('url', 'id') webpage = self._download_webpage(url, channel) re_id = r'<div([^>]*)data-uniquename=(["\'])[\w-]*(?P<id>%s)(\2)([^>]*?)>' % RaiBaseIE._UUID_RE video_id = self._html_search_regex(re_id, webpage, 'livestream-id', group='id') return { '_type': 'url_transparent', 'url': 'http://www.raiplay.it/dirette/ContentItem-%s.html' % video_id, 'ie_key': RaiPlayIE.ie_key() } ```
230-264 no copy pastes.
http://www.raiplay.it/dirette/ should not be matched.
What's the point capturing `url` when you already have it in `url`. Rewrite with `_match_id`.
Use `url_result` if you don't adjust any metadata.
Remove all pointless changes.
Do not capture empty strings.
No escape for `/`.
Move flags into regex.
Don't change extractor name.
Don't shadow built-in names.
It's not a display id.
No `url` and `formats` at the same time.
Title is mandatory.
Read coding conventions and fix all optional fields.
```suggestion 'md5': '048dab5c2f8ab97f2bd75ab4cf3f463a', ``` Maybe this video was changed, this now seems to be the correct hash.
No need to escape `\`.
`{}` does not work in python 2.6.
This regex does not make any sense.
Remove all pointless changes.
Don't mix unrelated changed in single PR.
This does not fix anything.
Breaks extraction if there is no `stream-labels` key.
Both should be extracted.
Breaks all videos embedded with exact `<ul class="media-list items" id="media-related-items"><li data-video-info`.
You should not mute errors. Instead you must catch them where necessary.
This does not make any sense. You must **catch** exceptions **where** you use `_extract_theplatform_smil` not here.
Empty string capture does not make any sense.
Will capture incorrect data for `data-audio-src="" some-attr="blah"`.
>This does however add unnecessary complexity that could break the extractor when it should be able to work. I'm guessing this is why you want it out? Yes.
Will capture incorrect data for `data-showname="" some-attr="blah"`.
`unescapeHTML` is already done.
Empty string capture does not make any sense.
Empty string capture does not make any sense.
Don't capture groups you don't use.
`/?` is pointless at the end.
`episode_controls` is not used. Remove 29-31.
`_search_regex` is enough.
`_search_regex` is enough.
This will break extraction if no `id` present.
This will result is `[None]` is no category extracted.
Move into method call.
Must be `list`.
https://rutube.ru/video/10b3a03fc01d5bbcc632a2f3514e8aab/?pl_id=4252 is **not a playlist** and it must be matched by non-playlist extractor.
Capture as `id` obviously.
Replace 245-257 with `entry.update({ ... })`.
Incorrect. 1. This matches **any** URL with `pl_id` in query. 2. You must check for both `pl_id` and `pl_type`.
I've already pointed out: `.*$` is pointless at the end.
Read coding conventions on optional fields.
Don't capture groups you don't use. This should not match `https://rutube.ru/video/10b3a03fc01d5bbcc632a2f3514e8aab/pl_id=4252`. `.*$` does not make any sense at the end.
No. Override `suitable`.
`default` is not used with `fatal`.
`\s` not ` `.
Use display id.
You are providing a fallback for title extraction thus you **assume** this situation normal and should not print any warning.
You will add it when there will be a playlist support. For now it's completely useless.
I've already pointed out: use `display_id` until you get real id.
No point in base class.
Eliminate all methods that is only used once.
This is checked by `_search_regex`.
`None` is not a `video_id`.
No `id` extracted.
This may be matched with `re:` prefixed string.
That's not enough. `Ð¸ÑÐ½Ñ` is month name.
This will fill up to `_TEST_FILE_SIZE` is the file is `< _TEST_FILE_SIZE`.
This will break all ffmpeg tests with md5 calculated prior to this change.
Both test the same extraction scenario. The rule is one test per extraction scenario.
`else` is superfluous.
Uppercase is used for constants. Also this is a template not format.
You already have `[]` in regex, capture it and parse there.
`--no-playlist` is not respected.
`default` is senseless here.
No need to escape `]` is character set.
Use bare `re.match`.
It's should not be greedy.
Check for `embed_code` instead.
Do not change the order of extraction scenarios.
Never use bare except.
- - [ ]
```suggestion webpage = self._download_webpage(url, video_id) ```
Audio is not 128.
Do not match exact URL.
No exact matches for URLs.
`width` and `height` instead.
It's not an id.
All formats should be extracted.
What's the point of this complication? Each clip has `show-clip` value for class attribute for `a`. Replace with single playlist extractor based on this approach.
Caps is used for constants.
Move into loop.
I did not suggest moving `subtitle_original_lang_code` into loop.
Don't lookup `lang_code` twice.
Don't lookup twice.
92-115, 133-157 codes are identical apart from entry tag and query.
You're already building query with `query` param.
Should not break extraction if missing.
Should not break extraction is download fails.
Available formats should be first extracted from subtitles XML.
Optional should not break extraction. Read FAQ.
There is no such param `only_matching`.
Query should go in `query=` .
Subtitles requiring additional network requests should only be extracted when explicitly requested.
Automatic captions should be treated as automatic captions.
Automatic captions are also available.
Use fatal=False or default=None in _html_search_regex to handle fallbacks instead of try-except patterns.
Use _html_search_meta for ```<meta>``` tags
Just use ```'ÐÐ¾ÑÐ»Ðµ ÑÐ¸ÑÑÐ¾Ð². Â«ÐÑÐ¸Ð¾Ð»Ð¸Ð½Â»'```
youtube-dl should be compatible with both Python 2 and Python 3. Use symbols from ```..compat``` instead.
`(?!/)` makes no sense.
New domain should be embedded here.
Move flags into regex. Regex should match `runParams={`.
Strings in JSON may contain `<`.
I've already pointed out: use `float_or_none`.
Breaks extraction if no `followBar`.
Breaks extraction if no `followBar`. `timestamp` instead of `upload_date`. `float_or_none`.
It's not a live stream.
It's already a unicode string.
Use `video_id`. Remove `.replace('\n', '')`.
You should not call this yourself instead you should define `_GEO_COUNTRIES`.
This should not process the whole page.
This should not process the whole page. Regex should be relaxed.
It's not always `True`.
Inner group is pointless.
In general we use single quotes unless there's a good reason. (single quotes in strings, etc.)
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
Codes not used anymore can be just deleted.
```not (foo is None)``` => ```foo is not None```
For long query strings, it's better to use ```query``` parameter here.
Make `https?:` optional instead.
Check code with flake8.
Should handle `src` with `https?:` also.
Dots must be escaped.
Don't carry URLs.
Should not break if `published_at` is missing.
Lack of information is denoted by `None`.
Reference before assignment for `show_image` if `not show`.
You must delegate with `url_result` instead.
Rename to something else.
This is supposed to be in a class that does extraction, not in a class that delegates.
Each should be a separate extractor.
`url` should not be `None`.
Specify `ie_key` and `video_id`.
Dots must be escaped.
`show = data.get('show') or {}`
Browser does not send such fields.
It sends "userid" and "password" instead. `<input type="text" name="userid" id="nameBox" value="" maxlength="50" size="50" class="slidTxtbox" title="Userid" style="color: rgb(170, 170, 170);">` `<input type="text" placeholder="Password" name="password" id="pwdBox" value="" maxlength="50" size="50" class="slidTxtbox" autocomplete="off" title="Password">`
`id` must not include anything apart from digits.
1. `id` is **not necessarily** 3 digits. 2. `id` must not contain irrelevant words.
`id` is of arbitrary length.
This matches multiple videos.
You are missing that it's a metadata provided by a 3dparty and there can by anything. So that you must ensure it's `int` before returning its value in info dict.
First group is superfluous.
Must not be `None`.
Must be `int`.
Must be `int`.
Breaks if `photo` is missing.
Read coding conventions on optional fields.
All debug code must be removed.
No. `_search_regex` is already fatal and reports proper error message. Revert.
No. Revert as it was.
`r'([\"\'])external_id\1\s*[:=]\1(?P<id>[0-9a-z_]+)\1'` matches **several times** on the webpage meaning that **wrong video will be downloaded** if actual video happens **not to be the first matched**.
`None` is default.
This should be extracted the very first.
`fmt_url_json` may be `None`.
54-58, 71-75 code duplication.
Optional fields should not break extraction if missing.
It's fine to use it until it produces longer/more complicated code. You should also limit `try-catch` section to the code it's actually applies for.
No. `KeyError` != `TypeError`.
Should not be fatal.
Breaks extraction if `release_date[0:4]` is not `int`.
Breaks extraction if `json.loads` fails.
`title` must never be `None`.
So what? You extract `release_date` from a `webpage`'s `album_release_date`. You have **absolutely no guarantee** `album_release_date` will match `YYYYMMDD` or `YYYY`.
`if '_high.' in video_url`.
It won't be longer.
This is the matter of **code reusage**, not personal preference. You should use the most generic solution available and not **reinvent the wheel**. Other code in this style was written before `update_url_query` was introduced.
`{}` won't work in python 2.6.
Revert. >Checking download_**url** video format **URL** makes even less sense.
`F821 undefined name 'video_itle'`.
`E226 missing whitespace around arithmetic operator`.
1. Code duplication. 2. Each should be non fatal.
```python for path, format_id in (('', 'audio'), ('video', 'sd'), ('videohd', 'hd')): self._download_xml( 'https://www.heise.de/ct/uplink/ctuplink%s.rss' % path, video_id, 'Downloading %s XML' % format_id, fatal=False) ```
This should be rewritten in terms of [`YoutubeIE._extract_urls`](https://github.com/rg3/youtube-dl/commit/66c9fa36c10860b380806b9de48f38d628289e03).
`title` must not be `None`.
Original order **was intentional** since `_og_search_title` provides incorrect titles for some extractors that have skipped tests now.
`title` can't be `None` here. Don't change the order of extraction.
`try_get`, single quotes.
Extracting duplicate code into a function obviously.
None of the optional fields should break extraction if missing.
1. 151-162 code duplication. 2. Missing format keys should not breaks extraction. 3. HLS and DASH should be extracted with appropriate `_extract_*` methods. 4. HLS and DASH extraction should be non fatal.
157, 160-162, 165-167 code duplication.
`if try_get(item_data, lambda x: x['streamInfo']['url'])` is enough.
As well as this.
It's a matching only test. Testing two identical URLs twice does not make any sense.
You don't get it. **Don't touch tests**. After your change there are two tests with identical scheme. Testing this does not make any sense. Each test must test **something special**.
Change 1 test to https://vplayer.nbcsports.com/p/BxmELC/nbcsports_embed/select/9CsDKds0kvHI. Change 2 test to https://vplayer.nbcsports.com/p/BxmELC/nbcsports_embed/select/media/_hqLjQ95yx8Z.
https://vplayer.nbcsports.com/p/BxmELC/nbcsports_embed/select/9CsDKds0kvHI is still available.
This is already checked in `float_or_none`.
1. Do not remove fallback to previous duration value. 2. `scale` of `float_or_none` instead.
Breaks extraction if `config.get('duration')` is not numeric.
No need for escaping in brackets.
`unicode` does not exist in python 3.
All formats should be extracted.
No. Use `_search_regex`.
No such field.
No such field.
No such field.
Optional data should not break extraction if missing. Read coding conventions.
`{}` won't work in python 2.6.
There is no point in that.
No, this is extractor specific and should not be here.
This no longer matches http://v.youku.com/player.php/sid/XNDgyMDQ2NTQw/v.swf and http://player.youku.com/v_show/id_XMTc1ODE5Njcy.html.
I'm well aware of that.
Make it match URLs from the first post obviously.
Do not remove the old pattern.
Should not match `src="/static.beeg.com/cpl/...`.
This is superfluous.
Id from URL is not always a video id. Correct id is in JSON.
No need to escape `{}`.
Make `www\.` part optional.
Regex should be more relaxed. Title part should be optional.
Breaks when `player` is `False`.
`default` implies non `fatal`.
This should be recursively delegated to pbs extractor instead.
Non fatal, proper prefix id.
Non fatal, proper prefix id.
This can be figured out from git log and diff thus should be removed.
This has been removed entirely, we aren't using Wistia for streaming any longer.
Don't shadow built-in names. Don't extract function you only use once.
Supposed to be `'segment_duration' in representation_ms_info`.
No. `duration` is optional attribute. See the spec.
`'thumb`' may not be present producing invalid thumbnail url.
No such meta field.
Use stricter regex instead.
Read coding conventions and fix all optional meta fields.
No need to specify this.
No need to specify this.
Should be `flv`.
Title is mandatory.
Don't shadow method argument name.
You don't get it. Group is fine, but it **should not** capture.
Don't capture groups you don't use.
`info_dict` is pointless with `only_matching`.
Dots should be escaped.
You're not using any dots in regex.
As said it should be more relaxed allowing both kinds of quotes, attributes in between and reversed order of attributes.
You must provide account credentials for testing.
This won't help if `owner` is set to `True`-ish non dict.
You can't make any assumptions on data returned. This pattern is flawed. Suggestion is to make it work in all possible cases.
>- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)
Avoid unnecessary verbosity. Collapse these 3 lines into one. Same for all other places.
Each piece used only once must be moved to the place where it's actually used.
`default` and `fatal` are not used together. Prefix keyword arguments with their names.
All formats should be extracted.
Don't shadow built-in names.
`int_or_none` for all int fields.
No bare except.
Code duplication. Prepare message and then throw only once.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
It's pointless to match URLs with strict regex, needless to say it's invalid due to `&` being delimiter in query. Match anything after `https?://` till closing quote or `&`.
This is pointless message since you don't support login.
`note` and `errnote` of `_download_json` instead.
This is fatal.
If processing of single format encoding fails it will incorrectly fallback here. `try` section should be minimal.
All methods only used once should be explicitly inlined.
Doesn't match https://www.servus.com/de/p/Die-Gr%C3%BCnen-aus-Sicht-des-Volkes/AA-1T6VBU5PW1W12/?foo=bar. Should not match https://www.servus.com/de/p/Kleines-Geschenk-Set-Anti-Stress/SM129658/.
Despite being keyword arguments avoid changing the original order.
`video_id` is idiomatic.
This is the behavior of `_search_regex` with defaults.
1. Youtube id can be passed directly. 2. `video_id=` should be passed as well.
1. This will never be reached. 2. Don't change the order.
All duplicate tests must be `only_matching`.
Each test the same scenario = duplicate.
>- [x] Checked the code with [flake8](https://pypi.python.org/pypi/flake8)
No, prove. At best it produces the same string.
It's been requested to make tests `only_matching` not to remove them.
Just `video_url = urljoin('https://ndtvod.bc-ssl.cdn.bitgravity.com/23372/ndtv/', filename)`.
Again: **do not remove the old code and do not change the order**.
`fatal=False` will cause wrong duplicate warnings in case of failures. `default=None` should be used instead.
Instead of trying to enumerate `(?:[^/]+\.)?ndtv\.com` is ok.
800 won't be reached due to `fatal == True`.
Move flags into regex.
All formats should be extracted.
10 is default.
Keep the old way also.
Same as in some previous PR.
Currently, video identifying is completely messed. This is how extractors should be organized: 1. `EllenTubeIE` accepting `https://api-prod.ellentube.com/ellenapi/api/item/uuid` and `ellentube:uuid`. 2. All other extractors must delegate to `EllenTubeIE` so download archive is always written as `ellentube`.
Don't mix unrelated changes in single PR.
jwplayer extraction code in generic extractor should be improved instead of adding this new extractor.
Remove all tests that test the same extraction scenario.
Nothing stops from renaming capture group to `id`.
`<display_id> is not a video`.
Test video must be a freely available one.
This is never matched.
1. It's called scheme, not protocol. 2. It's not an example. Example must use real arguments not parameters.
Should be reworded: `Supported schemes: http, https, socks, socks4, socks4a, socks5.`
No. Passwords should be retrieved at the time of usage, see `_get_login_info`.
No. youtube-dl should not store passwords.
ImportError and JSON parsing errors should be handled as well
Better to use more meaningful names than u or p. That helps users to understand what youtube-dl are using from their secret storages.
Better to wrap such long lines.
Option name should not contain `--password-from`. Select one that describes the mechanism in the most generic form. `--netrc` means the netrc mechanism will be used, symmetrically keyring option name should denote the keyring mechanism (or whatever it's properly called) will be used.
Should be `--gnome-keyring` or so for symmetry with `--netrc`.
Here's an example: https://github.com/rg3/youtube-dl/blob/909191de9154bf289b333cfe01b8e88e3ac1fefc/youtube_dl/downloader/hls.py#L5-L9
Did you use the netrc format for data stored in libsecret backends? If not don't use ```NetrcParseError```.
No. You should not shadow the original explicitly provided password.
No. Extractors are identified by special keys not domains, see `_NETRC_MACHINE`.
This must be a soft dependency.
Use `query` for query.
`acodec == 'none'`.
Move to the place of usage.
It's already done at L151.
`pzuid` does not look to be used anywhere.
This should not be fatal.
Yes, or `'%s/...' % self._HOST_URL`. Or even hide `self._HOST_URL` in a helper method.
They all are hardcoded strings known beforehand. `urljoin` is pointless.
Move into `_download_json`.
`info_dict['formats'] = formats`.
There should be fallbacks for these values since they are more or less static.
Same question for 'contains'
This will fail if the previous request failed.
Final bit, self._search_regex is better than re.search
This is not necessary. It's already guaranteed by regular expressions.
Same issue for re.search
Same for re.search
Is there a case that 'url' is not in mobj.groupdict()? From _LIST_VIDEO_RE, the 'url' group is mandatory.
Same for re.search
This is not necessary. self._download_webpage already reports that. Check parameters of _download_webpage if you want to use an alternative message. And this message is not so clear: ``` [ximalaya.com:album] 3 http://www.ximalaya.com/61425525/album/5534601?page=4: Downloading webpage ``` The common practice in youtube-dl is: ``` [ximalaya.com:album] 5534601: Downloading webpage for page 4 ```
I'm not sure if this approach is reliable. Better to let _process_page determine if there are more entries or not.
Using get_element_by_class is better
```XimilayaIE.ie_key()``` is better
This is now unused
Use ```audio_info['title']``` as this is a mandatory field. Check coding conventions
fatal=True is already the default
Using spaces in regular expressions is not a good idea. Check [coding conventions](https://github.com/rg3/youtube-dl/blob/master/README.md#youtube-dl-coding-conventions)
(?:[^>]+)? can be simplified as [^>]*
This blank line is unnecessary, and import statements for built-in modules should be sorted alphabetically.
The two URLs should use https if applicable
This TODO needs work
Many extractors for Chinese websites use localized names. You may want to include the term åé©¬æé here.
Trailing /? is not necessary here
Looks not so correct. As far as I understand, parsing the web page occurs before downloading the description file? (line 75~78)
The same. Should use https if ```url``` use https.
And (.+?) should be better than (.+) here
And I guess audio_info.get(k) can be simplified as audio_info[k] as k must be one of valid keys.
It's uncommon to include full domain name. ximalaya:album is better
If audio_uploader_id is None, this should also be None
errnote still needs a fix. "try to parse web page" sounds wrong
Better to create a variable for such a long regular expression
If audio_info does not have category_name, categories will become [None, 'xxx']. It should be only ['xxx']
group=1 is equivalent to the default behavior of _html_search_regex; just drop that.
In general we don't use full URLs to match thumbnails. Check out codes in step 4 of https://github.com/rg3/youtube-dl/blob/master/README.md#adding-support-for-a-new-site.
The name ```intro``` may be misleading as this is actually a regular expression matching object. Better as ```intro_mobj```.
Should use https here if ```url``` uses https (e.g., https://m.ximalaya.com/61425525/sound/47740352/)
Same for this test entry
It's better to also include other fields (uploader_url, thumbnail, category, duration)
Use ```room['rtmp_url']``` and ```room['rtmp_live']``` here as [video_url is a mandatory field](https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields)
do you have an example with TTML subtitles? all the videos that i've tested with has only VTT subtitles.
`self._TMP_FILES[name].name` can't be `None`.
Should be more relaxed. Any minor change in HTML layout of the first part will break it.
`fatal=True` is default.
`varvideoInfo` is not possible, regex is invalid.
`default` and `fatal` are not used together.
See another extractors.
You must make fatal the last source.
Read my message.
Only whitespace is allowed between `videoInfo` and `=`.
Matching between quotes is incorrect.
`'` can be used between `"` quotes and vice versa.
Lack of information is denoted by `None` not an empty string. Also `unified_strdate`.
Also do not use double quotes for string literals.
Must be fatal.
I've already pointed out - title **must be fatal**.
`default` and `fatal` are not used together.
Must be fatal.
Pass as list of regexes, don't bulk.
`[^?\s]` does not make any sense - you already matched `?` previously. I've already pointed out - no escapes for `/`. `(?:https*?:\/\/)*` does not make any sense. `s*` does not make any sense. `(?:www\.)*` does not make any sense.
Use whitespace characters consistently.
Breaks when `get_element_by_class` returns `None`.
No such metafield.
No exact URLs, use `re:`.
No. `title` is mandatory. Code duplication. Move all `_html_search_regex` into a single call.
No. `title` is mandatory. Read coding conventions and fix appropriately.
Code duplication. Must be single call to search regex.
Don't shadow outer names. No need for bracket when using single character.
Breaks on `default=False`. `title` must be fatal.
Breaks on `None`.
No escapes for `/`.
No exact URLs.
Stripped twice. Breaks on `None`.
Just modify the original extractor.
A minor bit: don't use a capturing group if it's not used.
@cacdpa: That's better. Just don't use the name ```url``` - it's already used as a parameter of ```_real_extract```.
@zachee @dstftw I had a go at removing the duplication. Is this any good? **Function** ``` def _get_first_valid_downloaded_webpage(urls, video_id, headers): for url in urls: webpage = self._download_webpage(url, video_id, headers=headers) if not ('File not found' in webpage or 'deleted by the owner' in webpage): return webpage raise ExtractorError('File not found', expected=True, video_id=video_id) ``` **Usage** ``` def _real_extract(self, url): video_id = self._match_id(url) url = 'https://openload.co/embed/%s/' % video_id url2 = 'https://openload.co/f/%s/' % video_id headers = { 'User-Agent': self._USER_AGENT, } webpage = self._get_first_valid_downloaded_webpage([url, url2], video_id, headers) ```
That's not an excuse. Code duplication must be avoided by generalizing common pieces of code.
Title is mandatory.
More video qualities are available.
No, catch exception.
CF detection and solving should happen centrally and automatically in `_webpage_read_content`. Otherwise it does not make much sense.
-`images that appear when hovering the cursor over a video timeline` This may not apply to other sites.
Breaks extraction completely if `params` does not match number of values to unpack.
Breaks if not `int`.
Code duplication. Keep the original `_write_thumbnails` code and generalize item part. `param_name` is a bad name for something that describes an item's kind.
Do not shadow built-in names.
Capturing empty string does not make any sense. What's the point capturing this at all? id and path occur only once in webpage.
1. `_extract_m3u8_formats`. 2. At least mpd is also available.
Do not mix unrelated changed in a single PR.
Do not split URLs.
More relaxed regex.
Breaks extraction if no `code` key exists.
There is a clear human readable error provided that must be used. Also must be `expected`.
Should allow arbitrary whitespace.
No. If `www` happens to be in path, query or fragment of mobile URL this will produce broken mobile URL.
Don't shadow built-in names.
Already pointed out: must be `ExtractorError`.
Breaks on https://www.zouzous.fr/heros/simon?abc. Single quotes.
Avoid spaces in regular expressions. See rutube.py for an example.
It's better to keep the original video ID or ```--download-archive``` will be broken for affected videos. ```smuggle_url``` and ```unsmuggle_url``` can help.
Iteration over dict not the hardcoded list.
Should be tolerate to missing keys.
Don't shadow existing name.
Should match from the beginning. It's senseless to replace mobile URL with itself.
Keep the original scheme.
Remove unnecessary verbosity.
`re` is not from `utils`.
1. Don't shadow outer names. 2. Inner parentheses are superfluous.
It's better to keep the original ID for existing patterns, or --download-archive will be broken.
1. Both should be extracted. 2. Update tests.
`(?P<channel>tv3|catradio|[^/*])` no, this imposes more strictness. `[^/*]` incorrect. `[^/]*` matching empty string does not make sense. `/?$` no.
Same, renames are not necessary
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
This is an optional field; use ```.get``` idiom. Check https://github.com/rg3/youtube-dl/blob/master/README.md#mandatory-and-optional-metafields
Dots should be escaped
You can just use 'url' in the final dict if there's only one format
Are all of these necessary? I think youtube-dl defaults suffice.
Use _ (underline) instead of webpage if the value is not used.
Use json.dumps instead
Use ```data``` and ```headers``` parameters of _download_webpage instead.
Use _download_webpage is urlh is not used. And note should be meaningful for typical users.
strip_jsonp should work here
urlencode_postdata should be better than urlencode + encode
Such renames are not necessary, and they may be confusing for others.
Better to use integers for supported_resolutions and use str() here
You can use tuples; supported_resolutions won't be changed
Use ```query``` parameter of ```_download_webpage``` instead.
Same issue for urlh
Must be a list.
Using `findall` and processing only first URL then is totally pointless.
Breaks if no URLs extracted.
Matching empty string is senseless.
Do not use exact URLs.
Playlist title should not be fatal.
fatal=True is default.
Breaks on `None` title.
What's the point of this? Use `url` as base.
Overload `suitable` instead.
Don't capture groups you don't use.
Use the original scheme and host.
Query to `query`,
Use the original scheme and host.
Code duplication 165, 176.
`False` is not a valid note.
1. Regex should match any variation of whitespace around `=`. 2. Once you provide default, fatal is not used thus it's pointless to provide fatal along with default.
Move data and query into `_download_webpage` call.
`False` is not a valid note.
1. Single quotes. 2. `expected`.
1. Relax whitespace. 2. default and fatal are not used together. 3. Should not match empty string.
Is the KeyError come from ```['html']```? If so using ```.get('html')``` and check playlist_data is better.
I'm addressing this concrete line of code.
Code duplication with tbs extractor.
Read coding conventions on optional meta fields.
Code duplication 286-288, 330-332.
The point is eliminating code duplication, not squeezing into one line.
46-47 code duplication.
No such scheme.
No. Must be the opposite since `--yes-playlist` is default.
No. Use `smuggle_url` if you need to pass some additional attribute.
Should capture behind quotes. Non greedy.
No. Empty download URL is pointless.
No. Must be fatal. Read coding conventions on optional fields.
No. Lack of information is denoted by `None`, again read coding conventions.
Must be optional.
Missing `src` should not break extraction completely.
Master m3u8 should be extracted with corresponding method.
Must be fatal. Read coding conventions.
More relaxed regex.
This may change as well. Add a fallback that just processes all videos without differentiation.
Just like I expected storage location has changed meanwhile so this code is completely broken. As I've already pointed out: match **only** `data-source` attribute.
Cause it will break on any minor change of storage location.
Do not shadow outer names.
More relaxed regex. Don't match exact URL.
No, completely the opposite. More precise URL imposes more restrictions thus easier to break. If they change storage host or path yours will break while matching by only attribute won't.
Inline it into actual method call.
Keep the old regex as well.
Do not remove `r` prefix.
```suggestion '-c', 'copy', '-map', '0', ``` `-map 0` is needed to copy all streams from the source file.
67-74 code duplication with mp3 path.
Read coding conventions on optional and mandatory fields.
Any unrelated suffixes should be removed.
Don't capture groups you don't use. Use proper regex to match all country codes.
Generic imports should go before youtube-dl's.
Formats not sorted.
No `..`. `.*` at the end is pointless.
`/video/ in url`.
`/kenh-truyen-hinh/ in url`.
`vcodec` of format should be set to `'none'`.
1. Relax regex. 2. Keep the old one as well.
`(?s)` == `DOTALL`. Current code works fine. This PR is noop.
Carry to the indented beginning of the line.
Read coding conventions on optional fields.
Relax regex, make group unnamed, don't capture empty dict.
I asked to add both not replace one with another,
Lack of information is denoted by `None` not `0`.
It's a field name not a step name.
`[^/]*$` is completely pointless here.
Should not be fatal.
Move flags into regex.
Again: relax regex.
None is default.
It should accept any combination of whitespace.
Carry to the indented beginning of the line.
No need for escapes inside a brace group, all dots outside must be escaped.
Extend `_VALID_URL` instead of adding a new extractor.
Keep the old TLD as well.
I've already pointed out: I won't accept this.
Keep the old fallback also.
Fix duration extraction instead.
Keep this test as `only_matching`.
Instead of this extractor you should create an extractor for apa.at/embed URLs and detect such iframes in generic extractor.
Should not accept `http://www.vol.at//5593454` or `http://www.vol.at/blue/man/group/5593454`.
Read code conventions on optional fields.
No. What's the point of video id here? `settings` part provides enough uniqueness.
Video id is much more likely to disappear than `settings` to be renamed.
Should not be greedy.
I'm not about whitespace.
All formats must be extracted.
Using `get` is pointless here.
* the first element of the path can have more than 1 digit * the tail `/player\.html` isn't needed (unless there are other tails that should find different media with the same id): ```suggestion _VALID_URL = r'https?://(?:www\.)?embed\.vidello\.com/[0-9]+/(?P<id>[a-zA-Z0-9]+)' ```
* move description extraction * add thumbnail ```suggestion 'description': clean_html(vidello_values.get('product_desc')), 'formats': formats, 'thumbnail': url_or_none(try_get(vidello_settings, lambda x: x['player']['poster'])), ```
* improve robustness * extract m3u8 manifest * get resolution for mp4 ```suggestion # may crash on these two, or on no formats found vidello_values = vidello_settings['cta'][0]['values'] title = vidello_values['product_title'] formats = [] video_sources = try_get(vidello_settings, lambda x: x['player']['clip']['sources'], list) for curr_entry in video_sources or []: if not isinstance(curr_entry, dict) or not curr_entry.get('src'): continue video_url = self._proto_relative_url(curr_entry['src']) if not video_url: continue if curr_entry.get('type') == 'video/mp4': fmt = { 'url': video_url, } width = int_or_none(video_url.rsplit('/', 2)[1]) if width is not None: fmt['width'] = width inv_ar = try_get(vidello_settings, lambda x: x['player']['ratio'], float) if inv_ar is not None: fmt['height'] = int(round(inv_ar * width)) fmt['format_id'] = 'mp4-%04d' % (fmt['height'], ) formats.append(fmt) elif curr_entry.get('type') == 'application/x-mpegurl': formats.extend(self._extract_m3u8_formats(video_url, video_id, ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) self._sort_formats(formats) ```
```suggestion r'settings\s*=\s*({.+?});', webpage, 'vidello settings', fatal=False) or {}, video_id, fatal=False) ```
Depending on later suggestions: ```suggestion clean_html, int_or_none, try_get, url_or_none, ```
Depending on later suggestions: ```suggestion 'description': "Start marketing your videos more effectively on \x03the web utilising vidello's premium hosting, streaming, \x03analytics & marketing features to grow your \x03online business fast.", 'thumbnail': r're:https?://.+\.jpg', }, 'params': { 'format': '[format_id!^=hls]', }, ```
Move before like youtube embed.
Adding another fallback here does not make much sense since title is not used when delegated to kaltura anyway.
This has no effect.
>Adding another fallback here does not make much sense since title is not used when delegated to kaltura anyway.
default and fatal are not used together.
This is pointless.
default and fatal are not used together.
>I'm open to suggestions for other ways of achieving the same goal. Did you read my message at all? `escaped_json_string.encode('utf-8').decode('unicode_escape')`.
No captures for groups you don't use.
Will never happen since it does not throw if not fatal.
Must not be fatal.
This is senseless. `_search_regex` always returns a unicode string so that `.encode('utf-8').decode('unicode_escape')` is applied directly.
You are using different ids in url and here. Don't do that.
1. Capturing empty string is pointless. 2. Using named groups when there is only one group is pointless. 3. Relax regex. 4. Webpage contains multiple videos.
Playlist metadata must not be fatal.
This is not used in single video extractor thus should not be here.
Won't work on `None`.
No trailing `$`. Override `suitable`.
Playlist id and title should not be fatal.
Code duplication 173, 213. There is no sense to extract fields explicitly.
I've already pointed out: no trailing `$`. Override `suitable` instead.
This must be checked **before** any processing.
Duration calculation is incorrect.
This must be checked **before** any processing.
This has no effect. Postprocessors work on info dict copy.
This must be checked **before** any processing.
It's a chapters track as explained in the ffmpeg issue you linked that appears after chapters metadata is embedded by `--add-metadata`.
Don't mix unrelated changes in single PR.
Keep the old pattern as well.
That's not a title. Revert.
`False` is default.
Should not be fatal.
`True` is default.
The title is `Staff Favorites: November 2013`.
**Never ever** use `eval` on data you don't control.
I'm not about the URL. **Do not touch** the global `std_headers`.
That's not necessarily correct. It may just have all specified ip version addresses filtered out from non empty getaddrinfo addresses list. So should be reworded.
`ipv4_addrs` is a misleading name since ip version depends on `filter_for` and not necessarily equals v4.
`filter_for = socket.AF_INET if '.' in source_address else socket.AF_INET6`.
Modify existing regex instead.
This is unreadable. Indent optional group properly.
The original regex is clear line by line expression without any optional parts.
This breaks all non ks embeds. ks part must be optional.
Must not break extraction if missing.
No, parse height from label.
Don't touch this.
It's already contained in video URL. Remove.
URL must not be `None`.
Pass list of regexes.
Refactor into a single `_search_regex` call.
`error.get('code')`, `error.get('message')` may be `None`.
Don't capture groups you don't use.
No such meta field.
youtube-dl does not use non-browser user agent.
Now you break extraction if any of these keys is missing.
Wrong key. Again breaks if no such key.
Don't shadow built-in names.
Code duplication. `url` must not be `None`.
Read coding conventions on optional fields.
Breaks if no error key.
Don't use named groups when there is only a single group.
Capturing empty URL is senseless.
Generic extractor must have only generic embed patterns.
Not used with formats.
No such meta field.
No. Use fatal search regex instead.
No. 2841-2847 does that. This code handles `20.detik.com/embed` page itself and it's not generic and should not be here.
Do you read my messages at all? I did not say to remove it completely. It must be moved to a separate extractor.
Instead of this extractor you should add an extractor for `https://20.detik.com/embed/...` URLs and add detection of such embeds in generic extractor.
1. Keep the old regex as well. 2. No need for escaping it's already a raw string.
`_match_id`. Do not shadow built-in names.
`if not content_url:`.
default and fatal are not used together. `True` is default.
`True` is default.
`None` is not an id.
Use raw strings.
No. Override `suitable` instead.
Eliminate excessive verbosity.
Breaks extraction if missing.
Captures `?#&` ending as id.
Breaks if no videos in season.
Check for `compat_str` also.
Should check for a list.
Capturing empty string is senseless. `\n` instead of `root[__env]`.
URLs must not be `None`.
Must be int.
Must be int.
Must be int.
Do not use underscore as prefix.
This intermediate dict is completely pointless. Build formats directly.
Breaks if no `@context` is present.
Breaks if `e.get('@context')` is not a string.
Remove excessive verbosity. This fits well on a single line.
Mandatory data must be accessed with `[]` not `get`.
`images` is not guaranteed to be a list.
`if images` implies the second check.
Read coding conventions on mandatory meta fields.
Parse drupal settings.
Incorrect quote matching. Expression in quotes may contain the other quote kind characters.
Incorrect. Should not be greedy.
Relax regex. Don't match empty string.
Must be extracted first.
No. As already said delegate `url` to generic always when main path fails.
Just rethrow active exception.
No need for this noise.
`fatal` must be added to `_extract_info`. No changes to core code.
Add `fatal` flag.
This method is used in another extractors. **Do not change the default behavior**.
Do not change the order of extraction.
Delegate to generic always when original path found nothing.
Remove new parameter. Check url to also start with `//` instead.
`if height and filesh:`
Bother to read coding conventions on optional meta fields.
Potential reference before assignment error.
This does not mean `next_url` is obtained.
This won't skip empty strings.
So what? If `not enclosure_nodes` is `False` it does not mean `next_url` was obtained.
Must not be fatal.
Query to `query`.
Carry long lines. Read coding conventions.
Same tolerance issue.
Query to `query=`.
Should not be fatal.
Should be tolerate to missing keys in `media`.
Must only contain description.
Must only contain title.
Should not be fatal.
v4 is already matched by the previous regex, this regex must only match v6.
Outer parenthesis are superfluous. `title` can't be `None` at this point.
Read coding conventions on optional metadata and fix code appropriately.
No such meta field.
No such meta field.
No such meta field.
No need for `ext`.
Inline to actual call place.
It does not necessarily mean that.
First parameter to `urljoin` is URL.
`try_get`. Same for all other places.
Single loop for all sources without any unnecessary intermediates.
`.*/?` is pointless at the end.
`default` implies non fatal.
This should be extracted very first.
This should be fixed in `js_to_json`.
Make `https?:` part optional.
Re-read my post with eyes please.
No, provide subtitles as URL.
No, override `suitable` instead. Also do not split strings.
What are you doing? These cookies are already in session, no need to bother with Cookie header at all.
I mean you must return URL of subtitles if you know it. Conversion is done by `--convert-subs` upon request.
I've already pointed out: this must be removed.
This does not necessarily mean that. There is a clear captured error message that should be output.
This branch is never reached.
1. This does not work [properly](https://github.com/rg3/youtube-dl/issues/14814). 2. This is ad hoc micro optimization exclusively for pluralsight for reducing number of requests and overall sleeping time while downloading. Remove.
There are also vtt subtitles available.
`/?` is senseless at the end.
Read coding convention on optional meta data.
Check code with flake8.
No hardcodes. Use API.
1. `_sort_formats`. 2. Must not break if any of these keys is missing.
Read: coding conventions, optional fields.
Read: coding conventions, mandatory fields.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
Code in bracket is a set of matching characters. `or`ing won't work.
All formats must be extracted.
Should not break if missing.
This exact line, namely the capture group.
Remove all unrelated changes.
I've provided **clear working** piece of code that you must just copy paste. Instead you introduced mess with base URL.
You: `[0-9|a-f]`. Stackoverflow: `[0-9a-f]`. Difference? You have to learn to distinguish `(...)` and `[...]`.
Too weak. Must be more concrete.
1. No additional requests here. 2. Will break extraction if failed. 3. Bother to read coding conventions before submitting PR.
Hi @dragonken , Yes, actually I don't really understand the naming from @chongjea , but I know what's the correct data. What do you mean to set video_id? Is the number correct? Line 173? `video_id` is not used evermore after line 173.
Hi @dragonken, The current commit by @chongjea does not work now. @atxalex 's code also does not work in my environment. I am currently using my patch privately. Should I open a new PR? Seems @chongjea not response anymore...
Try this: ``` mobj = {} ll = re.findall(r'cid(?:["\']:|=)(\d+),["\']page(?:["\']:|=)(\d+)', webpage) for e in ll: mobj[e[1]] = e[0] cid = mobj[page] or \ ```
Hi @beards, The variable name is hard to get the idea. It looks like I add the following before _download_json work. `Line 173: video_id = cid` But the downloaded filename title is not quite right after download.
Should not allow empty 3rd level domain. Should not be greedy. Inner group is superfluous.
It should match all non empty domain names.
Download archive behavior must not change, it must only take place after success of the actual download and post processing.
The class name should ends with FD
Don't use print() directly as -j will be broken. Use to_screen or related functions instead.
Such logic should be implemented in get_suitable_downloader()
Thining again and I believe there's no need for a separate class. The heart beat function can be implemented in the FileDownloader class. (not in HttpFD as other stream types, including HLS and HDS, may also need heart beats)
The number 25 should be configurable
Cause it's an **utility function** and it's used in other places you've broken with this change.
Never ever do that. You must handle it at the place of the actual method call.
Playlist title must not be fatal. Do not capture empty strings.
All this branch should go after unavailability checks and before `Cannot parse data`.
Move everything into `_download_webpage`.
metainfo is downloaded twice.
There is no point in base class.
Title is mandatory.
There are multiple formats, some may have `file` some may not. If some new hq format is introduced with different rendition your approach will break downloading by default while proper URL handling will not. Finally read coding conventions.
Whether it has changed or not does not mean there should be a format with invalid URL.
`field_preference` must be a list or a tuple.
Must not be `None`.
Code duplication 80-86, 89-94.
`/?` is pointless at the end.
Keep the regex on a single line.
@Nekmo You can make json POST with `_download_json`, and set expected statuses for POST use `data=urlencode_postdata(form_data)` From common.py ``` python def _download_json( self, url_or_request, video_id, note='Downloading JSON metadata', errnote='Unable to download JSON metadata', transform_source=None, fatal=True, encoding=None, data=None, headers={}, query={}, expected_status=None): """ Return the JSON object as a dict. See _download_webpage docstring for arguments specification. """ ```
eg find type by URL ext
Don't split. Same everywhere.
`expected_status` to `_download_json` instead.
Read coding conventions on optional/mandatory meta fields.
Should not break if no `type`.
Inline into actual `_download_*` call.
`expected_status` to `_download_json` instead.
There should not be any messages.
Class `username` matches multiple elements.
Breaks if `node_views_class` is `None`.
Forward slash does not need escaping.
Breaks if `node_views_class` is `None`.
No trailing $.
Description is description not title.
As already said: no trailing $. Override `suitable`.
`urls` is pointless. Build `entries` straightaway.
Breaks on missing key, breaks on download failure.
Breaks on missing keys.
Must not be fatal. Read coding conventions on optional/mandatory fields.
Don't shadow built-in names.
All debug garbage must be removed.
Must be separate extractor.
Breaks on missing keys.
Don't capture groups you don't use.
Nothing changed: 1. `re.sub` will break on `None` string. 2. Regex must match single quotes as well. 3. `utils.parse_count`.
Remove loop. Add a method extracting meta data by name, call it for each meta field separately.
Move into final return.
No. For mandatory data you should not use `get` and any other fail safe approaches since it make no sense to continue if you can't extract it.
Breaks on None.
Must not be fatal.
Breaks on unexpected data.
`[]` is superfluous in group with single character.
Breaks on unexpected data.
Read coding conventions on how mandatory data should be accessed.
Remove superfluous whitespace.
Breaks on unexpected data.
Consider using the safe extraction method `try_get` from `utils.py`. ```suggestion artist = try_get(item, lambda x: x['artist']['name'], compat_str) ```
Pass `default=None` to `_og_search_description` instead. Write proper [commit messages](https://github.com/rg3/youtube-dl/commits/master) prefixed with extractor name.
Do not duplicate extractor.
Keep the non-embed URL as fallback scenario.
1. This will try the same URL twice in case of embed URL. 2. Fallback must also apply when no formats was found at first try.
You don't need to. This is checked automatically once URL is included in test set of some other extractor.
Still more protocols possible. Still no schemeless support.
1. There are more schemes possible as well as missing scheme. 2. `'"` are allowed.
what is true for now, can differ in the future, so it's better to make the extraction process more robust.
use `in` operator and move `error = links_data.get('error')` statement instead of using `links_data.get('error')` multiple times and check for error availibility before using it.
this might change in the future, this is not necessary as you're checking for error in response page.
Must be separate extractor.
All of these will break extraction on unexpected data.
Must be separate extractor delegating to CNBCIE.
`_` is a throwaway name for a variable in python and should not be accessed.
If you accept this to be missing then it must be `default=` not `fatal`.
Won't match `digitalData=JSON.parse(...`.
It's already raised.
This is never reached cause sort formats will throw on `not formats`.
This metadata is still available on a page.
If you assume to capture a dict then capture curly braces. Whitespace must be `\s+`.
This should be placed after you've ensured that no formats were extracted
It should also output generic extractor error for all other kinds or errors.
This is pointless.
`data` must be `bytes` so you'll need to `encode` it.
Error handling broken.
JSON layout has changed. Error is not captured properly anymore.
`/?` makes no sense at the end.
Do not change extractor name.
Set empty `vcodec`.
Dots must be escaped.
Check code with flake8.
Links must stay id based.
Should only capture path part and should not capture query or fragment parts of URL.
Also looks like they redesigned the site so that extractor does not work any longer.
Move to initial title assignment.
Matching empty id is senseless.
Must not be `None`.
No direct URLs here.
Matching empty data is senseless.
Keep it on the single line.
Breaks if not found.
Breaks on failure.
Breaks on unexpected data.
1. `.*` at the end is senseless. 2. Don't capture groups you don't use. 3. Capture and use display id.
Don't shadow built-in names.
Pass video id.
I've already pointed out: **remove all duplicate tests**. Or make them only_matching.
Must allow arbitrary whitespace not only `\s`. Must allow single quotes. Group name is senseless when there is only one group.
My take is: `_VALID_URL = r'https?://(?:www\.)?tele5\.de/(?:mediathek/filme-online/videos(/|\?(.*&)?vid=)|tv/)(?P<id>[\w-]+)'` It matches all of the following: ``` https://www.tele5.de/mediathek/filme-online/videos?blah=1&vid=1550589 https://www.tele5.de/mediathek/filme-online/videos/schlefaz-atomic-shark https://www.tele5.de/mediathek/filme-online/videos?vid=1549415 https://www.tele5.de/tv/digimon/videos https://www.tele5.de/tv/digimon/videos/rikas-krise https://www.tele5.de/tv/kalkofes-mattscheibe/video-clips/politik-und-gesellschaft?ve_id=1551191 ```
Should be more relaxed.
Because all of them use the same extraction scenario.
Read some doc on regular expressions. Namely what does `[]` and `()` mean and how it should be used.
This is done automatically.
This does not mean it should not be included.
Do not remove the old code.
Have you even read it? Percent encoding is a plain mapping of characters to `%XX` representation per se. RFC 3989 determines the set of rules for applying this mapping to URIs, roughly speaking it determines the set of characters that should not be percent encoded so that this set is used in `escape_rfc3986` to fix some invalid URIs to meet the requirements. What `compat_urllib_parse.unquote` does is simply mapping `%XX` back to character representation for a plain string. This have nothing to do with URIs and with rules determined in RFC 3989.
You don't need dedicated function for that - you already have it `compat_urllib_parse_unquote`.
Nothing to do with RFC 3986.
Use `compat_urllib_parse_unquote` instead.
Don't shadow built-ins.
Read coding conventions on mandatory and optional meta fields.
Nothing changed. Also there is a video id available in JSON.
No exact URLs here.
Breaks on `None`.
Capturing empty string is senseless. If you expect a dict then capture group must contain `{}`.
Capture and output display id.
Default note is enough.
Read coding conventions on mandatory and optional meta fields.
Single quotes. `item` is already a string.
`.*(?!</li>)` does not make any sense. `['|"]` incorrect.
Search should use the default settings: most popular sort, no duration preference, no upload date preference.
Recursion should be replaced with plain loop.
Query should go as `query` to `_download_webpage`.
1. Do not shadow `url`. 2. Regex should not match across several tags.
Read coding conventions on optional meta fields. Fix all such meta fields added in this PR.
Use `{}` dicts instead.
1. This will duplicate source format when multiple non source formats are available. 2. Source format must be named `source`.
No. You don't need to grab any extension in the first place.
Again: you should not bother with that. It will be automatically extracted from the download URL.
Just replace the part appended to non source formats with empty string.
This is pointless. You already have `source_url` as marker.
There is already a for loop below you must integrate into.
Don't capture groups you don't use.
Read coding conventions and fix optional meta fields.
Do not mix unrelated changes in single PR.
Use `{}` dicts.
Remove unrelated changes.
Don't mix unrelated changes in single PR.
No `(?i)`, no `$`. Dots must be escaped.
For `url` type any metadata here have no effect.
No such meta field.
Don't touch unrelated code.
If you expect dict then capture a dict.
Don't touch unrelated code.
This URL is available from JSON.
1. This must only take place when it's not available from player JSON. 2. Query must be passed as `query`.
Must not be fatal.
Update `video_info` and return it directly.
Breaks on failed download.
Breaks on failed download.
Must not contain empty list for a key.
and this does not look to be geo restricted.
`self._parse_html5_media_entries` for formats extraction.
Optional group at the end does not make any sense. Triple quotes are pointless.
Bitrate should go to corresponding format meta field.
Not used with `formats`.
I don't see any technical difference. Both ways are "tinkering" with `m3u8` extension. The only difference in one case is that you use already written and tested code and get metadata extracted for free. `re.sub` is the tool.
Still does not handle aforementioned URLs.
Won't work for `info = {'title': None}`.
There is no point in before and after parts. Matching one capturing group from `<source` to `index` is enough.
Std lib imports should go before youtube-dl imports.
Procedure for what? Once description changes test will fail regardless of whether it's an md5 or complete description.
`url` should be passed as first arg.
This check does not make sense since `media` contains mandatory data continuing without which is pointless. Read coding conventions.
Use `merge_dicts` to cleverly merge metadata from `media[0]`, `info` and `result`.
Duplicate tests must be `only_matching`.
Invalid. Either both quotes must be present or none. Not one of them.
No need for such checks.
Removing useless noise.
No such key `thumbnailUrl` possible in `info`.
Won't work. Id must be stable persistent value appointed by remote party. `<source src=https://tiny.gjirafa.com/api/media/malltv/t0zzt0/index` `t0zzt0` is the id.
Append `.m3u8` in `webpage` source and use `_parse_html5_media_entries` instead.
Same, no such key possible.
No need for such checks.
Remove all unrelated changes.
The benefit in not wasting time reinventing the wheel and code reusage as it already extracts formats, subtitles, thumbnail and possibly more in future.
I've browsed quite some videos and haven't found any duplicate ids. Provide example URLs wth the same ids.
Again: original keys from `ld+json` have nothing to do with info dict returned by `_search_json_ld`. Bother to check its code.
No, it won't. Bother to read it carefully. `(["\']?)` is idiomatic and correct way to say `(["\']|)`.
`_parse_html5_media_entries` has nothing to do with this regex and has its own parsing routines. All you need to do is to add `.m3u8` after index, you don't need to even bother about what follows it.
`_search_regex` is enough here.
1. `_og_search_thumbnail`. 2. Read coding conventions on optional meta fields.
1. Escape dots. 2. Do not capture empty string. 3. Relax regex.
Won't work. See how this is done for output template.
`quality` must be used for quality.
Read coding conventions on how to write regexes.
Breaks if no such key.
Single regex is enough for extracting episode.
Incorrect regex for domain. Remove `$` from the end.
This will process the same URL twice overwriting the previous results.
Remove all useless debug noise.
Must ensure numeric.
Read coding conventions and fix code failsafeness.
Don't touch files' permissions.
Pass args directly to `_download_json`.
This is always true.
No `smuggle_url` here. `url_transparent`.
Broken python 3. Must be bytes. `urlencode_postdata`.
`try_get` is useless here.
`compat_urllib_parse_urlparse` and `compat_parse_qs` instead.
Title must contain only title.
Broken python 3. Must be bytes. `urlencode_postdata`.
Merge in single list comprehension.
Consts should be in uppercase.
Has no effect for url_transparent.
It does not matter here cause this data is mandatory (read coding conventions) and you can't proceed without it anyway. Do not raise `ExtractorError` on your own.
If you guarantee something exists you don't need to use `try_get`. The purpose of `try_get` is to be used when there is no such guarantee.
`try_get` is useless here.
Do not split literals.
`.*` is useless at the end.
Remove all garbage.
Inline all such methods and use regexes instead.
1. Do not remove the old regex. 2. `\s*` for matching whitespace. 3. When matching between quotes - match any non quote character. 4. Remove useless `;`.
I mean you must pass both regexes.
Should not match `varflashPlayerOptions...`.
```suggestion 'url': 'https://learning.oreilly.com/learning-paths/learning-path-python/9781788996396', ``` I assume it would be good to include one link starting with "learning.oreilly.com" in tests.
```suggestion (?:www\.)?(?:safaribooksonline|learning\.oreilly)\.com/ ``` This would handle also new URLs starting with learning.oreilly.
Should contain `quality` key.
Must not break extraction if missing.
Omit expected type.
Yes, it should accept any variation of whitespace.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
Invalid arguments for 4-5.
>1. Relax regex.
I've already pointed out: no unnecessary requests here. Extension is always the same and must be hardcoded.
49-95 code duplication.
Must not break extraction if missing.
1. Relax regex. 2. Do not capture empty dict.
1. This is mandatory. 2. All formats must be extracted.
`vardata` makes no sense.
`enumerate` on for range.
Must not break extraction if missing.
1. This should use `_search_json_ld`. 2. This should be done in generic extractor. 3. `_search_json_ld` should be extended to be able to process all JSON-LD entries when `expected_type` is specified.
Does not match https://narando.com/r/b2t4t789kxgy9g7ms4rwjvvw.
`\s*` is useless here. Again do not capture empty strings.
Rename to `NarandoIE`.
This does not make any sense, you already have `url`.
Relax `id` group.
`[\n\r].*` does not make much sense. `\s*` same. No escapes for forward slash. Does not make sense to capture empty string.
Do not capture empty strings.
Capture between tags.
No exact URLs here.
Remove superfluous whitespace.
Must not be fatal. Do not capture empty string.
Just make it optional with default=None.
Group name is superfluous.
Instead of this extractor you should implement extractor that processes [ViMP platform](https://www.vimp.com/de/) iframe URLs: http://k100186.vimp.mivitec.net/video/HDC-MAG-quotVirtuelle-Identitaetenquot/a22cd0ac9a14eaaafe0820d66c7971fd https://www.hd-campus.tv/video/City-of-Colour-Die-Stadt-der-vielen-Gesichter/11/12152f0f79d20e0affe363a2c56f83f1 https://www.schwaebische.de/landkreis/landkreis-ravensburg/ravensburg_video,-erkl%C3%A4rvideo-was-ist-eigentlich-die-kehrwoche-_vidid,138463.html http://vimp.schwaebische.de/video/Ausschenken-statt-Einschenken/c29897ac7bb59c5c83ad8b64f15e9e69 http://www.web-tv-produktion.de/category/video/Video-zur-Stadtrundfahrt-Stuttgart-mit-dem-Hop-on-Hop-off-Bus-Karrideo-Image-und-Eventfilmproduktion/57e70e43bc958d51d8e2f621f7cdebc8/8 Many more.
Please continue to make the code [PEP8](https://www.python.org/dev/peps/pep-0008/) compliant, which the extra space between the condition and the `:` violate, as does the extra space between your `else` and `:` below The parens around the test are also unnecessary, but I don't believe they're strictly speaking wrong
You should use exactly the method youtube expects you to, not try all methods in a row.
It's obviously possible since youtube ask you to use some particular method when logging in via web and not tries all the methods available. Trying all methods in a row only approaches risk of ban due to number of tries.
> Instead I should check which challenge it is and submit based on that, maybe switching to SMS by default if something else is selected? That would be my suggestion. There's also some additional challenges for login verification (if attempting to use an unrecognized device) that would easily be supported, although I expect that's out of scope for this PR.
The issue appears to be distinguishing between codes sent over SMS (`[None, tfa_code, True, 2]`), and codes generated from an authenticator app (Google Authenticator, Authy, etc). which is sent using `[tfa_code, True]`. The list of available challenges is returned (default first) from `challenge_results`, with a corresponding ID (from the above code appears to be available as `select_tfa_results[8]`) and some extra information for each challenge. Rather than switching challenges after failure, you can switch only if the default challenge is not `6` or `9`, and similarly determine which payload to use. I'd very, very much recommend looking at [this code](https://github.com/omarroth/invidious/blob/6215259/src/invidious.cr#L1004-L1084) for reference, which is heavily based on youtube-dl's implementation.
You can select preferred method when logging in via web. Then you'll have the payload for this method. I tried to figure out this several times either along with adding support for other TFA methods but each time I've been hit by TFA code limit so I gave up on this.
Looks like you have an unnecessarily nested if block. You should deindent everything you've changed, change the first if to an elif and remove the else above it.
provide an example with a `secondary type`.
yes, remove duplicate formats if the qualities are not available for all programs.
do not use names of Python built-in functions(https://docs.python.org/3/library/functions.html#format).
No exact URLs here.
from first test of `SverigesRadioPublicationIE`: https://sverigesradio.se/sida/playerajax/getaudiourl?id=7038546&type=publication&quality=low&format=iis ```json { "audioUrl": "https://lyssna-cdn.sr.se/isidor/ereg/ekot_nyheter_sthlm/2018/09/6_esa_fran_dek_3b44ebd_a32.m4a", "duration": 132, "codingFormat": 12, "state": 0, "isGeoblockEnabled": false } ``` https://sverigesradio.se/sida/playerajax/getaudiourl?id=7038546&type=publication&quality=medium&format=iis ```json { "audioUrl": "https://lyssna-cdn.sr.se/isidor/ereg/ekot_nyheter_sthlm/2018/09/6_esa_fran_dek_3b44ebd_a96.m4a", "duration": 132, "codingFormat": 13, "state": 0, "isGeoblockEnabled": false } ```
Query to `query`.
Do not capture groups you don't use.
Do not capture empty strings.
Do not capture empty strings.
breaks if `audioUrl` key is not available.
```suggestion from ..utils import int_or_none ```
the duration for `episode` file and `secondary` file is different, if the content of the files is different then a playlist should be used.
`formats` should be sorted.
`vcodec` to 'none'.
it's either one of two cases: they are identical -> keep them as they are(formats of the same entry). they are different -> separate them into a playlist with two entries.
Extract common URL base.
Carry long lines.
no need to call compat_str if it's already as `string` and `title` field should be extracted before formats.
other qualities not extracted(`low` and `medium`).
`('audioUrl' in audiourl) and audiourl['audioUrl'] != ""` this can be simplified to just `audiourl.get('audioUrl')`.
should not return empty formats.
you can just use `item.get('title')` in this case.
if the only difference for this case is the format of the URL, then use an `only_matching` test case.
26-29, 32-37 code duplication.
unless there is another example that has a `secondary` version that is significantly different version from the `episode` one, I think it can be dropped for now.
`audio_id` is already a string.
Move `_match_id` into `_extract_audio`.
I've no idea what you're about. But this looks fine apart from proper `url` extraction.
`audioUrl` may be missing.
`secondaryurl` is `False` if downloading fails.
Here you must use `urljoin`.
`emotion` is not guaranteed to be a dict.
Idiomatic way to check this is `'Video streaming is not available in your country' in error`.
`int_or_none`, remove `expected_type`.
Yes. It does not work as you expect, since you don't specify geo verification headers for this request. Moreover `thumbnail_php` works fine for me even without proxy and without any cookies, so all this seems pointless.
Must be fatal. Must be extracted very first.
Must not be fatal.
try_get is pointless here. Read coding conventions.
Simple string concatenation is enough here.
Incorrect. find returns -1 on failure that is Trueish value.
`tracks` is not guaranteed to be iterable.
`raise_geo_restricted` and specify `countries`.
Breaks on missing file key.
Breaks on uninitialized `h`. Breaks on None width and height.
try_get is pointless here.
`get` is pointless since availability of result key is mandatory.
Don't capture groups you don't use.
Again: read. It's all explained.
Don't capture empty list. `_search_regex`, `_parse_json`. Read coding conventions.
Playlist must be a **separate** extractor in the first place.
No need for that.
id must be captured correctly in all cases.
Mandatory. Read coding conventions.
Remove all garbage or add proper metadata.
Always return a playlist.
Remove all garbage.
No, delegation only via `url_result`.
What's the point of this? `set-cookie` headers are handled internally.
Mandatory. Read coding conventions.
`_search_regex`, `_parse_json`. Again: read coding conventions.
`[/&]?` does not make any sense at the end.
**Do not remove** `_search_regex` part.
Remove all garbage.
Don't touch code formatting.
Do not change variable names without need.
Passed where? It's a member, you don't and shouldn't pass it anywhere.
Constant names should be in uppercase.
Do not use leading underscore for parameters.
Do not use leading underscore for locals.
What are you even trying to do?! There is only **one single** valid tmpl, leave this code as it was.
What are you even trying to do? 1. Create a base class for playlist extractor. 2. Make this method of this base class. 3. Move all constants into derived classes' members.
What do you mean where? You return `playlist_result` that is exactly the same.
Remove unused groups. Relax regex.
Do not shadow existing variables.
Use plain characters not escape codes.
`default` is already not fatal.
Should not be fatal.
Do not capture empty strings.
Allow arbitrary whitespace and both quote types.
This breaks if `tahoe_secondary_data` request has failed.
This should be checked before sorting since currently `_sort_formats` raises exception on empty formats.
1. This must only be raised after ensuring the `formats` is empty. 2. Message should be changed to `This video is paid, subscribe to download it`.
Breaks matching of `http://ruv.is/sarpurinn/...` URLs.
Do not touch `only_matching` tests.
Avoid excessive verbosity.
This is too ambiguous.
There are more formats (https://fm4.orf.at/stories/2956045/), all should be extracted.
Alphabetic. If in parentheses, each import should be on a separate line.
I'd call it `SaltTVIE`.
If it does not require pre-processing anymore then it should be extracted as url not data.
By providing username and password in params obviously.
`SoundcloudIE.ie_key() if SoundcloudIE.suitable(permalink_url) else None` at the place of passing `ie`.
This does not guarantee you anything. What you should do is to check `suitable` on resolved URL.
This must be passed in `url_result`.
It's already fine. More idiomatic way is to use `url_result`.
Must be separate extractor.
`[]` is useless.
1. Breaks if div is not found. 2. `re.finall`.
Does not match `>Categories`.
This is too relaxed and should only be matched in categories part of the webpage.
Move method near the place of usage.
Match between divs.
Method name does not match with what it actually does.
Lack of data must be denoted by `None`.
1. Relaxl regex. 2. Should be "like count".
Move right after view_count.
Else branch is useless.
Again: video URLs are already available on playlist page.
I'm not talking about **these** URLs.
This check is pointless.
Playlist title should not be fatal.
This should not match playlist URLs.
`urljoin` or source `url`.
Don't shadow built-in names.
No. It's a job if the video extractor.
1. This is too broad, regex should be restricted by playlist id. 2. This captures duplicates.
Device id is not escaped.
This should be under `if signature:` else branch now I suppose.
Remove superfluous verbosity.
`{}` won't work in python 2.6.
Are you sure `signature`, `self._device_id` will not have `<`, `>`? Strictly speaking those should be escaped or better standard XML facilities should be used for XML building.
will break if `item` is `None`.
the timestamp is present in both playlist and single video pages.
instead of: ```python tracks = data['tracks'] for track_type in tracks: for video in tracks[track_type]: ``` you can just do something like: ```python for tracks in data.get('tracks', {}).values(): for video in tracks: ```
should be handled someway in `js_to_json`, at least add a `TODO` to indicate that this should be fixed in a general way.
1. `info_dict` may already contain correct values that will be overwritten by these possibly invalid ones. 2. Other metadata should also be extracted from this source. 3. `merge_dicts` for safe merging with priority.
Must not be None. Read coding conventions.
Does not work as expected in all cases.
There is totally no point in `extract_attributes`. Just fix the regex.
`AA-1T6VBU5PW1W12` != `aa-1t6vbu5pw1w12`.
Again: it's not part of the video's title and must not be in the title.
` - Servus TV` should not be in title.
Ids must stay exactly the same.
I'm not going to argue. You either implement it as requested or it's not going to be merged. ![image](https://user-images.githubusercontent.com/1908898/53301615-8d11b880-3887-11e9-9815-620837af85fe.png)
You should add that ability since it's a generic need.
This should be done in `_extract_m3u8_formats`.
Do not remove the old regex.
No trailing `$`, override `suitable`.
Breaks on empty `entries`.
1. Do not restrict `id` expression. 2. Do not capture groups you don't use. Read coding conventions.
No trailing `$`, override `suitable`.
`{}` won't work in python 2.6.
Remove all unrelated changes.
Capture with /album. Capture non greedy.
Dot is pointless here.
293, or is pointless.
`/?` does not make any sense.
It's not an album id.
Not a video id.
Parentheses are pointless.
This does not make any difference since you prepend album anyway.
This is error prone, `len` of the actual string should be used instead.
>besides the test is there to make sure that breakage in this part of code will be detected That's a doubtful argument considering broken core tests at the beginning of this PR. The length of this string is const until one decides to refactor here something. Using 10 is a variation of code duplication since the length is already implicitly defined in the string literal itself. Also using 10 indicates no relation to the string literal so that one unfamiliar with code who decides to refactor it may forgot to change the number and may be unaware of the tests at all.
There is no point to use `remove_start` since line is always a string.
Geo restricted. Test will always fail.
No unrelated changes.
Extractor must not return `None`.
Mandatory. Read coding conventions.
You must properly extract height instead.
No way. Return value type must not change.
`not json_lds` already does the second part.
This won't work as expected in case of `fatal=True` and `_json_ld` failure.
No trailing `?search`, match exact path.
Merge first two alternatives.
Breaks if not a list.
Read coding conventions.
1. This must be downloaded after JSON. 2. This must not be fatal.
Move flags into regex.
No formats should be skipped. Remove.
Do not use `sanitized_Request`.
Formats must be sorted.
Don't capture things you don't use.
Remove useless code.
Move into `_download_json`.
I'm not talking about capturing upload date. Do not capture AMPM.
This does not apply to all videos. Remove this dict completely.
Last part should be removed instead.
Do not remove old patterns.
format is pointless.
Videos formats are not handled.
No trailing markers - override `suitable`.
Query must go to `query`.
Too broad regex.
Also use generators or PagedList instead of list of entries.
87-90 code duplication.
Sorry for this Pull Request has long been reviewed. May I suggest adding referrer to the embed link, as suggested below? ```suggestion if mobj.group('id'): urls.append(VimeoIE._smuggle_referrer('https://player.vimeo.com/video/' + mobj.group('id'), url)) else: urls.append(mobj.group('url')) ``` I came across this Pull Request, since I was looking at Issue #24381. Adding vimeo-id tag support would help downloading video from Laracasts.com too. Although, we still need to implement an extra Extractor for playlist. Hopefully, your Pull Request can be approved.
this regex also isn't working for me, and I couldn't find a main.js that seemed relevant today
Do not remove old patterns.
Twitch collection IDs can have hyphens e.g. `HgTD8zFrghUb-Q`, so it should be: ```python _VALID_URL = r'%s/collections/(?P<id>[\w\d\-]+)' % TwitchBaseIE._VALID_URL_BASE ```
There is no point checking whole dicts and exact URLs.
Move query to `query`.
Read coding conventions on optional metadata.
DRY. Extract json format as well.
`.*` is useless at the end.
Delegation must be implemented using youtube-dl mechanisms.
url is not validated and may be not a URL or not a string at all.
Will capture incorrect id for https://iai.tv/video/darkness-authority-and-dreams#blah.
34-35 can be easily moved into `for`.
Capture dict if you expect dict.
Real id is in widget dict.
1. Don't shadow outer names. 2. `url_or_none`.
Should not be fatal.
Well, I'm not going to argue. You either fix it as requested or get PR rejected.
This is already fatal.
What's the point of all this mess? This can be easily expressed with a single capture group in regex.
Inline variable at the place of usage.
Should not be fatal.
Read coding conventions on optional metadata.
Do not use sanitized_Request.
Move flags into regex.
Breaks if not arr.
This should not be fatal now.
1. Does not work in python 3. 2. Sorting is invalid. Sorting must be done with _sort_formats.
Formats in webpage are still available and should be extracted.
This is pointless.
Remove if you use `thumbnails`.
All three thumbnails should be extracted.
Remove and make dvr path mandatory.
You already have it in get call.
No such meta field.
Extractor must not return None.
Mandatory. Read coding conventions.
Do not change this.
```suggestion _VALID_URL = r'https?://(?:www\.)?bajeczki\.org/(?P<id>.*)' ```
Never use bare except.
Use default. Read coding conventions and fix code.
1. This does not make any sense at all since there will never be an exception. 2. You must extract media URL not guess it.
URL is always available. In case of video it's just hidden behind base64.
Audio must have proper `vcodec` set.
Oh, I see. Thanks!
No need to encode.
Inline everything used only once.
Do not match by plain text.
Breaks. Read coding conventions on optional metadata.
Place on a single line.
Must not be fatal for playlist.
Escape dot. No need to split URL.
`_search_regex`. Read coding conventions.
Extract `height` for each format.
Plain `for x in l`.
Use descriptive names.
Capturing truncated `https://www.porntrex.com/get_file/` does not make any sense.
What are you even trying to do? `'1'` that's all.
Move to base class.
Capturing empty string does not make any sense.
Again: it will capture empty string for `data-playlist-item=""`.
Only whitespace is possible between src: and id.
No, this does not make any sense.
No, this does not give your any safety.
Read coding conventions on mandatory and optional data.
```suggestion from ..utils import urlencode_postdata ```
There is no need in named group when it's the only one.
Move query to `query`.
Carry arguments to the next lines not to waste space on the left. Keep 80 char len per line.
No such meta field.
No such meta field.
No such meta field.
No such meta field.
If it's not found you will try extracting from 404 page. Remove all this 404 mess.
Can't be None.
You must use `default` if there is a fallback after it.
This should not be fatal.
There should be a hardcoded fallback since it's always the same.
EntryId must be extracted the very first.
I mean exactly what it states: EntryId must be extracted the very first in code as extraction does not make any sense without it.
Breaks if no tags or it's not a string.
Breaks extraction if not available. Again read coding conventions.
Don't capture empty title.
Parse from flashvars JSON.
Don't carry URLs. Read coding conventions.
Parse from flashvars JSON.
Don't capture groups you don't use.
Inline everything used only once.
No, add another extractor and properly delegate.
Don't capture groups you don't use. Read coding conventions.
Just output complete stringified flashvars and consume in python code as JSON.
What's the point of this? Remove.
Do not capture empty strings.
Do not carry dict values.
Must not allow https://www.kanald.com.tr//1-bolum/10115.
`.*` at the end does not make any sense.
Rename to `KanalDIE`.
No trailing $, override suitable.
Rename to something else. This extractor must delegate to embed extractor.
I think `_search_json_ld` finds the first matching. There are 3 ld+json in the page and `VideoObject` is the last. Also code breaks [in this line](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L1243) due to `contentUrl` parameter doesn't contains any hostname. It has only path. So must be added hostname to `contentUrl` before calling `_json_ld` function I think
This line does not make any sense.
`/?` does not make any sense.
generator or PagedList instead of list.
Must not be fatal. Read coding conventions.
url and formats are not used together.
use an appropriate name for the variable(it's not a URL).
incorrect order, put the methods after the properties.
Must not capture.
Code duplication with base class method.
watch.videodelivery.net should also be added.
No such URL exists.
No such id `None`.
Must be a separate extractor delegating to SBS.
No `^` and `$`. Override `suitable`.
Not acceptable. Search `url_result`.
This is useless if you use `only_matching`.
As already pointed out: you must delegate to `SBSIE` extractor not inherit from it.
This must be in gdcvault extractor.
This test is pointless. You must test iframe extraction not regex match.
Superfluous brackets. Doesn't cover multiple spaces. Doesn't cover other kinds of whitespace.
Instead of such hacks you can name group differently and capture it without any issue.
the message will be `Checking None URL` if `format_id` is not available, provide a fallback for `format_id`.
This does not look to be possible on a clean session.
Don't capture empty dict.
If you pass cookie you must not pass credentials.
Remove unnecessary verbosity.
This must be delegated to brightcove extractor.
This must be an id of the media.
Move on a single line.
Possibly referenced before assignment.
Possibly referenced before assignment.
May be None.
Capture with regex in `_VALID_URL`.
Inline all these.
Entries with invalid `vid.get('id')` should be skipped.
There is no point in a separate method.
`video_data` is totally useless. Write directly to id variable when found.
As already said: parse as JSON not with regexes.
Must not be fatal.
1. Remove script tags, it's unique enough without them. 2. Do not use named group when there is only one group. 3. Curly braces don't need escaping. 4. Do not capture empty dict.
1. `v` may not be dict. 2. `v.get('slug')`.
It's already extracted as video_id.
@froiss Like he said, you can pass a list of regexes instead of a single regex: self._html_search_regex([r'old regex', r'new regex',], webpage, 'wat id')
This is ambiguous since regex matches multiple ids. Parse correct id from JSON matching by slug and only if it fails fallback to this regex pattern.
No `\W`. You must match arbitrary quotes and arbitrary whitespace.
No. Use list of regexes.
No. As already said you must keep the old pattern along with the new.
1. Do not remove the old pattern. 2. Relax regex.
Replace with `'md5:'`.
No, parse as JSON. Or from ld+json.
Don't capture unused groups./
This could be simplified as: ``` P<host>pornhub(?:premium)?\. ```
This must be a separate extractor.
Remove credentials from public code and change your pass.
This must be calculated once.
Don't touch this.
Plain `for` is enough.
Don't relax regexes.
Since this is an adult website, I think it is prudent to add: ``` 'age_limit': 18 ```
I have also seen some older videos that end with ``- ThisVid tube`` instead of ``- ThisVid.com``, which would be incorrectly matched by this regex.
Are underscores legal in IDs? I don't remember.
In my code, I also added the following fields: * ext * thumbnail * uploader_id * uploader_url Feel free to [copy my code](https://github.com/jhwgh1968/youtube-dl/blob/23c4fb5e3ea619e7da6adc9d3dee8170b544fe25/youtube_dl/extractor/thisvid.py#L90).
No. _extract_urls of youtube extractor.
Remove all garbage.
You must fix user_id extraction instead of this ugly hack.
could you add description from perex key
this will be done for you by just providing `width` and `height`
`from ..utils import urljoin` based on what you use from urlparse
All formats should be extracted not only mp4.
No tabs in python code.
Second group is useless.
```suggestion spl_url = data['episode']['spl'] + 'spl2,3' ```
Read coding conventions on optional metadata.
Do not escape `/`.
```suggestion 'ext': ext, ``` fix flake8 check
```suggestion formats.append({ 'format_id': r, ```
It also accepts `/video/bbq-salon/WHATEVER_I_WANT_AND_MAKES_SEO_SENSE-63940306` but that doesn't make it `id`, does it? Also when you examine their graphql query there is a attribute called `dotId` which has exactly that value eg. it is `ID` your `video_id` is in their data on attribute `urlName`. Do as you think, but this will be an issue when merging into master.
```suggestion 'url': urljoin(spl_url, v['urls'].get(ext)) ``` fix flake8 check
```suggestion format.update({'width': v['resolution'][0], 'height': v['resolution'][1]}) ``` to fix flake8 errors
```suggestion metadata = self._download_json(spl_url, video_id, 'Downloading playlist') if 'Location' in metadata and 'data' not in metadata: # they sometimes wants to redirect spl_url = metadata['Location'] metadata = self._download_json(spl_url, video_id, 'Redirected -> Downloading playlist') play_list = metadata['data'] ```
Extract id once before the loop.
Playlist title is optional.
This should not be here as done by downloader.
Braces in non inline dicts **should** be carried. Parentheses != braces.
If either of these attrs is missing whole playlist extraction is broken.
If nothing matches `None` will be returned.
`url` is not a video id.
No direct URLs in tests.
Add a rationale for that.
121, 124 - DRY.
No, since we don't need unnecessary traffic during tests. Basically decryption can be disabled while in test - in both cases it's technically just downloading bytes and checking whether it matches the expected checksum regardless of whether it's encrypted or not. But I'd just keep with current policy: encrypted stream should not run md5 file tests in the first place.
Downloader options should be read from `downloader_options` here.
This must be asserted.
Again: asserts are for **invariant checks** during **development**. Once you're done you may remove them freely that's why there are almost no asserts in code base. The point is that there is no need to throw in such cases. If you want to check while developing add an assert. If you're sure enough you won't stumble while developing don't use asserts at all.
1. No. 2. This won't have any effect anyway since post processors operate on info dict copy.
This is expected behavior, you named it preferredinfo not preferredmetadatatoembed. `track` from info must be preferred over `title` from preferredinfo. Again: you merge `_preferredinfo` with `info` into separate merged info **not touching** the initial dicts then operate with old code on merged info instead of `info`.
No, that won't work, nobody will bother crafting JSON file just to just override single info meta field. There should be an interface for overriding a single meta field from command line in the first place.
No. As already said it's an implementation detail and may be changed any time. [This](https://github.com/ytdl-org/youtube-dl/blob/a6e6673e825f6225c3a316b164ddca03fd20b5d2/youtube_dl/YoutubeDL.py#L140-L321) and [this](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py#L100-L325) are maintained. Embeddable metadata is not.
No. In this case the whole new notion of embeddable metadata meta fields should be introduced.
I guess you misunderstand the whole point of assertions. You should distinguish cases when data comes externally and internally. In this case the latter takes place. The input data is build by our own code internally therefore we ourselves are responsible for guaranteeing its validity so that it's pointless to always perform this runtime check. If it turns out to be invalid then it's a bug in our code that is idiomatically as precondition covered by assert in debug.
https://www.3sat.de/wissen/nano/190822-sendung-100.html for 3SAT https://www.zdf.de/kinder/wickie-und-die-starken-maenner/der-wegezoll-102.html for ZDF both running with the patch above download klappt mit dem Patch
There is already video id from `url`.
Breaks if no `name`.
Title is mandatory. Again read coding conventions.
Do not touch existing tests.
Read coding conventions on mandatory data.
Video id is already available from `url`.
You must not use `.get` with `try_get`.
Breaks. Read coding conventions.
jp also exists
Remove useless changes.
Once again: 1. Read coding conventions on mandatory metadata. 2. Pass `query` to `_download_json` not in URL. 3. Fix tests.
Read coding conventions.
Read coding conventions on mandatory metadata.
1. No `{}`. 2. Inline. 3. Query to `query`.
You can use self._download_json() here
Keep test with non-ad rendition.
Revert all noop changes.
Named groups are pointless if you are not using these names.
Must work for https://zattoo.com/program/DE_arte/170967775, https://zattoo.com/ondemand/videos/ZAiYp5EchgdAdM2KTgYFRUFP, https://zattoo.com/live/srf1 and so on.
This should be split into building url and extracting formats.
Must not return `None`.
Must also match https://zattoo.com/highlights?foo=1&channel=orf-1 and similar.
```suggestion iframe_url, video_id, ```
No brackets needed.
This is no longer actual.
Inline everything used only once.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
This is useless at the end.
Breaks. Read coding conventions.
Lack of data is denoted by `None` not 0.
Useless with timestamp available.
Referring `url` from `url` looks like nonsense. Provide rationale.
Don't capture groups you don't use. Unused captured group.
No exact URLs.
`url_or_none`, `str_or_none`. Read coding conventions.
Both URL and array should be tried if available.
No, as this will work incorrectly for empty `""` values.
Don't touch the old test.
Use formatted strings.
Remove all useless noise.
Again: read coding conventions.
Breaks on None.
Don't touch the old test.
First two groups are useless.
Superflous groups. Do not capture empty string.
No escapes for slash.
Move flags into regex.
Do not capture empty dict.
Move inside a loop.
None is not an id.
Remove superfluous verbosity.
Video id length is 8.
This test is identical to the first. Revert.
Read coding conventions.
Do not touch the old patterns.
Will break on `mundonick` lookup in dict below.
Breaks. Read coding conventions on optional data.
Do not touch extractor name.
Add `id` and `ie_key`.
Do not touch this.
Revert it exactly as it was.
Single quotes. No `{}` here.
Do not touch this.
Must not be fatal. Must fallback to generic if not found.
Do not touch existing `AtomicParsley` code. `mutagen` path must be a fallback if `AtomicParsley` is missing.
You must output to a temp file not the original file.
Must be `[mutagen] Adding thumbnail to "%s"` before the actual adding.
No. It must be a soft dependency. Message requesting installation of `AtomicParsley` or `mutagen` should only be output when thumbnail embedding is requested and neither of these dependencies is found.
Same as in previous review.
Read coding conventions.
Read coding conventions.
Part in the middle should not be greedy. Also use `_parse_html5_media_entries` as main path.
Do not remove previous pattern.
flake8, no long lines.
Filter invalid URLs.
True is default.
No, override `suitable`.
You technically can't login with this extractor apart from using cookies.
Do not capture groups you don't use.
Read coding conventions on optional metadata.
https://ok.ru/video/1705664645833 downloads fine without these changes.
>Video has been blocked due to author's rights infingement
`utils.urljoin` not from compat.
Not quite sure. Currently it redirects to `pornhub.com`. Possibly this was not the case in the past.
Move under `if` where it's used.
You must provide account credentials/cookies for testing.
Should also match `pornhubpremium.net`. Or extractor should not match `pornhubpremium.net`.
This should be in `_real_initialize`. Same for all other occurrences.
Does premium login also work on pornhub.com site? In any case it's better to have separate extractors with separate `_NETRC_MACHINE` with common base class in case one may want to use different credentials.
Then extractors must be separate. Move common code in the base class and there will be no duplicated code.
1. Having arbitrary cookies set for domain does not mean user is logged in so this will potentially skip login when it should not. 2. With current approach `cookies` will always be non empty since `age_verified` cookie is always set by the extractor.
Already extracted in code.
Dots not escaped.
Covered by generic extractor. Remove.
Read coding conventions on mandatory data.
1. Extract dict if you expect dict. 2. Relax regex. 3. Escape dots.
Again: float_or_none, not parse_duration.
Depends on what does it mean.
No such meta field.
DRY. url is mandatory.
You must make it **not break** as you delegate.
Playlist title is optional, description breaks.
You are **delegating** to brightcove that provides the id.
Carry long lines. Bother to finally read coding conventions.
Breaks once `userData` is not first.
If I understand the readme, `_og_search_title` is used to read the value of the `og:title` meta element in the head so should probably be used as a fallback for `playlist_title` instead. I haven't seen a fallback source for the individual video titles.
Looks like this `flat_list` generation only needs to be done once, so could move up outside the `playlist_dict` loop.
These should be `vid_info.get('short_description')` for the individual video descriptions, this is currently adding the playlist description to every video.
Suggest using `int` instead of (or in addition to) `round` because I'm seeing a lot of `x.0 min.` even though the default rounding is 0 decimals.
I missed subs on PBS, so I'm really interested in getting this merged. I'm not in any way familiar with the detailed coding guidelines and can't speak for dstftw, but I think this change of formatting of the list might be an issue (removing the comma and moving up the closing bracket ).
```suggestion captions = info.get('cc') or {} ```
not need, there is generic code that will extract the subtitle extension from the subtitle URL.
reduce indentation by using setdefault method, use `append` instead of `extend` when you're adding a single element at a time: ```suggestion for caption_url in (info.get('cc') or {}).values(): subtitles.setdefault('en', []).append({ 'url': caption_url }) ```
Again: you must eliminate code duplication. In both places.
Must return info dict.
Using preferences causes invalid sorting.
That's not acceptable. Extractor should not lose any functionality.
Current web player does not use any API on this host.
Read coding conventions on mandatory metadata.
Lack of data is denoted by None not empty string.
With this you wipe **all cookies** for `.google.com` domain with corresponding consequences. You must only touch minimal relevant cookies for minimal relevant domain.
Meaning that you must provide account credentials for testing or whatever else is needed.
Must be numeric not string.
I can confirm that this code works by providing the OAuth token. eg: ``` sh youtube-dl -f bestaudio --add-header "Authorization:OAuth <OAUTH TOKEN>" https://soundcloud.com/turborecordings/azari-iii-hungry-for-the -5 youtube-dl -f http_aac_256 --add-header "Authorization:OAuth <OAUTH TOKEN>" https://soundcloud.com/turborecordings/azari-iii-hungry-for- the-5 ``` You can get this code by opening up dev tools in the browser.
iterate over dictionary tuples ```python for key, src_url in vod.get('streamUrl').items(): ```
No. Spiegel does not match `nexx:`.
Again: no, do not touch token extraction code. Just eliminate duplication.
Breaks. You are not guaranteed `protection_data` is a dict. **Do not** change previous token extraction algo.
~~1. This won't work for UTF-8.~~ Nevermind, `BaseCookie.__ParseString` does not seem to be capable of UTF-8. 2. This will produce `"None"` string if there is no cookies, i.e. `cookie_header` is `None` that is completely wrong. 3. Also this should only taken place when `cookie_header` is a not a bytestring already.
Remove unnecessary captions as it reduces redability of code.
This will break unicode strings under python 2.
There is no need in a separate class since you only need a single cached set. Keep changes minimal.
There is really no need in that at the moment. The need will arise when this download archive v2 based on standalone db or whatever is about to be implemented. For now I'd prefer changing couple lines in existing code over premature refactoring even though it may not meet best OOP practices.
This is useless. `filepath` must be required to be a valid path. This must be asserted.
Not having archive != having dummy `Archive` object.
This is pointless.
m3u8 should also be extracted.
I suggest you split this pull request into two: one for the bugfix and another for the new feature. So one doesn't block the other.
The title is `Radio . - mrdd #608871369`, no extra spaces.
What are you even doing? It's already stripped in `_html_search_regex`.
Breaks. Read coding conventions.
Const must be uppercase. Protected must be prefix with underscore. Single quotes.
Again: **ONLY MATCHING** test, no downloading, no metadata, nothing.
No, only_matching as already pointed out.
Move these to individual extractors.
Shouldn't here be Tirol in the URL? ``` _VALID_URL = r'https?://(?P<station>tirol)\.orf\.at/player/(?P<date>[0-9]+)/(?P<show>\w+)' ```
This produces invalid results when being called in a non-english speaking country. At least for me, in germany, titles will have `, - Anschauen auf Crunchyroll` appended (which is the same phrase being cut off here, just in german) This was not the case before
Const means does not change after initial assignment.
Uppercase is used for const.
Again: remove. Any download attempt must have clear explicit message.
1. Nothing, CI is running in unrestricted country. 2. This is not necessarily the case, for me all hosts return the same formats.
Ok, then at least cache as class member field for subsequent extractions inside single run.
Read coding conventions.
Also cache last accessible host and try it first.
Then you must try all hosts. Without code duplication.
Remove debug garbage.
Remove unnecessary whitespace.
This must be in a separate try-except so that it does not break renaming to correct name if any of these statements fails.
1. No umask respected. 2. There is no `os.chmod` in python 3.2 according to python [docs](https://docs.python.org/3/library/os.html#os.chmod). 3. flake8.
Do not capture unused groups.
Sorry it took me so long to get to this, but I just tested #25216 and it works for me as-is. So my theory above is completely wrong.
I'm not sure yet but I think removing the original pattern (by replacing it with the new pattern) fixed the problem, and putting it back has re-broken it. My suspicion is that both patterns are matched on the page in question but one returns an invalid ID. Or perhaps a valid ID that nevertheless yields an error when the code later tries to retrieve it. If I'm right, undoing the requested change will get it working again. But that's not really a solution, and again I'm not sure yet.
Do not touch old patterns.
Sorry for not minding my own business, but it looks like your requested change was implemented by @tmthywynn8 a couple of months ago. I'm not that familiar with the GitHub workflow, but it looks like this is just waiting for your approval.
Should not fail if no such cookie is available.
Do not make the regex stricter.
Will not work if `id` is moved apart from `{`. Parse JSON instead.
Do not remove the old pattern.
1. You must delegate extraction not call `_real_extract` directly. 2. You must delegate to concrete station extractor not abstract base extractor.
This is already embedded into extractors. DRY.
This can be moved to station extractor regexes eliminating this extractor at all.
Do not touch this.
1. Provide clear evidence this field is supposed to store the video URL. According to http://atomicparsley.sourceforge.net/mpeg-4files.html `tvnn` is a TV Network Name and obviously has nothing to do with video URL at all. 2. This does not guarantee the actual container is `mp4`. 3. This won't work for audio.
At least it's an URL and not something completely unrelated. TV Network Name can't be interpreted as URL at all.
Move embed extraction code to `ViddlerIE._extract_urls`.
only_matching, move to the end.
`%d` in path still does not work: ``` [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['--embed-thu', 'https://www.youtube.com/watch?v=bSBTcQNPNqc', '-f', '18', '-v'] [debug] Encodings: locale cp1251, fs utf-8, out utf-8, pref cp1251 [debug] youtube-dl version 2020.09.06 [debug] Git HEAD: bf6c312 [debug] Python version 3.7.0 (CPython) - Windows-10-10.0.10240-SP0 [debug] exe versions: ffmpeg N-85653-gb4330a0, ffprobe N-85653-gb4330a0, phantomjs 2.1.1, rtmpdump 2.4 [debug] Proxy map: {} [youtube] bSBTcQNPNqc: Downloading webpage [youtube] bSBTcQNPNqc: Downloading thumbnail ... [youtube] bSBTcQNPNqc: Writing thumbnail to: How to print %d using printf in C language-bSBTcQNPNqc.jpg [debug] Invoking downloader on '...' [download] How to print %d using printf in C language-bSBTcQNPNqc.mp4 has already been downloaded [download] 100% of 8.39MiB [ffmpeg] Converting thumbnail "How to print %d using printf in C language-bSBTcQNPNqc.webp" to JPEG [debug] ffmpeg command line: ffmpeg -y -loglevel "repeat+info" -i "file:How to print %d using printf in C language-bSBTcQNPNqc.webp" "-bsf:v" mjpeg2jpeg "file:How to print _d using printf in C language-bSBTcQNPNqc.jpg" ERROR: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory Traceback (most recent call last): File "C:\Dev\youtube-dl\master\youtube_dl\YoutubeDL.py", line 2065, in post_process files_to_delete, info = pp.run(info) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\embedthumbnail.py", line 61, in run self.run_ffmpeg(thumbnail_filename, jpg_thumbnail_filename, ['-bsf:v', 'mjpeg2jpeg']) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 239, in run_ffmpeg self.run_ffmpeg_multiple_files([path], out_path, opts) File "C:\Dev\youtube-dl\master\youtube_dl\postprocessor\ffmpeg.py", line 235, in run_ffmpeg_multiple_files raise FFmpegPostProcessorError(msg) youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessorError: file:How to print %d using printf in C language-bSBTcQNPNqc.webp: No such file or directory ```
WebP header is clearly defined as 0-3 bytes == ASCII `RIFF`, 8-11 bytes == ASCII `WEBP`. This is what should be checked exactly and not for `WEBP` somewhere is the first 16 bytes.
WebP header occupies 12 bytes, there is no point reading more.
This makes no sense if extension is not `webp`, extension must be checked first.
Breaks if request fails. Read coding conventions on optional metadata.
It must be a separate extractor in the first place.
This is not equivalent, do not touch.
Do not encode query into URL but pass via appropriate `_extract_m3u8_formats` parameter.
`<h4 class="login_register_header"[^>]+>` matches multiple times and does not properly identify title.
Do not remove the old pattern.
```suggestion # coding: utf-8 from __future__ import unicode_literals ```
I think it's better to directly include `'?v_id=%s'` in `_VIDEO_INFO_BASE_URL`.
This could use `_og_search_thumbnail` instead: ```py thumbnail = self._og_search_thumbnail(webpage, default=None) ```
```suggestion 'title': 'ææ!éèã»ãã¡ããã#1ããéº»éã', ``` (according to json)
Doh, somehow I didn't realize the ref to `s` didn't exist if switching to that. My bad. Will need to use PyCharm 1st ...
I'm pretty sure you can replace this dual `for` with another list comprehension, too.
You should be able to replace those 2 lines with ```python3 subtitles[s['language']].extend({'url': s['url'], 'ext': s['category']} for s in ysubs) ``` . Untested but it should become a generator which gets implicitly iterated, which should be faster than appending 1 by 1.
The regexp can be problematic when the json payload contains ";", for example the anime title can be "Dat Girl;ï¼". In this case the matched result is not a valid json string.
This will be needed later: ```suggestion import re ```
These will be needed later: ```suggestion from ..utils import ( get_element_by_id, get_element_by_class, int_or_none, js_to_json, MONTH_NAMES, qualities, unified_strdate, ) ```
Add data to help `upload_date` extraction and a sorting key: ```suggestion _MONTH_NAMES_KA = ['áááááá á', 'ááááá áááá', 'ááá á¢á', 'ááá ááá', 'áááá¡á', 'ááááá¡á','ááááá¡á', 'ááááá¡á¢á', 'á¡áá¥á¢ááááá á', 'áá¥á¢ááááá á', 'ááááááá á', 'áááááááá á'] _quality = staticmethod(qualities(('SD', 'HD'))) ```
More ways to get `title`: ```suggestion title = ( self._og_search_title(webpage, default=None) or get_element_by_class('my_video_title', webpage) or self._html_search_regex(r'<title\b[^>]*>([^<]+)</title\b', webpage, 'title')) ```
Make the regex less strict and the search non-fatal: ```suggestion r'''(?s)jwplayer\s*\(\s*['"]mvplayer['"]\s*\)\s*\.\s*setup\s*\(.*?\bsources\s*:\s*(\[.*?])\s*[,});]''', webpage, 'jwplayer sources', fatal=False) or '', video_id, transform_source=js_to_json, fatal=False) ```
* Use built-in sorting (needs https://github.com/dirkf/youtube-dl/tree/df-kikuyan-jwplayer-patch) * Get `description` in different ways * Extract `upload_date` by translating month names from Georgian ```suggestion formats = self._parse_jwplayer_formats(jwplayer_sources or [], video_id) for f in formats or []: f['preference'] = self._quality(f['format_id']) self._sort_formats(formats) description = ( self._og_search_description(webpage) or get_element_by_id('long_desc_holder', webpage) or self._html_search_meta('description', webpage)) uploader = self._search_regex(r'<a[^>]+class="mv_user_name"[^>]*>([^<]+)<', webpage, 'uploader', fatal=False) upload_date = get_element_by_class('mv_vid_upl_date', webpage) # as ka locale may not be present roll a custom date conversion upload_date = (unified_strdate( # translate any ka month to en re.sub('|'.join(self._MONTH_NAMES_KA), lambda m: MONTH_NAMES['en'][self._MONTH_NAMES_KA.index(m.group(0))], upload_date, re.I)) if upload_date else None) ```
* Use the standard method for `og:image` * Get additional metadata ```suggestion 'thumbnail': self._og_search_thumbnail(webpage), 'upload_date': upload_date, 'view_count': int_or_none(get_element_by_class('mv_vid_views', webpage)), 'like_count': int_or_none(get_element_by_id('likes_count', webpage)), 'dislike_count': int_or_none(get_element_by_id('dislikes_count', webpage)), ```
Apparently the page data has changed: ```suggestion 'uploader': 'chixa33', 'description': 'md5:5b067801318e33c2e6eea4ab90b1fdd3', ```
```suggestion files = video.get('files') or [] for playlist in (video.get('streamingPlaylists') or []): ```
I think some of these should be [collapsed](https://github.com/ytdl-org/youtube-dl#collapse-fallbacks).
Must not fail if `media_data['streamInfo']['source']` not available.
Extract `media_data['streamInfo']['sourceId']` into variable.
Must be `url_transparent`.
DRY with `_limelight_result`.
> I didn't had subtitles on my radar. Can you have a look on my updated approach? I had a quick look over it. Looks good so far :) Good luck that your PR will be merged anytime in the future ð
You want to use `video_id = self._match_id(url)` where `_match_id` does this https://github.com/ytdl-org/youtube-dl/blob/f5863a3ea08492bd9fc04c55e1e912d24e92d49b/youtube_dl/extractor/common.py#L414-L419
This URL is the same as above.
I think it's customary to use `_VALID_URL` for id matching if possible.
```suggestion course, video_id = re.match(self._VALID_URL, url).group('course_name', 'id') ```
More maintainable: ```suggestion 'skip': 'Requires alura account credentials',# }, { ```
Pass `Referer` header to avoid 403: ```suggestion headers = {'Referer': 'https://www.duboku.co/static/player/videojs.html'} formats = self._extract_m3u8_formats(data_url, video_id, 'mp4', headers=headers) ```
Use the function defined earlier: ```suggestion mobj = _get_element_by_tag_and_attrib(html, tag='a') ```
Use `headers` as introduced above: ```suggestion 'http_headers': headers, ```
Simplify: ```suggestion series_title, title = None, None for html in get_elements_by_class('title', webpage_html): ```
Simpler: ```suggestion series_id, season_id, episode_id = video_id.split('-') ```
Page has changed: ```suggestion 'title': 'äº²ç±çèªå·± ç¬¬1é', ```
Fix test: ```suggestion 'ext': 'mp4', ```
* use the resulting match object * avoid excessive indentation * `r'\s'` includes any whitespace * simplify `clean_html()` expressions ```suggestion href = extract_attributes(html[mobj.start(0):mobj.start('content')]).get('href') if not href: continue mobj1 = re.search(r'/(?P<s_id>\d+)\.html', href) if mobj1 and mobj1.group('s_id') == series_id: series_title = clean_html(re.sub(r'\s+', ' ', mobj.group('content'))) title = clean_html(re.sub(r'\s+', ' ', html)) break ```
`#playlist2` has gone: use `#playlist1` instead: ```suggestion 'url': 'https://www.duboku.co/voddetail/1554.html#playlist1', 'info_dict': { 'id': '1554#playlist1', ```
Simplify: ```suggestion series_id = self._match_id(url) ```
Fix test: ```suggestion 'ext': 'mp4', ```
Require n-n-n in `id` field; no need to match the tail: ```suggestion _VALID_URL = r'(?:https?://[^/]+\.duboku\.co/vodplay/)(?P<id>(?:[0-9]+-){2}[0-9]+)\.html' ```
Harmonise with yt-dlp pt1: ```suggestion _TESTS = [{ ```
Harmonise with yt-dlp pt3: ```suggestion ```
Harmonise with yt-dlp pt2: ```suggestion 'age_limit': 18, ```
Harmonise with yt-dlp pt4: ```suggestion }] ```
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
Sarmonise with yt-dlp pt7: ```suggestion # Stripchat declares the RTA meta-tag, but in an non-standard format so _rta_search() can't be used 'age_limit': 18, } ```
Simplify, harmonise with yt-dlp pt5: ```suggestion if try_get(data, lambda x: x['viewCam']['show'], dict): raise ExtractorError('Model is in private show', expected=True) elif not try_get(data, lambda x: x['viewCam']['model']['isLive'], bool): raise ExtractorError('Model is offline', expected=True) ```
```suggestion 'description': compat_str, ```
Removed by final suggestion: ```suggestion ```
Currently these are right: ```suggestion 'title': 're:(?i)^Les Fondamentaux : Vocabulaire$', }, 'playlist_mincount': 25 ```
Roll this together: ```suggestion return self.url_result( 'francetv:' + video_id, ```
More maintainable: ```suggestion from .lumni import ( LumniIE, LumniPlaylistIE, ) ```
There's a utility method for this; also the regex can be relaxed: ```suggestion playlist_title = self._og_search_title(webpage) return self.playlist_from_matches( re.findall(r'''<a\b[^>]+\bhref\s*=\s*["']/video/([0-9a-z-]+)''', webpage), playlist_id, playlist_title, getter=lambda x: 'https://lumni.fr/video/' + x, ie=LumniIE.ie_key()) ```
No need escaping braces.
This does not necessarily mean geo restriction, they may have other reasons for video unavailability. It's better checking the message inside `p` that explicitly states about blocking content in `your country`.
Use full `--continue=true|false` notation instead.
I've already pointed out: by default it should work the way youtube-dl works. If user wants to override external downloader behavior he should explicitly provide desired options via `--external-downloader-args` (these options should override the defaults by any suitable way external downloader supports - by putting at front/back or whetever). As for link mentioned that's not a problem at all as it's easily solved by proper output template.
This has no effect for youtube-dl's `--continue`/`--no-continue` - in both cases downloading is resumed. This is because you must also pass `--allow-overwrite=true` and `--remove-control-file=true` in case of youtube-dl's `--no-continue`.
>Do you mean I should check for ydl's `no-overwrites` flag and add aria2's `allow-overwrite` here? No. `--no-overwrites` is for non media files. >But if you mean to add `allow-overwrite` for `no-continue`, you misunderstand my intention. The point is not your intention but replicating the actual behavior of youtube-dl itself. With `--continue` youtube-dl resumes download, aria2c as external downloader should do the same by default, so that a command should be passed to aria2c that will instruct it to resume. When `--no-continue` is used youtube-dl downloads from scratch, same should do aria2c.
This must be a complete separate extractor. All other extractors must delegate to it not inherit.
```suggestion ExtractorError, ) ```
Do not inherit brightcove, you must delegate instead.
Remove all debug garbage.
No exact URLs.
Noway. See other extractors on how to delegate properly.
The `title.replace(r'\"', '"')` is still needed.
I think that's the title is incorrectly parsed, i.e. the test case shouldn't be adjusted.
1. Superfluous second group. 2. invidiou.site does not work.
You've traded bad for worse. Just parse it with regex in `_search_regex`.
`true` is irrelevant here, keep tests minimal.
You can either use an array `["0x40"]` or a dict with both key and value set to `"0x40"`.
Don't capture groups you don't use.
Again: it must contain `vbr or abr` if available.
This will skip `format_id` completely even if `media_type` is available.
This should be fixed as well.
Should not be here.
The starting part that is already used for it.
The set of format string meta fields is strictly defined. This must not match arbitrary field.
capture only what is needed for the extraction.
add more fallbacks and extract `timestamp`.
i think you should not remove the formats, it's possible that they would be served from different server, protocol, etc...
use valid URLs.
again, extract all formats, there is no guarentee that those formats are available in all videos.
as i said there is no guarantee that this will be the case all the time.
then you should use `_remove_duplicate_formats` method.
but it would always remove formats without `acodec` even if they are only present as `EXT-X-MEDIA` formats.
they have diffirent bitrate.
don't use `modified_time` as publish timestamp.
should be removed.
then I think it would be better to use `fatal=False` instead of `default=None`.
looking again at this, there are multiple problems with the `og:published_time`, the timezone is important to calculate the correct timestamp so the `og:published_time` value for embeds is malformed, and also there is a discrepancy between the values from the video page and the values from the embed page, so unless there is a way to determine the correct value, it might be better to drop it.
don't use the image that has the play icon(`image_full_play`).
in the case of https://video.espresso.repubblica.it/tutti-i-video/01-ted-villa/14772/14870 the date extracted from `og:published_time` is `2020-10-15`. while the one that is shown on the webpage `08 ottobre 2020`(i think this is the right date). so it appears that `og:published_time` is also unreliable similar to `videotitle`.
i think the use of `videotitle` is not reliable(it also has a problem with escaped double quotes), instead of this cleanup, other sources should be prefered.
for now, merge with the main extractor and if a need for separation arises in the future, this can be done(only the common code).
> so I guess I'll remove it at that's it. right? yes, it would be better to remove it.
- extract all formats. - try not complicating things, try to avoid using hardcoded values.
the title should'nt have an escaped double quotes.
i'm not sure why you're adding this step.
match only valid URLs.
try to extract both formats(HLS and DASH formats).
No, use a value that matches the user agent used by youtube-dl(`chrome`).
reuse [the code that parses the old formats](https://github.com/ytdl-org/youtube-dl/blob/aa613ef7e1efe9f799a1209659f8d9d01e3de221/youtube_dl/extractor/francetv.py#L135-L168).
should not break the extraction here if a request fails or the `video` field is not accessible.
```suggestion is_live = (try_get( video, lambda x: x['plages_ouverture'][0]['direct'], bool) is True) or video.get('is_live') is True or '/live.francetv.fr/' in video_url) ```
still would break the extraction if one request fails(with `fatal=False` the return value of `_download_json` would be `None` and you can the `in` operator with `NoneType`). ```suggestion if fallback_info and fallback_info.get('video'): ```
use the already extracted value(`video_url`).
do not capture groups that you're not using.
```suggestion _VALID_URL = r'(?P<base>https?://(?:www\.)?raiplay\.it/programmi/(?P<id>[^/?#&]+))' ```
should not fail if the extraction of one set or item is not possible.
what the point of this change? the old code would work as well(without capturing additional groups).
there is no need to collect the `content_sets` and then iterate over them, you can process them directly.
capture the base URL in `_VALID_URL` regular expression and then add `.json` suffix(same for `RaiPlayPlaylistIE`).
> Any field apart from the aforementioned ones are considered optional. That means that extraction should be tolerant to situations when sources for these fields can potentially be unavailable (even if they are always available at the moment) and future-proof in order not to break the extraction of general purpose mandatory fields. - the playlist title and description are not mandatory. - extraction should not fail if any of the fields `blocks`, `sets`, and `id` are not available.
the part you're matching is static and won't change, I meant to match to the end of the URL(the same for `RaiPlayLiveIE`).
is there a URL where the `duration` is available.
this URL you have provided is not related to this part of the code(`RaiPlayLiveIE`).
```suggestion if reason_text and not error_message: ```
```suggestion formats.extend(self._extract_mpd_formats( format_url, video_id, 'mpd-%s' % protocol, fatal=False)) ```
This will break download archives.
`rights` is not guaranteed to be a dict.
```suggestion timestamp = unified_timestamp(rights.get('validFrom')) ```
This will break some extraction scenarios like `svt:1392003-001A`.
Looks like, it's off then.
```suggestion info_dict['thumbnail'] = thumbnail ```
Nothing conceptually changed: you still request `st_mtime` unnecessarily *n-1* times.
```suggestion _VALID_URL = r'https?://cooking\.nytimes\.com/(?:guid|recip)es/(?P<id>\d+)' ```
even if `clip_info` exist, it's possibe that there is no format there(empty `contentUrl*` values).
this does not handle the case where `contentUrl` value is `None`.
no need for parenthesis(prefered way in python), unless it's needed. ```suggestion if not clip_info: ```
check the other instances as well, those where just examples.
it's also to cover the other possiblility, both `error` and `contentUrl*` exist, the response may change for different errors. so in general it's prefered to check the existance of formats before raising an error(ex: [YahooIE](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/yahoo.py#L278-L279), [GoogleDriveIE](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/googledrive.py#L249), etc...).
the process to extract the format and the thumbnail is similar, so these part needs to be abstracted to remove duplication.
kind of, i will try to abstract it further later(the `source` format also shares a bit code with this part).
no need to set `fatal=True`, this is the default.
same if `clip_info` is `None`.
remove, youtube-dl will create the `thumbnail` key from the `thumbnails` list.
```suggestion 'timestamp': float_or_none(clip_info.get('created'), 1000), ```
will return `https://medal.tv/users/None` if an `author id` couldn't be extracted.
check the existence of the `contentUrl` before adding the format.
remove, will be set by youtube-dl using `formats` dict.
same if one of the values is `None`.
`<script>.*` this part does not play a roll in the regex, may be change to `<script[^>]*>\s*(var\s*)?`. Split Long lines(longer than 80 columns). ```suggestion hydration_data = self._search_regex( r'<script>.*hydrationData\s*=\s*({.+?})\s*</script>', webpage, 'hydration data', default='{}') ```
try put value that are used multiple times in a variable(ex: `author_info.get('id')`). i think the `{}` format is not supported in python 2.6.
```suggestion author_info = try_get(parsed, lambda x: list(x['profiles'].values())[0], dict) or {} ```
```suggestion clip_info = try_get(parsed, lambda x: x['clips'][video_id], dict) or {} ```
error into a common variable and reuse it. it would be better to check for error only after not been able extracted mandatory information(`video_id`, `title` and `formats`). ```suggestion error = clip_info.get('error') if error: ```
breaks the extraction if `clips` is `None`.
will break the extraction if `profiles` is empty or `None`.
```suggestion thumbnail = self._html_search_meta(['og:image', 'twitter:image'], webpage) ```
```suggestion 'thumbnail': r're:^https?://.*\.jpg', ```
Use `xpath_text` instead.
These are not identical since the latter does not cover `value` attribute.
do not use the program title as a description.
```suggestion if episodes: ```
I think you mean: ```suggestion is_video = m_type in ('video', 'program/video') ```
```suggestion raise ExtractrorError('No episodes returned for program with ID: %s' % program_id, expected=True) ```
the playlist description is not mandatory, if it's not supplied by the website, then do not fill it.
you have to test the code, and see if the result is usefull, this code will result in error message similar to this: ``` Cannot download episode: {'data': {'episodes': [{'pgm_gr_id': 'dwc', 'image': '/nhkworld/en/ondemand/video/9999011/images/buCOFSNvcN0ctbWvHm25sLdGTOKRUX32w7YLHHTS.jpeg', 'image_l': '/nhkworld/en/ondemand/video/9999011/images/dYFYZODb5cmlf6efJ9Cgb4bWIouBc6M83TEhRMFZ.jpeg', 'image_promo': '', 'voice_lang': 'en', 'caption_langs': [], 'voice_langs': ['en'], 'vod_id': 'lpZXIwaDE6_Z-976CPsFdxyICyWUzlT5', 'movie_lengh': '2:28', 'movie_duration': 148, 'analytics': "[nhkworld]clip;Dining with the Chef_Chef Saito's Family recipe: MENCHI-KATSU;en,001;9999-011-1970;", 'title': 'Dining with the Chef', 'title_clean': 'Dining with the Chef', 'sub_title': "Chef Saito's Family recipe: MENCHI-KATSU", 'sub_title_clean': "Chef Saito's Family recipe: MENCHI-KATSU", 'description': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'description_clean': 'Umami juice splashes out from crunchy Panko batter! Menchi-katsu is a mouthwatering deep fried meat ball from Japan. [From "Dining with the Chef"]', 'url': '/nhkworld/en/ondemand/video/9999011/', 'category': [17], 'mostwatch_ranking': 136, 'related_episodes': [], 'tags': ['washoku'], 'chapter_list': [], 'life': 0, 'life_category': [], 'promotion': []}]}} ``` the error message is meant to be readable and usefull to the user, that is the point that i'm trying pass.
I think that should be outside of this method because the value is also recalculated before calling the `_parse_episode_json` method.
it would be better to order the function parameter based on their importance(i think the most important one is `m_id`).
```suggestion class NhkBaseIE(InfoExtractor): ```
```suggestion from .nhk import ( NhkVodIE, NhkVodProgramIE, ) ```
> expected is now false no need to type `expected=False`, it's the default. > Oh I see what you mean about --dump-pages, it will print the API response. I guess that could mean I can remove the JSON from the string as well. Exactly.
> Does that seem more helpful? If this error was to actually happen, it would mean that what the API returns has changed beyond what we expect. When the happens, I think all the user can do is report an issue here. there is a generic option to do this(`--dump-pages`), and we would ask to the user to use this option if needed. > Is it typical practice to say in error messages "report this error to youtube-dl"? If so I can add that as well. you can do that, by removing `expected=True` from the options passed to `ExtractorError`,
this will break the extraction if `pgm_id` or `pgm_no` doesn't have a value(None)
- we don't directly use `str` in the project, instead we use `compat_str` to keep the code compatible with all support versions of Python. - when you call `str/compat` on a dict it won't give a good result, so, i think it should be ommited.
you already have `pgm_id` and `pgm_no` variables now.
there is no need for an `else` block.
```suggestion if not (pgm_id and pgm_no): ```
create a seperate extractor.
extract into a seperate method to reduce code duplication.
```suggestion from ..utils import ExtractorError ```
Second part should be allowed to be empty.
Must be int.
Lack of data is denoted by `None` not `0`.
```suggestion for kind, vid in re.findall(r'if\s+\(\s*imageQualityType\s*==\s*\'([^\']+)\'\s*\)\s*{\s*video_id\s*=\s*"(\d+)"', webpage): player_path = '/intent?id=%s&type=url' % vid ```
1. Relax regex. 2. Do not capture empty string.
`id` must not contain any irrelevant parts.
You've already checked for `qualityItems` two lines above.
`None` is not a video id.
This is not equivalent, not matching `qualityItems` does not mean it will match `media` or `quality`.
Regex must look for this kind of prefix.
Use character set instead.
Do not change this.
Do not mix different scenarios in single regex. Upload date should be extracted first. If this fails it should fallback on ago pattern extraction.
DRY 105, 107.
still fails if `uploader_data` not available.
`created_at` still available in the API response, extract `timestamp` field from it. and also there more metadata that can be extracted (`view_count`, `album`, etc...).
i think it would be better to keep the old variable name(`song`) as this response contain most of the data available in the old `API` response.
fails if `profile` is not available.
no need to call `_extract_m3u8_formats` method for HLS Media manifests.
use `query` argument.
extract all formats.
Breaks if `...['resolutions']` is not a `list`.
[The website of TASVideos](http://tasvideos.org) writes the name with uppercase `TAS` ```suggestion class TASVideosIE(InfoExtractor): ```
- `'%s/' % show_id` part is also repeated twice. - as i said before, the `info` request should not be break the extraction it fails.
`video_id` is not actually a video id, i think the variable name should be changed.
as the base URL and the paths are known beforehand, there is no need to use `urljoin`.
when you're using the kwargs in order, you don't have to use the kwarg name. you can just do(after moving `data` extraction to `_download_info`): ```suggestion return self.playlist_result( entries, show_id, show_info.get('title'), show_info.get('description')) ```
reduce the indentation. ```suggestion episodes = season.get('episodes') or [] for episode in episodes: ```
- if part of the API response(`data`) is used multiple times, `show_infos['data']` should not be repeated for every access. - all API response is encapsulate the information under a `data` dict, so it should be moved to the common code(`_download_info` method).
`(?P<id>[^/]+)` should not be mandatory, the information needed for extraction rely only on `show_id`(for API calls).
`playlist_description` can be extracted from the same data.
should not be fatal, the `playlist_title` and `playlist_description` are not mandatory.
this code is almost the same as the one `VVVVIDIE`, move into a seperate method in base class, and use it in both extractors.
still won't allow for https://www.vvvvid.it/show/156/psycho-pass/?foo=bar, `https?://(?:www\.)?vvvvid\.it/` can also be shared between extractors.
video_id(slug) is not the right value to be used for `playlist_title`, it can be extracted from the show info request with the description, otherwise drop the `playlist_title`.
I think it would be better to rename the extractor to `VVVVIDShowIE`.
use single quotes consistently, check for the availability of value before using them(`season_id` and `video_id`). `/title` part is not needed, can be simplified into(after checking for the values): ```suggestion video_url = '/'.join([url, season_id, video_id]) ```
try not to break the extraction when some info is missing(ex: if a `season` doesn't have `episodes`).
should match urls with query, fragment, etc...(https://www.vvvvid.it/show/156/psycho-pass?foo=bar, https://www.vvvvid.it/show/156/psycho-pass/, ...).
it would be preferable to parse the other metadata(`title`, `thumbnail`,etc...) for use with `--flat-playlist` option, otherwise just use the `url_result` method.
remove the duplication as much as possible(`https://www.vvvvid.it/vvvvid/ondemand/%s/` is always used).
would still fail if `episodes` isn't available for a perticular `season`.
```suggestion if not (season_id and video_id): ```
no, the title should match what is used in `VVVVIDIE`, and also this would prevent users from customizing the output filename(ex: the user may prefer to use `ep - title`).
everything needed to construct the nested playlists is already available in the `seasons` request, so it's just wasteful, to request the same data for every season again and again.
`playlist_title` should be set directly, it should be set in the playlist result title.
it's about the way you're setting `playlist_title`. `playlist_title` should not be set directly, it should be set in the playlist result title(by using the `playlist_result` method or using the playlist result type).
OK, so, it appears that directly nested playlists are not currently supported. unfortunately, at this point, I think reverting to the initial code(setting `playlist_title` directly) until there is proper support for nested playlists is in place is better than adding more wasted requests(i will try to reduce the size of the payload for video URLs as well).
for now, it would be better to go with setting `playlist_title`. what I mean by proper support is change would that involves modifying `__process_playlist` and other related methods instead of faking some of the fields. and I'm sorry for suggesting something that appeared to not be ready yet(however I still think in the long run it would be the best option).
the use of `yield` is not too important in this case, as it's not related to limiting the number of requests.
Use `_download_webpage` since you're not using handle anynway.
There is no point to use `get` here.
`if not videos:`.
Hmm, okay. Not the best soulution but it's also implemented in other extractores
Does youtube-dl passes to this location? You already set here the supported url: https://github.com/ytdl-org/youtube-dl/blob/b74e1295cfbd5c0a2d825053033af65285804246/youtube_dl/extractor/plutotv.py#L17
`ad_free_formats` is never empty here since `_sort_formats` will throw if's empty.
This could be moved several lines up.
`re.search` should be used instead as one may need partial matches.
For now this should not be printed or only printed in `verbose` mode.
Split into two `if` statements for uniformity with other places in code and better readability.
If such reconstructed URL is already processed it should be skipped.
This must be assert not exception.
This won't work for URLs with query.
Details on what? That's exactly how browser will work - if one requests to open http URL then http URL must be opened following by a redirect to https (if any). youtube-dl intention is to mimic browser's behavior in the first place, not introducing some levels of protection from leaking or whatsoever.
Do you even read what's addressed to you? If one explicitly wants to access via http you **must not** force it to https. Current code uses http because at the time of writing such URLs were not available via https at all. I'm not going to argue further - your either do what's requested or this will be rejected.
Use scheme of the input URL.
Do not touch this.
Do not touch formatting.
Do not escape quotes inside triple quotes.
215, 220 DRY.
Do not mix unrelated changes in single PR.
Add `<iframe` part to the regex and move this whole pattern to into previous `_search_regex` call.
Maybe could you return instant the value in Line 71: https://github.com/ytdl-org/youtube-dl/blob/9e995bdc4f2a875c25d4fe96de2c13b60c13e2e4/youtube_dl/extractor/gimy.py#L71 Instead of writing in a var and return the var.
yes, the function is used only once.
```suggestion subtitles.setdefault(sub_lang, []).append({ 'ext': determine_ext(sub_url), 'url': sub_url, }) ```
at this point `sub_lang` is guaranteed to be in the subtitles dict.
instead of this it's better to just put result of `determine_ext` in a variable and use it.
```suggestion subtitlesArray = video_data.get('subtitlesArray') or [] subtitlesArray.append({'url': video_data.get('subtitles')}) for subtitle in subtitlesArray: ```
use the common naming convention in Python.
```suggestion for k in ('subtitles', 'subtitlesUrl'): subtitles_array.append({'url': video_data.get(k)}) ```
use the extension extracted from `determine_ext`.
Keys are used for identification, names are used for display. Here key is at least required, name is optional but may be useful for supported postprocessors page generation and for overall symmetry with info extractor API.
`pp` should be matched against the list of available post-processors that should be build similarly to `gen_extractor_classes`.
`PP_NAME` should not be used for identification, instead `pp_key` similar to `ie_key` should be used.
`args` may be `None` here.
Post-processors are already identified by `key` in API same should be used here.
`cli_configuration_args` is supposed to be used with `params` dict, if you are working with `postprocessor_args` on your own you should not use it.
`EmbedThumbnail` does not start with ffmpeg as it uses not only ffmpeg. Either option is fine - this is just a human readable name after all.
Something like this (not tested): ```python args = self._downloader.params.get('postprocessor_args') if args is None: return default if isinstance(args, (list, tuple)): # for backward compatibility return args assert isinstance(args, dict) pp_args = args.get(pp_key) if pp_args is not None: return pp_args pp_args = args.get('default') # for backward compatibility if pp_args is not None: return pp_args return default ```
This should not affect child post-processor implementations names. Moreover there is even no guarantee that each child post-processor's name will start with ffmpeg at all as there may be custom post-processors.
This should be deduced from class name in `PostProcessor.PP_NAME` similarly to `InfoExtractor.IE_NAME`.
Actually, it's not. You still ignore `default` parameter of `_configuration_args`.
```suggestion if not (playlist_files and isinstance(playlist_files, list)): ```
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
this request should be done after sorting the formats(the extractor should start with mandatory info(id, title and formats) and then extract the optional fields).
will fail if `streaming` value is `None`(`null`), same applies to `show` and `video` variables.
try to reduce repetition(`https://gw.api.animedigitalnetwork.fr/`).
additional info can be extracted from `video` request.
the same for `streaming` key.
should not break the extraction if `subtitles` key is not present.
why not just pass the `video_id` instead of modifying `msg_template`.
both are know beforehand, so there is no need to use `urljoin`.
falback to a static URL.
- you can just do `e.cause.read()` instead of `e.cause.fp.read()`. - extract `message` into a variable as it's going to be used either way.
example for the URL of the test case: https://gw.api.animedigitalnetwork.fr/video/7778
why is 403 error catched? `expected_status` specify only allowed error codes, there is no need to specify 200 status code.
should be in the `else` block of the `for` loop.
`break` as soon as the `links_data` has been obtained.
surround only the part that will threw the exception.
```suggestion for _ in range(3): ```
when subfield(`user`) is used multiple times, extract the value into a variable and reuse it.
i think it's better to treat errors in the except block.
no, there is `season_number`, `episode_number`, `timestamp`, etc...
then, the extraction shoud stop when geo-restriction is detected as there is no point in making further requests.
Let's do it, then, if you like.
Yes, much more useful there.
`SearchInfoExtractor`? What happened with this equivalent change in yt-dlp (extractor/common.py)? ``` Instances should define _SEARCH_KEY and optionally _MAX_RESULTS """ _MAX_RESULTS = float('inf') ``` If it didn't break anything we might as well do it. If the CI tests break in new ways then back out.
As in, me personally? I think I knew that, but protocol.
Fair enough. It can be done in some pull of useful things from yt-dlp's common.py.
```suggestion _VALID_URL = r'(?P<mainurl>https?://(?:www\.)?daserste\.de/[^?#]+/videos(?:extern)?/(?P<display_id>[^/?#]+)-(?:video-?)?(?P<id>[0-9]+))\.html' ```
Basically, there should not be suffixes like ` | ARD Mediathek` so that title extraction should be fixed.
- does not match the correct image. - you're not following the coding conventions.
will easily match outside the element.
I suggest `default=video_id` in the `_html_search_regex` call.
I suggest `fatal=False`
`_html_search_regex` will just error out if you don't provide a `default` or set `fatal=False`.
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
split long lines.
use `_hidden_inputs` method.
will be extracted from the URL.
the fallback will never be reached this way.
`fatal=False` is already the default.
- extract mandatory information first. - incorrect fallback.
```suggestion 'thumbnail': r're:^https?://.+\.png', ```
- Alphabetic. - Vertical Hanging Indent.
```suggestion _VALID_URL = r'https?://(?:www\.)?samplefocus\.com/samples/(?P<id>[^/?&#]+)' ```
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
- incorrect fallback. - use `_hidden_inputs` method.
i did request multiple times to check the fallbacks, check that the code is working before pushing.
Extractor should not return `None`.
Do not shadow outer names.
don't Python builtin function names as variable names(`id`).
keep the line the same as it was(broken in two lines).
keep the same GQL query style(it was intentional).
this will break the extraction if `slug` is null.
the extraction can work just by having the track URL, so the entry `id` is an optional field, and the extraction should proceed even if an entry id has not been found.
the assumption was based on the fact that the cookie is set programatically and not using `Set-Cookie` headers, but as it's undefined wheather the value can change or not, i guess it's better to set the cookie value for every request.
```suggestion if gql_auth: ```
> I can't find any header setting function in `common.py`. `_real_initialize` is used for extractor initialization, it's up to every extrator to setup it's own initialization. > While it would be possible to set something like self.gql_auth_header in _real_initialize, this feels to me like adding gratuitous complexity. there is no complexty, you will set the header once and reuse them in every subsequent request, instead of constructing the same headers over and over again for a large collections.
Technically, cookie may change between requests so that moving `Authorization` calculation in `_real_initialize` may result in expired token.
this should be done once(in `_real_initialize`).
Just noting here that this still uses `api.viervijfzes.be` and `api.goplay.be` does not work (yet).
```suggestion (?:www\.)?(?P<site>vier|vijf|goplay)\.be/ ```
I just noticed the `video_id` is also present in the `data-io-video-id` attribute for the `video` element on the page, in a format without hyphens. This can easily be converted to a regular UUID and would not require to iterate the playlists below. For example (untested): ```python import uuid video_id = uuid.UUID(self._html_search_regex( r'data-io-video-id="([^"]*)"', webpage, 'video_id')) ```
```suggestion _VALID_URL = r'https?://(?:www\.)?(?P<site>vier|vijf|goplay)\.be/video/(?P<series>(?!v3)[^/]+)/(?P<season>[^/]+)(/(?P<episode>[^/]+)|)' ```
Those changes work correctly. ð
In case of a single episode (or a movie) this fails as there's no episode information in the title. This can be tested with https://www.vier.be/video/trio/trio/trio-s1-aflevering-1 for example.
> ```python > import uuid > video_id = uuid.UUID(self._html_search_regex( > r'data-io-video-id="([^"]*)"', > webpage, 'video_id')) > ``` It's actually only there once playback has started. Before it starts the `video_id` is not set in the `video` element yet.
I have yet to look into it but I believe this code has an issue for series with more than 9 episodes. E.g. for https://www.goplay.be/video/de-container-cup/de-container-cup-s2/de-container-cup-s2-aflevering-10#autoplay (and any episode between 10 and 19) it downloads episode 1.
`_sort_formats` should always be called for non youtube videos(to break the extraction with a proper message when no formats has been extracted).
remove this line(will be calculated automatically from `timestamp`).
same for `episode_number`(both lines where highlighted).
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
will return invalid URL if `search_url` is `null`.
there is not need to iterate twice, one iteration is enough while checking for the existence of mandatory values and filling the entries array.
`_` doesn't have to be escaped.
not mandatory(should not break the extraction if it couldn't be extracted).
again, use `_download_json` `query` argument.
no longer needed.
extraction should not break if an episode or all episodes couldn't be extracted.
there is not need for excess verbosity.
try to account for future content(there a possibilty that future content would be more then that limit).
incorrect URLs for Cook's Country.
make one of the tests an `only_matching` test.
`objectID` does not match the id from `AmericasTestKitchenIE`.
still the check for element class is missing.
`skip_download` is needed for the test to pass similar to the first test.
keep similar checks for element class and `get_element_by_class` value.
the `class` attribute of the `a` HTML element.
```suggestion r'<a[^>]+class="tag-[^"]+"[^>]*>([^<]+)</a>', tag_list ```
after the change to check the `a` HTML element `class` attribute, this is no longer needed.
Don't capture groups you don't use.
Do not remove the old pattern.
Video URL is mandatory. Read coding conventions.
Move before youtube-dl imports.
Move this inside `_download_webpage` call.
Move query to `query` parameter.
More or less correct not taking code duplication into account. >Aside from the fixes, should we loop and display all of the errors (instead of just `errors[0]`)? You can if this make sense. >Also, is `errors` always a list (remove the `isinstance` check)? There is no such guarantee.
You should not throw everything under try/except. Also read coding conventions.
Read coding conventions.
```suggestion _NETRC_MACHINE = 'animedigitalnetwork' ```
should be done once when the extractor is initialized.
- use single quotes consistently. - i think it would be better to keep errnote closer to note.
use the common Python variable names convension.
```suggestion self._API_BASE_URL + 'authentication/login', None, 'Logging in', data=urlencode_postdata({ ```
This is never reachable.
Remove excessive verbosity. Read coding conventions.
This must be done right after title extraction.
Do not shadow input `url`..
There must be two different extractors: for videos and for playlists.
This is mandatory. Read coding conventions.
Playlist title is optional.
This is default.
keep the old fallback code.
use `datetime` class directly.
that's why i think it's better to directly use python's datetime builtin module to parse the non standardized format.
There's a check in the caller for missing formats, so it's better to just return an empty format array.
I know what you mean :) I think `default=None` is actually better (without `fatal=False`). `fatal=False` makes it emit a warning and return `None`, while `default=None` will just silently return `None`. For optional metadata like `duration`, I think it's better not to have a warning.
If you have `default`, `fatal` is not used.
Id and title are mandatory for video. Read coding conventions.
There should be two different extractors: one for video, one for playlist.
You're not guaranteed `thumbnails` is a list.
Do not remove the old pattern.
no longer exists in the webpage.
use the more generic domain(Google Drive's domain).
add `name` meta tag value as fallback.
```suggestion if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403: ```
1. Lack of data is denoted by `None`, not 0. 2. int_or_none.
check the result of the fallback(in comparision with the primary source).
combine into a single call to `_html_search_meta`.
i think it would be better not to silence the warning when the description is not found(it's not expcted to not been able to extract description, failing would likely indicate that a change in the website has happened).
`strip_or_none` no longer needed.
`twitter:title` meta tag does not contain ` | Dagelijkse kost` in the end. if you want to keep `og:title` then you can use `remove_end` function to clean the title.
check that extracted description is what is expected(should not contain html tags).
`twitter:description` and `description` meta tags are also available.
match only supported URLs.
use single quotes consistently.
parentheses not needed.
fallback to other available values.
use `get_element_by_class` function.
use `_match_id` method.
extract mandatory information(title and formats) first, and sort formats.
should not break the extraction if the field is not available.
extract mandatory information(title and formats) before non mandatory info(description, thumbnail, ...)
as you can see it's from the video data, `mediaFiles` entries has the same fields. also extract the other matadata about the formats(`filesize`, `tbr`, ...)
> You mean both hls and hls_sec? yes, extract both `http` and `https` formats.
try to extract both HLS urls.
try to parse multiple formats, set `vcodec` to `none`.
accept `audio` `mediaType`.
```suggestion _VALID_URL = r'https?://(?:4d\.rtvslo\.si/(?:arhiv/[^/]+|embed)|www\.rtvslo\.si/(?:4d/arhiv|mmr/prispevek))/(?P<id>\d+)' ```
use `query` argument.
just use a hardcoded value for now.
use `getRecording` and remove `getMedia` call.
```suggestion unified_timestamp, try_get, ExtractorError ```
The filename will end up as `.list.mkv` which doesn't make sense. It should be either `.mkv.list` or just `.list`
Best not to make unrelated changes in a PR. Same issue below. Also these changes often don't obey the linter
better to make this an optional argument than changing the code in the other classes
This is already matched.
This does not matter, if you open https://youtu.be/BaW_jenozKc there is a video embedded. If you open https://redirect.invidious.io/watch?v=BaW_jenozKc there is no video embedded. >used for URLs floating over the net to share youtube videos. Prove that.
At the moment of writing, https://invidio.us/watch?v=BaW_jenozKc had BaW_jenozKc embedded so this patterns is kept for compatibility with old URLs possibly still floating over the net. https://redirect.invidious.io/watch?v=BaW_jenozKc does not embed any video and apparently never did thus has nothing to do in this case.
There is no video on this page.
i think it would better to put the default value once in the in referer policy processing function.
the name is too generic(there multiple ways that the website whould advertise the referer policy).
it would be better to use the terminology used in the specification.
don't make unrelated changes.
the `get_referrer_url` function can return None, there is not point in setting `Referer` to None.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
no, as i said you would extract the metadata and return immediately.
this is too generic and can lead to picking the wrong video.
should not break the extraction if it wasn't able to parse json data.
this can match outside the script element and can be easily broken if an attribute follow `data-json`.
use `_html_search_regex` and get rid of `unescapeHTML`.
extract mandatory information before optional information.
don't use both `fatal` and `default`.
parse json data and extract `versionID` from there.
do not set both `thumbnail` and `thumbnail`.
1. DRY. Generalize with download sleep interval. 2. Do not sleep if interval is invalid. 3. There must be min and max intervals for randomization.
This does not make any sense since you already sleep in `_request_webpage`.
as the `iptv-all` gives better quality in general, just drop `_MEDIA_SETS` from the extractor and just use the default value from `BBCCoUkIE`.
These looks like candidates for generalization and extracting into a separate method.
```suggestion settings = self._parse_json(settings, playlist_id, transform_source=js_to_json, fatal=False) ```
These looks like candidates for generalization and extracting into a separate method.
Why not just use regular expression? It's much easier to understand and figure out the original intention after URL pattern changes in future.
Then something like `float_or_none(bitrate, scale=1000) or parse_bitrate(bitrate)` should be used. Current approach also won't work if `bitrate` is not evenly divisible by 1000.
`title` can't be `None`.
`id` can't be `None`.
Resolution is used for textual representation of width and height and typically automatically deduced from these meta fields.
This must be inside a dict corresponding to `info_dict` key. `python -W error .\test\test_download.py TestDownload.test_BBC_13` must succeed.
Use outer double quotes instead of escaping single inner quotes.
Do not use exact URLs in tests.
>How about a regex there, say `r're:https?://.+/.+\.jpg'`? This is fine.
Jython issue is intermittent you can ignore it. Also there is no need to rebase as GitHub does this just fine.
The point is to always have copy-paste ready string literal in tests for debugging/testing purposes.
Breaks. Read coding conventions.
Do not reformat code and remove irrelevant changes.
`update_date` is pointless if `timestamp` is provided.
This does not make any sense.
Read coding conventions on optional and mandatory data extraction.
Most likely you don't need this.
Course extraction must be in a separate extractor.
This is total mess. You don't need to used braces for single character, you don't need to escape forward slash, you don't need positive lookahead here, you don't need optional groups.
Do not remove. This still can be used directly.
You don't need to specify a list comprehension for `''.join()`, a generator comprehension is faster anyway (implied inside parenthesis), and uses far less RAM.
Playlist title is optional.
Read coding conventions.
`_parse_json`. Read coding conventions.
This will break `--max-download`.
`autonumber` is not reset to zero in the first place.
<80 character limit is a soft limit; you are allowed to over 80 characters limit since it doesn't make it hard to read
Later, you're going to rely on `function/0/...'.split('/')` having at least length 8 (2 + 5 + 1). Make sure it has: ```suggestion video_url = self._html_search_regex(r"video_url:\s+'(function/0/(?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` Or, as `function/0` is never used (I think?), ```suggestion video_url = self._html_search_regex(r"video_url:\s+'function/0/((?:[^/']+/){5,}.+?)',", webpage, 'video_url') ``` and later (l.66 currently) ```diff - urlparts = video_url.split('/')[2:] + urlparts = video_url.split('/') ```
```suggestion new = '' ```
Redo a confusing suggestion that was accepted: ```suggestion retval = '' for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
```suggestion if title[t_len] == '-' and title[:t_len] == title[t_len + 1:]: result['title'] = title[:t_len] ```
```suggestion display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False) ```
```suggestion modlicense = compat_str(4 * abs(fronthalf - backhalf)) retval = "" for o in range(0, center + 1): for i in range(1, 5): retval += compat_str((int(license[o + i]) + int(modlicense[o])) % 10) ```
```suggestion get_element_by_class, int_or_none, ```
If the search was `fatal`, `kvs_version is None`: ```suggestion if not (kvs_version and kvs_version.startswith('5.')): ```
Import `sanitize_url` from `..utils` and: ```suggestion thumbnail = sanitize_url(thumbnail) ``` Or just allow a scheme-less URL to be handled by `info_dict['thumbnail'] = sanitize_url(thumbnail)` in YoutubeDL.py: ```suggestion ```
```suggestion center = len(modlicense) // 2 ```
As we're always going to want `type` later (but that's a Python keyword, so rename it): ```suggestion main_id, type_ = re.match(self._VALID_URL, url).group('id', 'type') ```
* relax REs -- allow whitespace and either '' or "" * use '' * make non-fatal * extract in one go ```suggestion uploader = self._html_search_regex(r'''(?s)<span\b[^>]*>Added by:\s*</span><a\b[^>]+\bclass\s*=\s*["']author\b[^>]+\bhref\s*=\s*["']https://thisvid\.com/members/([0-9]+/.{3,}?)\s*</a>''', webpage, 'uploader', default='') uploader = re.split(r'''/["'][^>]*>\s*''', uploader) if len(uploader) == 2: # id must be non-empty, uploader could be '' uploader_id, uploader = uploader uploader = uploader or None else: uploader_id = uploader = None ```
Perhaps: ```suggestion idx = i if idx == o: idx = l elif idx == l: idx = o new += newmagic[idx] ```
```suggestion from ..compat import ( compat_str, ) from ..utils import ( sanitize_url, ) ```
```suggestion 'noplaylist': True, ```
To be used later: ```suggestion clean_html, get_element_by_class, sanitize_url, url_or_none, urljoin, ```
```suggestion compat_str, compat_urlparse, ```
```suggestion next_page, 'next page link', group='url', default=None)) ```
```suggestion r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) if next_page else None) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin( url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( ```
```suggestion next_page = urljoin(url, next_page and self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None)) ```
```suggestion if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
Style: ```suggestion modlicense = license.replace('$', '').replace('0', '1') ```
Style: ```suggestion return '/'.join(urlparts) ```
Fix: ```suggestion urlparts = video_url.split('/') ```
As above: ```suggestion 'display_id': main_id, ```
Doesn't match, and `display_id` shouldn't be a URL anyway: ```suggestion ```
Fix: ```suggestion video_id = self._html_search_regex(r'''video_id:\s+'([0-9]+)',''', webpage, 'video_id') video_url = self._html_search_regex(r'''video_url:\s+'function/0/(https?://(?:[^/']+/){5,}.*?)',''', webpage, 'video_url') ```
Add check for private videos: ```suggestion video_holder = get_element_by_class('video-holder', webpage) or '' if '>This video is a private video' in video_holder: self.raise_login_required( (clean_html(video_holder) or 'Private video').split('\n', 1)[0]) # video_id, video_url and license_code from the 'flashvars' JSON object: ```
``` thumbnail = self._html_search_regex(r"preview_url:\s+'((?:https?:)?//media\.thisvid\.com/.+?\.jpg)',", webpage, 'thumbnail', fatal=False) ```
``` video_id = self._html_search_regex(r"video_id:\s+'([0-9]+)',", webpage, 'video_id') ``` And `\s+` in the next 2 lines, too.
Add redirect to main page for embeds to get more metadata: ```suggestion title = self._html_search_regex(r'<title\b[^>]*?>(?:Video:\s+)?(.+?)(?:\s+-\s+ThisVid(?:\.com| tube))?</title>', webpage, 'title') if type_ == 'embed': video_alt_url = url_or_none(self._html_search_regex(r'''video_alt_url:\s+'(%s)',''' % (self._VALID_URL, ), webpage, 'video_alt_url', default=None)) if video_alt_url: webpage = self._download_webpage(video_alt_url, main_id, note='Redirecting embed to main page', fatal=False) ```
Use tests with currently valid URLs! (Did those go 404 just now, or how did you test it?) ```suggestion 'url': 'https://thisvid.com/videos/sitting-on-ball-tight-jeans/', 'md5': '839becb572995687e11a69dc4358a386', 'info_dict': { 'id': '3533241', 'ext': 'mp4', 'title': 'Sitting on ball tight jeans', 'thumbnail': r're:https?://\w+\.thisvid\.com/(?:[^/]+/)+3533241/preview\.jpg', 'uploader_id': '150629', 'uploader': 'jeanslevisjeans', 'age_limit': 18, } }, { 'url': 'https://thisvid.com/embed/3533241/', 'md5': '839becb572995687e11a69dc4358a386', 'info_dict': { 'id': '3533241', 'ext': 'mp4', 'title': 'Sitting on ball tight jeans', 'thumbnail': r're:https?://\w+\.thisvid\.com/(?:[^/]+/)+3533241/preview\.jpg', 'uploader_id': '150629', 'uploader': 'jeanslevisjeans', 'age_limit': 18, ```
Untested, but this should catch the `//playlist/nnn/video/video-id` pattern. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed|(?P<pl>playlist))(?(pl)/\d+/video)/(?P<id>[A-Za-z0-9-]+/?)' ``` Full handling of playlists needs a separate set of changes, because the playlist format is a page with a video and a playlist. The yt-dl solution to this is to extract the playlist unless `--no-playlist`, so this pattern has to be caught in the playlist extractor and rewritten to be passed to this extractor in case `--no-playlist`.
Define a `thisvid:` URL scheme to be used as an API: ```suggestion _VALID_URL = r'(?:thisvid:|https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/)(?P<id>[A-Za-z0-9-]+/?)' ```
Use `sanitize_url()`, or let the core code, which does so, fix it.
```suggestion ThisVidMemberIE, ThisVidPlaylistIE, ```
Handle playlist pages that don't have explicit pagination links. The playlist pages are like `/playlist/pl_id/video/whatever`, `/playlist/pl_id/video/whatever/2`, `/playlist/pl_id/video/whatever/3`, ... ```suggestion if next_page: # member list page next_page = urljoin(url, self._search_regex( r'''<a\b[^>]+\bhref\s*=\s*("|')(?P<url>(?:(?!\1).)+)''', next_page, 'next page link', group='url', default=None) # in case a member page should have pagination-next with empty link, not just `else:` if next_page is None: # playlist page parsed_url = compat_urlparse.urlparse(page_url) base_path, num = parsed_url.path.rsplit('/', 1) num = int_or_none(num) if num is None: base_path, num = parsed_url.path.rstrip('/'), 1 parsed_url._replace(path=base_path + ('/%d' % (num + 1, ))) next_page = compat_urlparse.urlunparse(parsed_url) if page_url == next_page: next_page = None ```
Support full playlist handling: * revert single video pattern * also, final `/?` is unnecessary. ```suggestion _VALID_URL = r'https?://(?:www\.)?thisvid\.com/(?P<type>videos|embed)/(?P<id>[A-Za-z0-9-]+)' ```
`compat_str()`, here and in l.96.
The playlists are public: there should be tests for them. Watch this space.
Yep. Just that yt-dl uses `'string'` where a lot of Python code has the equally valid `"string"`. You could also have `'...(%s)...' % (kvs_version, )`.
``` display_id = self._search_regex(r'<link\s+rel\s*=\s*"canonical"\s+href\s*=\s*"%s"' % (self._VALID_URL, ), webpage, 'display_id', fatal=False)
Leave a line, and prepare for the pagination code. ```suggestion import re import itertools ```
Move this into `_VALID_URL`.
Should not be fatal.
Remove all unrelated changes.
Do not capture groups you don't use.
Do not use `requests`.
Do not remove existing tests.
DRY: 61-63, 66-68.
You should use `self._request_webpage`, preferably with a HEAD request
Avoid unrelated changes.
```suggestion height = int(self._search_regex(r'(\d+).mp4', format_url, 'height', default=360)) ```
Parse height with regex instead.
this is basically the same code repeated twice. It can be generalized
That's all correct code [53-93], do not remove it. Fix `add_m3u8_format` instead.
`format_id if format_id else 'hls'` can be shortened to `format_id or 'hls'`.
>`if...: ...; else: ...` is a better representation of the logic than `if: ...; continue; ...`. An `elif ...:` might need to be added in future. That's questionable and the matter of preference. Anyway one should avoid mixing fixes/new features and refactoring in the same commit.
Avoid unrelated changed.
Absolutely right, and I even looked it up to check, but probably only noticed the missing `default` :(( Must be going blind. However the `default=None` would override the warning if that was desired.
Would a format be clearer? ``` 'https://www.newgrounds.com/%s/%s' % (path, media_id),
If they send (eg) 480p, we won't find it; also we don't know that the URL is there. How about s/t like: ``` for resolution in filter(lambda x: re.match(r'\d+p', x), json_data['sources'].keys()): url = try_get(json_data['sources'][resolution], lambda x: x[0]['src'], compat_str) if not url: continue formats.append({ 'url': url, ... ``` Or perhaps try anything that parses as a resolution: ``` for resolution, parsed_resolution in filter(lambda x: x[1], map(lambda x: (x, parse_resolution(x),), json_data['sources'].keys())): url = try_get(json_data['sources'][resolution], lambda x: x[0]['src'], compat_str) if not url: continue formats.append({ 'url': url, 'height': parsed_resolution.get('height'), ... ```
Why not give the search a name instead of `''`, say `'media url'`? I know it was like that before, but it assists debugging since the name appears in the `-v` output.
Sorry, I should have noticed: this will crash if the thumbnail isn't found, and it's not a mandatory field. Add `default=None`, or `fatal=False` to report a warning and return `None`. Similarly with `'description'` below. Although the `_og_search_xxx()` methods could have appropriate defaults, I can see that might lead to a different sort of confusion.
That's OK as long as `json_data['sources']` (no need for `.get()`) is just a dict of media formats. The `_check_formats()` method, which I think is called later, will strip out any formats with null `url`. In case they start sending, say, '4k' rather than '2160p', this would be better: ``` ... for source in media_sources: format = { 'format_id': media_format, 'url': source.get('src'), } format.update(parse_resolution(media_format)) # next line only if height should sort ahead of bitrate, etc (see _sort_formats()) format['quality'] = format.get('height') formats.append(format) ```
it might be better to use the same condition that is used by [PeerTube's client code](https://github.com/Chocobozzz/PeerTube/blob/d55c466e1caff9082e8ca1734fbf079ac92e12b4/client/src/app/%2Bvideos/%2Bvideo-watch/video-watch.component.html#L216)
`unescapeHTML(n[1])`? (`utils.py`) See also line 59 below.
By convention, a title is required. Here, if the title isn't found, `.strip()` will crash. You could make `fatal=True`, or supply a default, say `default='Untitled video %s' % video_id`.
```suggestion 'url': 'http://www.rtve.es/play/videos/balonmano/o-swiss-cup-masculina-final-espana-suecia/2491869/', ``` This URL redirects to https://www.rtve.es/play/videos/balonmano/o-swiss-cup-masculina-final-espana-suecia/2491869/. This corresponds with the previous test L44 where we are using `play`.
If you want yt-dl to handle URLs containing `...playz/...`, leave the pattern as `...playz?/...`. Any redirection happens after the URL gets matched.
Either sloppy code or an anti-scraping measure.
Maybe simplify with: ``` decoded_url.translate(None, '"\',').decode('utf-8') ``` Or in case there could be spaces in `['"cod", "ed ", "URL"]`: ``` re.sub(r'[\s"\',]', '', decoded_url).decode('utf-8') ```
Definitely. This wouldn't pass the CI tests. Have one of these instead. Uncontroversial string operations: ``` title = ' '.join((title, part, )) ``` ``` title += ' ' + part ``` Old-style format, still supported in Python3: ``` title = '%s %s' % (title, part, ) ``` ``` title = '%(title)s %(part)s' % {'title': title, 'part': part, } ``` ``` # if title and part are local vars ... title = '%(title)s %(part)s' % locals() ``` Newer-style, but still OK in 2. ``` title = '{0} {1}'.format(title, part) ``` ``` title = '{title} {part}'.format(title=title, part=part) ``` You might get the impression that Guido couldn't make his mind up about how string formatting should work.
Could check here: ``` if 'Este episÃ³dio nÃ£o se encontra disponÃ­' in title: raise ExtractorError('Episode unavailable', expected=True) ```
Maybe make `default=''` and try `self._html_search_meta('twitter:title', webpage)` after l.52 if `title` is empty then? In case the `<title>...</title>` has nothing useful.
Here `if episode_json is False` can/should be replaced with `if not episode_json`, which is also the way you're doing it later.
Oh, ok. That makes sense. Shoulda read the statement above it.
Or you could finish with: ``` result = self.playlist_from_matches(urls, playlist_id, title) if uploader_id: result['uploader_id'] = uploader_id return result ```
As `title` is not a required field for playlists, add `default=None`. Similarly for `uploader_id` below.
Similarly, valid JS could have whitespace around `[.(),;]`, or use '' instead of "".
Valid HTML could include whitespace or other attributes in the tag, or use '' instead of "", something like this: ``` description = self._html_search_regex( r'<p\s[^>]*?class\s*=\s*(?P<q>["|\'])jove_content(?P=q)[^>]*>(?P<desc>[^<]+)</p>', webpage, 'description', fatal=False, group='desc') ``` In case of valid but deprecated HTML with unquoted attribute values, `(?P<q>"|\'|\b)` would match the start of the class attribute value.
Valid JS could include white space around `.`, `(`, `)`, `[`, `]`, and `,`, though it makes the RE a bit messier, something like this: ``` r'Page\s*\.\s*messaging_box_controller\s*\.\s*addItems\s*\(\s*\[\s*(?P<msg>{(?!.*}\s*,\s*{).+?})\s*\]\s*\)' ```
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
Could be better to use a pattern here in case Snapchat moves this image, eg: ``` 'thumbnail': 're:https://s\.sc-cdn\.net/.+\.jpg' ```
Strictly, the JSON may not have resulted in a `schema_video_object` that is a `dict`, even if that case is an error, and even if it is a `dict` the required `video_url` might not be found. You could consider, eg ``` try: video_url = str_or_none(schema_video_object['contentUrl']) if not video_url: raise ValueError('video_url must be non-empty string') except (TypeError, ValueError) as e: raise ExtractorError('Unexpected format for schema_video_object', cause=e, video_id=video_id) ``` After this 'type guard', `schema_video_object` is known to be something that has a `get()` method.
Consider using the library routine `try_get` (`utils.py`) for ll. 43-6: ``` views = try_get(schema_video_object.get('interactionStatistic'), lambda x: x['userInteractionCount'])
Similarly for ll.48-51: ``` uploader_id = try_get(schema_video_object.get('creator'), lambda x: x[alternateName'])
> * Done > > * There is no terminator, just JSON Fine, then. > * Done > > * I did not implement this as I do not understand it fully The attribute values in HTML tags can be 'quoted' or "double-quoted" or (historically) not quoted at all. To match all of these quoting styles, you could use a named capture group that matches any of the beginning quote styles `(?P<q1>"|\'|\b)`and then close the match with a back-reference `(?P=q1)`. However, a named capture group will count among the numbered groups considered by `_html_search_regex()`, so you'd also have to use a named capture group for the JSON that you're targeting `(?P<schema_obj>.+?)` instead of just `{.+)` and add `group='schema_obj'` to the method arguments to tell it to use that group. But I admit it's unlikely Snapchat will change its quoting style. > * Done > > * I did not implement this as I do not understand it fully The HTML tag before the target JSON `<script data-react-helmet="true" type="application/ld\+json">` would be equally valid as `<script type="application/ld\+json" data-react-helmet="true">`. However regular expression syntax is not good at expressing this. The Perl-style REs supported in Python have an 'alternative' syntax `(?(A)match-if-A matched|match-if-A-didn't-match)`, where `A` is a capture group name or number. Thus `(?P<A>A-pattern)B-pattern(?(A)|A-pattern)` matches both `A-patternB-pattern` and `B-patternA-pattern`. So your match (without any of the other suggested changes) could be `r'<script (?P<helmet>data-react-helmet="true" )type="application/ld\+json"(?(helmet)| data-react-helmet="true")>(.+)</script>'` Introducing a named capture group means tweaking the method call as above, and again it's probably unlikely that Snapchat will change the order of the attributes, unless they upgrade their React toolset to something that accidentally generates a different order. Thanks for listening!
Or (probably the same result): ``` title = self._generic_title(url) ```
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
These are already in compat as `compat_filter` and `compat_map` [here](https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/compat.py#L2970).
Perhaps gather `description` too? Say, ``` description = self._html_search_meta(('og:description', 'twitter:description', ), webpage) ``` Also perhaps an item in the `categories` list, `timestamp`, `creator`, from these elements: ``` <div class="news-info"> <div class="news-section-date news-text"> <span class="section"> China </span> <span class="date"> 11:30, 06-Jun-2021 </span> </div> ... <div class="news-author news-text news-statistics-info J_news-statistics"> <div class="news-author-name"> CGTN </div> </div> ... </div> ``` For `timestamp`, apparently `unified_timestamp()` doesn't know about the date-time format used above (and confuses itself that the year is a timezone if it does), so either decode it yourself, or try also importing the `DATE_FORMATS_DAY_FIRST` list from `utils.py and appending '%H:%M %d/%b/%Y' to it, then replace `-` with `/` in the date-time string before passing it to `unified_timestamp()`. See #29948.
Since #29201 is not merged, youtube-dl does not have any need for gcm
With this: ``` for n, res in enumerate(common_res): ``` you could replace `len(thumbnails)` by `n` as the value of `preference`.
You could consider using the library routine `parse_resolution()` (`utils.py`) ``` thumb_info = parse_resolution(res) thumb_info['url'] = base_url.replace(replace, res) thumb_info['preference'] = n # as above thumbnails.append(thumb_info) ```
Looks like a good solution.
Rather than introducing a new metadata parameter, why shouldn't this just be the `timestamp` of the item, when it's a live show? > timestamp: UNIX timestamp of the moment the video became available
It seems rather silly to stuff extracted information into an `info` dictionary only to extract each key back one by one at the end. The churn of changing `formats` into `info['formats']` later in the extractor inflates the diff size and makes code harder to review without actually accomplishing anything.
This is no longer extracted. The `info` dictionary issue is masking a bug here.
Instead of escaping the inner double quotes you could single-quote the string.
Or better, use the utility method that gets the value of the content attribute of a `<meta>` tag with any of these attributes having the specified value (or a value from an iterable): itemprop, name, property, id, http-equiv: ``` content_url = self._html_search_meta('contentURL', webpage) ``` And also below for author.
``` from .common import InfoExtractor from ..compat import ( compat_str, compat_urllib_parse_unquote ) ``` Also, if @dstfw were to review this, he would probably complain about the next two IE imports being moved as not related to the actual change.
idk what the maintainers think about this, but I personally think this change is out of the scope of this PR. If this function is desired, it can be added separately. For now, you could just replace `self._match_valid_url(url)` with `re.match(self._VALID_URL, url)` as many other extractors already do.
`info.get('description')` ? Not a mandatory item. Similarly with `info['uploaded_at']` below.
`()` is a better 0-length iterable than `""`, which implies text.
Every existing extractor imports individual functions rather than the whole file.
Now `info['data']` needs to be a dict. As an `['id']` is mandatory (as is `['title']`), you could get it here and give up otherwise: ``` display_id = video_id # this can be included as the 'display_id' of the result video_id, title = try_get(info, lambda x: (x['data']['id'], x['data']['title'], ) title = str_or_none(title) if video_id is None or not title: raise ExtractorError('Unable to extract id/title') ``` Eventually `video_id` and `title` can be used in the result dict.
Some 62 out of 64 other extractors that do a similar thing have called the corresponding method `_call_api()`. I'm only pointing this out in case you might want to do so.
Above, it's still possible that the JSON download works but doesn't result in a `dict`. So this would be better (setting `None` on error to help with the second point below): ``` status = try_get(info, lambda x: x['status']) ``` Then, if the API makes a breaking change without us noticing, is that `expected` or not? As the site is unlikely to revert the change, it becomes our bug and so not `expected`. I suggest `expected` should correspond to the API returning an actual status that is not OK, and nothing else, like so: ``` raise ExtractorError(status or 'something went wrong', expected=status not in ('ok', None)) ``` But you are obviously familiar with the API and I'm not ...
Should the extractor crash if the category download fails? It's not going to be used for a mandatory field.
Actually I think the line I fingered was fine, but the replacement's fine too.
Actually yt-dl will set the `'upload_date'` from the `'timestamp'` if it's present, so you could leave this line out.
Yes, of course. Somehow I stopped reading the line too soon :-(
At this point `subtitle` isn't known to be a dict (and subtitles aren't a mandatory item). How about: ``` base_url = try_get(subtitle, lambda x: x['subtitleUrl'], compat_str) ``` Then get() can be used freely afterwards.
This may work because `compat_str` is (at the moment) imported in `utils.py`, but properly it should be imported from `..compat` directly.
imo it would be better to let this function accept a `**kwargs` and update it as-is into the dict
Many extractors call this function like `playlist_result(entries, id, title)` without giving the field names. So, for compatibility, the id, title and desc will need to be kept as normal args. But otherwise, yes, that is what I meant. ```py @staticmethod def playlist_result(entries, playlist_id=None, playlist_title=None, playlist_description=None, **kwargs): """Returns a playlist""" video_info = {'_type': 'playlist', 'entries': entries} video_info.update(kwargs) if playlist_id: video_info['id'] = playlist_id if playlist_title: video_info['title'] = playlist_title if playlist_description is not None: video_info['description'] = playlist_description return video_info ```
Instead, as above, just have: ``` playlist = self.playlist_result( self._entries(selected_tab, item_id, webpage), playlist_id=playlist_id, playlist_title=title, playlist_description=description) playlist['view_count'] = view_count playlist['last_update'] = last_updated ``` Here we're being just as careless as the original code in failing to check that the returned `playlist` is actually a dict ...
`_html_search_regex()` throws unless given a `default`(returns it silently) or `fatal=False` (returns `None` with a warning), so the OG search won't execute.
Do you want the first &playlistId=... or the last? Maybe `query['playlistId'][-1]`? Obvs these are probably the same, but the _VALID_URL is capturing the last match, like this: ``` >>> re.match(r'''(?:(?P<pl>pl=\d+)|&)+''', 'pl=1&pl=2').groupdict() {'pl': 'pl=2'} ``` Maybe scrap lines 266-270 and just have `video_id = self._match_id(url)`? Or, ll.266-271 could become: ``` # must return a valid match since it was already tested when selecting the IE matches = re.match(self._VALID_URL, url).groupdict() # id is not enforced in the pattern, so do it now; ditto integration video_id = matches['id'] integration = matches['integration'] ```
The style checker will complain that there's no exception class/tuple (eg, `(ExtractorError, RegexNotFoundError, KeyError, )`). Otherwise you could make all the calls non-fatal and (as appropriate) test for falsity or supply defaults.
Since you're using `'''`, you don't need to escape any quotes (in fact this might break it due to `r'''`) unless it's the same one in the start or end of the string, touching the same quote. Same for the other such strings later in the code.
Missing dot escape.
No need to escape a double quote inside a single (in fact that might break this due to `r'`).
When I tried this it crashed ('datetime.datetime has no timestamp'). But we have `unified_timestamp()` in `utils.py` and `unified_timestamp(episode.get('air_date'))` seems to work OK.
The URL should have been `compat_str` already, so the regex groups will be too.
To put `-` in `[]`, just make it the last item, hence `[\w-]`
Sorry, that's quite right. The framework in use is saving the page state in a stringified JSON and it needs two decodes to get it.
Move the capture group inside the quotes: then `_parse_json()` will work the first time: ``` data = self._search_regex( r'window.__APP_STATE__\s*=\s*["\']?({.+?})["\']?;</script>', webpage, 'app state') ```
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
You could just write this like so, as other extractors seem to do: ``` info_dict = { 'id': video_id, ... ```
At this point `iteminfo` isn't known to be a dict. If it isn't, or it has no key `status_code`, this will crash, when it might be preferable to catch all these potential failures in the ExtractorError. ``` ... iteminfo = self._download_json('https://www.douyin.com/web/api/v2/aweme/iteminfo', video_id, query={'item_ids': video_id}) or {} status_code = iteminfo.get('status_code', 'status_code missing') if status_code: raise ExtractorError('%s (%s)' % (iteminfo.get('status_msg', 'status_msg missing'), status_code), video_id=video_id) ... ```
For these 2, add expected_type `int`, eg: ```py 'width': try_get(item, lambda x: x['video']['width'], compat_str), ``` (equivalent in effect to wrapping in `str_or_none()`). Add `from ..compat import compat_str` after line 4.
For these 3, add expected_type `int`, eg: ```py 'width': try_get(item, lambda x: x['video']['width'], int), ``` (equivalent in effect to wrapping in `int_or_none()`).
Prefer `post.get()` for these two.
As `title` is a mandatory field, you could use`video_data['clipTitle']`.
Or at least, it's not `unicode`. In yt-dl `str` should almost always be `compat_str`, but as above it's not needed here.
```suggestion r'(?:\b|[^a-zA-Z0-9$])(?P<sig>[a-zA-Z0-9$]{2,})\s*=\s*function\(\s*a\s*\)\s*{\s*a\s*=\s*a\.split\(\s*""\s*\);[a-zA-Z0-9$]{2,}\.[a-zA-Z0-9$]{2,}\(a,\d+\)', ``` This works for me.ð
Suffer. In addition to that they can install python and run this themselves quite fine.
ðwork for me, modify the file by hand
These 2 regexes ought to be merged, though maybe not in the scope of this PR: ``` r'(?:\b|[^a-zA-Z0-9$])(?P<sig>[a-zA-Z0-9$]{2,})\s*=\s*function\(\s*a\s*\)\s*{\s*a\s*=\s*a\.split\(\s*""\s*\)(?:;[a-zA-Z0-9$]{2}\.[a-zA-Z0-9$]{2}\(a,\d+\))?', ```
works also for me :+1:
This worked for me.
This isn't safe: if `media.get('track_info')` isn't the expected `dict`, it will crash. Use `try_get`.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
The `publishTime` isn't known to be a `dict`, so `try_get(video_detail, lambda x: x['publishTime']['timestamp'], int)`. Similarly with `series` below.
Try `r'(?s)window\s*\.\s*APP_SERVER_STATE\s*=\s*(\{.+?\})\s*;'`: * `(?s)` to make `.` match across lines (or pass `re.DOTALL` in `flags`) * all the `\s*` are at points where arbitrary JS whitespace could occur * `.+?` (1) `.` in case of `;` inside strings in the JSON (2) `+` definitely need some contents (3) `?` don't match `"{...}; ...x = {...};` * finally force a terminating `}` inside the group and `;` afterwards.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
The code typically leaves a trailing `,` in lists, dicts, tuples (even when not required) as it's valid syntax and avoids having to change two lines to insert a new line. You could leave this as-is.
Depending on where in the page the target may be, consider `self._html_search_regex()` which unescapes the returned match (eg, if it contains `&amp;` that should be `&`, or just `&#0049;` that should be `1`).
Avoid crashing and skip the item on a missing title: ``` title2 = try_get(info2, lambda x: x['video']['title'], compat_str) title = try_get(info2, lambda x: x['video']['custom']['show_title'], compat_str) title = ' - '.join(x for x in (title, title2, ) if x) if not title: continue return { 'id': video_id, 'title': title, ... ``` You could also generate a diagnostic if this is a "really shouldn't happen" or `raise ExtractorError()` if it's a "can't happen" event.
Use `(try_get(info, lambda x: x['tracks'], dict) or {}).items()` to avoid crashing.
Use `try_get(info, lambda x: x['plugins']['thumbnails']['url'], compat_str)` to avoid crashing on optional metadata.
Here (ll.77-9) you can use `try_get()` to simplify and enforce the `format_url`: ``` format_url = try_get(format_dict, lambda x: url_or_none(x['src'])) if not format_url: continue # now guaranteed: format_dict is a dict-like object, format_url is not None ```
The site could randomly insert syntactically valid white space in the target JS expression: ``` r'processAdTagModifier\s*\(\s*(\{.*)\)\s*,\s*\{\s*"video"\s*:', ``` Also, consider these points: * `.*` won't match newlines unless you set the `s` flag (`r'(?s)...'`) * if the site should send several consecutive sequences like this `{...}, {"video: ...}, {...}, {"video: ...}, {...}, {"video: ...`, the pattern will match the entire string and not just the first `{...}, {"video: ...}, `: maybe use `.*?` so as to get the shortest match. And at l.65 below.
`url.split('/')[-2]` is a valid expression, but the test and splits can be combined in one step with more assurance that a1, a2 aren't empty: ``` mobj = re.search(r'(?=.*/radio/).+/([^/]+)/([^/]+)$', url) # or if 'radio' shouldn't match the a1/a2 groups # mobj = re.search(r'/radio(?:/[^/]+)*/([^/]+)/([^/]+)$', url) if mobj: # now mobj.groups() is essentially (a1, a2, ) embed = self._download_webpage( 'https://www.rtvs.sk/embed/radio/archive/%s/%s' % mobj.groups(), video_id) ... ```
Here and in the next line consider `self._parse_json()` instead of calling `json.loads()` directly.
Although this is fine logically, this change is a bit shorter without changing the indentation of following lines. ``` grps = re.search(r'/myspass2009/\d+/(\d+)/(\d+)/(\d+)/', video_url) for group in grps.groups() if grps else []: ```
You could make the regex more robust: ``` r'window\s*.\s*__PRELOADED_STATE__\s*=\s*(.*?)\s*</script' ``` * `\s*` for all the places where JS/HTML allow whitespace * `.*?` to avoid capturing "target_json</script>...<script...>...</script>", etc * `</` don't need to be escaped. Also, as you're presumably hoping to get some brace expression, maybe this? ``` r'window\s*.\s*__PRELOADED_STATE__\s*=\s*({.*?});?\s*</script' ```
:Mandatory field: either `video_player['name']`, or provide an alternative, eg from the webpage (the home page offers several: `<title>` element, `og:title` and `twitter:title` `<meta>` elements).
Again, `''` wouldn't be a helpful default: you'd just get an AttributeError ` ... object has no attribute 'get'`. It would be OK if both defaults are `{}`, or use `try_get(video_player, lambda x: x['video']['servers'], dict) or {}`.
If a media URL extracted into `formats` may sometimes fail (eg HTTP error 403, 404), you can call `self._check_formats(formats)` to discard any such links.
This would fail more gracefully: ``` props = self._parse_json(initialData, course_name, default={}) props = try_get(props, lambda x: x['initialProps'], dict) or {} entries = [] for chapter_num, chapter in enumerate(props.get('concepts') or [], 1): ``` Or if you want to flag the absence of concepts expand the last line ``` concepts = try_get(props, lambda x: x['concepts'], list) if concepts is None: raise ExtractorError('No concepts found', expected=True) for chapter_num, chapter in enumerate(concepts, 1): ```
If there really is no `description`, omit it. The public pages have a description in `<meta name="description" content="...">` as well as in the `og:description` and `twitter:description` `<meta>` items.
`(server_json.get('id') or '?')` (or some other default value). Or server_json.get('id', '?') Similarly l.128.
Avoid crashing randomly if JSON items are moved or renamed or otherwise unexpected: ``` video_player = try_get(data_preloaded_state, lambda x: x['videoPlayer'], dict) title = video_player.get('name') # if there may be other ways to get the title, try them here, then ... if not title: raise ExtractorError('No title for page') duration = video_player.get('duration') formats = [] for server, server_json in (try_get(video_player, lambda x: x['video']['servers'], dict) or {}).items(): ```
There's still a loop on `format_id` which doesn't seem right: ``` for server, server_json in (try_get(video_player, lambda x: x['video']['servers'], dict) or {}).items(): if not isinstance(server_json, dict): continue for format_id in ('hls', 'dash'): if format_id not in server_json.keys(): continue if format_id == 'hls': formats.extend(self._extract_m3u8_formats( server_json[format_id], lecture_id, 'mp4', entry_protocol='m3u8_native', m3u8_id=format_id, note='Downloading %s m3u8 information' % server_json.get('id', '?') , elif format_id == 'dash': formats.extend(self._extract_mpd_formats( server_json[format_id], lecture_id, mpd_id=format_id, note='Downloading %s MPD manifest' % server_json.get('id', '?'), fatal=False)) self._sort_formats(formats) ... ```
Don't give a default, especially not of the wrong data type; just let it default to `None`. Also, perhaps move this down to l.129 where `duration` is further processed.
Use the Python 2 and low 3 Time Machine: `'url too short: %s' % (video_pre_parts, )` or: `'url too short: %(video_pre_parts)s' % {'video_pre_parts': video_pre_parts, }` or: `'url too short: {video_pre_parts}'.format(video_pre_parts=video_pre_parts)` or: `'url too short: {0}'.format(video_pre_parts)` No doubt there are other ways (eg `....format(**locals())`
Generally, I'd have put the static regex directly as the first argument of `re.search()`.
If this should be anywhere it should be in `utils.py`.
I'm not an expert in youtube-dl's conventions and helpers but intuition says `title = info.get('titre')` and let the caller check if empty or not. Otherwise it can possible end up with a stack trace from KeyError.
mixing quotes and double quotes
For 2.6 (yes, I know) compatibiility, `{0}` rather than `{}`, apparently. And number other formats elsewhere.
Use `try_get()`, which does this for you: ``` thumbnail = try_get(api_metadata, lambda x: x['video'][0]['poster']) ``` You could also condition the value with `url_or_none()`
Mandatory field: use `metadata['title']`, and better to do this earlier: see below.
Have you checked if this works with the multi-video post test? IIRC you'd need to extract `carousel_media` for it also ```suggestion best_quality = items0['video_versions'][0] ``` or make the array indexing optional
The test page has a perfectly good `application/ld+json` script element with `@type: VideoObject` which should be the default source for metadata.
```suggestion description = try_get(additional_data, lambda x: x['caption']['text']) ``` otherwise this would error out when no caption object
Have you tried the test page with yt-dlp? I'd rather pull changes from the downstream extractor than make new changes here, if that deals with the issue.
Instead, let's have `int_or_none(quality)`, or if the `quality` string may have punctuation, like '1,000' or '+42', use `str_to_int()`. These functions are in utils.py. It's better to return a None bitrate than an incorrect number.
Use `\s+` for spaces in the pattern. Also possibly `\s*=\s*`.
Sure, but `strip_` **or_none**. The regex matches a whitespace title --> `None`.
`title` could be stripped to `None`: provide an alternative title, such as the last-but-one element of the URL path, capitalised (`... or url.rsplit('/', 2)[1].capitalize()`).
How about: ``` display_id = mobj.groupdict.get('display_id') or mobj.group('id') ```
A lot of extractors use the format "{extractor} says: {error_message}", which may be clearer to the user.
I guess so.
Let's hope that doesn't happen, then, but in case it does a specific log message, rather than "... object has no attribute 'get'", would surely be more helpful. Up to you, anyway.
As there's an error check, be aware that the returned JSON object may not be a `dict`, so maybe `not isinstance(info_raw, dict) or ...`, or previously: ``` if not isinstance(info_raw, dict): info_raw = {'code': '', 'msg': 'No valid JSON data', } ``` I'm sure you can think of a suitable solution.
These 2 lines could be replaced by: ```python uploader_url, creator = creator_data[0][0:2] ```
These 2 lines could be replaced by: ```python uploader_url, uploader_id = uploader_data[0][0:2] ```
Conventionally: `from ..compat ...`
`compat_str(...)` if you want it to be unicode text.
You already had `item.get('lyric')` so no need for `try_get(item. ...)` in the dict presentation, as the extraction would have crashed for `item == None`, eg. Similarly `album` is known to be a dict so you can freely `.get()` from it.
`title` is required, so use the original pattern `item.get('title') or item['alias']` Or if you want to make the ensuing crash report clearer, `dict_get(item, ('title', 'alias')) or item['title']`
`_process_data(x,...)` expects `x` to be a `dict`. The downloaded 'JSON' may turn out to be a list or just `False`. So: ``` info = self.download_json(api, song_id) info_data = try_get(info, lambda x: x['data'], dict) if not info_data: # if the site might often do this, not just when the API changes, add # `, expected=True` raise ExtractorError('API returned empty or invalid song data') return self._process_data(info_data, song_id, type_url) ```
if u r going to port `traverse_obj`, port it properly to utils imho. It is very useful and superseeds most instances of `try_get` and `dict_get` while being able to do much more. If for just this extractor, you can easily replace traverse_obj with equivalent `try_get`s PS: `is_user_input`, `traverse_string` will never be used by extractors. They are designed to allow the item access in `--print`. case_sense won't mostly be used either
yt-dlp's `--print` is a bit more complex, allowing printing data at multiple stages. Eg: `-O "after_move:%(filepath)s" will print the final file path. This function is needed to (easily) support that syntax, (and also other similar options)
Please consider the same for `manifest_url`. See my explanation in #30703.
You only need a simplified version of this. Actually, optparse's built-in action `append` should be sufficient here
I did this so I can deprecate `--get-url` (along with all other --get options). Unless you want to do that too, `urls` field isn't really needed
It would be useful to have access to these in `-o` and even in --match-filter (https://github.com/yt-dlp/yt-dlp/issues/1309)
This breaks under quite a few circumstances: 1. With `%(episode_number)d` when there is no episode number, etc (gives NA in filename) 2. `%(playlist_index)s` doesnt pad like it does in filename 3. `--autonumber-start`, `--output-na-placeholder` etc won't work etc Generalizing code with `prepare_filename` (splitting out only the sanitization) will avoid these issues
As suggested in my refactor idea, IMO this list should not exist in the first place; rather, its values should be collected from the sections in code that implement them. I will not oppose anyone who decides to enact your solution, but I won't do anything that may look like approval.
That was a mere typo.
Either `default=''` or, better, let it crash (no default), since nothing else can work if this fails. Otherwise failure leads to a mysterious attempt to append `None` to a `compat_str`.
`duration` isn't a required field, so: ```py # title is required, so let it fail early attrs = content['data']['attributes'] title = attrs['title'] # this could be moved above l.40 and used there audio_info = content['data']['attributes']['audioLinks'][0] # these could be merged into the `dict` return duration = audio_info.get('duration') description = clean_html(attrs.get('description')) ```
```suggestion content_id = self._html_search_regex(r'"contentId":"(.+?)"', webpage, 'content_id') ```
Ensure there's a media URL: ```suggestion audio_url = content['data']['attributes']['audioLinks'][0]['url'] audio_info = content['data']['attributes']['audioLinks'][0] duration = audio_info.get('duration') description = clean_html(attrs.get('description')) ```
You could possibly extract these further fields, at least from the page I looked at: * thumbnail: `<meta property="og:image" content="https://www.mujrozhlas.cz/sites/default/files/styles/facebook/public/rapi/98e3cbb1471da07cca3960049ccb1de4.jpg?h=6c71f50d&amp;itok=F326BYA9" />` `thumbnail = self._og_search_thumbnail(webpage)` * timestamp: `<meta property="og:updated_time" content="2022-03-14T16:45:01+0100" />` `timestamp = parse_iso8601(self._og_search_property('updated_time', webpage, fatal=False))`
This pattern automatically strips whitespace from the returned value, and allows for attributes in the `<title>`: ```py title = self._html_search_regex(r'<title\b[^>]*>\s*(.+?)\s*</title>', webpage, 'title') ```
As this isn't a required item, add `fatal=False` to the args of `_og_search_property()`.
To access both groups from the URL match, use `mobj = re.match(self._VALID_URL, url)` (`import re`) and then access the matches as `video_id = mobj.group('id')`. In this case the fact that the extractor is running means that the `id` and `display_id` groups matched. If you had an optional group, like `(?P<display_id>.+)?`, something like `display_id = mobj.groupdict().get('display_id')` would be appropriate.
If the final suggestion above is used (and import urljoin from ..utils): ```suggestion sub = urljoin('https://commons.wikimedia.org', sub) ```
With the last change, `info['subtitles']` hasn't yet been set: ```suggestion subtitles[lang] = [{"ext": "srt", "url": sub}] ```
Don't use `str()`, as above. Also, improve the RE, allowing whitespace, either type of quote, no unnecessary `\`: ```suggestion for sub in re.findall(r'''\bsrc\s*=\s*["']/w/api\s*(.*?)\s*srt\b''', webpage): ``` Is the pattern being targeted actually `src="/w/api/path/to/sttl.srt`? If so, remove `\s*` and add `\.`: ```suggestion for sub in re.findall(r'''\bsrc\s*=\s*["']/w/api/(.*?)\.srt\b''', webpage): ``` Finally, if the URL path is in the target attribute, extract that instead of just the sub name: ```suggestion for sub in re.findall(r'''\bsrc\s*=\s*["'](/w/api/.*?\.srt)\b''', webpage): ```
suggestion: ```py subtitles = {} for sub in re.findall(r'\bsrc="/w/api\s*(.+?)\s*srt\b', webpage): ... subtitles.setdefault(lang, []).append(...) return { 'url': video_url, ... } ```
The url exactly needing to match is tooo specific imo and could easily break in the future (without actual extraction breaking)
you should use md5 for long fields like this ```py 'description': 'md5:xxx', ```
pukkandan: >why not just capture the whole field in regex? dirkf: >Assuming the intention is to collapse any spaces around the licence name, the regex should use \s+ anyway. ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license') ``` Also, should this crash extraction? ```suggestion licenze = self._html_search_regex(r'\bThis\s+(.*?)\s+license\b', webpage, 'video license', fatal=False) or 'unspecified' ```
Instead of importing `KNOWN_EXTENSIONS` (instead, `ExtractorError`, which should have been used anyway): ```suggestion ext = determine_ext(url, None) if ext is None: raise ExtractorError("invalid video url") ``` Or if this is a normal case: ```suggestion ext = determine_ext(url, None) if ext is None: raise ExtractorError("invalid video url", expected=True) ``` But I'm not convinced that this check is useful.
Shouldn't crash if absent. Also: * don't use `str()`. `webpage` will already be a `compat_str` and so the right type to be passed to `re` search functions in either Py2 or Py3 * relax the RE * since you fixed the `unicode_literals` import, remove all `u` from explicit `u'...'`, here and anywhere else. ```suggestion author = self._html_search_regex(r'<td\b[^>]*>\s*([^<]*?)\s*</td>', webpage, 'video author', default=None) ``` If you want a diagnostic for missing author: ```suggestion author = self._html_search_regex(r'<td\b[^>]*>\s*([^<]*?)\s*</td>', webpage, 'video author', fatal=False) ```
* Since you fixed the unicode_literals import, remove all u from explicit u'...', here and anywhere else. * relax the RE. ```suggestion video_url = self._html_search_regex(r'''<source\b[^>]+\bsrc\s*=\s*("|')(?P<src>(?:(?!\1).)+)\1''', webpage, 'video URL') ```
Preferably per line (makes it easier to see changes) and sorted: ```suggestion from ..utils import ( clean_html, determine_ext, get_element_by_class, KNOWN_EXTENSIONS ) ``` You'll also have to add some imports from `..compat` if other suggestions are accepted.
Add a description too. ```suggestion IE_NAME = 'wikimedia.org' IE_DESC = 'Some description of the extractor that will be extracted automatically into help ...' ```
Don't re-invent URL parsing. Also, why `sub = sub.replace(';', '&')? ```suggestion parsed_url = compat_urlparse.urlparse(url) qs = compat_parse_qs(parsed_url.query) lang = qs.get('lang', [None])[-1] if not lang: continue ``` A URL query string might in principle have several `lang=xx` elements. The URL is parsed; the query string is returned as a `dict` of `list`s. We assume that, when it only makes sense for a query parameter to have one value, the last one in the list (-1) is meant. In this case it's probably the first as well.
You could (should) implement @rautamiekka's suggestion to rename the `licenze` variable, unless there's a good reason for it not having an English spelling.
Use ```py description = get_element_by_class('description', webpage) ``` (`from ..utils import get_element_by_class`)
You need this at the top: ```py # coding: utf-8 from __future__ import unicode_literals ```
Assuming the intention is to collapse any spaces around the licence name, the regex should use `\s+` anyway.
This should be the URL being extracted from, not the URL of the extracted media link. Have you run the test as described in the Developer section of the yt-dl manual? Or am I completely confused? ``` python test/test_download.py TestDownload.test_Wikimedia ```
Separate lines, please.
You can use a `dict` expression, and the return value shouldn't be a list: ```py return { 'url': video_url, 'ext': 'webm', 'id': video_id, 'title': video_id, 'license': licenze, } ```
The second of these is a real f-string and won't run in Py2; also we need to use the compat version of `urllib.parse` (replace its import with `from ..compat import compat_urllib_parse'): ```py subtitle_url = ( 'https://commons.wikimedia.org/w/api.php?action=timedtext&lang=nl&title=File%3A{0}&trackformat=srt'.format(compat_urllib_parse.quote(video_id))) ```
This won't run in Py2. Also, it's not really an f-string, as it has no `{expression}` in it. Also, you don't need to use the lookahead/behind assertions, as the method will find the first plain group (or whatever you specify with `group=...` in the args). Instead try something on these lines: ```py licenze = self._html_search_regex(r'\bThis\s*(.*?)\s*license\b', webpage, u'video license') ``` I've used `.*?` to avoid matching too much, `\b` to enforce a word boundary, and `\s*` to strip whitespace from around the `licenze`.
Nothing wrong with this, but most extractors don't do it. If there's a problem it'll be possible to localise it without it.
As you don't use mobj after the next line, replace the two by ```py video_id = self._match_id(url) ```
You should test this on DASH formats. I don't remember exactly what the issue was, but I remember FfmpegFD not working well with them
Maybe move the error check into `_call_api()` since it's already liable to raise an exception? You could make it so that if the method returns its value is guaranteed to be a `dict`.
The (non-empty) matches succeeded (or an exception would have been raised) so the values are both truthy: ```suggestion headers['rfApiProfileId'] = api_token headers['rfWidgetId'] = widget_id ```
```suggestion r'''widgetId:\s+["'](\w+)''', rf_token_js, 'widgetId') ```
```suggestion r'''apiToken:\s+["'](\w+)''', rf_token_js, 'apiToken') ```
```suggestion self._downloader.report_warning( 'Search api returned no items (if matches are expected rfApiProfileId may be invalid)') ```
Simplify error reporting as below, or skip this check and just let the `_html_search_regex()` calls raise the exception? ```suggestion for token in ('apiToken', 'widgetId'): if token not in rf_token_js: raise ExtractorError( 'Unable to fetch ' + token, expected=True) ```
* `\n` matches `\s`. * Use the `s` flag in case the content of `<h1>` is wrapped. * Relax matching case, quotes and whitespace. * Strip whitespace from the title using the pattern. Thus: ```suggestion title = self._html_search_regex(r'''(?is)<div\s[^>]+\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.+?)\s*</h1>''', webpage, 'title') ```
Is the match on trailing `\s+title` necessary to avoid false matches? ```suggestion for video_id in re.findall(r'(?is)<div\s[^>]*\bclass\s*=\s*["'.*?\bvideo-item\b.*?["'][^>]*>\s*<a\s[^>]*\bhref\s*=\s*["'](.*?)["']', webpage): ```
```suggestion _MORE_PAGES_INDICATOR = r'''<li\s[^>]*\bclass\s*=\s*["']next["'][^>]*><a\s[^>]*href\s*=\s*["']#videos["']''' ```
```suggestion if re.match(r'^preview_url\d$', k): ```
```suggestion _TITLE_RE = r'''(?is)<div\s[^>]+?\bclass\s*=\s*["']headline["'][^>]*>\s*<h1\s[^>]*>\s*(.*?)'s\s+New\s+Videos\b''' ```
What actually is the format of these `video_urlN_text` values? If they are like `1080p`, import and use `parse_resolution`: ```suggestion } f.update(parse_resolution(flashvars.get(k + '_text')) ``` Otherwise: * `height` is optional * use `int_or_none()` (and import it from `..utils`) : ```suggestion 'height': int_or_none(flashvars.get(k + '_text', [None])[:-1]) } ```
Good practice since the `%` operator can be ambiguous: ```suggestion return self._extract_videos(model_id, self._BASE_URL_TEMPL % (model_id, )) ```
No need to escape " inside r'...'. As before, relax the pattern: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\bhref\s*=\s*["']http.+/["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Omit matching the `href` attribute, if the page has only one tag with class `avatar`, or the first one is wanted: ```suggestion uploader = self._search_regex(r'''(?s)\bclass\s*=\s*["']avatar["'][^>]+?\btitle\s*=\s*["'](.+)["']''', webpage, 'uploader', ``` Or match the entire tag that contains the attributes and use `extract_attributes(tag).get('title')`
Again `\n` matches `\s`, and allow line breaks and whitespace: ```suggestion self._search_regex(r'(?s)var\s+flashvars\s*=\s*({.+?})\s*;', webpage, 'flashvars', default='{}'), ```
```suggestion 'url': r're:^https?://www\.camwhoresbay\.com/get_file/7/55259a27805bf1313318c14b2afb0dae1fef6e1dd4/484000/484472/484472_720p\.mp4/\?rnd=.+', ```
There isn't any point to `_TITLE`? ```suggestion return self._html_search_regex( ```
```suggestion 'url': 'https:' + v, ``` Or import `sanitize_url` from `..utils` ```suggestion 'url': sanitize_url(v), ```
As with formats, `height` is optional: ```suggestion 'height': int_or_none(flashvars.get(k.replace('_url', '_height'))), ```
Is the trailing part `/.+` required? The id match will terminate at end of string or any non-digit.
```suggestion if re.match(r'^video_(?:alt_)?url\d?$', k): ```
Better to use `'url': update_url_query(v, {'rnd': rnd, }),` (and import `update_url_query`). For backward compatibility, format fields must be numbered (throughout, if any remain).
Just use `'thumbnail'` to match the default image URL.
Although I think you may have taken "specific" more literally than I intended!
Let's try to force the CI tests, and also ensure no final `/`: ```suggestion cdn_url = '/'.join((data['cdns'][0], 'replays', user, video_id)).rstrip('/') ```
Wrap these to ensure the correct type: ```suggestion 'duration': float_or_none(data.get('duration')), 'view_count': int_or_none(data.get('peakViewers')), ```
As above, you can omit this and let the core set it from `release_timestamp`: ```suggestion ```
Conversely, calculate these easy required fields; then get the formats: ```py info = { # if this might be missing, maybe data.get('_id') or video_id ? 'id': data['_id'], 'title': data['title'], } formats = self._extract_m3u8_formats(cdn_url + '/index.m3u8', video_id, ext='mp4', m3u8_id='hls') # then call this, which also raises an exception if there were no formats self._sort_formats(formats) info.update({ 'formats': formats, 'display_id': video_id, ... }) return info ```
Maybe better: ```suggestion data = self._download_json('https://api.cozy.tv/cache/%s/replay/%s' % (user, video_id), video_id) ```
Move this to the dict, but you don't need to set upload_date as the core will calculate it from the `timestamp`: ```py 'timestamp': unified_timestamp(data.get('date')), ```
You can do this in one line: ```suggestion video_id, user = re.match(self._VALID_URL, url).group('id', 'user') ```
Whereas, if there's just one item (and you're not planning to add another extractor), this can be one line: ```suggestion from .cozytv import CozyTVReplayIE, ```
Maybe better: ```suggestion cdn_url = '/'.join((data['cdns'][0], 'replays', user, video_id)) ```
If these `,`s are thousands separators, might they be `.`s for some locales (plainly not a decimal point for a count)? ```suggestion view_count = str_to_int(self._html_search_regex( (r'<strong>([\d,.]+)</strong> views', r'Views\s*:\s*<strong>([\d,.]+)</strong>'), webpage, 'view count', fatal=False)) ``` (and `from ..utils import str_to_int` at the top).
Not part of your PR, but should convert HTML-encoding to text, also strip surrounding spaces: ```suggestion title = self._html_search_regex( ```
If the attribute names are all `src` and not `xxxsrc`: ```suggestion r'\bsrc\s*:\s*(["\'])(?P<url>(?:https?://)?(?:(?!\1).)+)\1', ```
Consistency, and `.` is a regex character: ```suggestion height = int(self._search_regex(r'(\d+)\.mp4', format_url, 'height', default=360)) ``` The default is a different type from the successful method return, but in the context it doesn't matter.
I suppose these are available, but are they meaningful? That may not be an issue for this PR, though. If the `tags` don't matter, the test could have: ```suggestion 'tags': list, ```
There's a `_check_formats()` method for this: ```suggestion formats = [] if len(sources) > 1: formats = [sources[1][1]] self._check_formats(formats, video_id) if len(sources) > 0: formats.append(sources[0][1]) map(add_format, formats) ```
```suggestion try_get, url_or_none, ```
Preferably for such a long field use `'md5:...'`
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False)) or {} video_url = url_or_none(try_get(url_json, lambda x: x['sources'][0]['src'], compat_str) or self._og_search_video_url(webpage)) # Get the video url ```
* If there may be a newline in the 'videodescription', the `s` (dotall) flag is needed. * The regex as proposed will match everything between the first `<p>` and the last `</p>` (in the test video there's only one `<p>` element, but that's not robust). * A named group isn't really required. ```suggestion article = self._search_regex( r'(?s)<article\b[^>]*?\bclass\s*=\s*[^>]*?\bpost\b[^>]*>(.+?)</article\b', webpage, 'post', default='') video_description = ( self._html_search_regex(r'(?s)<p>\s*([^<]+)\s*</p>', article 'videodescription', fatal=False) or self._og_search_description(webpage)) ``` The suggestion (untested) tries to get the `<article>` with class `post`. Then the first `<p>` element in the article's innerHTML is selected, with the description being its stripped text. An alternative could be to find all the `<p>` elements in the page and return the one that matches the start of the ld+json or og:description text.
This will normally be set automatically. You could apply `mimetype2ext()` from `utils.py` to the extracted `video_type` to get `mp4` (in the test video), but that's just what yt-dl should set as `format` anyway -- and if you rely on ld+json `video_type` won't be available.
If this is retained, use `url_or_none()` from `utils.py` to condition the values: ```suggestion video_url = url_or_none(url_json.get('sources')[0].get('src')) or self._og_search_video_url(webpage) # Get the video url video_type = url_json.get('sources')[0].get('type') # Get the video type -> 'video/mp4' video_thumbnail = url_or_none(url_json.get('splash')) or self._og_search_thumbnail(webpage) # Get the thumbnail ``` (and `from ..utils import url_or_none` at the top).
```suggestion from ..compat import compat_str ```
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False) or {} ```
```suggestion more_opts += ['-b:a', compat_str(max(6, int((9-int(self._preferredquality)) * 24))) + 'k'] ``` (and `from ..compat import compat_str` at the top if necessary).
Is the issue that the `uri` is a manifest (m3u8)? Or is it actually a direct mp4 link with a confusing format? If the former, the typical pattern for handling format URLs is ```py ext = determine_ext(uri) if ext == 'm3u8': formats.extend(self._extract_m3u8_formats(uri, ...) elif ext == ...: # other manifests, eg mpd ... else: formats.append({ 'url': uri, ... }) ``` This is so common that it should have been abstracted in a common routine, I think, but perhaps one with too many arguments. Anyhow it automatically extracts all the available formats from the manifest so that they can be listed. If appropriate, do the same here.
So, looking at (1 of) the manifests, they're playlists and you could pass `preference=quality(version)` into `_extract_m3u8_formats()`. Probably also `entry_protocol='m3u8_native' as the manifests look harmless. Incidentally the API data appears to be in the webpage hydration JSON as well `window.__DUMPERT_STATE__ = JSON.parse(...)` but as you don't look at the page itself that doesn't matter.
Supposing that we had a link leading to a useful page ... ```suggestion video_id = self._match_id(url) ```
Perhaps there is more metadata to be extracted, but I can't tell from the OK page.
* `_html_search_regex()` has default `fatal=True`: add a `default` to fall back to `_og_search_title()` * allow line break in `.*` ```suggestion title = self._html_search_regex(r'(?s)<title\b[^>]*>(.*)</title>', webpage, 'title', default=None) or self._og_search_title(webpage) ```
This link just gives a page whose source is `OK`, and obviously it won't be possible to extract anything useful from that `webpage`.
Use `_search_regex()` to get proper error reports in yt-dl: ```suggestion url = self._search_regex( r'''("|')audioUrl\1\s*:\s*("|')(?P<url>(?:(?!\2).)*)\2''', webpage, 'audio URL') test_url = url_or_none(url) if not test_url: raise ExtractorError('Invalid audio URL %s' % (url, )) url = test_url ```
Don't capture patterns that aren't used: ```suggestion _VALID_URL = r'(?:https?://)?(?:www\.)?m\.(?:qingting\.fm|qtfm\.cn)/vchannels/\d+/programs/(?P<id>\d+)' ```
The same URL that was passed in! You actually have to do some extraction to get the media URL. Did you run this code? Also relax the regexes, but improving metadata extraction is not the first priority.
But this should not fail. ```suggestion result['creator'] = try_get(apiResponse, lambda x: x['streamer']['label'], compat_str) ```
The normal style is to stash results in variables and then construct the result dict as it's about to be returned. But if you're using this style, why not do: ```suggestion result = { 'id': self._match_id(url), } ```
This should fail if the data isn't available. Eg: ```suggestion result['url'] = 'https://livestreamfails-video-prod.b-cdn.net/video/' + apiResponse['videoId'] ```
Similarly, should fail if no `label`, or default to `IE._generic_title()`.
Right, that's fine for `display_id`.
I'd bring these two out as separate statements before the return so that if one of them fails the diagnostics are clearer. ```suggestion video_url = 'https://livestreamfails-video-prod.b-cdn.net/video/' + api_response['videoId'] title = api_response['label'] return { 'id': id, 'url': video_url, 'title': title, ```
```suggestion # coding: utf-8 from __future__ import unicode_literals from .common import InfoExtractor ```
This should fail if the data is not available, or default to `IE._generic_id()`: ```suggestion result['display_id'] = apiResponse.get['sourceId'] ``` After this, `apiResponse` is known to be a dict that you can `.get()` from without crashing.
```suggestion result['timestamp'] = parse_iso8601(apiResponse.get('createdAt')) ```
Similarly: ```suggestion result['thumbnail'] = url_or_none(try_get(apiResponse, lambda x: 'https://livestreamfails-image-prod.b-cdn.net/image/' + x['imageId'])) ```
```suggestion media_url = 'https://www.newgrounds.com/portal/video/' + media_id ```
This method needs to be back-ported from yt-dlp.
```suggestion 'language': compat_str(lang), ```
It's only 2 characters less, but: ```suggestion obj_pid, session_id, abs_time = (timestamp.get(x) for x in ('ObjectPublicIdentifier', 'SessionID', 'AbsoluteTime')) ```
```suggestion info = self.playlist_result( OnDemandPagedList( functools.partial(self._fetch_page, base_url, query_params, display_id), self._PAGE_SIZE), playlist_id=display_id, playlist_title=display_id) if folder_id: info.update(self._extract_folder_metadata(base_url, folder_id)) ```
`only_once` parameter isn't in yt-dl, yet.
What do you think `expected_type=lambda x: x or None` is doing? Apparently, a callable `expected_type` only fails if it returns None, so this is filtering out any falsy values!
Eg: ```suggestion 'url': update_url_query(base_url + '/Pages/Viewer/Image.aspx', { 'id': obj_id, 'number': obj_sn, }), ```
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
Doesn't exist in yt-dl, but there is a back-port in outstanding PR #30713.
Doesn't exist in yt-dl, but is easy to add.
This, and the `_extract_*_formats_and_subtitles()` methods of InfoExtractor, need another PR to be back-ported from yt-dlp, not yet done.
Doesn't exist in yt-dl. See also: * parse_qs * srt_subtitles_timecode * traverse_obj In this case, it's simple function that you could implement locally.
Py2 compat: ```suggestion # coding: utf-8 from __future__ import unicode_literals import re ```
As dstftw used to say, unrelated change. And the next two as well.
Although it's less neat, it's good to try for the mandatory field first, so that a log shows that failing rather than an intermediate stage: ```suggestion url = status['data'][0]['primary']['video_data']['videoSrc'] # now we know this exists and is a dict data = status['data'][0]['primary'] ```
```suggestion api_url = 'https://parler.com/open-api/ParleyDetailEndpoint.php' ```
Project convention is ''; also, as 5 other extractors have defined it already and probably others should have, prepare for utils.UUID_RE to be defined: ```suggestion _UUID_RE = r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}' _VALID_URL = r'https://parler\.com/feed/(?P<id>%s)' % (_UUID_RE, ) ```
`clean_html()` removes newlines; also yt-dl expects a non-empty title: ```suggestion title = clean_html(data.get('full_body')) or self.generic_title(url) ```
Use `urlencode_postdata()` from `utils.py`: ```suggestion payload = urlencode_postdata({'uuid': video_id}) ```
"" -> ''
Parentheses make this clearer although not required: ```suggestion uploader_url = ('https://parler.com/' + uploader_id) if uploader_id else None ```
```suggestion timestamp = unified_timestamp(data.get('date_created')) ```
For API string data you can condition the values with `strip_or_none()` from `utils.py`: ```suggestion uploader = strip_or_none(data.get('name')) uploader_id = strip_or_none(data.get('username')) ```
"" -> ''
"" -> ''
Please format like this for mantainability; also, add routines per other suggestions: ```suggestion from ..utils import ( clean_html, strip_or_none, unified_timestamp, urlencode_postdata, ) ```
Could you explain to me the reasoning behind this change? All your tests pass in yt-dlp without this
Same. The `filter` doesn't make sense to me
nvm, I figured it out
Maybe some unwanted `mediaAsset` has no `type`: ```suggestion if mediaAsset.get('type') == 'SOURCE': ```
Insert this alphabetically (after veehd and before veoh).
Maybe some unwanted `asset_format` has no `mime_type`: ```suggestion if asset_format.get('mimeType') == 'video/mp4': ```
You can use `update_url_query()`: ```suggestion asset = self._download_json( update_url_query('https://www.veejoy.de/api/service/get-media-summary', { 'mediaIri': self.get_asset_ref(video_data), 'locale': 'en', }), video_id) ```
As above, `final_asset.get("contentUrl")` could return `None`. Get the mandatory value early; crash if that fails.
The JSON returned by the API call is in `application/ld+json` format; we have a method `_json_ld()` for handling that, but the data here uses constructs that it doesn't (yet) support. Anyhow, try handling all the `asset_format`s here; maybe this version will work; only the actual format URL is mandatory: ```suggestion # actually, quality processing doesn't improve on just the width # a filesize is available but is 0 or doesn't change with quality qualities = qualities('hq', 'mq', 'lq') for asset_format in asset_formats: f_url = url_or_none(asset_format.get('contentUrl')) if not f_url: continue ext = determine_ext(f_url) transcodingFormat = try_get(asset_format, lambda x: x['transcodingFormat'], dict) or {} label = (strip_or_none(transcodingFormat.get('label') or '').split('-') extra = ( ('width', int_or_none(transcodingFormat.get('videoWidth'))), ('quality', qualities(label[0])), ('language', asset_format.get('language')), ) if ext == 'm3u8': # expect 'mimeType': 'application/vnd.apple.mpegurl' fmts = self._extract_m3u8_formats( # if the yt-dl HLS downloader doesn't work: `entry_protocol='m3u8'` f_url, video_id, ext=mp4, entry_protocol='m3u8_native', m3u8_id=transcodingFormat.get('formatType'), fatal=False)) for f in fmts: f.update((k, v) for k, v in extra if f.get(k) is None) formats.extend(fmts) else: # expect 'mimeType': 'video/mp4' fmt = {'url': f_url} fmt.update(extra) formats.append(fmt) ``` Then call `self._sort_formats(formats)` either here or, better, in `_real_extract()` after returning from here: if there aren't any formats, it raises.
Still needs to be more tolerant for optional data: ```suggestion return ( strip_or_none( try_get(video_data, lambda x: x['studioDetails']['item']['title'], compat_str)) or 'Veejoy') ``` Imports: ```py from ..compat import compat_str from ..utils import ( try_get, strip_or_none, ) ```
Do the mandatory fields first: ```suggestion final_asset = self.get_original_file_url(video_data, video_id)['contentUrl'] producer = self.get_producer(video_data) thumbnails = self.get_thumbnails(video_data) ```
After this, check for mandatory `title` field, and use the value in the returned info_dict: ```suggestion video_data = self.get_video_data(url, video_id) title = video_data['title'] ```
You don't need anything but the `id` to match, but there might be other attributes ahead of it: ```suggestion next_data = self._search_regex(r'''<script\b[^>]+\bid\s*=\s*(['"])__NEXT_DATA__\1[^>]*>(?P<json>[^<]+)</script>', webpage, 'next_data', group='json') ``` As NextJS is a common feature, there should be a standard function for this. This method is in yt-dlp but not yet back-ported, but you could pull in a copy: ```py def _search_nextjs_data(self, webpage, video_id, transform_source=None, fatal=True, **kw): return self._parse_json( self._search_regex( r'(?s)<script[^>]+id=[\'"]__NEXT_DATA__[\'"][^>]*>([^<]+)</script>', webpage, 'next.js data', fatal=fatal, **kw), video_id, transform_source=transform_source, fatal=fatal) ``` Then: ```py return self._search_nextjs_data(webpage, video_id)['props']['pageProps']['media'] ```
Conventionally yt-dl uses single quotes `['props']['pageProps']['media']`.
As thumbnails aren't required, this should tolerate errors: ```suggestion for res in ('3_4', '16_9'): thumb = try_get(video_data, lambda x: x['teaserImage'][res], dict) if not thumb: continue thumb = url_or_none(try_get(thumb, lambda x: x['srcSet'][1].split(' ')[0])) if thumb: thumbnails.append({ 'id': res, 'url': thumb, }) ``` Add this around l.4: ```py from ..utils import ( try_get, url_or_none, ) ```
Since this only replicates `_match_id()`, use that instead. A separate method makes the reader wonder what fancy stuff is being done, when actually there isn't any.
```suggestion from ..utils import ( ```
```suggestion from .utils import ( ```
```suggestion help='Specify webdriver type when you want to use Selenium to execute YouTube's "n_function" in order to avoid throttling: "firefox", "chrome", "edge", or "safari"') ```
```suggestion # coding: utf-8 from __future__ import unicode_literals import re ```
```suggestion from ..compat import ( compat_str, ) from ..utils import ( strip_or_none, try_get, url_or_none, ) ```
Use the `_TESTS` var instead or `_TEST` (deprecated). ```suggestion _TESTS = [{ 'url': 'https://www.bandlab.com/post/f5f04998635a44ea819cacdba7ae2076_f8d8574c3bdaec11b6562818783151a1', 'info_dict': { 'id': 'f5f04998635a44ea819cacdba7ae2076_f8d8574c3bdaec11b6562818783151a1', 'ext': 'm4a', 'title': 'ON MY OWN (unreleased)', 'artist': 'Michael MacDonald', }, }] ```
Prefer '' where possible. Allow `.*` to match line breaks. ```suggestion media_server = self._html_search_regex(r'var\s*mediaServer\s*=\s*\{.*"vod"\s*:\s*"([^"]+)"', webpage, 'vod', 'ms02.w24.at') mp4_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'video/mp4'", webpage, 'mp4_path') m3u8_path = self._html_search_regex(r"(?s)src:.*\+ '([^']+)'.*type:'application/x-mpegURL'", webpage, 'm3u8_path') ```
```suggestion import re ```
Maybe add this (if it works!): ```suggestion 'upload_date': unified_strdate(self._search_regex(r'Sendung\s+vom\s+([0123]?\d\.[01]?\d\.\d{4}', webpage, 'broadcast data', default=None)), } ```
You could remove the station ident from the title: ```suggestion 'title': re.sub(r'\s+-\sW24\s*$', '', self._og_search_title(webpage)), ``` If you didn't want the fixed station name, you could use `self._og_search_property('site_name', webpage, default='W24')` or similar.
You could get a thumbnail, and perhaps a (constant) uploader: ```suggestion 'extension': 'mp4', 'thumbnail': self._og_search_thumbnail(webpage), 'uploader': self._og_search_property('site_name', webpage, fatal=False), ``` That seems to be as much metadata as can be pulled from the page.
Prefer `entry_protocol='m3u8_native'` if it works.
I'm not saying this is much better but it might be a bit more resilient to format changes: ```suggestion media_server = self._parse_json( self._search_regex(r'mediaServer\s*=\s*(\{[^}]+})\s*;', webpage, 'media servers', default='{}'), video_id) src_txt = self._search_regex(r'videoPlayer%s\.src\s*\(\s*(\[[^\]+])' % (video_id, ), webpage, 'media paths') srcs = [] try: srcs = eval( src_txt.replace('mediaServer.vod', 'vod').replace(',\n',', '), {'vod': media_server.get('vod', 'ms02.w24.at'), 'src': 'src', 'type': 'type'}) # expect a non-empty list srcs[0] except Exception as e: raise ExtractorError('Unable to extract media links', cause=e) formats = [] for src in srcs: fmt_url = url_or_none(src.get('src')) if not fmt_url: continue ext = mimetype2ext(src.get('type')) or determine_ext(fmt_url) if ext == 'm3u8': formats.extend(self._extract_m3u8_formats( fmt_url, video_id, 'mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) else: formats.append({ 'url': fmt_url, 'ext': ext, }) ```
Again: ```suggestion {'ids': '[{0}]'.format(song_id), 'br': bitrate, 'header': cookie}, separators=(',', ':')) message = 'nobody{0}use{1}md5forencrypt'.format( URL, request_text).encode('latin1') ```
Again: ```suggestion ['{0}={1}'.format(k, v if v is not None else 'undefined') ```
Again: ```suggestion return ('params={0}'.format(encrypted_params), headers) ```
Use utils.try_get() to simplify and increase robustness; also condition more values: ```suggestion for song in try_get(data, lambda x: x['data'], list) or []: song_url = try_get(song, lambda s: x['url']) if self._is_valid_url(song_url, info['id'], 'song'): formats.append({ 'url': song_url, 'ext': details.get('extension'), 'abr': float_or_none(song.get('br'), scale=1000), 'format_id': song_format, 'filesize': int_or_none(song.get('size')), 'asr': int_or_none(details.get('sr')), }) ```
Condition the value? ```suggestion bitrate = int_or_none(details.get('bitrate')) or 999000 ```
Prefer `'md5:...'` for long/non-ASCII values (maybe also in existing tests).
Use built-in method? Also, should it crash (raise) if `json.loads()` fails, or just return an empty dict again? ```suggestion try: return self._download_json( url, song_id, data=data.encode('ascii'), headers=headers) except ExtractorError as e: if type(e.cause) in (ValueError, TypeError): # JSON load failure raise except Exception: pass return {} ```
Let's sort the import list. ```suggestion bytes_to_intlist, float_or_none, ```
Py2.6 compat (!): ```suggestion 'requestId': '{0}_{1:04}'.format(now, rand), ```
Again: ```suggestion data = '{0}-36cd479b6b5-{1}-36cd479b6b5-{2}'.format( ```
As this may need to be updated, consider 1. make the value a class var `_USER_AGENT` 2. import utils.std_headers and let non-null `std_headers['User_Agent']` override this value, so that `--user-agent ... ` or `--add-headers "User-Agent: ..."` take precedence (allows cli work-around if the site needs a new UA).
New test should use the current URL format. ```suggestion 'url': 'https://www.rtve.es/play/audios/a-hombros-de-gigantes/palabra-ingeniero-codigos-informaticos-27-04-21/5889192/', ```
Also need `m/alacarta`: ```suggestion _VALID_URL = r'https?://(?:www\.)?rtve\.es/(?P<kind>(?:playz?|(?:m/)?alacarta)/(?:audios|videos)|filmoteca)/[^/]+/[^/]+/(?P<id>\d+)' ```
