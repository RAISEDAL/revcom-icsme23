Test failure is because this is missing `geography=True` on PostGIS.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
put closing parenthesis on the next line
Ah! Of course, sorry I missed that.
chop "one of" add comma before "or"
I'd rather raise an exception to account for the fact that assertions are ignored with `python -O` unless there's some reason to prefer the assert.
Perhaps "Cannot convert DateField '%s' to DateTimeField." (I'm not certain it's an issue here, but we're favoring '%s' over %r until we drop Python 2 support to avoid possible u'' prefixes.)
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Test failure is because this is missing `geography=True` on PostGIS.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
put closing parenthesis on the next line
Ah! Of course, sorry I missed that.
chop "one of" add comma before "or"
I'd rather raise an exception to account for the fact that assertions are ignored with `python -O` unless there's some reason to prefer the assert.
Perhaps "Cannot convert DateField '%s' to DateTimeField." (I'm not certain it's an issue here, but we're favoring '%s' over %r until we drop Python 2 support to avoid possible u'' prefixes.)
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Test failure is because this is missing `geography=True` on PostGIS.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
I'd rather raise an exception to account for the fact that assertions are ignored with `python -O` unless there's some reason to prefer the assert.
I removed the NotIn assertions in my edits because they are brittle since a typo in the message means they would inadvertently pass. I suppose if error messages were class attributes that would make it more robust, but it seems unlikely to me that a regression could be introduced such that both messages are displayed.
put closing parenthesis on the next line
chop "one of" add comma before "or"
please limit line lengths so horizontal scrolling isn't required, something like: ``` self.assertHTMLEqual( form.as_p(), '<p><input id="id_custom" name="custom" type="text" /> ' '...' ) ```
```suggestion template_name = 'forms_tests/form_snippet.html' ```
dashboardId parameter is missing.
Maybe I'm missing sth, but can we use `@contextmanager`? ```python class TestCase(TransactionTestCase): ... @contextmanager def captureOnCommitCallbacks(self, *, using=DEFAULT_DB_ALIAS, execute=False): self.callbacks = [] start_count = len(connections[using].run_on_commit) try: yield self.callbacks finally: run_on_commit = connections[using].run_on_commit[start_count:] self.callbacks[:] = [func for sids, func in run_on_commit] if execute: for callback in self.callbacks: callback() ```
Argh, I just realized the PluginLoader attribute stuffing (update_object) has been happening on `class_only` cases, which is broken for "same name, different behavior" cases (since subsequent loads will overwrite the class vars on the existing type object, not instance vars). It works here, since `all` is `yield`ing the plugin class immediately after calling `update_object` on it, but the whole pattern is busted for `class_only`, esp for things that might consult the class var during or after `__init__`. We've needed to rethink that for awhile, and this kinda sets it in a little more concrete. :(
ignore it then, I stopped reading at import ssl, did not realize it is an encrypted tcp socket connection and assumed http/s
~~ use the shared open_url function, it takes care of many issues with python's ssl ~~
I'd would directly compare `callbacks`: ```suggestion self.assertEqual(callbacks, [branch_1, branch_2, leaf_3, leaf_1, leaf_2]) ```
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
```suggestion # try to get collection world name first ```
This implies that constructing some callbacks doesn't result in an instance, but also doesn't raise an Exception? That doesn't sound right...
Are we doing singleton `__new__`/`__call__` magic internally on some of the callback plugin types? Seems like this would never be `False` otherwise...
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
"Both Y and X must be provided". Switch the Y and X in the error.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
I think this should be a `ValueError`
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
```not (foo is None)``` => ```foo is not None```
`return '%s-%s-%s' % (y or 0, m or 0, d or 0)` can be moved here.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
I think we can move entire `ordering` logic to a separate branch i.e. ```python if self.ordering: ... sql, sql_params = super().as_sql(compiler, connection, ordering=( 'ORDER BY ' + ', '.join(ordering_expr_sql) )) return sql, sql_params + ordering_params return super().as_sql(compiler, connection, ordering='') ```
Please chop all unnecessary blank lines.
It might be worth compiling the regexp in the class or [module level and reuse](https://github.com/django/django/blob/master/django/contrib/localflavor/ca/forms.py#L16-L17).
Maybe `_parse_where()` -> `_get_condition_sql()`, I would return condition and add `WHERE` in `constraint_sql()`, e.g. ```python def _get_condition_sql(self, compiler, schema_editor, query): if self.condition is None: return None where = query.build_where(self.condition) sql, params = where.as_sql(compiler, schema_editor.connection) return sql % tuple(schema_editor.quote_value(p) for p in params) def constraint_sql(self, model, schema_editor): ... condition = self._get_condition_sql(compiler, schema_editor, query) return self.template % { ... 'where': ' WHERE (%s)' % condition if condition else '', } ```
As far as I'm aware, for backward compatibility we should assigned errors from `UniqueConstraint`s with a single field to this field :thinking:
```suggestion with self.assertRaisesMessage(ValidationError, msg): ```
```suggestion Call clean_fields(), clean(), validate_unique(), and validate_constraints() on the model. ```
Wrap at 79 chars.
```suggestion variety, *_, salt, data = argon2_hash.split('$') ```
Maybe `argon2_hash` -> `rest`
@puzan `rabbitmqctl status` doesn't support `vhost`. I run RabbitMQ 3.6.16. As quick fix, I added `add_vhost=True` as default argument to `_exec` to have something like this : ```python def _exec(self, args, run_in_check_mode=False, split_lines=True, add_vhost=True): .... some code here .... if add_vhost: args.insert(1, '-p') args.insert(2, self._vhost) ```
need to catch BotoCoreError here too.
```python mo_file_en.with_suffix('.po').touch() ```
Maybe a helper method would help eliminate the redundancy of these methods? e.g. `return self._value_or_setting(self._location, settings.MEDIA_ROOT)`
excellent handling of congestion control
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
this is already done by argspec when param is defined as boolean, all redundant
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` required_together required_one_of require_if ```
Can be simplified if you do `type='int'`.
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
Can be simplified if you do `type='int'`.
this is already done by argspec when param is defined as boolean, all redundant
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` required_together require_if ```
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive ```
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` required_together required_one_of require_if ```
Can be simplified if you do `type='int'`.
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
Can be simplified if you do `type='int'`.
this is already done by argspec when param is defined as boolean, all redundant
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` required_together require_if ```
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive ```
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` required_together required_one_of require_if ```
Can be simplified if you do `type='int'`.
You've named this check_params in other modules. Not a major issue.
Can be simplified if you do `type='int'`.
Can be simplified if you do `type='int'`.
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` required_together require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive ```
For integers please: use `type='int` remove the `isdigit` check from `check_params`
Add a trailing comma.
It used to be that way (in Python 2 era). Now gettext is an alias to ugettext and the latter will be deprecated in the future.
Use `gettext` instead of `ugettext`
+1 it's better to just patch `time.time`
@samdoran then, I'd say that there's something wrong with the setup and TZ should be adjusted somewhere.
I think we don't need to patch `datetime.datetime.fromtimestamp()` and `datetime.datetime.utcfromtimestamp()` methods as they just return a datetime object from patched `time.time()`.
Mind putting this magic value into a constant with a descriptive name? I'd read much better if it was ```suggestion sys.exit(RC_CLI_INIT_FAILURE) ``` or something like that.
`foo`-> `{@code foo}`
maybe also here `"foo"` -> `{@code foo}`
Space missing between `}` and `is`.
The `('443' if self.is_secure() else '80')` block is repeated twice - can we extract it to a variable at the start? ``` port_in_x_fw_host = False default_port = ('443' if self.is_secure() else '80') ```
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
to match the behavior of `_get_raw_host()` I think this reconstruction should only occur in the `SERVER_NAME` scenario.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
This fails on python 3 and doesn't look too good.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
argument ordering should be reversed
this is not a safe across py2 and py3, use the provided `string_types` instead ... but better yet, jsut define the option as type: list and this will be automatically handled for you. Single element, comma separated string and 'actual list' will all be normalized so you always get 'a list'.
We already test this exact configuration in upgrade test (from 0.9 to 0.10, using both 0.9 producer and consumer, and default timestamp type). I would change timestamp_type of this test to LogAppendTime to make it different.
you should not need to checked disabled, as the plugin itself wont be called at all if true
again, code not needed as this is already 'required'
Given that 1.0 was released 2 years ago, I'd even go with 1.1 as the minimum version.
Do we want to support all these versions? I'd vote for only testing 1.0 and newer. 1.0 was released roughly 2 years ago.
If it doesn't add too much to the runtime, I think it would be good to include some more cases like you suggest.
this check is not needed, the default is `False` if you should never get a `None` at this point
check is redundant as you already flagged as 'required'
```python async with contextlib.aclosing(aiter(self._iterable_class(...))) as agen: async for item in agen: yield item ``` You should explicitly aclose your async generators when you create them: https://vorpus.org/blog/some-thoughts-on-asynchronous-api-design-in-a-post-asyncawait-world/#cleanup-in-generators-and-async-generators
I partly restored `dates_or_datetimes` (removed in e88d2dfcf4daa2b4ee451f518085413bb3b8deeb), it looks simpler IMO.
Would it be better to raise `NotImplementedError()` so backends don't overlook implementing it? They can always return an empty list if it's not supported.
I pushed a commit with some edits, let me know if it looks okay.
I've removed an extra newline that caused a flake8 error.
`changed=False` (this module doesn't do any change).
``` def __init__(self, module): super(VmwareConfigFactsManager, self).__init__(module) cluster_name = self.params.get('cluster_name', None) esxi_hostname = self.params.get('esxi_hostname', None) if cluster_name: cluster_obj = self.find_cluster_by_name(cluster_name=cluster_name) if cluster_obj: self.hosts = [host for host in cluster_obj.host] else: module.fail_json(changed=False, msg="Cluster '%s' not found"%cluster_name) elif esxi_hostname: esxi_host_obj = self.find_hostsystem_by_name(host_name=esxi_hostname) if esxi_host_obj: self.hosts = [ esxi_host_obj ] else: module.fail_json(changed=False, msg="ESXi '%s' not found"%esxi_hostname) ``` @Akasurde What do you think ? No variable except `hosts` has to be attributes. The module should fail if the cluster or ESXi isn't found.
It should be in `YoutubeDL.process_ie_result` instead, and it's better to open a new pull request for this change.
Perhaps "Cannot convert DateField '%s' to DateTimeField." (I'm not certain it's an issue here, but we're favoring '%s' over %r until we drop Python 2 support to avoid possible u'' prefixes.)
It might be worth including the bridgename in the error message.
This sentence is too long, maybe: ```python choice = self._choice_input( f"It is impossible to add the field '{field_name}' with " f"'auto_now_add=True' to {model_name} without providing a " f"default. This is because the database needs something to " f"populate existing rows.\n", [ ... ```
Read: coding conventions, mandatory fields.
Read: coding conventions, optional fields.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
1. `_sort_formats`. 2. Must not break if any of these keys is missing.
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
You could use lambda to save some lines defining the function
Use `self._search_regex` and `utils.unified_strdate` instead.
You should be able to use `SimpleTestCase` (which prevents any queries) by setting an `id` manually on your score instance, avoiding the `save()` call and passing a singleton list containing `score` to `serializer.serialize()`: ``` python data = serializer.serialize([Score(id=1, score=3.4)]) ``` With this approach you should be able to hardcode the `"pk": 1` below as well.
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
you can use `state` to avoid the 'or' to the user
```suggestion self, objs, fields, batch_size, on_conflict=None, update_fields=None, unique_fields=None, ```
```suggestion item, fields=fields, using=self.db, ```
Ditto for `[]` â†’ `None` and `ON_CONFLICTS_NONE` â†’ `None`.
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
Yeah, it's fine.
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
single line as above
```suggestion item, fields=fields, using=self.db, ```
use a single line or use hanging indent (we avoid non-4 space indents)
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
Minor but I'd move this control flow block after the `weights` one to match the args order.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
put closing parenthesis on the next line
Co-locate this with Lag and the base class rather than defining alphabetically. Same as First/LastValue.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Defining `__init__()` to specify a default `output_field` is required until we re-arrange `runtests.py` to avoid importing settings dependant modules before calling `django.setup()`.
`arity = 1`
class attribute `output_field = IntegerField()` * fairly sure that's acceptable Then drop the `__init__`
class attribute `output_field = IntegerField()` * fairly sure that's acceptable Then drop the `__init__`
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
you can use `state` to avoid the 'or' to the user
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
not a list of dicts, just pass a dict to the with: ```yaml with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ```
@pilou-, cyberark-bizdev I had a chance to talk to bcoca today and he said the best way to do this is to pass a list of dicts to the lookup. Then either of these playbook constructs can be valid: ``` with_cyberaskpassword: - {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - {'appid': 'Application2', 'query': 'safe=Safe2;Folder=root;Object=User2', 'output': 'password,passprops.username,passprops.address'} with_cyberarkpassword: {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - debug: msg='{{ lookup('cyberark_password', {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'}) }}' ``` Implementing that will take a little restructuring. I'll go into that in the run method below.
Okay, so @cyberark-bizdev take out my first example. The last two are what we're shooting for. (The middle one is the same as bcoca's example, just in compact yaml instead of "readable" yaml ;-)
Okay, so to implement the examples like this: ``` with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ``` We'll want to take the dict from terms instead of from kwargs. terms will hold the dict that the user passed in so you should be able to drop in replace it in your constructor like so: ```CyberarkPassword(**terms)```
s/write target file {0}/fetch file to {0}/
kushal found that check_call is 2.5 ; check_output is what is 2.7.
No longer using a temporary file. This issue is resolved
<nod> But it seems like it's vulnerable to both symlink attacks and denial of service attacks unless it's randomized. (if it is randomized, it is resistant to but not immune to DOS. Using mkstemp should solve symlink attacks.)
Is this function used to retrieve large files? If so, I suggest redirecting directly to the file, instead of using PIPE -> 'stdout' variable and writing to the file only then.
this should probably go to the debug level
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Cases from lines 361 and 363 work with the previous implementation.
True, I mixed up sth when switching branches. Nevertheless they work on all databases except PostgreSQL, so we need to fix this.
rename variable name as it will shadow built-in 'type'.
blank line not needed
`copy()` in unnecessary.
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
I suggest handling the response failure would yield better error messages. ``.json()`` will always raise a ``JSONDecodeError`` Example: ```python r = requests.get(...) r.raise_for_status() # if 404, 401, 500 etc ... hosts_list = r.json() ```
I think it would be better to use ``params`` with ``requests.get`` rather than building url query.
preferred format: "#12554 - Make sure ..."
I'd omit the blank line after the docstring as you've done in most places.
`copy()` in unnecessary.
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
I suggest handling the response failure would yield better error messages. ``.json()`` will always raise a ``JSONDecodeError`` Example: ```python r = requests.get(...) r.raise_for_status() # if 404, 401, 500 etc ... hosts_list = r.json() ```
I think it would be better to use ``params`` with ``requests.get`` rather than building url query.
preferred format: "#12554 - Make sure ..."
It might be worth including the bridgename in the error message.
This syntax is not supported in python2.6. You will need to index your format like `{0}`
This syntax is not supported in python2.6. You will need to index your format like {0}
This syntax is not supported in python2.6. You will need to index your format like {0}
Ok, there are a bunch of these that need addressed, to index your format string for py2.6 support.
Nesting under distribution ID is useful when you know it, but when returning a single item it's nice if you can make it available under the key the user asked for it with. Take for example: `ansible -m cloudfront_facts -a 'region=us-east-1 distribution=yes domain_name_alias=rsb.io' localhost`. As a user, I'd like to be able to say something like: `{{ cloudfront['rsb.io].ARN }}` to get the ARN, instead of (currently) `{{ cloudfront.[cloudfront.keys()[0]].distribution.ARN }}`. You could return both, so users could either use the ID or one of the CNAMEs to access the facts. They would be unique, since in CloudFront you can't have overlapping CNAMEs ever.
`type='str'` is the default so you can remove that. `default=None` and `required=False` are also defaults that can be removed.
It looks like you're double-nesting `ansible_facts` dictionary, on line 406/409/412 you have the facts being saved in `result = { 'ansible_facts': { 'cloudfront':...` but then here you pass to exit_json under `ansible_facts` so your output looks like: ``` localhost | SUCCESS => { [479/1550] "ansible_facts": { "ansible_facts": { "cloudfront": { "E22CS9R7XQ0CWU": { "distribution": { .... ``` To fix this you can instead do `module.exit_json(msg="Retrieved cloudfront facts.", **result)` or stop nesting the `cloudfront` key under `ansible_facts` on 402/405/408/411.
`try` block should contain fewer statements. `Exception` should be replaced with a more precise exception type.
I think you can just do `result.pop('ResponseMetadata', None)` and then call module.exit_json with result
It looks like this module is not idempotent. Running the same task twice with `state: present` or `state: absent` will show changed every time.
I wonder if it's worth pointing to the alternative here. ðŸ¤”
I would leave only `The django.utils.datetime_safe module is deprecated.`. This a private API, we don't see to provide an alternative.
Right, but the order in which you're doing things would at a glance result in the not-uncommon case of, eg `foo (cr)(lf)` not being properly treated, since the break `sub` wouldn't see that, but the char drop would remove the (cr), leaving a line ending that wouldn't format the right way.
This doesn't really matter since only a single instance of ForemanInventory is created in this script but config_paths should really be an instance attribute (created and given an initial value in ```__init__```) rather than a class attribute. Class attributes exist a single time per class. All instances of the class would share the same instance of that attribute. Instance attributes exist, one per instance and thus can change independently. Modifying config_paths later in the code is a tip-off that this should be an instance-attribute rather than a class-attribute. Also, you can probably move the setting from an environment variable to be with this code when you do that. It would seem to make sense to keep that all together.
The json library entered the python stdlib with python-2.6. Code that runs on the controller (which includes dynamic inventory scripts) need python-2.6 or greater. So there's no need to fallback to simplejson here. the json stdlib library should always be available.
py3.x-only code; can safely ditch the args to `super()`
On further thought, this actually might break something with the new stuff, since you're relying on pyyaml blindly `call`ing whatever is passed in, but the prototype logic that supports object instances only does that call if `isinstance(loader, Reader)` is true. We could probably tweak that somehow, like `callable()` instead, which might be a little more resilient/Pythonic anyway... So this is definitely fine for released code, and it's something I'll keep in mind for the new stuff.
this is not in core and will result in exceptions if not present, use same pattern as we do with boto
Minor consistency nit- our stuff seems to be using `[action]_[backend]()`, where these aliased imports have that reversed. I'd vote for `[action]_[backend]()` over this- makes the code a little easier to read...
Detail why this may be useful
It is usually a good idea to prepend some text so that it is easier to find out where an error was raised (in case it's not clear from the error message).
```suggestion for key, value in self.parameters.plugin_options.items(): ```
Please don't use lists for tracking differences, but `DifferenceTracker`. That produces a much better output.
If for some reason you want to be able to enable without installing, you could add a flag which for `enable`/`disable` fails instead of installing (if the plugin doesn't exist). But the default behavior should be "install if not there, then make sure you have the correct state".
`enabled` should mean the same as `present` and make sure the plugin is enabled, i.e. if it is not installed, install it. (Same for `disabled`.) `present` means "make sure it is there, I don't care if enabled or not".
That's **not** how a module should return differences. Some docker_* modules have done this in the past (and some might still do that), but it's simply wrong (and won't show up when the user runs `ansible-playbook --diff`).
That should only be called if the plugin should in `enabled` state afterwards.
You should check if the options actually changed before reconfiguring the plugin. It only makes sense to reconfigure if this actually changes something.
we want want -> we want
Just in case it's probably a good idea to use `.get()` instead of dict access syntax in case the key is unset.
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
excellent handling of congestion control
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
hm maybe that's what I've searched for
I think this may read better: ```suggestion if missing_keys: ```
I think this approach might read simpler: ```suggestion new_galaxy_yml = dict.from_keys(optional_strings) new_galaxy_yml.update(dict.from_keys(optional_lists), []) new_galaxy_yml.update(dict.from_keys(optional_dicts), {}) new_galaxy_yml.update(galaxy_yml) ```
Are you only doing this to support Python 2.6? Those keys are already unique.
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
I believe it's ```suggestion raise ImportError("We weren't able to import the module {0}".format(module_name)) ```
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
excellent handling of congestion control
I think this may read better: ```suggestion if missing_keys: ```
I think this approach might read simpler: ```suggestion new_galaxy_yml = dict.from_keys(optional_strings) new_galaxy_yml.update(dict.from_keys(optional_lists), []) new_galaxy_yml.update(dict.from_keys(optional_dicts), {}) new_galaxy_yml.update(galaxy_yml) ```
Are you only doing this to support Python 2.6? Those keys are already unique.
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
I believe it's ```suggestion raise ImportError("We weren't able to import the module {0}".format(module_name)) ```
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
excellent handling of congestion control
I think this may read better: ```suggestion if missing_keys: ```
I think this approach might read simpler: ```suggestion new_galaxy_yml = dict.from_keys(optional_strings) new_galaxy_yml.update(dict.from_keys(optional_lists), []) new_galaxy_yml.update(dict.from_keys(optional_dicts), {}) new_galaxy_yml.update(galaxy_yml) ```
Are you only doing this to support Python 2.6? Those keys are already unique.
You added the return statement to the above if the condition which means no need else statement. You should remove it to make it easier to read.
I thought you wanted to remove `return`. Nevertheless I'd also leave the `else` as it increases readability.
That's not true, `return` is to avoid setting new migrations.
Do you think else statement is required in the following example? ```py def greet(): condition = False if condition: return "Hi" else: return "Hello" ```
I prefer the following one rather than the above one ```py def greet(): condition = False if condition: return "Hi" return "Hello" ``` Feel free whether follow the things I point out.
```suggestion sys.exit(1) ```
I removed it.
Ahh true, sorry for the noise. No changes are required.
Do we need an indentation in the message? ```suggestion self.stdout.write("No optimizations possible.") ``` We can also leave an indentation and add a heading: ```python if self.verbosity > 0: self.stdout.write(self.style.MIGRATE_HEADING("Optimizing...")) optimizer = MigrationOptimizer() new_operations = optimizer.optimize(migration.operations, migration.app_label) if len(migration.operations) == len(new_operations): if verbosity > 0: self.stdout.write(" No optimizations possible.") ```
No indentation is needed: ```suggestion "Optimizing from %d operations to %d operations." % ```
`return '%s-%s-%s' % (y or 0, m or 0, d or 0)` can be moved here.
```not (foo is None)``` => ```foo is not None```
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
For long query strings, it's better to use ```query``` parameter here.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
you can use `state` to avoid the 'or' to the user
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Defining `__init__()` to specify a default `output_field` is required until we re-arrange `runtests.py` to avoid importing settings dependant modules before calling `django.setup()`.
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
`copy()` in unnecessary.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
This should be: ``params = config_params + params``
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
@charettes Ideas welcome. Can you take a look? I really don't like the idea of using `psycopg2` in expressions.
I would prefer to wrap value with `Value()` and compile `options` separately.
Defining `__init__()` to specify a default `output_field` is required until we re-arrange `runtests.py` to avoid importing settings dependant modules before calling `django.setup()`.
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
`copy()` in unnecessary.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
This should be: ``params = config_params + params``
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
@charettes Ideas welcome. Can you take a look? I really don't like the idea of using `psycopg2` in expressions.
I would prefer to wrap value with `Value()` and compile `options` separately.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
put closing parenthesis on the next line
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
I'd rather raise an exception to account for the fact that assertions are ignored with `python -O` unless there's some reason to prefer the assert.
chop "one of" add comma before "or"
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Perhaps "Cannot convert DateField '%s' to DateTimeField." (I'm not certain it's an issue here, but we're favoring '%s' over %r until we drop Python 2 support to avoid possible u'' prefixes.)
Too long line.
Default type is `str` so you don't have to set it explicitly. Just use `size=dict(),` instead. The same bellow.
Too long line.
```not (foo is None)``` => ```foo is not None```
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
We don't need tags in this function, I think.
need to catch BotoCoreError here too.
Add a task ```.yaml - debug: var: result.docker_host_facts ``` And similar tasks after the other examples.
```suggestion if self.client.module.params[docker_object]: ```
```suggestion if self.client.module.params['disk_usage']: ```
I think this description is off :)
```suggestion elif docker_object == 'networks': ```
`InvalidInternetID` doesn't seem to exist according to botocore's source
Shouldn't this be `When I(containers) is C(yes)`? (Same for the others.)
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
We don't need tags in this function, I think.
need to catch BotoCoreError here too.
Add a task ```.yaml - debug: var: result.docker_host_facts ``` And similar tasks after the other examples.
```suggestion if self.client.module.params[docker_object]: ```
```suggestion if self.client.module.params['disk_usage']: ```
I think this description is off :)
```suggestion elif docker_object == 'networks': ```
`InvalidInternetID` doesn't seem to exist according to botocore's source
Shouldn't this be `When I(containers) is C(yes)`? (Same for the others.)
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
It would be more readable to raise an error explicitly (like previously), e.g. ```python db_features = connections[self.db].features if ignore_conflicts and not db_features.supports_ignore_conflicts: raise NotSupportedError('This database backend does not support ignoring conflicts.') if update_conflicts: if not db_feature.supports_update_conflicts: raise NotSupportedError( 'This database backend does not support updating conflicts.' ) if unique_fields and not db_features.supports_update_conflicts_with_target: raise NotSupportedError( 'This database backend does not support updating conflicts with ' 'specifying unique fields that will trigger the upsert.' ) ``` (I used new feature flags.)
This condition is the problem. It leads to `icinga2 feature list` never beeing executed in `check_mode`, which results in empty `out`. The condition should be removed alltogether. `self._exec` is only invoked twice anyway: first to run `icinga2 feature list` (should be done regardless of `check_mode` or not) and second at a place in code that is never reached in `check_mode` anyway.
`_search_regex`, `_parse_json`. Again: read coding conventions.
Mandatory. Read coding conventions.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Sorry, I know it was in my changes, but I forgot to remove that lineÃ¢Â€Â¦ Changing the data seems a bit ugly here, could you remove this line and changing the next one by: ``` prefix = ipaddress.ip_network(data["prefix"] + "/" + data["prefix_length"]) ```
For integers please: use `type='int` remove the `isdigit` check from `check_params
It would be more readable to raise an error explicitly (like previously), e.g. ```python db_features = connections[self.db].features if ignore_conflicts and not db_features.supports_ignore_conflicts: raise NotSupportedError('This database backend does not support ignoring conflicts.') if update_conflicts: if not db_feature.supports_update_conflicts: raise NotSupportedError( 'This database backend does not support updating conflicts.' ) if unique_fields and not db_features.supports_update_conflicts_with_target: raise NotSupportedError( 'This database backend does not support updating conflicts with ' 'specifying unique fields that will trigger the upsert.' ) ``` (I used new feature flags.)
This condition is the problem. It leads to `icinga2 feature list` never beeing executed in `check_mode`, which results in empty `out`. The condition should be removed alltogether. `self._exec` is only invoked twice anyway: first to run `icinga2 feature list` (should be done regardless of `check_mode` or not) and second at a place in code that is never reached in `check_mode` anyway.
`_search_regex`, `_parse_json`. Again: read coding conventions.
Mandatory. Read coding conventions.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Sorry, I know it was in my changes, but I forgot to remove that lineÃ¢Â€Â¦ Changing the data seems a bit ugly here, could you remove this line and changing the next one by: ``` prefix = ipaddress.ip_network(data["prefix"] + "/" + data["prefix_length"]) ```
For integers please: use `type='int` remove the `isdigit` check from `check_params
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
Make a method to determine constructing dict from which object will be confused
That makes sense, let's keep it in that sense. EDIT: Actually, I'm wondering that if the `monitor` would always grep the same log4j entry in the outside verification or it always try to grep the new lines after the inner verification? If it's the first case, then the outside verification would always be redundant as we are doomed to just grep the same lines.
Fail here make the process exit, but we need to create it if the state is present
My bad, I thought the `upgradePhase-Starting a REBALANCE` message is used somewhere in verification and thought with the augmented log4j it can now be replaced, but now I see it is only for debugging purposes.
The properties' key-value and default/required can be defined in the spec.
Duplicate with `get` method? This logic can be handled as `if get_traffic_manager_profile`
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
excellent handling of congestion control
Please including a trailing comma in the last item of a dictionary so if more items are added we don't need to modify this line again.
Please use a style like this: ``` python reverse( 'admin:%s_%s_change' % (opts.app_label, opts.model_name), args=(quote(pk_value),), current_app=self.admin_site.name, ) ```
Maybe @felixxm or @carltongibson can guide, but I believe it'd be good practice to use a `warnings.warn` in `__init__`, although a deprecation timeline has not been determined for `django.contrib.postgres.field.JSONField`.
Do we need this check? All tests pass without it.
`form_class` is defined in `RangeField.formfield()` so this is redundant.
WDYT about adding `or self.title` here? ``` python self.title = title or self.title or self.lookup_title ``` This would add support for the following: ``` python class MyStaffFilter(BooleanFieldListFilter): title = "By can haz admin" # no effect :( overwritten in __init__ @admin.register(User) class UserAdmin(UserAdminBase): list_filter = ( ("is_staff", MyStaffFilter), ) ```
Do note that this does not take `self.principal` into account, neither is that being checked. So you might return with `changed=False` if there's a tgt for a totally different principal.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
Again, this only works on the primary credential cache. If the ticket is in another, this might not work.
also need to check whether the workspace is name or resource id
I don't see a need for string interpolation in cases like this.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
for py2/3 and i18n issues we've created a to_text function to use for stringification of errors, avoid str()
nit: this doesn't need to be a field, you can just use a local variable
probably inside the `if` instead of before since it's kind of confusing to see this for a test that isn't going to actually read the DLQ
not a list of dicts, just pass a dict to the with: ```yaml with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ```
Okay, so @cyberark-bizdev take out my first example. The last two are what we're shooting for. (The middle one is the same as bcoca's example, just in compact yaml instead of "readable" yaml ;-)
@pilou-, cyberark-bizdev I had a chance to talk to bcoca today and he said the best way to do this is to pass a list of dicts to the lookup. Then either of these playbook constructs can be valid: ``` with_cyberaskpassword: - {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - {'appid': 'Application2', 'query': 'safe=Safe2;Folder=root;Object=User2', 'output': 'password,passprops.username,passprops.address'} with_cyberarkpassword: {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - debug: msg='{{ lookup('cyberark_password', {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'}) }}' ``` Implementing that will take a little restructuring. I'll go into that in the run method below.
Okay, so to implement the examples like this: ``` with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ``` We'll want to take the dict from terms instead of from kwargs. terms will hold the dict that the user passed in so you should be able to drop in replace it in your constructor like so: ```CyberarkPassword(**terms)```
Unnecessary blank lines here, removing these blank lines is preferable.
I would also detect if the value is a bool and set it accordingly to `on`/`off`.
I don't like this warning. If the force parameter is given, this is expected behavior. So no warning is needed. It just clutters the Ansible output. Logging something that is not shown by default seems enough if needed.
Should the default be https, if so update docs
this is not a 1.0 callback, its using 2.0 API
Line is too long.
Too long line.
Too long line.
Default type is `str` so you don't have to set it explicitly. Just use `size=dict(),` instead. The same bellow.
Too long line.
use python bool for the default `default=False`
Use same `env_fallback` as `X_AUTH_TOKEN`
This is unnecessary, AnsibleAWSModule handles it.
This whole connection block can be replaced with `conn = module.client('ssm')`
```suggestion elif paramname == 'SubnetIds': ```
This one still needs to be swapped.
```suggestion for modulesubnet in modparams['SubnetIds']: ```
```suggestion SubnetIds=module.params.get('subnet_ids'), ``` and here as well
```suggestion SubnetIds=module.params.get('subnet_ids'), ``` Here this var needs to stay as you had it originally - ansible prefers snake cased but boto typically needs camelcase. `instance_parameters` will be passed into the boto connection so needs to match what the API expects, both here and later when you access the returned parameters. https://docs.aws.amazon.com/dms/latest/APIReference/API_CreateReplicationSubnetGroup.html#API_CreateReplicationSubnetGroup_RequestSyntax
Seems nicer to infer private_zone if vpc_id is set. But not a blocker
Remove the two extra double-quotes here.
Ah, i'd expect a bool to always be True/False, if that is not the case (no time to test really right now :)), then indeed, this needs to stay (and this goes for my other bool remarks)
Unessecary blank line
~~~diff - if self.hcloud_volume.size <= size: + if self.hcloud_volume.size < size: if not self.module.check_mode: self.hcloud_volume.resize(size).wait_until_finished() self._mark_as_changed() + elsif self.hcloud_volume.size > size: + self.module.warn("Shrinking of volumes is not supported") ~~~
I mean not to actually shrink it but doing a "no-op" but showing a warning to the user, that volumes can not be shrinked.
`enable_3D` is a bool, so it will always be set, i think this can be a little simplified as well ```suggestion video_spec.device.enable3DSupport = self.params['enabled_3D'] enabled_3d = self.params['enabled_3D'] if self.params['enable_3D'] != video_card_facts['enable_3D_support']: self.change_detected = True ```
If the minimum value is `1.17` as per module docs, shouldn't this be a float? ```suggestion video_memory_mb=dict(type='float'), ```
```suggestion module.fail_json(msg='Unable to find the specified virtual machine : %s' % (module.params.get('uuid') or module.params.get('name'))) ```
- does not match the correct image. - you're not following the coding conventions.
yes, we've been wanting to change that for a while but were waiting until we made the architecture more pluggable to allow for old/new formats to be used transparently.
CTR mode doesn't actually require padding, so this is unnecessary. That said, I assume you're staying compatible with existing vault implementations which already do this. It's not a security thing, just a few wasted bytes/CPU cycles.
- extract mandatory information first. - incorrect fallback.
```suggestion - If C(false) (NO CYCLE) is specified, any calls to nextval after the sequence ```
use `_hidden_inputs` method.
- does not match the correct image. - you're not following the coding conventions.
Why have both `schema` and `newschema`? I would assume that if I specify another value for `schema`, that the schema will be changed.
Rename DO to DigitalOcean to avoid acronyms
+1 for adding `name:` lines
Ah, i'd expect a bool to always be True/False, if that is not the case (no time to test really right now :)), then indeed, this needs to stay (and this goes for my other bool remarks)
`enable_3D` is a bool, so it will always be set, i think this can be a little simplified as well ```suggestion video_spec.device.enable3DSupport = self.params['enabled_3D'] enabled_3d = self.params['enabled_3D'] if self.params['enable_3D'] != video_card_facts['enable_3D_support']: self.change_detected = True ```
```suggestion module.fail_json(msg='Unable to find the specified virtual machine : %s' % (module.params.get('uuid') or module.params.get('name'))) ```
comma after tuple
chop "one of" add comma before "or"
put closing parenthesis on the next line
I think `enumerate` would work here
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
Please use a single quote.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
I'd rather raise an exception to account for the fact that assertions are ignored with `python -O` unless there's some reason to prefer the assert.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
Could you add at the top of Apply the following line. This will create an ems log event for users with auto support turned on. netapp_utils.ems_log_event("na_ontap_ldap", self.server)
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
I suggest handling the response failure would yield better error messages. ``.json()`` will always raise a ``JSONDecodeError`` Example: ```python r = requests.get(...) r.raise_for_status() # if 404, 401, 500 etc ... hosts_list = r.json() ```
You probably should just exit here with `changed=False`
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
excellent handling of congestion control
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
```suggestion # RemovedInDjango50Warning: When the deprecation ends, revert to # FORM_RENDERER="django.forms.renderers.Jinja2", ```
missing space after comma (check code with flake8)
Ah -- I see you want to use a user name for the ESTABLISH DOCKER CONNECTION line later... we can save docker_remote_user for that purpose as well.
So we weren't going to issue a warning if _play_context.remote_user matches with what docker is going to use anyways. So I suppose we probably want to do something like this in the else: ``` python p = subprocess.Popen([self.docker_cmd, 'inspect', '--format', '{{.Config.Image}}', self._play_context.remote_addr], shell=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE) out, err = p.communicate() if p.returncode != 0: display.warning() # Warning message about being unable to get user to login as from docker + out + err docker_remote_user = out.strip() or 'root' if docker_remote_user != self._play_context.remote_user: display.warning('docker {0} does not support remote_user, using container default'.format(docker_version)) ```
`topics` defined here and in next test, maybe move up to `init`
SR-IOV can be enabled only if `C(num_virt_func) > 0.`
optional parameter, related to `C(num_virt_func)`.
Given that 1.0 was released 2 years ago, I'd even go with 1.1 as the minimum version.
Do we want to support all these versions? I'd vote for only testing 1.0 and newer. 1.0 was released roughly 2 years ago.
If it doesn't add too much to the runtime, I think it would be good to include some more cases like you suggest.
Also missing parentheses: > during vm execution (e.g. due to a vm label update),
Lines 58-60 have inconsistent indent.
I would make it a list so user can pass more versions
> during vm execution (e.g. due to an update),
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I think we can move entire `ordering` logic to a separate branch i.e. ```python if self.ordering: ... sql, sql_params = super().as_sql(compiler, connection, ordering=( 'ORDER BY ' + ', '.join(ordering_expr_sql) )) return sql, sql_params + ordering_params return super().as_sql(compiler, connection, ordering='') ```
I don't see a need for string interpolation in cases like this.
please use a variable for this string so that if it changes, we don't have to update it below as well
I meant for the entire string here to be a constant; otherwise looks good to me.
"From Django 5.0" isn't clear to me. Does it mean, "Starting with Django 5.0"? If so, some other ways to say this could be "Starting in Django 5.0," "In Django 5.0 and up," "From Django 5.0 onward," or perhaps simply, "In Django 5.0" if the behavior won't carry forward.
"In Django 5.0" :+1:
This can be single-lined: ```suggestion warnings.warn(self._deprecation_message, category=RemovedInDjango50Warning) ```
I don't think we need underscore prefixes: - `_DeprecatedConvertValueMixin` -> `DeprecatedConvertValueMixin` - `_deprecation_value` -> `deprecation_value` - `_deprecation_message` -> `deprecation_msg`
It might be worth compiling the regexp in the class or [module level and reuse](https://github.com/django/django/blob/master/django/contrib/localflavor/ca/forms.py#L16-L17).
Running the entire test suite I could reproduce the error, but not if only running `django.contrib.gis`. Not sure exactly the cause but this code seems suspicious as we leave a different model admin registered for other tests.
I think that there is no need to check all empty values, so maybe: ``` # Default should be populated on an empty value. pub_form = PubForm({}) pub_form.mocked_mode = '' pub = mf2.save(commit=False) self.assertEqual(pub.mode, default_mode) ```
This should be: ``params = config_params + params``
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
`copy()` in unnecessary.
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
WDYT about adding `or self.title` here? ``` python self.title = title or self.title or self.lookup_title ``` This would add support for the following: ``` python class MyStaffFilter(BooleanFieldListFilter): title = "By can haz admin" # no effect :( overwritten in __init__ @admin.register(User) class UserAdmin(UserAdminBase): list_filter = ( ("is_staff", MyStaffFilter), ) ```
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
@charettes Ideas welcome. Can you take a look? I really don't like the idea of using `psycopg2` in expressions.
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
is not specified.
put `warnings.warn()` outside the if/else so you don't need to repeat it
Is there a need to hardcode pks? This is generally to be avoided, I think.
Wrap lines closer to 79 characters and use () when referring to a function. ``` # get_current_site() will lookup a Site object, so these must match the # domains in the MockSite model. ```
Yes, fine with me.
This inserts the `RenamePermissions` operations _after_ the `RenameModel` operations. That's correct for forwards migrations. However, if you're rolling back a model rename you'd need to rename the permissions afterwards as well, hence add it _before_ the `RenameModel` operation into the migration.
@charettes, any reply here? I guess we shouldn't block the patch about the issue with backwards migrations if we can't find a simple solution.
Having the migration name hard-coded in here doesn't strike me like the cleanest solution. I don't have an alternative right now, tho.
Ah, didn't know this existed yet. I see that this PR is mostly a "copy" of the ContentTypes one. Sounds alright for now, then.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
I think that there is no need to check all empty values, so maybe: ``` # Default should be populated on an empty value. pub_form = PubForm({}) pub_form.mocked_mode = '' pub = mf2.save(commit=False) self.assertEqual(pub.mode, default_mode) ```
`copy()` in unnecessary.
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
I am not able to create a new monitor - ``` fatal: [localhost]: FAILED! => { "changed": false, "invocation": { "module_args": { "apikey": "VALUE_SPECIFIED_IN_NO_LOG_PARAMETER", "check_type": null, "monitorid": null, "name": "myMonitor_002", "state": "present", "url": "http://www.my-domain.com" } }, "msg": "Could not perform action newMonitor" } ``
```suggestion Kwargs: ```
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
Why not also do this when `module.check_mode` is `True`? Then the check mode behavior is closer to the real thing.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
Raise `AnsibleCallbackError`, which can be imported from `ansible.errors`.
It also looks you you are missing the variable to be inserted for the `%s`.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
`capabilities` can be cached instead of fetching it from remote host each time. https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/network/vyos/vyos.py#L80
colors should all be configurable
Maybe a helper method would help eliminate the redundancy of these methods? e.g. `return self._value_or_setting(self._location, settings.MEDIA_ROOT)`
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
Function calls are complex. For example: ``` from youtube_dl.jsinterp import JSInterpreter jsi = JSInterpreter(''' function a(x) { return x; } function b(x) { return x; } function c() { return [a, b][0](0); } ''') print(jsi.call_function('c')) ```
We don't need tags in this function, I think.
> They can't be multiline, can they? Yep. According to [ECMA 262 5.1](http://www.ecma-international.org/ecma-262/5.1/), CR (U+000D), LF (U+000A), LS (U+2028) and PS (U+2029) are not allowed in RegExp literals
need to catch BotoCoreError here too.
I think this should be `if tags and tags != igw.tags:`. Otherwise if we're creating a gateway with no tags this condition will be met and it will crash later since there are no tags. On second thought, just make the default of tags be an empty dict instead of None, [here](https://github.com/ansible/ansible/pull/23782/files#diff-7c7439a69b0a017fb8e03a769c5ef29bR179) and it will fix the problem.
omit the blank line
`InvalidInternetID` doesn't seem to exist according to botocore's source
Add here that the `key_alias` or `key_arn` are both ways to provide it.
You don't need this conditional, since Ansible enforces that these are the only choices.
I learned recently that you can use actual separate literals to improve readability: ```suggestion @pytest.mark.parametrize(['url', 'expected'], [ ```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
Oh I see :)
We don't need tags in this function, I think.
need to catch BotoCoreError here too.
I think this should be `if tags and tags != igw.tags:`. Otherwise if we're creating a gateway with no tags this condition will be met and it will crash later since there are no tags. On second thought, just make the default of tags be an empty dict instead of None, [here](https://github.com/ansible/ansible/pull/23782/files#diff-7c7439a69b0a017fb8e03a769c5ef29bR179) and it will fix the problem.
move to finally
this should be in `finally` just in case the commands before throw an exception
`InvalidInternetID` doesn't seem to exist according to botocore's source
Add here that the `key_alias` or `key_arn` are both ways to provide it.
You don't need this conditional, since Ansible enforces that these are the only choices.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
Oh, I just meant to suggest the `value` to `values` (plural) renaming, not more...
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
Both cases test the same thing, don't repeat yourself. Test things based on their properties. Here you'd probably want to test iterables, not just sets.
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
I removed the NotIn assertions in my edits because they are brittle since a typo in the message means they would inadvertently pass. I suppose if error messages were class attributes that would make it more robust, but it seems unlikely to me that a regression could be introduced such that both messages are displayed.
It probably makes sense to test that the exception reason also matches expectations
chop "one of" add comma before "or"
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
Oh I see :)
We don't need tags in this function, I think.
need to catch BotoCoreError here too.
I think this should be `if tags and tags != igw.tags:`. Otherwise if we're creating a gateway with no tags this condition will be met and it will crash later since there are no tags. On second thought, just make the default of tags be an empty dict instead of None, [here](https://github.com/ansible/ansible/pull/23782/files#diff-7c7439a69b0a017fb8e03a769c5ef29bR179) and it will fix the problem.
move to finally
this should be in `finally` just in case the commands before throw an exception
`InvalidInternetID` doesn't seem to exist according to botocore's source
Add here that the `key_alias` or `key_arn` are both ways to provide it.
You don't need this conditional, since Ansible enforces that these are the only choices.
Can you make this into a couple statements for easier readability? Something like: ``` invalid_entries = [x for x in statement['Principal']['AWS'] if not x.startswith('arn:aws:iam::)] if clean_invalid_entries and len(invalid_entries): for entry in invalid_entries: statement['Principal']['AWS'].remove(entry) ``` Or there's always: ``` original_size = len(statement['Principal']['AWS']) statement['Principal']['AWS'] = [x for x in statement['Principal']['AWS'] if x.startswith('arn:aws:iam::)] have_invalid_entries = (original_size == len(statement['Principal']['AWS'])) ```
Oh I see :)
need to catch BotoCoreError here too.
`InvalidInternetID` doesn't seem to exist according to botocore's source
We don't need tags in this function, I think.
I think this should be `if tags and tags != igw.tags:`. Otherwise if we're creating a gateway with no tags this condition will be met and it will crash later since there are no tags. On second thought, just make the default of tags be an empty dict instead of None, [here](https://github.com/ansible/ansible/pull/23782/files#diff-7c7439a69b0a017fb8e03a769c5ef29bR179) and it will fix the problem.
Use the [boto3 exception guidelines](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/amazon/GUIDELINES.md#boto3-2)
Add here that the `key_alias` or `key_arn` are both ways to provide it.
You don't need this conditional, since Ansible enforces that these are the only choices.
Please remove `no_log=True` from username
put closing parenthesis on the next line
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
probably inside the `if` instead of before since it's kind of confusing to see this for a test that isn't going to actually read the DLQ
for py2/3 and i18n issues we've created a to_text function to use for stringification of errors, avoid str()
Unnecessary blank lines here, removing these blank lines is preferable.
nit: this doesn't need to be a field, you can just use a local variable
Okay, so to implement the examples like this: ``` with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ``` We'll want to take the dict from terms instead of from kwargs. terms will hold the dict that the user passed in so you should be able to drop in replace it in your constructor like so: ```CyberarkPassword(**terms)```
@pilou-, cyberark-bizdev I had a chance to talk to bcoca today and he said the best way to do this is to pass a list of dicts to the lookup. Then either of these playbook constructs can be valid: ``` with_cyberaskpassword: - {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - {'appid': 'Application2', 'query': 'safe=Safe2;Folder=root;Object=User2', 'output': 'password,passprops.username,passprops.address'} with_cyberarkpassword: {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - debug: msg='{{ lookup('cyberark_password', {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'}) }}' ``` Implementing that will take a little restructuring. I'll go into that in the run method below.
Okay, so @cyberark-bizdev take out my first example. The last two are what we're shooting for. (The middle one is the same as bcoca's example, just in compact yaml instead of "readable" yaml ;-)
not a list of dicts, just pass a dict to the with: ```yaml with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ```
`copy()` in unnecessary.
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
Sure, a separate PR sounds good.
you can move it to before `if` as just `docs = {}` line, this should read better.
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
`self.base_command` is already list so I think this is useless.
Please remove deprecated parameters as this is new module.
Need a colon at the end here
`copy()` in unnecessary.
Sure, a separate PR sounds good.
you can move it to before `if` as just `docs = {}` line, this should read better.
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
You should probably use `transform_commands` from `ansible.module_utils.network.common.utils` instead.
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
`self.base_command` is already list so I think this is useless.
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
Need a colon at the end here
`copy()` in unnecessary.
you can move it to before `if` as just `docs = {}` line, this should read better.
Sure, a separate PR sounds good.
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
`self.base_command` is already list so I think this is useless.
This is unnecessary, AnsibleAWSModule handles it.
Need a colon at the end here
chop "one of" add comma before "or"
okay, but would be helpful to say _why_ we need to always return True.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
put closing parenthesis on the next line
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
`copy()` in unnecessary.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
chop "one of" add comma before "or"
I'd rather raise an exception to account for the fact that assertions are ignored with `python -O` unless there's some reason to prefer the assert.
okay, but would be helpful to say _why_ we need to always return True.
put closing parenthesis on the next line
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
`copy()` in unnecessary.
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
prefer hanging indent style with 1 arg per line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
I prefer putting the closing ) on the next line
put closing parenthesis on the next line
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
chop "one of" add comma before "or"
, keeping a reference to the cyptes object so that the vsimem file...
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
I think this should be false (not a string)
This should be: ``params = config_params + params``
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Okay, so @cyberark-bizdev take out my first example. The last two are what we're shooting for. (The middle one is the same as bcoca's example, just in compact yaml instead of "readable" yaml ;-)
@pilou-, cyberark-bizdev I had a chance to talk to bcoca today and he said the best way to do this is to pass a list of dicts to the lookup. Then either of these playbook constructs can be valid: ``` with_cyberaskpassword: - {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - {'appid': 'Application2', 'query': 'safe=Safe2;Folder=root;Object=User2', 'output': 'password,passprops.username,passprops.address'} with_cyberarkpassword: {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - debug: msg='{{ lookup('cyberark_password', {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'}) }}' ``` Implementing that will take a little restructuring. I'll go into that in the run method below.
not a list of dicts, just pass a dict to the with: ```yaml with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ```
I think it is better to pass module as argument to build_xml() and handle lxml install check-in module_utils instead of having it in each module
Having this check in each module will result in code duplication and an overhead for module writer. `module` param will be used if the check is moved to utils function
Need a colon at the end here
Okay, so to implement the examples like this: ``` with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ``` We'll want to take the dict from terms instead of from kwargs. terms will hold the dict that the user passed in so you should be able to drop in replace it in your constructor like so: ```CyberarkPassword(**terms)```
prefer hanging indent style with 1 arg per line
`band_input`, you don't get much by saving one char :-)
I think `enumerate` would work here
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
nit: this doesn't need to be a field, you can just use a local variable
probably inside the `if` instead of before since it's kind of confusing to see this for a test that isn't going to actually read the DLQ
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
move to finally
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
this should be in `finally` just in case the commands before throw an exception
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
I realize this predates your PR, but why is 'unused' not used? It appears to be the target field provided by the SQLUpdateCompiler during as_sql(). This seems like a really good spot to allow a backend to hook in and CAST the expression to a type that is allowed by the target column.
What do you think about using `argparse.SUPPRESS` instead (as suggested in the previous patch)? e.g. ```suggestion parser.add_argument( '-v', '--verbosity', default=1, type=int, choices=[0, 1, 2, 3], help=argparse.SUPPRESS if 'verbosity' in self.suppressed_arguments else ( 'Verbosity level; 0=minimal output, 1=normal output, ' '2=verbose output, 3=very verbose output' ), ) ``` This way the list of options will not be misleading anymore and at the same time default values will be available for subcommands :thinking: This should increase backward compatibility.
chop "one of" add comma before "or"
put closing parenthesis on the next line
should probably be validating whichever components aren't part of the vlan's "primary key" (seems like parent interface/node/interface_name, but not sure off the top of my head) and either allow them to be modified or at least warn/error if they're not in the requested state
nit: this doesn't need to be a field, you can just use a local variable
I'd rather raise an exception to account for the fact that assertions are ignored with `python -O` unless there's some reason to prefer the assert.
colors should all be configurable
If we're just testing broker compatibility I don't think we even need this part of the test.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
remove u'' prefix (syntax error on Python 3.2 and unnecessary since this file has `from __future__ import unicode_literals`).
```suggestion return ''.join([ self.handle_word(word) for word in words ]) ``` I changed `handle_word` to return `word` in remaining cases.
put closing parenthesis on the next line
Passing `lead` and `trail` to the `trim_punctuation()` looks unnecessary. This method is called only here so we always pass `('', word, '')`. Maybe: ```suggestion lead, middle, trail = self.trim_punctuation(word) ``` and ```python def trim_punctuation(self, word): """ Trim trailing and wrapping punctuation from `word`. Return the items of the new state. """ lead, middle, trail = '', word, '' ... return lead, middle, trail ```
a single file or a list ...
chop "one of" add comma before "or"
Perhaps "Cannot convert DateField '%s' to DateTimeField." (I'm not certain it's an issue here, but we're favoring '%s' over %r until we drop Python 2 support to avoid possible u'' prefixes.)
I'd rather raise an exception to account for the fact that assertions are ignored with `python -O` unless there's some reason to prefer the assert.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
You want to do it the other way around; no adjustments should be required to the base schema adaptor. In order to achieve that you'll want to have the PostGIS `_create_index_sql` method *not* pass `fields` but `expressions` when necessary ```python def _create_index_sql(self, model, *, fields=None, **kwargs): if fields is None or len(fields) != 1 or not hasattr(fields[0], 'geodetic'): return super()._create_index_sql(model, fields=fields, **kwargs) field = fields[0] template = None if field.geom_type == 'RASTER': # For raster fields, wrap index creation SQL statement with ST_ConvexHull. # Indexes on raster columns are based on the convex hull of the raster. template = self.rast_index_wrapper % '%(expressions)s' elif field.dim > 2 and not field.geography: # Use "nd" ops which are fast on multidimensional cases template = "%%(expressions)s %s" % self.geom_index_ops_nd expressions = None if template is not None: fields = None expressions = [Func(Col(field.column), template=template)] using = ' USING %s' % self.geom_index_type return super()._create_index_sql(model, fields=fields, expressions=expressions, using=using) ```
I don't think that a separate ticket is necessary, using `super()._create_index_sql()` will fix described issue.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
To be consistent with the rest of the codebase, I'd import `from django.utils.six.moves import range` first.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
```suggestion return ''.join([ self.handle_word(word) for word in words ]) ``` I changed `handle_word` to return `word` in remaining cases.
put closing parenthesis on the next line
chop "one of" add comma before "or"
Perhaps "Cannot convert DateField '%s' to DateTimeField." (I'm not certain it's an issue here, but we're favoring '%s' over %r until we drop Python 2 support to avoid possible u'' prefixes.)
For long query strings, it's better to use ```query``` parameter here.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
Similarly, ```if tc['skip'].get('i')```
```not (foo is None)``` => ```foo is not None```
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
Does the result provide any insight into if anything's changed? Looks like put and delete are currently both hard coded to return ``changed=True``
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
Since we expect the name to start with 'http://' or 'https://' I would so this: ```python if name.startswith('http://') or name.startswith('https://'): ... ```
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
For long query strings, it's better to use ```query``` parameter here.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
Similarly, ```if tc['skip'].get('i')```
```not (foo is None)``` => ```foo is not None```
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
Does the result provide any insight into if anything's changed? Looks like put and delete are currently both hard coded to return ``changed=True``
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
Since we expect the name to start with 'http://' or 'https://' I would so this: ```python if name.startswith('http://') or name.startswith('https://'): ... ```
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
I prefer putting the closing ) on the next line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
prefer hanging indent style with 1 arg per line
I think `enumerate` would work here
`band_input`, you don't get much by saving one char :-)
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
move to finally
this should be in `finally` just in case the commands before throw an exception
This can be changed to look for only one space now that #57209 is merged. Thanks again for writing tests. Much appreciated.
This is usually not needed
For algorithmic code, it can make sense to test private methods and private functions in isolation from the rest of the code. This does seem to be a place where that could be justified. The code being tested is functional (meaning it operates via parameters and return values rather than callbacks) and it plugs into a larger framework which is outside of our control. What I'll sometimes do is push all the permutations of data that I care about at the private function and then push a small subset at the public interface to make sure that the interaction between the public and private code is working as expected.
Usually, testing private interfaces doesn't make sense.
It doesn't matter whether it's a method or a function. A private function is related to the module scope, a private method is related to the class. Still, both are private, it's just a different level of namespacing. If a module name starts with an underscore it'd be also private.
Plz also use `match` arg here
Both cases test the same thing, don't repeat yourself. Test things based on their properties. Here you'd probably want to test iterables, not just sets.
No need to parametrize with just one case.
(Similarly, mark any string that you are going to call decode on with as a b"string".)
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
not Python 3 compatible
Remove print statement: ```suggestion ```
this is a setting 'resolved' not the definition, you are mixing the concepts here.
Looking at collections source code I think you mean `collections.Iterator` here.
This code isn't correct. (2, 7) is the check you want, and then with >= if I am not mistaken.
This seem to be duplicated for some reason.
I would avoid cloaking the `resolver` variable, I like your `sub_resolver` naming above.
Please swap the order in the assertions and put `response` on the left, `self.assertEqual(response['Expires'], 'Sun, 17 Jul 2016 10:00:02 GMT')`
Mind putting this magic value into a constant with a descriptive name? I'd read much better if it was ```suggestion sys.exit(RC_CLI_INIT_FAILURE) ``` or something like that.
It's a standard practice to use separate args for different params. Also, autogenerated param ids aren't very readable when they are complex so in such cases it's better to assign them meaningful names (they are displayed in the pytest report): ```suggestion @pytest.mark.parametrize( ('returned_items_count', 'patched_dc_stdout'), ( (3, (DOCKER_OUTPUT_MULTIPLE, '')), (2, (PODMAN_OUTPUT, '')), (0, ('', '')), ), ids=('docker JSONL', 'podman JSON sequence', 'empty output'), ) def test_docker_images(docker_images, mocker, returned_items_count, patched_dc_stdout): mocker.patch( 'ansible_test._internal.docker_util.docker_command', return_value=patched_dc_stdout) ret = docker_images('', 'quay.io/ansible/centos7-test-container') assert len(ret) == returned_items_count ```
As per PEP257 you should have a sentence here, hence it should end with a period: ```suggestion """Test that lenient_lowercase() proper results.""" ``` Also, let's rephrase it to contain useful info.
For the author information we normally only keep name and GitHub handle.
This can be changed to look for only one space now that #57209 is merged. Thanks again for writing tests. Much appreciated.
```suggestion short_description: Manage volumes on Vexata VX100 storage arrays ```
Ã°ÂŸÂ‘Â to not needing this
This is usually not needed
Plz also use `match` arg here
Missing period at end.
Can we rename this parameter to `state` in order to match other modules ? ``` present == enable absent == disable ```
There in no module in VMware space, which is absent / present for enable / disable but we can add `state` with multiple choices like `[ absent, present, enabled, disabled ]` .
If changing to `None` from `''` in `.slice_expression()` above, then: ```suggestion if self.end is None: return f'{lhs}[%s:]', params + [self.start] else: return f'{lhs}[%s:%s]', params + [self.start, self.end] ```
IMO it's valuable, because it explains why we can always use `self.start` without an extra check.
Just a thought -- would an `expression.flatten()` method be useful? The check below would then be `any(expr for expr in expression.flatten() if isinstance(expr, F))`. Might need to look more closely at other places in the ORM to figure out if it'd be a useful construct.
Inner functions are slow, especially for pypy - best to extract this!
There is no need to provide a value for `max_pages_num` - I think it should be calculated automatically: ```python (self.on_each_side + self.on_ends) * 2 ``` Otherwise developers can provide values that are incompatible with each other...
I think you meant `get_ellipsized_page_range` here. Use `number` instead of `page_num` which matches the argument name used by `Paginator.get_page()`.
F() expressions aren't the only ones that can refer to other columns in the query. How about Q(other_field__isnull=True). Also, expressions are free to resolve columns of the query without using F-expressions. We need some other way to know if the expression refers to columns of the query. Maybe we could first resolve the expression, the check for Col references? That might be better. The check should be done in the expression, so that the expression tells Django if it is referring to any columns. Making the compiler guess this is the wrong way in my opinion.
```suggestion params = self.settings[alias].copy() ```
I had to change this to `template_params['subquery'], sql_params = self.subquery.query.get_compiler(connection=connection).as_sql()` in order to compile correctly when using a database other than the `DEFAULT_DB_ALIAS`.
Yeah that's what I suspected too. Stupid SQL.
Remove print statement: ```suggestion ```
not Python 3 compatible
Looking at collections source code I think you mean `collections.Iterator` here.
This code isn't correct. (2, 7) is the check you want, and then with >= if I am not mistaken.
This seem to be duplicated for some reason.
Pretty sure you shouldn't be removing ANSIBLE_METADATA.
should probably address this too at some point. But I can always fix this after. Usually ``` from ansible.module_utils.basic import AnsibleModule from ansible.module_utils.ec2 import ec2_argument_spec, get_aws_connection_info, connect_to_aws ``` suffices but flake8 will tell you if you've missed anything.
I guess this is `seconds` but unit must be added. When `state` is `active` and `wait_for_public_IPv` is set, then value of `wait_timeout` is used twice (hence max waiting time is 2*`wait_timeout`): it should be specified.
Mind putting this magic value into a constant with a descriptive name? I'd read much better if it was ```suggestion sys.exit(RC_CLI_INIT_FAILURE) ``` or something like that.
I think I'd trim it right here rather than in tests ```suggestion """.lstrip() # noqa: E501 ```
Match the error message
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
You're checking two separate properties here. This should be in a separate test.
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
It probably makes sense to test that the exception reason also matches expectations
You should probably expect unicode strings
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
(Similarly, mark any string that you are going to call decode on with as a b"string".)
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
Line is too long.
Too long line.
Too long line.
This line is too long. Max line length allowed in Ansible is 120 characters.
Default type is `str` so you don't have to set it explicitly. Just use `size=dict(),` instead. The same bellow.
Too long line.
use python bool for the default `default=False`
Line is too long.
Line is too long.
it might not be relevant in this particular case but be aware to better handle LANG when searching in output. use `module.run_command_environ_update = dict(LANG='C', LC_ALL='C', LC_MESSAGES='C', LC_CTYPE='C')` early after `module` has been instantiated to get that covered.
The failures on MySQL, PostgreSQL and likely Oracle seems to be an indicator that it should not work on SQLite either. There's only so much that Django can do when coercing types in a database agnostic way and I'm not sure trying to support cases where `float` are implicitly properly converted to `Decimal` at the ORM level is a pattern we should encourage. If you're filtering against decimal/numeric data with floats you're better off defining your coercion rules explicitly at the application level and pass _stable_ numeric data to the database to avoid surprises down the road when a specific float value happens to take an unexpected rounding/loss of precision path along the way to the query executor.
Please ignore, my suggestion is invalid syntax.
note: the 'startswith' _ is still needed for deprecations (but that is handled elsewhere), so we only skipped when it was a symlink (rename deprecating old name, not module itself) so this should 'work'tm as it is now
Breaks if not `int`.
Breaks extraction completely if `params` does not match number of values to unpack.
I'm really not sure what this is supposed to do. You've proved that `diff` is truthy above, and now you're asserting that whatever object `diff` is is symmetric over `not`, but I'm not sure why. You don't seem to be making any claims about the contents of `diff`, so I don't quite see what the value of this test is.
colors should all be configurable
Line is too long.
This line is too long. Max line length allowed in Ansible is 120 characters.
Too long line.
`George. R. R. Martin` â†’ `George R. R. Martin` (Remove the extra period, and throughout below.)
Please rewrite `@override_settings` into a single line: ```python @override_settings(STATICFILES_DIRS="a string") ```
> both `CombinedExpression` and `Lookup` combine a left and a right expression. Not always, many lookups are complicated expressions or even function calls. The only common factor for me is that both have two arguments. Also `CombinedExpression` has a lot of unnecessary logic, e.g. `SQLiteNumericMixin`. We should probably compare `Lookup` subclassing `Expression` vs. `CombinedExpression` :thinking:
Why the `CombinedExpression` and not `Expression`? IMO it's misleading, I know that `CombinedExpression` has the concept of right-hand and left-hand sides but for other purposes.
Or split the args over two lines if it passes 119 chars.
We usually avoid hanging indents, and prefer this: ``` WindowTestModel.objects.create( name='Jones', salary=45000, department='Accounting', hiredate=datetime.datetime(2005, 11, 1) ) ```
Since you don't actually need the instances of all those models, can you use `WindowTestModel.objects.bulk_create()` please.
I'd put `hire_date` on the next line -- the longer line isn't helping readability here. Maybe it would be DRYer to put the data in a list of tuples, e.g. `'Williams', 37000, 'Accounting', datetime.datetime(2009, 6, 1),` and use a comprehension to create the objects.
`cls.staff_user = User.objects.create_user(username='user', password='secret', email='user@example.com', is_staff=True)`
check that -> and that (no comma needed since the two clauses are independent)
With this simplification, the pythonic way of generating a list is to use list comprehension. You can replace the whole function body with: ```python return [parse_to_obj(r) for r in parse_to_logical_rows(vlan_out)] ```
Oh, I see. `run_commands()` runs list of commands and returns list of results.
Needs to be `run_commands(module, ['show vlan brief'])[0]`.
prefer if you use hanging indent style for this assertion to match the other tests
@pierremahot we'll need a test for this
Do note that this does not take `self.principal` into account, neither is that being checked. So you might return with `changed=False` if there's a tgt for a totally different principal.
Trick credit: https://twitter.com/raymondh/status/967927989752098816
Oh.. I missed the part about "more than two lines", so please post example output, so we could take a closer look at the issue together :)
You won't need this complexity with the snippet I've posted above
Again, this only works on the primary credential cache. If the ticket is in another, this might not work.
`SearchedCase` is no longer it's own type so this should be named `Case conditions must be lookups`. Alternatively, do this checking directly inside the `When` types, since users will be using these directly.
You can also drop the parentheses :wink:
I don't think we need the extra assignment here as this is only used once.
Also make the comparison case-insensitive, i.e. ```python if self.index_type.lower() != 'gist': ```
I would move it to `_init__()`: ```python def __init__(self, *, name, expressions, index_type=None, condition=None): ... self.index_type = index_type or `GIST` self.condition = condition super().__init__(name=name) ``` and skip the default in `deconstruct()` and `__str__()`: ```python def deconstruct(self): ... if self.index_type != 'GIST': kwargs['index_type'] = self.index_type ```
We can use `compiler = query.get_compiler(connection=schema_editor.connection)`
It will be `django.contrib.postgres.constraints.XXX` but I don't think that's an issue for core constraints we return `django.db.models.XXX`.
Why we don't use `super().deconstruct()`? ```python def deconstruct(self): path, args, kwargs = super().deconstruct() kwargs['expressions'] = self.expressions if self.condition is not None: kwargs['condition'] = self.condition if self.index_type != 'GIST': kwargs['index_type'] = self.index_type return path, args, kwargs ```
Also here: ```python if self.index_type.lower() != 'gist': ```
All tests work without it that's why I'm wondering if we need it.
We can also go faster by using a list comprehension instead of a generator - they're cheaper to create: ```suggestion obj.combined_queries = tuple([query.clone() for query in self.combined_queries]) ``` Simple benchmark: ``` In [2]: items = [1, 2, 3, 4] In [3]: %timeit tuple(x*2 for x in items) 508 ns Â± 10 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each) In [4]: %timeit tuple([x*2 for x in items]) 358 ns Â± 14.7 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each) ```
`os.chmod` should be before the `try`. It doesn't make a big difference but that's the common pattern in general.
The point of `RenameMethodsBase` is to insure that if someone overloads the method then the custom method still gets called. But that's only useful if the implementation of the underlying method hasn't changed. I believe using `RenameMethodsBase` doesn't buy us much here.
You could use RenameMethodsBase.
`if it encounter` => `if it encounters`
It's not actually a comprehension - this could just use a tuple literal.
@Ian-Foote thanks for the clarification I always mix up the two terms.
`mentionned` => `mentioned`
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
Please rewrite as ``` if __name__ == '__main__': main() ```
Code duplication 80-86, 89-94.
I think names like `float_nan` would be more consistent with our coding style.
I simplified this test with `@mock_inputs()`.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
I prefer putting the closing ) on the next line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
Must not be `None`.
Whether it has changed or not does not mean there should be a format with invalid URL.
There are multiple formats, some may have `file` some may not. If some new hq format is introduced with different rendition your approach will break downloading by default while proper URL handling will not. Finally read coding conventions.
Try except blocks should be as small as possible. In this case, if is only needed around the calls to modify and delete. Perhaps put the try except around the if state conditional to limit the statements that it surrounds
Code duplication 80-86, 89-94.
I think names like `float_nan` would be more consistent with our coding style.
I simplified this test with `@mock_inputs()`.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
I prefer putting the closing ) on the next line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
Must not be `None`.
Whether it has changed or not does not mean there should be a format with invalid URL.
There are multiple formats, some may have `file` some may not. If some new hq format is introduced with different rendition your approach will break downloading by default while proper URL handling will not. Finally read coding conventions.
Try except blocks should be as small as possible. In this case, if is only needed around the calls to modify and delete. Perhaps put the try except around the if state conditional to limit the statements that it surrounds
what will happen if modification of `.git` file fails while `.git/` dir is already moved? I think we could try to roll it back.
should we ask for a file name? (default to stdout is fine)
> wirte write
I think you might want to rollback at this point.
Yes, the revisions you'v emade to this look good to me.
I took a closer look at the module and the concern here is still valid but I have (for real!) left the information in the module PR now :-) So we can decide what to do there. It doesn't block this one from going in.
option should deal with the case the dependencies are not avaiable and give a warning.
This should probably just be done with a mutually exclusive group in the CLI argparse declaration.
I don't think this will work correctly on python2. Pretty sure you need parenthesis there: ``` python except (ConfigParser.NoSectionError, ConfigParser.NoOptionError): ```
colors should all be configurable
no restructured text (:class:) in docstrings please
Please rewrite as ``` if __name__ == '__main__': main() ```
Any problem with: ``` @property def media(self): ```
@timgraham It might be more appropriate in another commit then. I believe I wanted to make sure nothing was logged if a m2m backed inline was submitted without changes.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
missing space between `,` and `and`
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
no restructured text (:class:) in docstrings please
Please rewrite as ``` if __name__ == '__main__': main() ```
Any problem with: ``` @property def media(self): ```
@timgraham It might be more appropriate in another commit then. I believe I wanted to make sure nothing was logged if a m2m backed inline was submitted without changes.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
missing space between `,` and `and`
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
no restructured text (:class:) in docstrings please
Please rewrite as ``` if __name__ == '__main__': main() ```
Any problem with: ``` @property def media(self): ```
@timgraham It might be more appropriate in another commit then. I believe I wanted to make sure nothing was logged if a m2m backed inline was submitted without changes.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
missing space between `,` and `and`
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
Any problem with: ``` @property def media(self): ```
You probably should just exit here with `changed=False`
having `main()` call `run_module()` feels a bit redundant, I see no reason to not put all of the main logic in `main` (and splitting actions into functions where it makes sense)
the `and retries >= CONFIRM_UPDATE_MAX_RETRY` is redundant here. If the execution got here, it'll always be `True`
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
FYI, in case you switch to working on whole config, `get_config.config` gives you the whole tree. (`get_config(:foo).config` is generally equivalent to `get_config.config[:foo]`.) Also, VMDB::Config is deprecated. May be better to use `::Settings.to_hash`.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
You probably should just exit here with `changed=False`
having `main()` call `run_module()` feels a bit redundant, I see no reason to not put all of the main logic in `main` (and splitting actions into functions where it makes sense)
the `and retries >= CONFIRM_UPDATE_MAX_RETRY` is redundant here. If the execution got here, it'll always be `True`
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
FYI, in case you switch to working on whole config, `get_config.config` gives you the whole tree. (`get_config(:foo).config` is generally equivalent to `get_config.config[:foo]`.) Also, VMDB::Config is deprecated. May be better to use `::Settings.to_hash`.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
You probably should just exit here with `changed=False`
having `main()` call `run_module()` feels a bit redundant, I see no reason to not put all of the main logic in `main` (and splitting actions into functions where it makes sense)
the `and retries >= CONFIRM_UPDATE_MAX_RETRY` is redundant here. If the execution got here, it'll always be `True`
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
`client.start_execution` is idempotent but `start_execution` always returns `changed=True` if `action=start`. We probably want to `client.list_executions` to see if the execution is already running before proceeding.
I haven't actually used stepfunctions myself so I'm just going off the docs. AIUI, `ExecutionAlreadyExists` would only be returned on a running execution if the `execution_input` is different, not in a case where the input is the same. As an ansible user who is new to step functions, I feel like if I gave the same input and ran the playbook twice I would expect the 2nd time to be `changed=false` if the first execution is still running. But that doesn't mean that's how AWS users familiar with step functions will expect it.
```suggestion module.exit_json(changed=True,**camel_dict_to_snake_dict(res))) ```
No need to split the line.
You can achieve this same result output with `module.exit_json(changed=True,**camel_dict_to_snake_dict(execution))` which can be imported from `ansible.module_utils.ec2`. That will automatically cover if AWS ever starts returning additional keys from this API and is generally easier to maintain.
Various modules allow to specify the name of the executable in order to use a binary from a non-standard location (i.e. a binary not located in the PATH). Look at e.g. the isoextract on how to do this.
Also here, I don't know why you are splitting lines. It does not make it easier to read. Usually it is done out of necessity (e.g. PEP8 rules require it), but in this case there is no need to make this weird.
This whole block should be changed into: ```python module = AnsibleModule( argument_spec=dict( name=dict(type='str', required=True), remote=dict(type='str'), state=dict(type='str', default="present", choices=['absent', 'present']) ), supports_check_mode=True, ) ``` Assuming that **name** is always required, and **repo** is required when **state == 'present'**. - Put the type-option first, as this one is usually always added - Lines in Ansible can be 160 chars wide, so there's usually no need to split it a line
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Migrations plans with both forwards and backwards migrations are not supported.
That code is marginally longer, but I think it's clearer. I'll leave it up to what you think is best though.
I don't like how this entire if block looks. Is `plan` allowed to be `None` considering the check/assignment on line 73 and 74? I'd prefer to call out that it can not be forwards and backwards by doing something like: ``` if all_forwards == all_backards: raise.. # this may not be correct if `plan` can be `None` elif all_forwards: migrate_forwards() else: migrate_backwards() ``` I feel this highlights the invariant, and avoids potential problems of misreading the bool combinations in the original code.
shorten line (rule of thumb I'm using is ~120 characters so I don't have scroll horizontally in github review)
Is `plan` meant as the second argument? Looks like we're missing a test for this branch.
We've been using "Take / apply" verb-style in new docstrings.
You can achieve this same result output with `module.exit_json(changed=True,**camel_dict_to_snake_dict(execution))` which can be imported from `ansible.module_utils.ec2`. That will automatically cover if AWS ever starts returning additional keys from this API and is generally easier to maintain.
```suggestion module.exit_json(changed=True,**camel_dict_to_snake_dict(res))) ```
`client.start_execution` is idempotent but `start_execution` always returns `changed=True` if `action=start`. We probably want to `client.list_executions` to see if the execution is already running before proceeding.
I haven't actually used stepfunctions myself so I'm just going off the docs. AIUI, `ExecutionAlreadyExists` would only be returned on a running execution if the `execution_input` is different, not in a case where the input is the same. As an ansible user who is new to step functions, I feel like if I gave the same input and ran the playbook twice I would expect the 2nd time to be `changed=false` if the first execution is still running. But that doesn't mean that's how AWS users familiar with step functions will expect it.
Maybe `_parse_where()` -> `_get_condition_sql()`, I would return condition and add `WHERE` in `constraint_sql()`, e.g. ```python def _get_condition_sql(self, compiler, schema_editor, query): if self.condition is None: return None where = query.build_where(self.condition) sql, params = where.as_sql(compiler, schema_editor.connection) return sql % tuple(schema_editor.quote_value(p) for p in params) def constraint_sql(self, model, schema_editor): ... condition = self._get_condition_sql(compiler, schema_editor, query) return self.template % { ... 'where': ' WHERE (%s)' % condition if condition else '', } ```
You can also drop the parentheses :wink:
We can use `compiler = query.get_compiler(connection=schema_editor.connection)`
I don't think we need the extra assignment here as this is only used once.
I would move it to `_init__()`: ```python def __init__(self, *, name, expressions, index_type=None, condition=None): ... self.index_type = index_type or `GIST` self.condition = condition super().__init__(name=name) ``` and skip the default in `deconstruct()` and `__str__()`: ```python def deconstruct(self): ... if self.index_type != 'GIST': kwargs['index_type'] = self.index_type ```
Also make the comparison case-insensitive, i.e. ```python if self.index_type.lower() != 'gist': ```
Why we don't use `super().deconstruct()`? ```python def deconstruct(self): path, args, kwargs = super().deconstruct() kwargs['expressions'] = self.expressions if self.condition is not None: kwargs['condition'] = self.condition if self.index_type != 'GIST': kwargs['index_type'] = self.index_type return path, args, kwargs ```
It will be `django.contrib.postgres.constraints.XXX` but I don't think that's an issue for core constraints we return `django.db.models.XXX`.
Also here: ```python if self.index_type.lower() != 'gist': ```
`SearchedCase` is no longer it's own type so this should be named `Case conditions must be lookups`. Alternatively, do this checking directly inside the `When` types, since users will be using these directly.
It seems this URL doesn't work anymore.
I think a simple `django.template.Context` will do here.
Use single quotes consistently.
Unless I'm missing something you don't need to define a class and nest an instance of it in a list to reproduce your use case. Simply passing a callable that throws an exception should do. e.g. ``` python engine = Engine(loaders=[ ('django.template.loaders.locmem.Loader', { 'child': '{{ raises }}', }), ], debug=False) def raises(): raise Exception engine.from_string('{% include "child" %}').render(Context({'raises': raises})) ```
```suggestion Test that the returned value for timezone consists of only uppercase ```
Use another lookup instead of `epoch` e.g. `second`.
does not match docs
To me it looks the module does handle check mode (https://github.com/ansible/ansible/pull/20734/files#diff-672a20e0686da08f7554286ee4283346R423). So in my opinion `supports_check_mode` an be set to true
you should not need to checked disabled, as the plugin itself wont be called at all if true
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
no restructured text (:class:) in docstrings please
Is there a typo? I'm not sure what "hub" means in this sentence.
Any problem with: ``` @property def media(self): ```
I guess some tests might be needed for the router stuff.
Oh, I see. `run_commands()` runs list of commands and returns list of results.
Needs to be `run_commands(module, ['show vlan brief'])[0]`.
With this simplification, the pythonic way of generating a list is to use list comprehension. You can replace the whole function body with: ```python return [parse_to_obj(r) for r in parse_to_logical_rows(vlan_out)] ```
True, I mixed up sth when switching branches. Nevertheless they work on all databases except PostgreSQL, so we need to fix this.
Cases from lines 361 and 363 work with the previous implementation.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
no restructured text (:class:) in docstrings please
Is there a typo? I'm not sure what "hub" means in this sentence.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Any problem with: ``` @property def media(self): ```
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
I guess some tests might be needed for the router stuff.
Cases from lines 361 and 363 work with the previous implementation.
True, I mixed up sth when switching branches. Nevertheless they work on all databases except PostgreSQL, so we need to fix this.
Can you use `assertContains()` here to prevent errors due to attribute order in the HTML tags.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Any problem with: ``` @property def media(self): ```
I guess some tests might be needed for the router stuff.
Needs to be `run_commands(module, ['show vlan brief'])[0]`.
Oh, I see. `run_commands()` runs list of commands and returns list of results.
True, I mixed up sth when switching branches. Nevertheless they work on all databases except PostgreSQL, so we need to fix this.
Cases from lines 361 and 363 work with the previous implementation.
With this simplification, the pythonic way of generating a list is to use list comprehension. You can replace the whole function body with: ```python return [parse_to_obj(r) for r in parse_to_logical_rows(vlan_out)] ```
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
`test_jsonfield` looks good to me (since this is InspectDBTests, _introspection seems redundant).
no restructured text (:class:) in docstrings please
Use single quotes consistently.
Is there a typo? I'm not sure what "hub" means in this sentence.
I'd use rename_forwards/backwards for consistency with other methods like database_forwards.
Any problem with: ``` @property def media(self): ```
I guess some tests might be needed for the router stuff.
chop trailing ", "
I think either name should be mandatory or this should take a label selector.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
No need for get(key, None) as None is the default fix also for following get()
no restructured text (:class:) in docstrings please
Use single quotes consistently.
It would have been more appropriate if `run_checks()` took an `Apps` instance as a parameter instead of a list of `AppConfig`s.
Any problem with: ``` @property def media(self): ```
you should not need to checked disabled, as the plugin itself wont be called at all if true
The code is the same, just the data differs, so I think it would be clearer to move the loop outside the if. ``` python output_data = self.py3_output_data if sys.version_info[0] >= 3 else self.py2_output_data for msg, param in output_data.items(): ... ```
I think either name should be mandatory or this should take a label selector.
> Speaking of which, I should submit a PR to add Python 3.5 to tox.ini and .travis.yml #12627.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
DRY 105, 107.
There is a minor behaviour change here. Previously calling `decr()` with `delta=0` would call `self._cache.decr()`, but now it'll call `self._cache.incr()` instead. In theory this shouldn't be a problem, but am highlighting it.
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
Remove blank line (and below).
Remove blank line (and below).
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
Undo unrelated change (and below).
Personally, I think `new_key` would be a more commonly used choice.
I see the idea, but for me if a function is called only once and only contains some simple lines, the function call overhead is not worth it. You can let this for now and wait for the Django fellows opinion.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
I realize this predates your PR, but why is 'unused' not used? It appears to be the target field provided by the SQLUpdateCompiler during as_sql(). This seems like a really good spot to allow a backend to hook in and CAST the expression to a type that is allowed by the target column.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Won't this result in a confusing SQL-level error if you pass in `None` for `expression` or `pos` by accident? I'm assuming `length` is the only one we actually expect to possibly be `None`. If that's true, I think it would be better to do something like: ``` expressions = [expression, pos] if length is not None: expressions.append(length) ``` Or, if you prefer: `expressions = (expression, pos, length) if length is not None else (expression, pos)`
okay, but would be helpful to say _why_ we need to always return True.
chop "one of" add comma before "or"
put closing parenthesis on the next line
This conditional is not required anymore given the check above.
Alright, missed that!
We should omit `default_bounds` when the default value is used: ```suggestion if self.default_bounds and self.default_bounds != '[)': kwargs['default_bounds'] = self.default_bounds ```
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Single quotes please. Also, can we use `size` instead of `dims` for consistency with the other tests? ```suggestion size = images.get_image_dimensions('missing.png') self.assertEqual(size, (None, None)) ```
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
I think we can drop the empty line here.
And this one.
Chop blank line.
In MySQL introspection we use `table_schema = DATABASE()`, I think we should use it here.
Chop blank line.
```suggestion with self.connection.cursor() as cursor: cursor.execute(""" SELECT table_name, table_rows FROM information_schema.tables WHERE table_schema = %s AND table_name IN %s """, (schema_name, tables)) rows = cursor.fetchall() ```
We dropped `sequence_reset_by_name_sql()` intentionally in d28396f5268f1974ef1e84d13bcf1ac107005ced. It's redundant after `TRUNCATE`, we should call it only in `DELETE FROM` branch.
```suggestion def sequence_reset_by_name_sql(self, style, sequences): return [ '%s %s %s %s = 1;' % ( style.SQL_KEYWORD('ALTER'), style.SQL_KEYWORD('TABLE'), style.SQL_FIELD(self.quote_name(sequence_info['table'])), style.SQL_FIELD('AUTO_INCREMENT'), ) for sequence_info in sequences ] ```
```suggestion - The plugin also sets standard host variable ansible_ssh_common_args to '-o StrictHostKeyChecking=no'. ``` Same in the line below.
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
`video_id` literal is not a video id.
Doesn't work in python 2.6.
Remove all debug output.
No need to use `_search_regex` here. It's already guaranteed that regex is matched.
`height` must be int. Any optional metadata must not break extraction. Bother to read coding conventions.
Regex should be relaxed. Dots should be escaped.
Title should contain title, not air date.
Audio must have proper `vcodec` set.
Who cares about effectiveness when it's broken? With your comparator `'1080' < '720'` that is incorrect.
... and then the call to instantiate AnsibleModule can look like this: ``` python self.module = AnsibleModule(argument_spec=merged_arg_spec, required_if=merged_required_if, **kwargs) ```
@hramezani I think you removed `setattr(options, opt_name, os.path.normpath(opt_val))` by mistake. My proposition was to remove only `else`, i.e. ```python if '.' in opt_val: print('Aborting: --%s must be a top-level module.' % opt_name.replace('_', '-')) sys.exit(1) setattr(options, opt_name, os.path.normpath(opt_val)) ```
IMO `else`is unnecessary i.e. ```python if '.' in opt_val: ... setattr(options, opt_name, os.path.normpath(opt_val)) ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
```suggestion query=dict(type='list', elements='str'), ```
```suggestion ) ```
I would also detect if the value is a bool and set it accordingly to `on`/`off`.
You can also add a deprecation warning for usages like `from django.forms.extras.widgets import SelectDateWidget`.
```suggestion Kwargs: ```
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
also need to check whether the workspace is name or resource id
`enumerate` on for range.
Should contain `quality` key.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
Breaks. Read coding conventions.
Lack of data is denoted by `None` not 0.
Don't capture groups you don't use. Unused captured group.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
No brackets needed.
This is no longer actual.
Useless with timestamp available.
`enumerate` on for range.
Should contain `quality` key.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
Breaks. Read coding conventions.
Lack of data is denoted by `None` not 0.
Don't capture groups you don't use. Unused captured group.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
No brackets needed.
This is no longer actual.
Useless with timestamp available.
You could use RenameMethodsBase.
The point of `RenameMethodsBase` is to insure that if someone overloads the method then the custom method still gets called. But that's only useful if the implementation of the underlying method hasn't changed. I believe using `RenameMethodsBase` doesn't buy us much here.
We can also go faster by using a list comprehension instead of a generator - they're cheaper to create: ```suggestion obj.combined_queries = tuple([query.clone() for query in self.combined_queries]) ``` Simple benchmark: ``` In [2]: items = [1, 2, 3, 4] In [3]: %timeit tuple(x*2 for x in items) 508 ns Â± 10 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each) In [4]: %timeit tuple([x*2 for x in items]) 358 ns Â± 14.7 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each) ```
@Ian-Foote thanks for the clarification I always mix up the two terms.
```suggestion # Run masked_action when enabled param is not provided or service is supposed to be enabled/started. if not module.check_mode and module.params['enabled'] in [None, True]): ```
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
`if it encounter` => `if it encounters`
Missing `=dict` on this and the next few lines
`mentionned` => `mentioned`
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
You could use RenameMethodsBase.
The point of `RenameMethodsBase` is to insure that if someone overloads the method then the custom method still gets called. But that's only useful if the implementation of the underlying method hasn't changed. I believe using `RenameMethodsBase` doesn't buy us much here.
We can also go faster by using a list comprehension instead of a generator - they're cheaper to create: ```suggestion obj.combined_queries = tuple([query.clone() for query in self.combined_queries]) ``` Simple benchmark: ``` In [2]: items = [1, 2, 3, 4] In [3]: %timeit tuple(x*2 for x in items) 508 ns Â± 10 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each) In [4]: %timeit tuple([x*2 for x in items]) 358 ns Â± 14.7 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each) ```
@Ian-Foote thanks for the clarification I always mix up the two terms.
```suggestion # Run masked_action when enabled param is not provided or service is supposed to be enabled/started. if not module.check_mode and module.params['enabled'] in [None, True]): ```
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
`if it encounter` => `if it encounters`
Missing `=dict` on this and the next few lines
`mentionned` => `mentioned`
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Need spaces around `+` sign.
immediatelly -> immediately
Yeah, I think what you have is fine, since we have to iterate over the generator one way or another, and the set comparison it's really straightforward either. I guess the only downside of this algorithm is that, if the broker nefariously listed the same topic multiple times, it would give us a false positive. But I seriously doubt that can happen.
Does this need to be a separate method? Seems unnecessary to me.
okay, but would be helpful to say _why_ we need to always return True.
Not sure it makes a difference but before it looks like we got `form=None` in the context.
Do we need to define `async_generator` in `__aiter__`? ```python async def _async_generator(self): # Generators don't actually start running until the first time you call # next() on them, so we make the generator object in the async thread # and then repeatedly dispatch to it in a sync thread. sync_generator = self.__iter__() # So, you can't raise StopIteration up through sync_to_async as # too much of the Python async system was originally built on # generators. Instead, we capture it inside a custom sync function # and change it to an exception we *can* propagate. # We should probably move this logic inside asgiref at some point. def custom_next(gen): try: return next(gen) except StopIteration: raise StopAsyncIteration() while True: try: yield await sync_to_async(custom_next)(sync_generator) except StopAsyncIteration: return # Note: __aiter__ is a *synchronous* method that has to then return an # *asynchronous* iterator/generator. Thus, we nest an async generator # inside it. # Also note: This is a generic iterable converter for now, and is going to # suffer a performance penalty on large sets of items due to the cost of # crossing over the sync barrier each time. We should write custom ones # for each Iterable subclass, but we'll need some work in Compiler first. def __aiter__(self): return self._async_generator() ```
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
```suggestion Test that the returned value for timezone consists of only uppercase ```
- i don't think that `type="hidden"` is important. - checking for ext is not needed here.
You can also add a deprecation warning for usages like `from django.forms.extras.widgets import SelectDateWidget`.
```suggestion ) ```
Ah, I think this ought to be `local_size == remote_size`, since this conditional causes the file to be skipped for this strategy. That explains the odd behavior I saw with `date_size`
It looks like this is just a serial upload, how is this faster than the current S3 module? I definitely see the benefit of the glob & sync strategies, but it seems like this would be just as fast.
I would also detect if the value is a bool and set it accordingly to `on`/`off`.
use `trail['LogFileValidationEnabled'] = ct_params['EnableLogFileValidation']`
also need to check whether the workspace is name or resource id
Use the Python 2 and low 3 Time Machine: `'url too short: %s' % (video_pre_parts, )` or: `'url too short: %(video_pre_parts)s' % {'video_pre_parts': video_pre_parts, }` or: `'url too short: {video_pre_parts}'.format(video_pre_parts=video_pre_parts)` or: `'url too short: {0}'.format(video_pre_parts)` No doubt there are other ways (eg `....format(**locals())`
Generally, I'd have put the static regex directly as the first argument of `re.search()`.
There seems to be an assumed structure of what is returned by the API endpoint, operating under the pretense that the structure won't change since the API is versioned, is there any chance that this assignment could fail and cause an unhandled exception? (similar question for other functions doing similar things below)
@sdodsley sounds good, thank you for the clarification
```suggestion raise Exception ```
```suggestion module.fail_json(msg='Initiator {0} remove failed.'.format(ini_id)) ```
```suggestion module.exit_json(msg=msg, changed=changed) ```
```suggestion msg = 'Added initiator {0}'.format(ini['id']) module.log(msg=msg) ```
```suggestion msg = 'Initiator {0} removed.'.format(ini_id) module.log(msg=msg) ```
```suggestion msg = "" ```
```suggestion module.fail_json(msg='wwn is required for adding initiator.') ```
```suggestion wwn = validate_wwn(module) ```
How about this - ```suggestion msg = "No corresponding incident" if len(incidents) == 0: if state in ('acknowledged', 'resolved'): return msg, False return msg, True elif state != incidents[0]["status"]: return incidents[0], True return incidents[0], False ```
Duration calculation is incorrect.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
To be consistent with the rest of the codebase, I'd import `from django.utils.six.moves import range` first.
Same here, make this a regular function
This should be an @staticmethod nad self removed.
Line is too long.
This line is too long. Max line length allowed in Ansible is 120 characters.
Line is too long.
Line is too long.
I would remove all aliases if possible.
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
(In general, I don't think modules should have such options.)
You should emphasize that the module can and will not do any idempotence checking for this.
A general remark: you should always use complete sentences. So this should end with a period.
(Same for the related options.)
What kind of values are valid here? I would expect this to be `float` based on the `0.0` in the description.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
I would remove this `device` here, and similar the option name in the examples for `device_read_bps`, `device_read_iops` etc.
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
This isn't needed, since we now only support 2.6+ anyway.
```suggestion """Render as <p> elements.""" ```
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
Can you use `assertContains()` here to prevent errors due to attribute order in the HTML tags.
Same here, can you use `assertContains()` here to prevent errors due to attribute order in the HTML tags.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
```suggestion """Render as <p> elements.""" ```
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
```suggestion template_name = 'forms_tests/form_snippet.html' ```
Code duplication 80-86, 89-94.
Whether it has changed or not does not mean there should be a format with invalid URL.
There are multiple formats, some may have `file` some may not. If some new hq format is introduced with different rendition your approach will break downloading by default while proper URL handling will not. Finally read coding conventions.
Must not be `None`.
```suggestion module.exit_json(changed=True,**camel_dict_to_snake_dict(res))) ```
I haven't actually used stepfunctions myself so I'm just going off the docs. AIUI, `ExecutionAlreadyExists` would only be returned on a running execution if the `execution_input` is different, not in a case where the input is the same. As an ansible user who is new to step functions, I feel like if I gave the same input and ran the playbook twice I would expect the 2nd time to be `changed=false` if the first execution is still running. But that doesn't mean that's how AWS users familiar with step functions will expect it.
`client.start_execution` is idempotent but `start_execution` always returns `changed=True` if `action=start`. We probably want to `client.list_executions` to see if the execution is already running before proceeding.
You can achieve this same result output with `module.exit_json(changed=True,**camel_dict_to_snake_dict(execution))` which can be imported from `ansible.module_utils.ec2`. That will automatically cover if AWS ever starts returning additional keys from this API and is generally easier to maintain.
Try except blocks should be as small as possible. In this case, if is only needed around the calls to modify and delete. Perhaps put the try except around the if state conditional to limit the statements that it surrounds
this seems like it should be shared with the main one rather than duplicated here
Can we display a similar message with the test results? It can be helpful when you have plenty of errors, .e.g. ``` ... FAILED (errors=206, skipped=1094, expected failures=4) Used shuffle seed: 556738431 (generated) ```
I don't see any need for this attribute.
I think [`_extract_m3u8_formats`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L847) does the work better. If the existing method does not fit the need, feel free to modify it.
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
I'm wondering why you decided to override this optional API. Is this impl expected to be faster than pulling the iterator and calling next in a loop? (this is what the default impl does)
``` for i, video_url in enumerate(video_urls): ```
You have some unmerged lines here
its not clear to me we aren't double-wrapping here if leafreaders were shared from the previous.
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
`field_preference` must be a list or a tuple.
Code duplication 80-86, 89-94.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
Must not be `None`.
There are multiple formats, some may have `file` some may not. If some new hq format is introduced with different rendition your approach will break downloading by default while proper URL handling will not. Finally read coding conventions.
Whether it has changed or not does not mean there should be a format with invalid URL.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
`form_class` is defined in `RangeField.formfield()` so this is redundant.
this seems like it should be shared with the main one rather than duplicated here
This function is too complex. Could you please factor and avoid: - more than 2 level of condition - inner loop. You can probably break down the function in several small functions.
with -> width
point -> points
comma after tuple
width, height, and offset
happy new year ! yeah effectively finally this is required for python < 3.5 as you explain (I just tested it in an interactive session). I'm happy to open a PR with this specific change so.. should not be too long to come..
hmmm.. now I think I'm wrong, you want to only have the first partial func.. `func.func` shouldn't anyway be another partial or else that would be problematic I think, as views have to return something else.. Sorry for disturbance ;)
chop "one of" add comma before "or"
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
This function is too complex. Could you please factor and avoid: - more than 2 level of condition - inner loop. You can probably break down the function in several small functions.
with -> width
point -> points
comma after tuple
width, height, and offset
happy new year ! yeah effectively finally this is required for python < 3.5 as you explain (I just tested it in an interactive session). I'm happy to open a PR with this specific change so.. should not be too long to come..
hmmm.. now I think I'm wrong, you want to only have the first partial func.. `func.func` shouldn't anyway be another partial or else that would be problematic I think, as views have to return something else.. Sorry for disturbance ;)
chop "one of" add comma before "or"
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
This function is too complex. Could you please factor and avoid: - more than 2 level of condition - inner loop. You can probably break down the function in several small functions.
with -> width
point -> points
comma after tuple
width, height, and offset
happy new year ! yeah effectively finally this is required for python < 3.5 as you explain (I just tested it in an interactive session). I'm happy to open a PR with this specific change so.. should not be too long to come..
hmmm.. now I think I'm wrong, you want to only have the first partial func.. `func.func` shouldn't anyway be another partial or else that would be problematic I think, as views have to return something else.. Sorry for disturbance ;)
chop "one of" add comma before "or"
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
This function is too complex. Could you please factor and avoid: - more than 2 level of condition - inner loop. You can probably break down the function in several small functions.
with -> width
point -> points
comma after tuple
width, height, and offset
happy new year ! yeah effectively finally this is required for python < 3.5 as you explain (I just tested it in an interactive session). I'm happy to open a PR with this specific change so.. should not be too long to come..
hmmm.. now I think I'm wrong, you want to only have the first partial func.. `func.func` shouldn't anyway be another partial or else that would be problematic I think, as views have to return something else.. Sorry for disturbance ;)
chop "one of" add comma before "or"
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
This function is too complex. Could you please factor and avoid: - more than 2 level of condition - inner loop. You can probably break down the function in several small functions.
with -> width
point -> points
comma after tuple
width, height, and offset
happy new year ! yeah effectively finally this is required for python < 3.5 as you explain (I just tested it in an interactive session). I'm happy to open a PR with this specific change so.. should not be too long to come..
hmmm.. now I think I'm wrong, you want to only have the first partial func.. `func.func` shouldn't anyway be another partial or else that would be problematic I think, as views have to return something else.. Sorry for disturbance ;)
chop "one of" add comma before "or"
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
For long query strings, it's better to use ```query``` parameter here.
```not (foo is None)``` => ```foo is not None```
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
@slafs non-existing setting names, e.g. `TEST_SETTING=True`
This can be single lined: ```python '%s\n%s' % (combined_list[prev], combined_list[k]), ```
Please use a single quote.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
@slafs non-existing setting names, e.g. `TEST_SETTING=True`
This can be single lined: ```python '%s\n%s' % (combined_list[prev], combined_list[k]), ```
Please use a single quote.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
When there is no `topicTitle`, `categories` will be `[None]` that does not make any sense.
Sorry, I had a typo in https://github.com/django/django/pull/12675#discussion_r418965162 -- what I ment to say was: keep `work factor` in safe summary (since it's for display and already translated) and `work_factor` in `decode`. This will also require less changes in the tests.
Should also update based on `block_size` and `parallelism`
Alternatively, we can get rid of those lists by just matching on whether the test name ends with "join" (we match on table name elsewhere in this PR)
Ah, I didn't realize load takes that long, but in retrospect, it should have been obvious. Then, of course we shouldn't always load both, and my suggestion is just to match on the test name.
Would it be the acme of foolishness to suggest that we just always "load-two"? Then, we wouldn't have to maintain the extra lists of tests at the top of the file, and we wouldn't need this condition here. And also, we'd eliminate a difference among the benchmarks, since the brokers would be loaded with the same dataset in all cases (even if one of them is unused).
Same as above, I'd revert to the simpler version. The main idea behind `decode` for me is a consistent interface where it makes sense, we don't have to bend backwards just to use it.
We confirmed that `parallelism` should be taken into account.
`self._test_scrypt_upgrade('parallelism', 'parallelism', 2)` fails, it seems we shouldn't take `parallelism` into account.
> My main worry here is: Is this correct and does it make sense to implement for such a complex hasher (notably we already have others where we argue it is simply not possible in a sensible way). > > Since scrypt can raise errors like this: > > > ValueError: Invalid parameter combination for n, r, p, maxmem. > > I am wondering if `must_update` couldn't also trigger this condition. Or can we always calculate `extra_iterations` and `extra_block` and be sure that the combinations are valid? You're right parameters may no be valid, e.g. ``` self.work_factor = 2 **14 decoded['work_factor'] = 2 ** 11 ``` both are a power of 2, however `extra_iterations = 14336` is not and raises `ValueError: n must be a power of 2`.
Test failure is because this is missing `geography=True` on PostGIS.
``` # Transform minus sign prefixed strings into an OrderBy() expression. ordering = [ (OrderBy(F(o[1:]), descending=True) if isinstance(o, str) and o[0] == '-' else o) for o in ordering ] ```
Use `super()` since Python 2 is no longer supported. Single line looks okay.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
Chop "Helper function that" "Return ..." (use PEP 257 verb style)
Turn (capitalize) add period. sql -> SQL
Put the close ] on the next line.
`"""Return the index at which the ordering expressions start."""`
`# Filter out fields contributed....` (chop comma before as)
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
argument ordering should be reversed
We don't need to test multiple cases because we want to ignore only locales with hyphens.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
Use `super()` since Python 2 is no longer supported. Single line looks okay.
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
Turn (capitalize) add period. sql -> SQL
`# Filter out fields contributed....` (chop comma before as)
If you use a catchall exception handler like this then you should also provide the traceback like this: ``` python import traceback from ansible.module_utils._text import to_native [...] module.fail_json(msg='Unanticipated error running atomic: %s' % to_native(e), exception=traceback.format_exc()) ```
A message string would good to say that image is not preset or something similar.
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
@slafs non-existing setting names, e.g. `TEST_SETTING=True`
This can be single lined: ```python '%s\n%s' % (combined_list[prev], combined_list[k]), ```
Please use a single quote.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
Is anything `required_together`, if not please remove this line
Oh I see :)
I think this should be `if tags and tags != igw.tags:`. Otherwise if we're creating a gateway with no tags this condition will be met and it will crash later since there are no tags. On second thought, just make the default of tags be an empty dict instead of None, [here](https://github.com/ansible/ansible/pull/23782/files#diff-7c7439a69b0a017fb8e03a769c5ef29bR179) and it will fix the problem.
`gluster_peer_ops` is unconditionally called and this method always calls `get_nodes`. `get_nodes` fails when `nodes` parameter isn't set: this parameter must be mandatory (and non empty). For that, you could use a custom method, meaning something like that: ``` class AnsibleModuleCheckListNotEmpty(AnsibleModule): def _check_type_list_not_empty(self, value): value = self._check_type_list(value) # default checks for a list if not value: raise ValueError("list must not be empty") return value [...] module = AnsibleModuleCheckListNotEmpty( argument_spec=dict( force=dict(type='bool', required=False), nodes=dict(type=self._check_type_list_not_empty, required=True), [...] ``` Once implemented you could remove the `get_nodes` method.
Can you make this into a couple statements for easier readability? Something like: ``` invalid_entries = [x for x in statement['Principal']['AWS'] if not x.startswith('arn:aws:iam::)] if clean_invalid_entries and len(invalid_entries): for entry in invalid_entries: statement['Principal']['AWS'].remove(entry) ``` Or there's always: ``` original_size = len(statement['Principal']['AWS']) statement['Principal']['AWS'] = [x for x in statement['Principal']['AWS'] if x.startswith('arn:aws:iam::)] have_invalid_entries = (original_size == len(statement['Principal']['AWS'])) ```
Add here that the `key_alias` or `key_arn` are both ways to provide it.
You don't need this conditional, since Ansible enforces that these are the only choices.
Ah, yes, I miss read the code. Nothing to change here.
argument ordering should be reversed
Please separate this into two separate error checking blocks: one for buckets and one for keys.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
argument ordering should be reversed
We don't need to test multiple cases because we want to ignore only locales with hyphens.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
Use `super()` since Python 2 is no longer supported. Single line looks okay.
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
Turn (capitalize) add period. sql -> SQL
`# Filter out fields contributed....` (chop comma before as)
If you use a catchall exception handler like this then you should also provide the traceback like this: ``` python import traceback from ansible.module_utils._text import to_native [...] module.fail_json(msg='Unanticipated error running atomic: %s' % to_native(e), exception=traceback.format_exc()) ```
A message string would good to say that image is not preset or something similar.
`gluster_peer_ops` is unconditionally called and this method always calls `get_nodes`. `get_nodes` fails when `nodes` parameter isn't set: this parameter must be mandatory (and non empty). For that, you could use a custom method, meaning something like that: ``` class AnsibleModuleCheckListNotEmpty(AnsibleModule): def _check_type_list_not_empty(self, value): value = self._check_type_list(value) # default checks for a list if not value: raise ValueError("list must not be empty") return value [...] module = AnsibleModuleCheckListNotEmpty( argument_spec=dict( force=dict(type='bool', required=False), nodes=dict(type=self._check_type_list_not_empty, required=True), [...] ``` Once implemented you could remove the `get_nodes` method.
http://docs.ansible.com/ansible/latest/dev_guide/developing_modules_general.html#new-module-development There is written: ``` # during the execution of the module, if there is an exception or a # conditional state that effectively causes a failure, run # AnsibleModule.fail_json() to pass in the message and the result if module.params['name'] == 'fail me': module.fail_json(msg='You requested this to fail', **result) ``` AFAIK module should return with `module.exit_json` or `module.fail_json`, not `raise ValueError` for example.
Also you may get here(or on many other places) an exception, and you don't catch any. AFAIK Ansible modules should return always with `module.exit_json` or `module.fail_json` rather then raising `Exception`.
Will this parsing also work when gluster isn't installed at all? In that case any meaningfull message would be nice.
I meant `IndexError` not `ValueError`, but it don't really matter. What I meant is, that isn't it better to have this safe contruct there: ```python try: main() except Exception as e: module.fail_json(msg=str(e), exception=traceback.format_exc()) ``` It's then better to debug issues.
Line is too long.
argument ordering should be reversed
This line is too long. Max line length allowed in Ansible is 120 characters.
use python bool for the default `default=False`
Default type is `str` so you don't have to set it explicitly. Just use `size=dict(),` instead. The same bellow.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
```suggestion query=dict(type='list', elements='str'), ```
I would also detect if the value is a bool and set it accordingly to `on`/`off`.
```suggestion description: Whether the domain is eligible for submission of "OV" certificates. Will never be C(false) if I(ov_eligible) is C(true) ```
```suggestion returned: success and I(ev_eligible) is C(true) and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING). ```
```suggestion returned: success and I(ov_eligible) is C(true) and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING). ```
```suggestion description: Whether the domain is eligible for submission of "EV" certificates. Will never be C(true) if I(ov_eligible) is C(false) ```
```suggestion sample: true ```
```suggestion sample: true ```
You can also add a deprecation warning for usages like `from django.forms.extras.widgets import SelectDateWidget`.
And as the return is a dictionary of headers, instead of tracking `self._ccsrftoken` here, you should be able to set your return dictionary to be `{'x-csrftoken': csrftoken_search.group(1)}`, and everything should work as expected.
If you're only going to get `APSCOOKIE_` _or_ `ccsrftoken`, then you can just return `None` if you don't find anything and the existing token will be reused. If you are expecting to have both, then I would just dedent the next line to be outside the for loop, so that the token is always added to the dictionary on every run. Then you should be able to at least remove the manual headers building in `send_request`.
Above, you wrote that `ov_eligible` is returned when `success and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING).`. But here, you only return it if it has value `True`.
```suggestion returned: success and I(ev_eligible) is C(true) and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING). ```
I think this can raise botocore.exceptions.ProfileNotFound too.
```suggestion sample: true ```
```suggestion description: Whether the domain is eligible for submission of "EV" certificates. Will never be C(true) if I(ov_eligible) is C(false) ```
```suggestion returned: success and I(ov_eligible) is C(true) and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING). ```
```suggestion sample: true ```
```suggestion description: Whether the domain is eligible for submission of "OV" certificates. Will never be C(false) if I(ov_eligible) is C(true) ```
with -> width
width, height, and offset
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
comma after tuple
flake8 complains about missing spaces around `*`
need to x2 check tmrow, but this should be an error, we don't allow for circular parenting
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
bump (it looks like this didn't get changed)
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
```suggestion # just get value from attribute itself as normal ```
I would prefer to avoid the try/except block. Because it can potentially hide a problem and it reduces the readability. This being said, it was already here, so it's up to you.
prefer if you use hanging indent style for this assertion to match the other tests
Actually result could be a property of the main object in this case.
The interface of __present and __bulk_present work differently. One is passing the result dict, the other is setting it as a global variable. I think you should stick to one way of working to make it easier to understand what's going on.
I would also detect if the value is a bool and set it accordingly to `on`/`off`.
Could we use `vm` too, maybe as an alias. This is shorter, clear enough IMO and it's actually already VMware naming convention.
could you please isolate the lines from 130 to 146 in a new function.
I think this should be false (not a string)
No need to specify `choices`
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
I think `enumerate` would work here
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
```suggestion ({b'str'.encode(DEFAULT_ENCODING): b'str'.encode(DEFAULT_ENCODING)}, {u'str': u'str'}), ```
Having a class for just one test method is unnecessary.
You could use lambda to save some lines defining the function
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
I think `enumerate` would work here
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
```suggestion ({b'str'.encode(DEFAULT_ENCODING): b'str'.encode(DEFAULT_ENCODING)}, {u'str': u'str'}), ```
Having a class for just one test method is unnecessary.
You could use lambda to save some lines defining the function
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
`band_input`, you don't get much by saving one char :-)
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
I think `enumerate` would work here
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
```suggestion ({b'str'.encode(DEFAULT_ENCODING): b'str'.encode(DEFAULT_ENCODING)}, {u'str': u'str'}), ```
Having a class for just one test method is unnecessary.
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
`_search_regex`, `_parse_json`. Again: read coding conventions.
Mandatory. Read coding conventions.
These 2 lines could be replaced by: ```python uploader_url, uploader_id = uploader_data[0][0:2] ```
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Sorry, I know it was in my changes, but I forgot to remove that lineÃ¢Â€Â¦ Changing the data seems a bit ugly here, could you remove this line and changing the next one by: ``` prefix = ipaddress.ip_network(data["prefix"] + "/" + data["prefix_length"]) ```
I would replace these 3 lines by: ``` # Add these imports from ansible.module_utils.compat import ipaddress from ansible.module_utils._text import to_text ### prefix = ipaddress.ip_network(data["prefix"]) network = to_text(prefix.network_address) mask = prefix.prefixlen ``` This way, it will not fail if `data["prefix"]` doesn't have a netmask (default it to /32 for ipv4, /128 for ipv6).
Or if you want your module to fail if the prefix sent as parameter doesn't have a netmask, you should handle the error.
Yeah we need to handle all exceptions that would be otherwise handled in `TaskExecutor` (`_execute()`, `run()`) and `Worker.run()`.
Please use a single quote.
This can be single lined: ```python '%s\n%s' % (combined_list[prev], combined_list[k]), ```
```suggestion description: Can be used to request certificates from ECS, with C(provider=entrust). ```
```suggestion description: Whether the domain is eligible for submission of "OV" certificates. Will never be C(false) if I(ov_eligible) is C(true) ```
```suggestion sample: true ```
```suggestion returned: success and I(ov_eligible) is C(true) and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING). ```
```suggestion description: Whether the domain is eligible for submission of "EV" certificates. Will never be C(true) if I(ov_eligible) is C(false) ```
```suggestion sample: true ```
```suggestion returned: success and I(ev_eligible) is C(true) and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING). ```
Above, you wrote that `ov_eligible` is returned when `success and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING).`. But here, you only return it if it has value `True`.
```suggestion - There is a small delay (typically about 5 seconds, but can be as long as 30 seconds) before obtaining the random values when requesting a validation. ```
This is always a tough question, and I'm not sure what's the best solution :) In Ansible, things are usually lower-case. I guess you have to decide what you want in the end :)
How about lower-case? ```suggestion choices: [ 'dns', 'email', 'manual', 'webserver'] ```
```suggestion description: Can be used to request certificates from ECS, with C(provider=entrust). ```
```suggestion returned: success and I(ev_eligible) is C(true) and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING). ```
```suggestion description: Whether the domain is eligible for submission of "EV" certificates. Will never be C(true) if I(ov_eligible) is C(false) ```
```suggestion sample: true ```
```suggestion returned: success and I(ov_eligible) is C(true) and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING). ```
```suggestion sample: true ```
```suggestion description: Whether the domain is eligible for submission of "OV" certificates. Will never be C(false) if I(ov_eligible) is C(true) ```
This will consume the `streaming_content` generator on Python 2. Use `django.utils.six.moves.map` instead.
do we really need that `is not None` check? `check_token` should return `False` for None tokens.
immediatelly -> immediately
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
It might be smarter to validate the token first and only modify the session + redirect if it's valid. Otherwise it makes it really easy to create a session just by GET'ing a url (possible DoS vector). It also means you can't pass `accounts/password_reset` as the token and take advantage of our `request.path.replace()` code. It probably means validating the token twice, which is slightly slower. Seems fine to me if an invalid token gets leaked.
@romgar If you find the time that would be great!
@collinanderson That is a good point, at first I wanted to be safe against leaking anything (just to be on the safe side), but the DOS argument is more important.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Not sure it makes a difference but before it looks like we got `form=None` in the context.
Double checking the commit, this change, in this form leaks some state across migrations. Testing on CI right now. ``` python ERROR: test_alter_id_type_with_fk (migrations.test_executor.ExecutorTests) ---------------------------------------------------------------------- Traceback (most recent call last): File "/home/markus/Coding/django/django/db/backends/utils.py", line 64, in execute return self.cursor.execute(sql, params) psycopg2.ProgrammingError: foreign key constraint "book_app_book_author_id_93532c48_fk_author_app_author_id" cannot be implemented DETAIL: Key columns "author_id" and "id" are of incompatible types: integer and character varying. The above exception was the direct cause of the following exception: Traceback (most recent call last): File "/home/markus/Coding/django/django/test/utils.py", line 182, in inner return test_func(*args, **kwargs) File "/home/markus/Coding/django/tests/migrations/test_executor.py", line 401, in test_alter_id_type_with_fk executor.migrate([("author_app", "0002_alter_id")]) File "/home/markus/Coding/django/django/db/migrations/executor.py", line 94, in migrate self.apply_migration(states[migration], migration, fake=fake, fake_initial=fake_initial) File "/home/markus/Coding/django/django/db/migrations/executor.py", line 131, in apply_migration state = migration.apply(state, schema_editor) File "/home/markus/Coding/django/django/db/migrations/migration.py", line 118, in apply operation.database_forwards(self.app_label, schema_editor, old_state, project_state) File "/home/markus/Coding/django/django/db/migrations/operations/fields.py", line 201, in database_forwards schema_editor.alter_field(from_model, from_field, to_field) File "/home/markus/Coding/django/django/db/backends/base/schema.py", line 482, in alter_field old_db_params, new_db_params, strict) File "/home/markus/Coding/django/django/db/backends/base/schema.py", line 635, in _alter_field params, File "/home/markus/Coding/django/django/db/backends/base/schema.py", line 106, in execute cursor.execute(sql, params) File "/home/markus/Coding/django/django/db/backends/utils.py", line 79, in execute return super(CursorDebugWrapper, self).execute(sql, params) File "/home/markus/Coding/django/django/db/backends/utils.py", line 64, in execute return self.cursor.execute(sql, params) File "/home/markus/Coding/django/django/db/utils.py", line 95, in __exit__ six.reraise(dj_exc_type, dj_exc_value, traceback) File "/home/markus/Coding/django/django/utils/six.py", line 658, in reraise raise value.with_traceback(tb) File "/home/markus/Coding/django/django/db/backends/utils.py", line 64, in execute return self.cursor.execute(sql, params) django.db.utils.ProgrammingError: foreign key constraint "book_app_book_author_id_93532c48_fk_author_app_author_id" cannot be implemented DETAIL: Key columns "author_id" and "id" are of incompatible types: integer and character varying. ``` However, integrating this with the second commit on my PR fixes the issue. I thus squash those commits there and close your PR here.
Migrations plans with both forwards and backwards migrations are not supported.
Is `plan` meant as the second argument? Looks like we're missing a test for this branch.
Use single quotes for strings, unless there's a nested single quote. ([Python style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style))
That code is marginally longer, but I think it's clearer. I'll leave it up to what you think is best though.
I don't like how this entire if block looks. Is `plan` allowed to be `None` considering the check/assignment on line 73 and 74? I'd prefer to call out that it can not be forwards and backwards by doing something like: ``` if all_forwards == all_backards: raise.. # this may not be correct if `plan` can be `None` elif all_forwards: migrate_forwards() else: migrate_backwards() ``` I feel this highlights the invariant, and avoids potential problems of misreading the bool combinations in the original code.
We've been using "Take / apply" verb-style in new docstrings.
`if it encounter` => `if it encounters`
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
`mentionned` => `mentioned`
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
Yeah, it's fine.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
```suggestion item, fields=fields, using=self.db, ```
I don't think that a separate ticket is necessary, using `super()._create_index_sql()` will fix described issue.
```suggestion poly = Polygon(((0, 0), (0, 1), (1, 1), (1, 0), (0, 0))) ```
I would use kwargs ```suggestion check=models.Q(geom__within=poly), ```
Please add trailing comma.
Can we check that the constraint actually exists? ```python with connection.cursor() as cursor: constraints = connection.introspection.get_constraints( cursor, Neighborhood._meta.db_table, ) self.assertIn(constraint_name, constraints) ```
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
We can ignore `name`, `path`, and `args`, e.g. `*_, kwargs = field.deconstruct()`
Chop blank line.
Test failure is because this is missing `geography=True` on PostGIS.
Minor but I'd move this control flow block after the `weights` one to match the args order.
this module is marked as "legacy" and therefore the CI is not complaining about pep8 here, but it would help if we try to be pep8 compliant.
yep, that is what I meant, basically make sure new code is pep8 compliant
Prefer this indent style: ``` response = self.client.post(reverse('...'), { .... }) ```
please include the trailing comma on the last item in a dictionary so if more items are added we don't have to modify this line again
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
chop newline for consistency with other tests
`assertNotContains` is a bit fragile; instead I'd like if you could check `response.context` variables.
if a case is only used in 1 file like `FinderTestCase`, I'd put it there.
Yes please, I didn't audit for all instances.
Minor but I'd move this control flow block after the `weights` one to match the args order.
We try to avoid accessing the database connections when not necessary, so I'd move `db_features`: ```suggestion if ignore_conflicts and update_conflicts: raise ValueError( 'ignore_conflicts and update_conflicts are mutually exclusive.' ) db_features = connections[self.db].features ```
Having an overridable method seems like the most orthodox OOP solution (it's what a Java programmer would do :-) ) but I'm not convinced it really gives a useful abstraction: by coincidence it's the right place to make this one change, but I'm not sure there's a useful class of future modifications it opens up, so it feels like overkill to me. My thought with an instance variable was just to set it in the constructor in the base class, and overwrite it in the subclass constructor (not exposing it as a kwarg). I'm not sure there's any advantage to this; I think I was thinking about this because it's what I'd do in C++. I don't have a particularly strong feeling on this. I think if I were writing it I'd go with the class-level attribute.
The parameter to this method seems odd to me. As it stands, no caller is explicitly using the parameter in a call to the method and I can't see many legitimate uses for it in derived classes. I think you're effectively just using the default value as a space to store a class-scope variable. Would it be simpler to just assign a member on the class? Or on the instance, if you prefer.
Why `IntegrityError`? This should raise a `ValueError`: ``` raise ValueError('ignore_conflicts and update_conflicts are mutually exclusive.') ```
I would use an early return (in both cases): ```suggestion return OnConflict.IGNORE ```
Was already highlighted [here](https://github.com/django/django/pull/13065#discussion_r684521409) but was missed.
```suggestion def _select_on_conflict(self, ignore_conflicts, update_conflicts, update_fields, unique_fields): ```
use US spelling (behavior)
@timgraham It might be more appropriate in another commit then. I believe I wanted to make sure nothing was logged if a m2m backed inline was submitted without changes.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
If you use `to_text(xxx, errors='surrogate_or_strict')` it won't throw exceptions.
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
It'd be good to include a default sleep here so this loop doesn't continuously hit the oVirt endpoint. Even just a one or two second sleep would be fine between tries.
Just in case it's probably a good idea to use `.get()` instead of dict access syntax in case the key is unset.
`self._options_context` stores the context of parent options in case of nested suboptions. As the `elif` block does not have a recursive call to `_handle_options` context handling is not required here. However, context information can be added as part of error message in case this argument is part of nested sub-options (similar to other error messages in sub-option handling).
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
this class has a bunch of details of WSGI environ so I think it more accurately belongs in `django.http.request`
we want want -> we want
Upon further reflection, I think these three lines along with `check_mutually_exclusive` are erroneous for subspecs. We don't currently have a way to specify `required_if/one_of/together` or `mutually_exclusive` for subspecs, so in every case *but* the networking "aggregate" thing, this is severely broken (as you'd be making nonsensical checks in most cases)
It'd be good to include a default sleep here so this loop doesn't continuously hit the oVirt endpoint. Even just a one or two second sleep would be fine between tries.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
Just in case it's probably a good idea to use `.get()` instead of dict access syntax in case the key is unset.
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
`self._options_context` stores the context of parent options in case of nested suboptions. As the `elif` block does not have a recursive call to `_handle_options` context handling is not required here. However, context information can be added as part of error message in case this argument is part of nested sub-options (similar to other error messages in sub-option handling).
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
we want want -> we want
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
Just in case it's probably a good idea to use `.get()` instead of dict access syntax in case the key is unset.
Can we also add the ability to traverse a list of dicts? ``` def generate_final_config(self, cfg_dict): """ Generate final config dictionary :param cfg_dict: A dictionary parsed in the facts system :rtype: A dictionary :returns: A dictionary by eliminating keys that have null values """ final_cfg = {} if not cfg_dict: return final_cfg for key, val in iteritems(cfg_dict): dct = None if isinstance(val, dict): child_val = self.generate_final_config(val) if child_val: dct = {key: child_val} elif (isinstance(val, list) and val and all([isinstance(x, dict) for x in val])): child_val = [self.generate_final_config(x) for x in val] if child_val: dct = {key: child_val} elif val not in [None, [], {}, (), '']: dct = {key: val} if dct: final_cfg.update(dct) return final_cfg ``` something like this ^^^
This currently does not handle lists of items. We need to support it here.
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
just a small typo, serached->searched
we want want -> we want
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
Minor but I'd move this control flow block after the `weights` one to match the args order.
can we factor the lentient handling part out in a single mehtod? ``` private Query rethrowUlessLentient(RuntimeException e) { if (settings.lenient()) { return null; } throw e; } ``` man I with we had support for annonymous functions here or macros even :)
> scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.), compute a sum You missed the parentheses.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
As the Python `urlencode()` already handles scalars, I think this could be slightly simplified to: ```py if isinstance(value, (list, tuple)): query_val = [ item if isinstance(item, bytes) else str(item) for item in value ] else: query_val = value ```
Nitpick: Append a \ to the end so it doesn't generate an empty first line.
You are completely right. What about this? Too ugly?: ``` js_catalog_template = \ r"""{% autoescape off %} ... ```
I'd expect to see a test for this. Please audit test coverage carefully and make sure all lines are covered.
use basic.py's `missing_required_lib` function as it gives details on host and python to avoid user confusion
Upon further reflection, I think these three lines along with `check_mutually_exclusive` are erroneous for subspecs. We don't currently have a way to specify `required_if/one_of/together` or `mutually_exclusive` for subspecs, so in every case *but* the networking "aggregate" thing, this is severely broken (as you'd be making nonsensical checks in most cases)
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
In accordance with the naming of `DatetimeSerializer` this should be `TimedeltaSerializer`.
`self._options_context` stores the context of parent options in case of nested suboptions. As the `elif` block does not have a recursive call to `_handle_options` context handling is not required here. However, context information can be added as part of error message in case this argument is part of nested sub-options (similar to other error messages in sub-option handling).
this class has a bunch of details of WSGI environ so I think it more accurately belongs in `django.http.request`
Then use `enumerate()` instead.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
Match the error message
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
@lindong28 I tried this a bit locally, and realized the command now has two pipes to `tee` (see line 58 as well) When I drop the pipe to tee here on line 51 and keep the one below, the producer runs as expected.
My only concern with this in general is if `run_command()` fails or raises an exception, the file may not get deleted, leaving the `sudo` password in clear text on disk. Use `module.add_cleanup_file()` to add the tempfile to the list of files that will be cleaned up up in the event of failure.
bcoca's patch has been merged so we can use `play_context.executable` instead of C.DEFAULT_EXECUTABLE. And the default value should be built in so no need for the final `else /bin/sh`
Unfortunately, shlex.split() needs a bit of code to make it compatible with both python-2.6 and python3.x On python-2.6, it only works on byte strings. In python3.x it only works with text strings. So you have to test for python version and then convert appropriately. Code like the following is what I use: ``` python from ansible.compat import six from ansible.module_utils._text import to_bytes, to_text [...] nspawn_args = self._play_context.nspawn_args if six.PY2: nspawn_args = shlex.split(to_bytes(nspawn_args, errors='surrogate_or_strict')) else: nspawn_args = shlex.split(to_text(nspawn_args, errors='surrogate_or_strict'))
It''s desirable to call to_bytes with errors='surrogate_or_strict' so that it raises an exception instead of replacing undecodable bytes with a "?". Also, map() is more pythonically expressed as a list comprehension. ``` python local_cmd = [to_bytes(i, errors='surrogate_or_strict') for i in local_cmd] ```
I think these should get taken care of by `uninstallService`? Or is the point here that we want to assert exit code 0 when uninstalling it in these tests
Usually we also make a few API calls to the server, e.g. https://github.com/elastic/elasticsearch/blob/2aba52de8f9315b0e384e1c657d7b0401d26a1b0/qa/vagrant/src/main/java/org/elasticsearch/packaging/test/PackageTestCase.java#L121-L122 I'm not completely sold on the value of those though
Yes, I think it isn't needed. You can remove it and see if it still works if you limit to `all` :)
Nit: can you put this and `runWithoutJava` next to each other
assumes native strings in line, you should convert to_native or ensure regex matches 'line' string type
Doesn't isatty bomb on none? It does for me...
"Mixin for combining with a lookup"
Instead of defining `success`, just return directly in the above code. I think it is simple enough.
Now that Django only supports Python 3.5+ this can be much more simply defined: ```python try: ... except FileNotFoundError: return False ``` You don't need to catch any other type of `IOError` as it is just re-raised. Also don't forget to remove the `errno` import that was added.
Bravo on tackling one of the gnarlier test setups ;-> :+1:
remove "0" in {0}
single line looks more readable here
Returns -> Return use period
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Do note that this only looks at the primary credential cache. This means that if you use a collection credential cache such as DIR, KEYRING or KCM (default in most distributions these days), you might not see a ticket that exists in a non-primary credential cache. Maybe consider using `klist -A`, or try a `kswitch -p <principal>`? The latter is easier to parse, but might break stuff if people are still using the old krb5 APIs (rather than GSSAPI) and were using the primary credential that you just made secondary.
Why do you have a different code path here? The non-check line does the same but provides a bit more info, so just using that in the check case should be fine I'd say.
Yeah that's what I suspected too. Stupid SQL.
put closing parenthesis on the next line
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
chop "one of" add comma before "or"
IIRC get_cols() was renamed to get_group_by_cols() some time ago - no need to have both.
I had to change this to `template_params['subquery'], sql_params = self.subquery.query.get_compiler(connection=connection).as_sql()` in order to compile correctly when using a database other than the `DEFAULT_DB_ALIAS`.
Similarly, I don't see much advantage to creating indirection with a method.
A bit DRYer? ``` python value = self.rhs if isinstance(value, datetime.datetime): output_field = models.DateTimeField() elif isinstance(value, datetime.date): output_field = models.DateField() else: output_field = None if output_field: value = models.Value(value, output_field=output_field) self.rhs = value.resolve_expression(compiler.query) ```
```suggestion pkg_version = line.partition(':')[-1].strip() ```
Is it possible ? For me it would mean that it exist a package without version.
It doesn't work if pkg_provides contain a version: ``` max@mde-oxalide % pacman -Qqei perl | grep Provide Provides : perl-archive-tar=2.30 perl-attribute-handlers=1.01 perl-autodie=2.29 perl-autoloader=5.74 perl-autouse=1.11 perl-b-debug=1.26 perl-base=2.27 perl-bignum=0.49 perl-carp=1.50 perl-compress-raw-bzip2=2.074 perl-compress-raw-zlib=2.076 perl-config-perl-v=0.29 perl-constant=1.33 perl-cpan-meta-requirements=2.140 perl-cpan-meta-yaml=0.018 perl-cpan-meta=2.150010 perl-cpan=2.20 perl-data-dumper=2.170 perl-db_file=1.840 perl-devel-ppport=3.40 perl-devel-selfstubber=1.06 perl-digest-md5=2.55 perl-digest-sha=6.01 perl-digest=1.17_01 perl-dumpvalue=1.18 perl-encode=2.97 perl-encoding-warnings=0.13 perl-env=1.04 perl-experimental=0.019 perl-exporter=5.73 perl-extutils-cbuilder=0.280230 perl-extutils-constant=0.25 perl-extutils-install=2.14 perl-extutils-makemaker=7.34 perl-extutils-manifest=1.70 perl-extutils-parsexs=3.39 perl-file-fetch=0.56 perl-file-path=2.15 perl-file-temp=0.2304 perl-filter-simple=0.95 perl-filter-util-call=1.58 perl-getopt-long=2.5 perl-http-tiny=0.070 perl-i18n-collate=1.02 perl-i18n-langtags=0.43 perl-if=0.0608 perl-io-compress=2.074 perl-io-socket-ip=0.39 perl-io-zlib=1.10 perl-io=1.39 perl-ipc-cmd=1.00 perl-ipc-sysv=2.07 perl-json-pp=2.97001 perl-lib=0.64 perl-libnet=3.11 perl-locale-codes=3.56 perl-locale-maketext-simple=0.21_01 perl-locale-maketext=1.29 perl-math-bigint-fastcalc=0.5006 perl-math-bigint=1.999811 perl-math-bigrat=0.2613 perl-math-complex=1.5901 perl-memoize=1.03_01 perl-mime-base64=3.15 perl-module-corelist=5.20180622 perl-module-load-conditional=0.68 perl-module-load=0.32 perl-module-loaded=0.08 perl-module-metadata=1.000033 perl-net-ping=2.62 perl-params-check=0.38 perl-parent=0.236 perl-pathtools=3.74 perl-perl-ostype=1.010 perl-perlfaq=5.021011 perl-perlio-via-quotedprint=0.08 perl-pod-checker=1.73 perl-pod-escapes=1.07 perl-pod-parser=1.63 perl-pod-perldoc=3.2801 perl-pod-simple=3.35 perl-pod-usage=1.69 perl-podlators=5.006 perl-safe=2.40 perl-scalar-list-utils=1.50 perl-search-dict=1.07 perl-selfloader=1.25 perl-socket=2.027 perl-storable=3.08 perl-sys-syslog=0.35 perl-term-ansicolor=4.06 perl-term-cap=1.17 perl-term-complete=1.403 perl-term-readline=1.17 perl-test-harness=3.42 perl-test-simple=1.302133 perl-test=1.31 perl-text-abbrev=1.02 perl-text-balanced=2.03 perl-text-parsewords=3.30 perl-text-tabs=2013.0523 perl-thread-queue=3.12 perl-thread-semaphore=2.13 perl-threads-shared=1.58 perl-threads=2.22 perl-tie-file=1.02 perl-tie-refhash=1.39 perl-time-hires=1.9759 perl-time-local=1.25 perl-time-piece=1.3204 perl-unicode-collate=1.25 perl-unicode-normalize=1.26 perl-version=0.9923 perl-xsloader=0.30 ```
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
If you're going to do partition, I'd rather use the fact that it'll always return a 3-tuple instead of using `-1` to index the third element. i.e.: ```python line.partition(':')[2] ```
@thomwiggers `line.partition(':')[2]` yes, I have no problem with that.
```suggestion pkg_name = line.partition(':')[-1].strip() ``` idem, no need to lower.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
A bit DRYer? ``` python value = self.rhs if isinstance(value, datetime.datetime): output_field = models.DateTimeField() elif isinstance(value, datetime.date): output_field = models.DateField() else: output_field = None if output_field: value = models.Value(value, output_field=output_field) self.rhs = value.resolve_expression(compiler.query) ```
```suggestion pkg_version = line.partition(':')[-1].strip() ```
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
Is it possible ? For me it would mean that it exist a package without version.
It doesn't work if pkg_provides contain a version: ``` max@mde-oxalide % pacman -Qqei perl | grep Provide Provides : perl-archive-tar=2.30 perl-attribute-handlers=1.01 perl-autodie=2.29 perl-autoloader=5.74 perl-autouse=1.11 perl-b-debug=1.26 perl-base=2.27 perl-bignum=0.49 perl-carp=1.50 perl-compress-raw-bzip2=2.074 perl-compress-raw-zlib=2.076 perl-config-perl-v=0.29 perl-constant=1.33 perl-cpan-meta-requirements=2.140 perl-cpan-meta-yaml=0.018 perl-cpan-meta=2.150010 perl-cpan=2.20 perl-data-dumper=2.170 perl-db_file=1.840 perl-devel-ppport=3.40 perl-devel-selfstubber=1.06 perl-digest-md5=2.55 perl-digest-sha=6.01 perl-digest=1.17_01 perl-dumpvalue=1.18 perl-encode=2.97 perl-encoding-warnings=0.13 perl-env=1.04 perl-experimental=0.019 perl-exporter=5.73 perl-extutils-cbuilder=0.280230 perl-extutils-constant=0.25 perl-extutils-install=2.14 perl-extutils-makemaker=7.34 perl-extutils-manifest=1.70 perl-extutils-parsexs=3.39 perl-file-fetch=0.56 perl-file-path=2.15 perl-file-temp=0.2304 perl-filter-simple=0.95 perl-filter-util-call=1.58 perl-getopt-long=2.5 perl-http-tiny=0.070 perl-i18n-collate=1.02 perl-i18n-langtags=0.43 perl-if=0.0608 perl-io-compress=2.074 perl-io-socket-ip=0.39 perl-io-zlib=1.10 perl-io=1.39 perl-ipc-cmd=1.00 perl-ipc-sysv=2.07 perl-json-pp=2.97001 perl-lib=0.64 perl-libnet=3.11 perl-locale-codes=3.56 perl-locale-maketext-simple=0.21_01 perl-locale-maketext=1.29 perl-math-bigint-fastcalc=0.5006 perl-math-bigint=1.999811 perl-math-bigrat=0.2613 perl-math-complex=1.5901 perl-memoize=1.03_01 perl-mime-base64=3.15 perl-module-corelist=5.20180622 perl-module-load-conditional=0.68 perl-module-load=0.32 perl-module-loaded=0.08 perl-module-metadata=1.000033 perl-net-ping=2.62 perl-params-check=0.38 perl-parent=0.236 perl-pathtools=3.74 perl-perl-ostype=1.010 perl-perlfaq=5.021011 perl-perlio-via-quotedprint=0.08 perl-pod-checker=1.73 perl-pod-escapes=1.07 perl-pod-parser=1.63 perl-pod-perldoc=3.2801 perl-pod-simple=3.35 perl-pod-usage=1.69 perl-podlators=5.006 perl-safe=2.40 perl-scalar-list-utils=1.50 perl-search-dict=1.07 perl-selfloader=1.25 perl-socket=2.027 perl-storable=3.08 perl-sys-syslog=0.35 perl-term-ansicolor=4.06 perl-term-cap=1.17 perl-term-complete=1.403 perl-term-readline=1.17 perl-test-harness=3.42 perl-test-simple=1.302133 perl-test=1.31 perl-text-abbrev=1.02 perl-text-balanced=2.03 perl-text-parsewords=3.30 perl-text-tabs=2013.0523 perl-thread-queue=3.12 perl-thread-semaphore=2.13 perl-threads-shared=1.58 perl-threads=2.22 perl-tie-file=1.02 perl-tie-refhash=1.39 perl-time-hires=1.9759 perl-time-local=1.25 perl-time-piece=1.3204 perl-unicode-collate=1.25 perl-unicode-normalize=1.26 perl-version=0.9923 perl-xsloader=0.30 ```
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
```suggestion pkg_name = line.partition(':')[-1].strip() ``` idem, no need to lower.
If you're going to do partition, I'd rather use the fact that it'll always return a 3-tuple instead of using `-1` to index the third element. i.e.: ```python line.partition(':')[2] ```
@thomwiggers `line.partition(':')[2]` yes, I have no problem with that.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I'd put the trailing `}` in a new line.
I think you can use `django.utils.deconstruct` to decorate the class, the `path` argument can be passed explicitly. Since Django 2.0+ is Python 3 only, you can use keyword-only arguments with `*, arg1=None, arg2=None`.
I think normally we don't `raise NotImplemented` but just return `False` so ```py return ( isinstance(other, CheckConstraint) and self.name == other.name and self.constraint == other.constraint ) ```
```python hanging = ( indentation, has, a, newline, after, opening, bracket, ) ```
1: I'm definitely happy with this. I'm not sure why I didn't go for it myself. 2: Fine by me, I don't feel strongly.
no restructured text (:class:) in docstrings please
We should pass `using` from the queryset ```suggestion ct_id = self.get_content_type(model=model_cls, using=ct_queryset.db).pk ```
could switch to single quotes for consistency
It would be nice to be consistent about the ordering in `assertEqual` using it's `(variable, 'expected value')` but here and a couple other places it's opposite.
You should be able to use direct attribute access here: `remote_field.through`
shorten line (rule of thumb I'm using is ~120 characters so I don't have scroll horizontally in github review)
Thanks for the patch @Giebisch, I think we should adjust the `expected_outputs` instead to be properly formatted in order to ensure formatting is actually used. https://github.com/django/django/blob/78657a201308a0b53661e2b267914407a9fd7fa1/tests/test_runner/test_debug_sql.py#L90-L111 https://github.com/django/django/blob/78657a201308a0b53661e2b267914407a9fd7fa1/tests/test_runner/test_debug_sql.py#L124-L133 The adjustments made make the tests pass with and without the proposed changes.
I'm thinking these parameters shouldn't be filled in, since we're popping `stack_description` so if a user wanted a hard-fail for nonexistent stacks, they could do: ``` - cloudformation_facts: stack_name: not-real failed_when: not cloudformation['not-real'] ```
Also probably put that in the examples, for if users want that behavior but maybe don't know about `failed_when`.
`test_jsonfield` looks good to me (since this is InspectDBTests, _introspection seems redundant).
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
Must not be `None`.
Whether it has changed or not does not mean there should be a format with invalid URL.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
`field_preference` must be a list or a tuple.
shorten line (rule of thumb I'm using is ~120 characters so I don't have scroll horizontally in github review)
Code duplication 80-86, 89-94.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
There are multiple formats, some may have `file` some may not. If some new hq format is introduced with different rendition your approach will break downloading by default while proper URL handling will not. Finally read coding conventions.
Whether it has changed or not does not mean there should be a format with invalid URL.
Must not be `None`.
this seems like it should be shared with the main one rather than duplicated here
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
It would be clearer to the end-user if the help was "Shows output from passing tests."
These 2 lines could be replaced by: ```python uploader_url, uploader_id = uploader_data[0][0:2] ```
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
`_search_regex`, `_parse_json`. Again: read coding conventions.
Mandatory. Read coding conventions.
I would replace these 3 lines by: ``` # Add these imports from ansible.module_utils.compat import ipaddress from ansible.module_utils._text import to_text ### prefix = ipaddress.ip_network(data["prefix"]) network = to_text(prefix.network_address) mask = prefix.prefixlen ``` This way, it will not fail if `data["prefix"]` doesn't have a netmask (default it to /32 for ipv4, /128 for ipv6).
Or if you want your module to fail if the prefix sent as parameter doesn't have a netmask, you should handle the error.
Sorry, I know it was in my changes, but I forgot to remove that lineÃ¢Â€Â¦ Changing the data seems a bit ugly here, could you remove this line and changing the next one by: ``` prefix = ipaddress.ip_network(data["prefix"] + "/" + data["prefix_length"]) ```
We could lowercase the vendored files, that would help at least for the `zh-*` variants.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
Please ignore, my suggestion is invalid syntax.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
Ditto. ```suggestion # Check empty result with non-execution optimization. with self.assertNumQueries(0): ```
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
I think you might want to rollback at this point.
Please ignore, my suggestion is invalid syntax.
Is this needed? You were the author and if you used code from win_psmodule the copyright is enough.
This is still something we shouldn't really want to explicitly support.
I don't think this should be a supported option, the source should also be set when `state: present`.
point -> points
I don't think this will work correctly on python2. Pretty sure you need parenthesis there: ``` python except (ConfigParser.NoSectionError, ConfigParser.NoOptionError): ```
This usage of filter won't work on python3. filter on python3 will return a generator which will evaluate to true in the conditional below. Use a list comprehension so that found_networks ends up with an empty list on python3 which will evaluate to false.
Same change needs to be made to many other uses of filter.
I don't think this works? block_id defaults to the boolean False in the parameters to this function. So you probably need to check: ``` python if block_id is not False: ``` rather than checking against a string.
Seems like it would result in less confusing code in the long run. If you do defer it and leave the TODO, I'd suggest to use your GitHub username instead of first name.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
I realize this predates your PR, but why is 'unused' not used? It appears to be the target field provided by the SQLUpdateCompiler during as_sql(). This seems like a really good spot to allow a backend to hook in and CAST the expression to a type that is allowed by the target column.
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
okay, but would be helpful to say _why_ we need to always return True.
```suggestion template_name = 'forms_tests/form_snippet.html' ```
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
Similarly, ```if tc['skip'].get('i')```
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Yeah, I think what you have is fine, since we have to iterate over the generator one way or another, and the set comparison it's really straightforward either. I guess the only downside of this algorithm is that, if the broker nefariously listed the same topic multiple times, it would give us a false positive. But I seriously doubt that can happen.
You can use `self.fail('Cyclic reference in Exception Reporter.get_traceback_frames()')`
use the `flatten_dict` method from module_utils.network.nxos.utils.utils.py
I think this could be increased to 10 just in case the system is really slow. The important thing is that it doesn't loop forever.
Single quotes please.
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
set the parameters to `default` by prepending `no`, not appending default values.
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
Not sure it makes a difference but before it looks like we got `form=None` in the context.
You can use `self.fail('Cyclic reference in Exception Reporter.get_traceback_frames()')`
I think this could be increased to 10 just in case the system is really slow. The important thing is that it doesn't loop forever.
Single quotes please.
Does this need to be a separate method? Seems unnecessary to me.
immediatelly -> immediately
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
Do we need to define `async_generator` in `__aiter__`? ```python async def _async_generator(self): # Generators don't actually start running until the first time you call # next() on them, so we make the generator object in the async thread # and then repeatedly dispatch to it in a sync thread. sync_generator = self.__iter__() # So, you can't raise StopIteration up through sync_to_async as # too much of the Python async system was originally built on # generators. Instead, we capture it inside a custom sync function # and change it to an exception we *can* propagate. # We should probably move this logic inside asgiref at some point. def custom_next(gen): try: return next(gen) except StopIteration: raise StopAsyncIteration() while True: try: yield await sync_to_async(custom_next)(sync_generator) except StopAsyncIteration: return # Note: __aiter__ is a *synchronous* method that has to then return an # *asynchronous* iterator/generator. Thus, we nest an async generator # inside it. # Also note: This is a generic iterable converter for now, and is going to # suffer a performance penalty on large sets of items due to the cost of # crossing over the sync barrier each time. We should write custom ones # for each Iterable subclass, but we'll need some work in Compiler first. def __aiter__(self): return self._async_generator() ```
@romgar If you find the time that would be great!
Not sure it makes a difference but before it looks like we got `form=None` in the context.
Yeah, I think what you have is fine, since we have to iterate over the generator one way or another, and the set comparison it's really straightforward either. I guess the only downside of this algorithm is that, if the broker nefariously listed the same topic multiple times, it would give us a false positive. But I seriously doubt that can happen.
You can use `self.fail('Cyclic reference in Exception Reporter.get_traceback_frames()')`
I think this could be increased to 10 just in case the system is really slow. The important thing is that it doesn't loop forever.
use the `flatten_dict` method from module_utils.network.nxos.utils.utils.py
Single quotes please.
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
set the parameters to `default` by prepending `no`, not appending default values.
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
Not sure it makes a difference but before it looks like we got `form=None` in the context.
May as well do the following as a field name can only legally have a single `-` at the start: ```python field_name = part.lstrip('-') ```
Would it be enough to check `form.fields`? This might make the test a bit easier to follow instead of having to parse the HTML to see what's expected..
please multiline these strings so they aren't longer than 120 chars. ``` row_html = ( '...' '...' ) ```
Missing asserts here? Also, please split the test into smaller parts.
It's fine if it's a placeholder for future capabilities.
This regex does not make any sense.
Use hanging indent as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style). ``` values = AggregateTestModel.objects.aggregate( arrayagg=ArrayAgg('char_field', ordering=ordering) ) ```
Remove all pointless changes.
Why do you have a different code path here? The non-check line does the same but provides a bit more info, so just using that in the check case should be fine I'd say.
Please remove `no_log=True` from username
This seems overly simplistic, we should at least be sure to the extend that we should check the signature of the function on whether or not it supports more than two arguments.
Any case where the method raises a `TypeError` which is not caused by the missing argument
This line is related to the `save` method, and should sit beneath it, rather than the new `get_updated_model` method.
`self.real_apps` is always a set, `set()` is unnecessary (here and in many other lines).
```suggestion raise InvalidCacheBackendError('Could not find backend {backend!r}: {e}') from e ```
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
I would rephrase this as "Only set this to yes when you want private information about this key to leave the remote machine", the way it's written now I read "don't" instead of "do want"
Use another lookup instead of `epoch` e.g. `second`.
`self.assertEqual(uct.call_count, 4)` But IMO it is better to use patch as decorator instead of context manager.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
Generally, I'd have put the static regex directly as the first argument of `re.search()`.
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
Use the Python 2 and low 3 Time Machine: `'url too short: %s' % (video_pre_parts, )` or: `'url too short: %(video_pre_parts)s' % {'video_pre_parts': video_pre_parts, }` or: `'url too short: {video_pre_parts}'.format(video_pre_parts=video_pre_parts)` or: `'url too short: {0}'.format(video_pre_parts)` No doubt there are other ways (eg `....format(**locals())`
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
nit: formatting, add some whitespaces
Would be nice to add this as part of this PR unless something big blocks it.
nit: formatting, add some whitespaces
looks like there are two levels of indentation instead of one
nit: formatting, add some whitespaces
I liked generator approach more: it's more readable in terms of separating a distinct logical step from a higher-level flow.
This whole block could've been done with a nested list comprehension without any need for temporary variable introduction: ```python names = [ name for name_list in names for name in name_list.split(',') ] ``` Alternative FP approach would look like: ```python from itertools import partial, reduce ... names = reduce( lambda l, e: l + e, map( partial(str.split, sep=','), names ), [], ) ``` (but Pythonistas don't like such style in general)
`list`s have `.append()` method. You probably don't want to re-allocate a new instance on each iteration of the loop.
Lists also have .extend() which might be what you need here
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
`band_input`, you don't get much by saving one char :-)
I think `enumerate` would work here
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
Would be nice to add this as part of this PR unless something big blocks it.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
hmmm.. now I think I'm wrong, you want to only have the first partial func.. `func.func` shouldn't anyway be another partial or else that would be problematic I think, as views have to return something else.. Sorry for disturbance ;)
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
You don't need this conditional, since Ansible enforces that these are the only choices.
looks like there are two levels of indentation instead of one
Missing space after the `for`
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: Invalid number of points in LineString (found 1 - must be 0 or >= 2) at __randomizedtesting.SeedInfo.seed([3BC798163FFF812D:3A8577712E4A2AD2]:0) at com.vividsolutions.jts.geom.LineString.init(LineString.java:102) at com.vividsolutions.jts.geom.LineString.<init>(LineString.java:93) at com.vividsolutions.jts.geom.GeometryFactory.createLineString(GeometryFactory.java:539) at com.vividsolutions.jts.geom.GeometryFactory.createLineString(GeometryFactory.java:531) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseLineString(GeoWKTShapeParserTests.java:99) ... ```
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: cannot create point collection with empty set of points at __randomizedtesting.SeedInfo.seed([3BC798163FFF812D:918E10886BC43EC1]:0) at org.elasticsearch.common.geo.builders.ShapeBuilder.<init>(ShapeBuilder.java:97) at org.elasticsearch.common.geo.builders.LineStringBuilder.<init>(LineStringBuilder.java:49) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseMultiLineString(GeoWKTShapeParserTests.java:112) ... ```
nit: formatting, add some whitespaces
```suggestion assert isinstance(wrap_var(b'foo'), type(b'')) ```
```suggestion assert isinstance(wrap_var(dict(foo='bar')), dict) ```
This can be changed to look for only one space now that #57209 is merged. Thanks again for writing tests. Much appreciated.
No need to parametrize with just one case.
Both cases test the same thing, don't repeat yourself. Test things based on their properties. Here you'd probably want to test iterables, not just sets.
It Python, you should use a proper camel case for classes: ```suggestion class TestJsonEncodeFallback: ```
It doesn't matter whether it's a method or a function. A private function is related to the module scope, a private method is related to the class. Still, both are private, it's just a different level of namespacing. If a module name starts with an underscore it'd be also private.
For algorithmic code, it can make sense to test private methods and private functions in isolation from the rest of the code. This does seem to be a place where that could be justified. The code being tested is functional (meaning it operates via parameters and return values rather than callbacks) and it plugs into a larger framework which is outside of our control. What I'll sometimes do is push all the permutations of data that I care about at the private function and then push a small subset at the public interface to make sure that the interaction between the public and private code is working as expected.
Usually, testing private interfaces doesn't make sense.
An example of using the `mocker` fixture. ```suggestion def mock_NamedTemporaryFile(mocker, **args): mock_ntf = mocker.MagicMock() mock_ntf.write = mocker.MagicMock() mock_ntf.close = mocker.MagicMock() ```
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
Please rewrite as ``` if __name__ == '__main__': main() ```
Missing `=dict` on this and the next few lines
missing space between `,` and `and`
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
Missing `=dict` on this and the next few lines
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
Please rewrite as ``` if __name__ == '__main__': main() ```
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
missing space between `,` and `and`
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
Ditto. ```suggestion # Check empty result with non-execution optimization. with self.assertNumQueries(0): ```
Please ignore, my suggestion is invalid syntax.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
I needed to add a `set_source_expressions` method which sets `self.subquery = source_expressions` here to get relabelling to work.
Usage of `template=None, **extra_context` params seems untested. Not sure if it's important.
I had to change this to `template_params['subquery'], sql_params = self.subquery.query.get_compiler(connection=connection).as_sql()` in order to compile correctly when using a database other than the `DEFAULT_DB_ALIAS`.
Notice that, conceptually, the `set(a) | set(b)` operation is different than `a + b`. The former performs set union meaning that duplicates are removed, whereas the latter is a simple concatenation with the possibility for the same item to appear twice. If these are considered "set operations", then the else part should be replaced by something like: ``` c = a + filter(lambda x: x not in a, b) ```
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
Please rewrite as ``` if __name__ == '__main__': main() ```
Missing `=dict` on this and the next few lines
missing space between `,` and `and`
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
```suggestion # Run masked_action when enabled param is not provided or service is supposed to be enabled/started. if not module.check_mode and module.params['enabled'] in [None, True]): ```
Missing `=dict` on this and the next few lines
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Please rewrite as ``` if __name__ == '__main__': main() ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
For integers please: use `type='int` remove the `isdigit` check from `check_params`
missing space between `,` and `and`
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
Please rewrite as ``` if __name__ == '__main__': main() ```
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Missing `=dict` on this and the next few lines
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
For integers please: use `type='int` remove the `isdigit` check from `check_params`
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
missing space between `,` and `and`
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
Please rewrite as ``` if __name__ == '__main__': main() ```
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Missing `=dict` on this and the next few lines
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
For integers please: use `type='int` remove the `isdigit` check from `check_params`
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
missing space between `,` and `and`
Please use triple double quotes around docstrings. ([PEP 257](https://www.python.org/dev/peps/pep-0257/#what-is-a-docstring))
The leading underscore in the '_meta' key is missing here.
need to x2 check tmrow, but this should be an error, we don't allow for circular parenting
This should probably just be done with a mutually exclusive group in the CLI argparse declaration.
option should deal with the case the dependencies are not avaiable and give a warning.
I see the idea, but for me if a function is called only once and only contains some simple lines, the function call overhead is not worth it. You can let this for now and wait for the Django fellows opinion.
Do you think we should remove internal variable too. ```python host_vars = self._get_host_variables(host) if host_vars: self._remove_internal(host_vars) result.extend(self._show_vars(host_vars, depth + 1)) ```
colors should all be configurable
I wonder if it is worth compressing the hitCount into byteSequence? That way you only have to store an array of ints? You'd have to cap `hitCount` at 255 but maybe that is ok? Or you could use an array of long and have tons of precision on your `hitCount`.
I'm fine with the answer being "no, you are crazy Nik" or "not right now".
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
This looks to be incorrect, there should be if not os.path.exists() protection, or better yet, have _createdir() do (very pseudocodish) try: makedirs(); except: if already-exists: pass; else raise EDIT: Sorry, was reading the old version of _createdir()...
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
Remove this since it doesn't do anything and `Mock` isn't defined.
If you turn this into ```suggestion if not os.path.exists(collection_output): _build_collection_tar( collection_path, collection_output, collection_manifest, file_manifest, ) return ``` you could reduce some nesting which improves readability.
This `close()` is extraneous in modern Pythons; the `with open()` implicitly closes the file.
Again, error handling changed.
Note that a _plugin option_ should be used for this configuration path. This would require: - add a new parameter in plugin metadata ([example](https://github.com/ansible/ansible/blob/03794864c2c3267f1fbff6ac49e2d883090284ad/lib/ansible/plugins/connection/ssh.py#L188)) - use `get_option` method in order to fetch option value ([example](https://github.com/ansible/ansible/blob/03794864c2c3267f1fbff6ac49e2d883090284ad/lib/ansible/plugins/connection/ssh.py#L973))
User of this inventory plugin should be able to use `v6_main_ip` for value of `ansible_host` (i don't know if it makes sens to be able to use `internal_ip`) or no `ansible_host` at all (meaning that `server['label']` will be used instead).
It might be safer to perform the `sys.stdout` switcharoo *inside* the `try: finally`: ```python try: old_stdout = sys.stdout sys.stdout = captured_stdout = StringIO() ... finally: sys.stdout = old_stdout ```
In the future it might be nice to add the `file` as an argument to these `tower-cli` modules. That way, you could just pass a `StringIO` in directly instead of having to monkey-patch `sys.stdout` in this way: https://github.com/pallets/click/blob/67f39e4ff9093ee347207d1cc1abe0130c21268e/click/utils.py#L206
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
This is not changed.
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
This is not changed.
Although `operator.xor()` has the signature `(a, b)`, it might make sense to stick with `(x, y)` for consistency? ```suggestion def _sqlite_bitxor(x, y): if x is None or y is None: return None return x ^ y ```
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
This is not changed.
Although `operator.xor()` has the signature `(a, b)`, it might make sense to stick with `(x, y)` for consistency? ```suggestion def _sqlite_bitxor(x, y): if x is None or y is None: return None return x ^ y ```
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
This is not changed.
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
Any problem with: ``` @property def media(self): ```
`copy()` in unnecessary.
I think it is better to pass module as argument to build_xml() and handle lxml install check-in module_utils instead of having it in each module
Test failure is because this is missing `geography=True` on PostGIS.
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
I would chop blank lines in this test.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
Chop `Ensure that`.
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
is this test needed? I'd suspect it's already tested.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
I would chop blank lines in this test.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
Chop `Ensure that`.
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
is this test needed? I'd suspect it's already tested.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Please remove this blank line as requested by Paolo.
I would change this to a `NotSupportedError`, e.g. ```python raise NotSupportedError( 'Bulk create a multi-table inherited model is not supported ' 'on this database backend.' ) ```
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
```suggestion item, fields=fields, using=self.db, ```
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
(And round-tripping of the messages is already tested in other tests)
I don't see any need for this attribute.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think [`_extract_m3u8_formats`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L847) does the work better. If the existing method does not fit the need, feel free to modify it.
You have some unmerged lines here
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
Maybe the following will be more explicit: ```python first_author_books = result[0].books.all() self.assertEqual([self.book1], [first_author_books]) self.assertEqual([self.reader], [first_author_books[0].read_by.all()])) ```
``` for i, video_url in enumerate(video_urls): ```
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
single line as above
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
```suggestion item, fields=fields, using=self.db, ```
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Maybe the following will be more explicit: ```python first_author_books = result[0].books.all() self.assertEqual([self.book1], [first_author_books]) self.assertEqual([self.reader], [first_author_books[0].read_by.all()])) ```
Unindent by 1 space. Indention must be a multiple of 4 spaces.
I would suggest to also warn a bit about so users won't "forget" they are using the sandbox url ~~~python self.module.warn(warning="Sandbox is enabled. All actions are made against the URL %s" % self.baseurl) ~~~
Revert unrelated change.
Use ```python from ansible.module_utils.basic import env_fallback ... auth_token=dict(aliases=['x_auth_token'], no_log=True, fallback=(env_fallback, ['X_AUTH_TOKEN'])), ```
Mind putting this magic value into a constant with a descriptive name? I'd read much better if it was ```suggestion sys.exit(RC_CLI_INIT_FAILURE) ``` or something like that.
A small oversight I noticed in an old Python 3.7.0 virtualenv: https://github.com/django/django/pull/13393
+1 to that for the yaml configuration.
I've changes to `django.utils.inspect.func_supports_parameter()`.
Maybe merge all the groovy securing code into one place? It feels funky to have the default receiver whitelist here but method blacklist above.
not sure `6.` is needed as you have `from __future__ import division`
See my previous review for indentation style of this. Perhaps the common qs stuff before the last filter can be moved to setUpTestData.
I have to admit that I turned several module-level functions into engine methods without giving it much thought. Feel free to move engine methods in other classes as needed.
I'm reluctant to pass a `Template` to the `Engine` because that creates a circular relation between the two classes. If you keep this design, you must at least rename `compile_string`, because it no longer accepts a string in argument. In fact I would add a new method and deprecate `compile_string`.
Yes, your version is simpler, Simon.
I'm not sure this `next()` dance is really required, I find the following much easier to read ```python for engine in engines.all(): if isinstance(engine, DjangoTemplates): return engine.engine raise ImproperlyConfigured('No DjangoTemplates backend is configured.') ```
How about: ``` if storage_engine == 'InnoDB': return self.connection.mysql_version >= ( (10, 2, 2) if self.connection.mysql_is_mariadb else (5, 7, 5) ) return storage_engine in ('MyISAM', 'Aria') ```
up with Django imports
prefer including a trailing comma in kwargs so if more items are added in the future we don't have to modify this line again
^ Just updating this if anything is looking at it. That's a general Python3 thing (no .message), not specific to this exception.
The OptionGroupNotFoundFault exception doesn't seem to have a .message. Neither does the exception for invalid parameters (though maybe we don't care about that since that's user error). This should be something like `if e.response['Error']['Code'] != 'OptionGroupNotFoundFault':` or `if 'OptionGroupNotFoundFault' not in str(e):`
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
Please add a trailing comma: ```suggestion on_conflict=None, update_fields=None, unique_fields=None, ```
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
Ditto for `[]` â†’ `None` and `ON_CONFLICTS_NONE` â†’ `None`.
This hook is unnecessary, IMO. I would move the logic to `_select_on_conflict()`.
```suggestion item, fields=fields, using=self.db, ```
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
You'll want to branch off `< 3.6`.
`always_text` is gone.
Can you just do `name.startswith('__') and not name.endswith('__')`? Simpler is better
I would consider that as not working
I also still don't understand why it's useful to allow writing code that doesn't work.
I think silently failing to cache the property should be considered not working.
Would be fine I guess. I'm not too sure about the safety margin aspect in the first place, but it's SQLite so whatever works, I guess.
Rename DO to DigitalOcean to avoid acronyms
Rename DO to DigitalOcean to avoid acronyms
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
Please add a trailing comma: ```suggestion on_conflict=None, update_fields=None, unique_fields=None, ```
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Ditto for `[]` â†’ `None` and `ON_CONFLICTS_NONE` â†’ `None`.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
It would be more readable to raise an error explicitly (like previously), e.g. ```python db_features = connections[self.db].features if ignore_conflicts and not db_features.supports_ignore_conflicts: raise NotSupportedError('This database backend does not support ignoring conflicts.') if update_conflicts: if not db_feature.supports_update_conflicts: raise NotSupportedError( 'This database backend does not support updating conflicts.' ) if unique_fields and not db_features.supports_update_conflicts_with_target: raise NotSupportedError( 'This database backend does not support updating conflicts with ' 'specifying unique fields that will trigger the upsert.' ) ``` (I used new feature flags.)
This hook is unnecessary, IMO. I would move the logic to `_select_on_conflict()`.
```suggestion item, fields=fields, using=self.db, ```
What if the prepared value contains unicode? (As I said before, you should use a list comprehension instead of a generator, it'll be about 2 times faster here)
Avoid using `IntegrityError` here. It is a `ValueError` to not provide `update_fields` with `update_conflicts`. ```suggestion if ignore_conflicts and update_conflicts: raise ValueError('The ignore_conflicts and update_conflicts flags are mutually exclusive.') if ignore_conflicts: self._check_on_conflicts_supported(OnConflict.IGNORE, unique_fields) return OnConflict.IGNORE if update_conflicts: self._check_on_conflicts_supported(OnConflict.UPDATE, unique_fields) if not update_fields: raise ValueError('The update_conflicts flag requires update_fields to be specified.') for name in update_fields: if name == 'pk': name = self.model._meta.pk.name try: self.model._meta.get_field(name) except exceptions.FieldDoesNotExist: raise ValueError(f'The update_fields list contains an unknown field: {name}.') return OnConflict.UPDATE return None ```
Please add a trailing comma: ```suggestion update_conflicts=False, update_fields=None, unique_fields=None, ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
> No exception for Python 3.5 but docs say name='_Person__friends' is required. It's required because it won't be cached and no exception is raised because there is no way to detect it.
Need a colon at the end here
Don't use except without an exception type. What could be the exceptions here ? It would be better to check if `get_param` returns `None`.
[`get_param`](https://github.com/spotify/pyfg/blob/master/pyFG/forticonfig.py#L249) should not raise an exception. I guess you could check if `f.running_config[path]` contains `module.params['name']ÃŒÂ€`.
You may want to consider using set operations here.
Parentheses around `e.message` are useless.
It seems like no_log and deprecation are separate things and should be handled in separate functions.
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
It's better to actually say that there's no file in place or it's inaccessible.
does this indicate that the module itself is broken? it should be indicated then that the user cannot fix the problem and that a bug should be opened.
Need a colon at the end here
It doesn't make sense to turn a unicode string into a unicode string. It's just `str(str(''))`, which is probably not what you want. Maybe you wanted to turn bytes into text here? In such case check'd be against `binary_type` and the translation should use `to_text()` shim.
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
I think it is better to pass module as argument to build_xml() and handle lxml install check-in module_utils instead of having it in each module
- try to use `refreshTokenUrl` and fallback to static URL. - break long lines when possible. - `data` should not be a dict. - extract `token` directly.
when subfield(`user`) is used multiple times, extract the value into a variable and reuse it.
surround only the part that will threw the exception.
falback to a static URL.
```suggestion for _ in range(3): ```
both are know beforehand, so there is no need to use `urljoin`.
i think it's better to treat errors in the except block.
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
Could you add at the top of Apply the following line. This will create an ems log event for users with auto support turned on. netapp_utils.ems_log_event("na_ontap_ldap", self.server)
Need a colon at the end here
what will happen if modification of `.git` file fails while `.git/` dir is already moved? I think we could try to roll it back.
I think it should be a KafkaException (or a subclass of it). IllegalStateException in this class refers to an invalid configuration that is more compile time than runtime, whereas a partitioning problem is purely runtime and only occurs with custom partitioners.
I think you might want to rollback at this point.
> wirte write
I think this might now work when there is dict of dict in response? It might break idempotency in those cases. I think you need to call it recursively when there is dict of dict or dict of list of dict.
This should probably just be done with a mutually exclusive group in the CLI argparse declaration.
option should deal with the case the dependencies are not avaiable and give a warning.
Not sure if we need `normalize_interface` when fetching interface names from running-config.
colors should all be configurable
I think these should be `is True` since the `.assertTrue` being replaced would fail if on something like `"foo"` which is truthy. So the `assert "foo"` would pass where `self.assertTrue("foo")` wouldn't.
A message string would good to say that image is not preset or something similar.
If you use a catchall exception handler like this then you should also provide the traceback like this: ``` python import traceback from ansible.module_utils._text import to_native [...] module.fail_json(msg='Unanticipated error running atomic: %s' % to_native(e), exception=traceback.format_exc()) ```
while purging all username we should preserve the username which is used to run the playbook otherwise we might hit connection timeout in middle and leave the box with partial configurations
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
small typo ```suggestion # table availability in check_src/dst. ```
ACk. Just FYI OS discovery can be done after CLICONF is implemented.
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
I would call `split()` once only. ``` entry = x.split() if len(entry) > 0: ldp = {} ldp['neighbor'] = entry[1] ldp['source'] = entry[3] ```
Please wrap at 79 chars.
Ah. lambda is prettier though :). maybe we can run pep8 with ignore lambda error.
I would use `choices` in `labels` and `values` to simplify implementation, e.g. ```python @property def labels(cls): return [label for _, label in cls.choices] @property def values(cls): return [value for value, _ in cls.choices] ```
I think we can use `in cls` instead of `in cls.__members__.values()` (here and in `names`).
Lack of data is denoted by `None` not 0.
Breaks. Read coding conventions.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
I don't see any need for this attribute.
You have some unmerged lines here
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
Can we display a similar message with the test results? It can be helpful when you have plenty of errors, .e.g. ``` ... FAILED (errors=206, skipped=1094, expected failures=4) Used shuffle seed: 556738431 (generated) ```
(And round-tripping of the messages is already tested in other tests)
I think [`_extract_m3u8_formats`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L847) does the work better. If the existing method does not fit the need, feel free to modify it.
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
``` for i, video_url in enumerate(video_urls): ```
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
I'm wondering why you decided to override this optional API. Is this impl expected to be faster than pulling the iterator and calling next in a loop? (this is what the default impl does)
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
you can use `state` to avoid the 'or' to the user
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
The mixup to me is that people using incorrect units will silently be using bytes, whereas it should report this as an error.
This means that if you'd be using 16m, it will be 16 bytes too. You will have to do a lot more to avoid mixups here.
The final command should probably return its stdout, stderr and rc back to the playbook.
That's up to you. There is no requirement to have a 1:1 mapping between the command and the Ansible interface. And I personally prefer to always have units (e.g. the AIX LV interface assumes no unit to mean MB, not MiB so...)
There is a much easier way to do this, by using the index in the list. Something like: ```python unit = size[-1] units = list('b', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y') try: multiplier = 1024**units.index(unit) except ValueError:: e = get_exception() module.fail_json(msg="No valid size unit specified. %s" % e) ```
You could make the unit mandatory (which is what I would do), or you could allow no unit (which means in bytes) and in that case you will have to consider that case as well. (i.e.if unit is a digit)
I don't think this is working correctly, did you test it for all possible inputs ? - First of all the size_unit is only correct if it is a known unit. - And if that's not true, and it's not a digit, you have to escalate. - If it is a digit, you likely won't have a unit and then you could assume bytes.
The mixup to me is that people using incorrect units will silently be using bytes, whereas it should report this as an error.
This means that if you'd be using 16m, it will be 16 bytes too. You will have to do a lot more to avoid mixups here.
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
There is a much easier way to do this, by using the index in the list. Something like: ```python unit = size[-1] units = list('b', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y') try: multiplier = 1024**units.index(unit) except ValueError:: e = get_exception() module.fail_json(msg="No valid size unit specified. %s" % e) ```
You could make the unit mandatory (which is what I would do), or you could allow no unit (which means in bytes) and in that case you will have to consider that case as well. (i.e.if unit is a digit)
That's up to you. There is no requirement to have a 1:1 mapping between the command and the Ansible interface. And I personally prefer to always have units (e.g. the AIX LV interface assumes no unit to mean MB, not MiB so...)
The final command should probably return its stdout, stderr and rc back to the playbook.
I don't think this is working correctly, did you test it for all possible inputs ? - First of all the size_unit is only correct if it is a known unit. - And if that's not true, and it's not a digit, you have to escalate. - If it is a digit, you likely won't have a unit and then you could assume bytes.
The mixup to me is that people using incorrect units will silently be using bytes, whereas it should report this as an error.
This means that if you'd be using 16m, it will be 16 bytes too. You will have to do a lot more to avoid mixups here.
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
There is a much easier way to do this, by using the index in the list. Something like: ```python unit = size[-1] units = list('b', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y') try: multiplier = 1024**units.index(unit) except ValueError:: e = get_exception() module.fail_json(msg="No valid size unit specified. %s" % e) ```
You could make the unit mandatory (which is what I would do), or you could allow no unit (which means in bytes) and in that case you will have to consider that case as well. (i.e.if unit is a digit)
That's up to you. There is no requirement to have a 1:1 mapping between the command and the Ansible interface. And I personally prefer to always have units (e.g. the AIX LV interface assumes no unit to mean MB, not MiB so...)
The final command should probably return its stdout, stderr and rc back to the playbook.
I don't think this is working correctly, did you test it for all possible inputs ? - First of all the size_unit is only correct if it is a known unit. - And if that's not true, and it's not a digit, you have to escalate. - If it is a digit, you likely won't have a unit and then you could assume bytes.
These should be handled in the arg_spec using `aliases`.
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
This logic seems ignore the use case of removing all tags.
Seems nicer to infer private_zone if vpc_id is set. But not a blocker
This need to be or'd with the existing value of changed
can be ignored
hm maybe that's what I've searched for
can be ignored
Same order, type first.
I'm not a huge fan of this, we should be able to build the result set from module.params and the response.
Does this need to be a separate method? Seems unnecessary to me.
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
Do we need to define `async_generator` in `__aiter__`? ```python async def _async_generator(self): # Generators don't actually start running until the first time you call # next() on them, so we make the generator object in the async thread # and then repeatedly dispatch to it in a sync thread. sync_generator = self.__iter__() # So, you can't raise StopIteration up through sync_to_async as # too much of the Python async system was originally built on # generators. Instead, we capture it inside a custom sync function # and change it to an exception we *can* propagate. # We should probably move this logic inside asgiref at some point. def custom_next(gen): try: return next(gen) except StopIteration: raise StopAsyncIteration() while True: try: yield await sync_to_async(custom_next)(sync_generator) except StopAsyncIteration: return # Note: __aiter__ is a *synchronous* method that has to then return an # *asynchronous* iterator/generator. Thus, we nest an async generator # inside it. # Also note: This is a generic iterable converter for now, and is going to # suffer a performance penalty on large sets of items due to the cost of # crossing over the sync barrier each time. We should write custom ones # for each Iterable subclass, but we'll need some work in Compiler first. def __aiter__(self): return self._async_generator() ```
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
Not sure it makes a difference but before it looks like we got `form=None` in the context.
Yeah we need to handle all exceptions that would be otherwise handled in `TaskExecutor` (`_execute()`, `run()`) and `Worker.run()`.
You could use lambda to save some lines defining the function
Since the same result is expected as above, wouldn't it be clearer to save the output in a variable? Something like: ``` expected = ( "[ ] author_app.0001_initial\n" "[ ] book_app.0001_initial\n" "[ ] author_app.0002_alter_id\n" "[ ] mutate_state_b.0001_initial\n" "[ ] mutate_state_b.0002_add_field\n" ) ```
Yes and no. Keeping the output explicit feels easier to read.
Swap the apps in `call_command()` but leave them sorted in the error message.
Copy / paste? I would have expected the ordering to change for the `duth1` and `duth2` arguments.
I made a few edits and squashed commits but before I push those updates I wanted to ask if this test is really needed. None of the changes seem related to verbosity so this test seems unnecessary to me.
Chop "Makes sure" prefix in favor of stating the expected behavior as described in https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/.
non existing -> nonexistent
It would help readability to use a name like "nonexistent_app" rather than "duth..".
Why's that? It's non-obvious at first glance.
Can you make this into a couple statements for easier readability? Something like: ``` invalid_entries = [x for x in statement['Principal']['AWS'] if not x.startswith('arn:aws:iam::)] if clean_invalid_entries and len(invalid_entries): for entry in invalid_entries: statement['Principal']['AWS'].remove(entry) ``` Or there's always: ``` original_size = len(statement['Principal']['AWS']) statement['Principal']['AWS'] = [x for x in statement['Principal']['AWS'] if x.startswith('arn:aws:iam::)] have_invalid_entries = (original_size == len(statement['Principal']['AWS'])) ```
Add here that the `key_alias` or `key_arn` are both ways to provide it.
colors should all be configurable
You don't need this conditional, since Ansible enforces that these are the only choices.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
Can you make this into a couple statements for easier readability? Something like: ``` invalid_entries = [x for x in statement['Principal']['AWS'] if not x.startswith('arn:aws:iam::)] if clean_invalid_entries and len(invalid_entries): for entry in invalid_entries: statement['Principal']['AWS'].remove(entry) ``` Or there's always: ``` original_size = len(statement['Principal']['AWS']) statement['Principal']['AWS'] = [x for x in statement['Principal']['AWS'] if x.startswith('arn:aws:iam::)] have_invalid_entries = (original_size == len(statement['Principal']['AWS'])) ```
Add here that the `key_alias` or `key_arn` are both ways to provide it.
colors should all be configurable
You don't need this conditional, since Ansible enforces that these are the only choices.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
Can you make this into a couple statements for easier readability? Something like: ``` invalid_entries = [x for x in statement['Principal']['AWS'] if not x.startswith('arn:aws:iam::)] if clean_invalid_entries and len(invalid_entries): for entry in invalid_entries: statement['Principal']['AWS'].remove(entry) ``` Or there's always: ``` original_size = len(statement['Principal']['AWS']) statement['Principal']['AWS'] = [x for x in statement['Principal']['AWS'] if x.startswith('arn:aws:iam::)] have_invalid_entries = (original_size == len(statement['Principal']['AWS'])) ```
Add here that the `key_alias` or `key_arn` are both ways to provide it.
colors should all be configurable
You don't need this conditional, since Ansible enforces that these are the only choices.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
Can you make this into a couple statements for easier readability? Something like: ``` invalid_entries = [x for x in statement['Principal']['AWS'] if not x.startswith('arn:aws:iam::)] if clean_invalid_entries and len(invalid_entries): for entry in invalid_entries: statement['Principal']['AWS'].remove(entry) ``` Or there's always: ``` original_size = len(statement['Principal']['AWS']) statement['Principal']['AWS'] = [x for x in statement['Principal']['AWS'] if x.startswith('arn:aws:iam::)] have_invalid_entries = (original_size == len(statement['Principal']['AWS'])) ```
Add here that the `key_alias` or `key_arn` are both ways to provide it.
colors should all be configurable
You don't need this conditional, since Ansible enforces that these are the only choices.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
you can use `state` to avoid the 'or' to the user
For long query strings, it's better to use ```query``` parameter here.
```not (foo is None)``` => ```foo is not None```
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
Any problem with: ``` @property def media(self): ```
I guess some tests might be needed for the router stuff.
Perhaps it isn't worth breaking consistency. For sure it can wait and be done separately.
The add_geometry_metadata sql, is, as far as I can see, not DDL -- and so, it can be executed by passing arguments rather than splicing them. This would require the elements of geometry_sql to be (sql, args) pairs instead of just strings.
We could lowercase the vendored files, that would help at least for the `zh-*` variants.
This will probably break for `zh-CN` and others because Django returns lower cased language names. I do not have a good idea on how to fix that though.
not a blocker, I would probably not error out here, instead you could print out a warning message, up to your decision: ~~~python module.warn(warning='Cannot change type of an existing volume.') ~~~
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
As noted in https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style, we're not so strict about it in code.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
In the case of exception print the exception e. Getting self-heal status can fail for various reasons, this error message is wrong. If the status option is wrong ansible takes care of printing it, since we have provided possible options.
This will throw an exception every time when a server is down. When glusterfsd is down the output looks like this: Brick 10.70.43.200:/mnt/engine Status: Transport endpoint is not connected Number of entries: - And you'll be trying to do int('-') which will throw ValueError. And the module throws error: fatal: [10.70.42.25]: FAILED! => {"changed": false, "msg": "Invalid heal status option."} in the function main.
Can you move this function above main() as per ansible guildelines: " Ansible follows C-style code flow where the caller functions/methods are towards the bottom of the file and the callee implementations are above them. "
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
Add here that the `key_alias` or `key_arn` are both ways to provide it.
Can you make this into a couple statements for easier readability? Something like: ``` invalid_entries = [x for x in statement['Principal']['AWS'] if not x.startswith('arn:aws:iam::)] if clean_invalid_entries and len(invalid_entries): for entry in invalid_entries: statement['Principal']['AWS'].remove(entry) ``` Or there's always: ``` original_size = len(statement['Principal']['AWS']) statement['Principal']['AWS'] = [x for x in statement['Principal']['AWS'] if x.startswith('arn:aws:iam::)] have_invalid_entries = (original_size == len(statement['Principal']['AWS'])) ```
You don't need this conditional, since Ansible enforces that these are the only choices.
Any problem with: ``` @property def media(self): ```
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
we want want -> we want
Can you re-warp this block to 79 chars? (First line is too short.)
We could lowercase the vendored files, that would help at least for the `zh-*` variants.
Just in case it's probably a good idea to use `.get()` instead of dict access syntax in case the key is unset.
Why not also do this when `module.check_mode` is `True`? Then the check mode behavior is closer to the real thing.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
okay, but would be helpful to say _why_ we need to always return True.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
IIRC get_cols() was renamed to get_group_by_cols() some time ago - no need to have both.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Why not also do this when `module.check_mode` is `True`? Then the check mode behavior is closer to the real thing.
Can you name this a little more verbosely? I can't unsee "get best"
we want want -> we want
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
These parens aren't necessary for unpacking the return values.
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
```suggestion print('hijacked sys.path to use ansible-vendored files, bwahaha', file=sys.stderr) ``` (because this log entry is added after the fact)
```suggestion print('hijacking sys.path to use ansible-vendored files, bwahaha Ã°ÂŸÂ’Â£Ã°ÂŸÂ’Â£Ã°ÂŸÂ’Â£') ```
Indent isn't really necessary here: ```suggestion if already_loaded_vendored_modules: print( 'doh, some vendored stuff was already loaded: {0}'. format(already_loaded_vendored_modules), file=sys.stderr, ) ```
I'd like to see here something like a `ResourceWarning` reported using the default Python warnings infra.
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
`mentionned` => `mentioned`
This could be moved inside the conditional that follows to avoid checking when there are no vendored modules.
I'd sort the set when showing the error message, to guarantee consistent results for the same package list.
Please rewrite as ``` if __name__ == '__main__': main() ```
Read: coding conventions, mandatory fields.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
1. `_sort_formats`. 2. Must not break if any of these keys is missing.
URLs must not be `None`.
All client calls should have exception handling. https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/amazon/GUIDELINES.md#using-fail_json_aws
This intermediate dict is completely pointless. Build formats directly.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
Do not use underscore as prefix.
Read: coding conventions, optional fields.
Similarly, ```if tc['skip'].get('i')```
yeah I think it would be worthwhile to at least test a single JOIN scenario.
Same style as above.
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
**Never ever** use `eval` on data you don't control.
Same as above, I'd revert to the simpler version. The main idea behind `decode` for me is a consistent interface where it makes sense, we don't have to bend backwards just to use it.
I'm not about the URL. **Do not touch** the global `std_headers`.
Might want to use simple quote here.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
yeah I think it would be worthwhile to at least test a single JOIN scenario.
Same style as above.
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
**Never ever** use `eval` on data you don't control.
Same as above, I'd revert to the simpler version. The main idea behind `decode` for me is a consistent interface where it makes sense, we don't have to bend backwards just to use it.
I'm not about the URL. **Do not touch** the global `std_headers`.
Might want to use simple quote here.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
like diff = load_config(self._module, config_xml, [])
`id` isn't used, it is sufficient to iterate on keys.
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
User of this inventory plugin should be able to use `v6_main_ip` for value of `ansible_host` (i don't know if it makes sens to be able to use `internal_ip`) or no `ansible_host` at all (meaning that `server['label']` will be used instead).
`Check the configuration files` seems vague, I propose: `Check inventory file and vultr configuration files`.
use a single loop? ~~~python for server in _retrieve_servers(api_key): server = Vultr.normalize_result(server, SCHEMA) .... ~~~ ~~~
`e` isn't used: remove it or use it.
When `hostname_preference` is equal to `name`, there is no need to define `ansible_host`.
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
Please ignore, my suggestion is invalid syntax.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
You could just return this dict, same below.
`del` is a builtin, not a function. These parens don't have to be here
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
1. `_sort_formats`. 2. Must not break if any of these keys is missing.
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Read: coding conventions, optional fields.
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
I don't think extending `Subquery.__init__` to allow any `QuerySet` some method (e.g. `filter`, `order_by`, ...) is desirable.
Test failure is because this is missing `geography=True` on PostGIS.
Chop "Helper function that" "Return ..." (use PEP 257 verb style)
Use hanging indent as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style). ``` values = AggregateTestModel.objects.aggregate( arrayagg=ArrayAgg('char_field', ordering=ordering) ) ```
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
Put the close ] on the next line.
`"""Return the index at which the ordering expressions start."""`
`# Filter out fields contributed....` (chop comma before as)
`del` is a builtin, not a function. These parens don't have to be here
Can you make this into a couple statements for easier readability? Something like: ``` invalid_entries = [x for x in statement['Principal']['AWS'] if not x.startswith('arn:aws:iam::)] if clean_invalid_entries and len(invalid_entries): for entry in invalid_entries: statement['Principal']['AWS'].remove(entry) ``` Or there's always: ``` original_size = len(statement['Principal']['AWS']) statement['Principal']['AWS'] = [x for x in statement['Principal']['AWS'] if x.startswith('arn:aws:iam::)] have_invalid_entries = (original_size == len(statement['Principal']['AWS'])) ```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Add here that the `key_alias` or `key_arn` are both ways to provide it.
You don't need this conditional, since Ansible enforces that these are the only choices.
If you use a catchall exception handler like this then you should also provide the traceback like this: ``` python import traceback from ansible.module_utils._text import to_native [...] module.fail_json(msg='Unanticipated error running atomic: %s' % to_native(e), exception=traceback.format_exc()) ```
ACk. Just FYI OS discovery can be done after CLICONF is implemented.
a list action or state is something we have accepted in the past, but not anymore for new modules. please create a separate facts module for this functionality.
colors should all be configurable
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
This can instead be `continue` and let the `else` unnest.
`del` is a builtin, not a function. These parens don't have to be here
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
Can you make this into a couple statements for easier readability? Something like: ``` invalid_entries = [x for x in statement['Principal']['AWS'] if not x.startswith('arn:aws:iam::)] if clean_invalid_entries and len(invalid_entries): for entry in invalid_entries: statement['Principal']['AWS'].remove(entry) ``` Or there's always: ``` original_size = len(statement['Principal']['AWS']) statement['Principal']['AWS'] = [x for x in statement['Principal']['AWS'] if x.startswith('arn:aws:iam::)] have_invalid_entries = (original_size == len(statement['Principal']['AWS'])) ```
Add here that the `key_alias` or `key_arn` are both ways to provide it.
You don't need this conditional, since Ansible enforces that these are the only choices.
ACk. Just FYI OS discovery can be done after CLICONF is implemented.
colors should all be configurable
Can you move the global declarations to one place? Easy for the future maintainer.
Can you move this function above main() as per ansible guildelines: " Ansible follows C-style code flow where the caller functions/methods are towards the bottom of the file and the callee implementations are above them. "
If you use a catchall exception handler like this then you should also provide the traceback like this: ``` python import traceback from ansible.module_utils._text import to_native [...] module.fail_json(msg='Unanticipated error running atomic: %s' % to_native(e), exception=traceback.format_exc()) ```
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
A message string would good to say that image is not preset or something similar.
not a blocker, I would probably not error out here, instead you could print out a warning message, up to your decision: ~~~python module.warn(warning='Cannot change type of an existing volume.') ~~~
You don't need this conditional, since Ansible enforces that these are the only choices.
Add here that the `key_alias` or `key_arn` are both ways to provide it.
In the case of exception print the exception e. Getting self-heal status can fail for various reasons, this error message is wrong. If the status option is wrong ansible takes care of printing it, since we have provided possible options.
Please rewrite as ``` if __name__ == '__main__': main() ```
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
Chop "Helper function that" "Return ..." (use PEP 257 verb style)
`SearchedCase` is no longer it's own type so this should be named `Case conditions must be lookups`. Alternatively, do this checking directly inside the `When` types, since users will be using these directly.
`"""Return the index at which the ordering expressions start."""`
Put the close ] on the next line.
`# Filter out fields contributed....` (chop comma before as)
Need a colon at the end here
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I don't think extending `Subquery.__init__` to allow any `QuerySet` some method (e.g. `filter`, `order_by`, ...) is desirable.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
Test failure is because this is missing `geography=True` on PostGIS.
I don't think extending `Subquery.__init__` to allow any `QuerySet` some method (e.g. `filter`, `order_by`, ...) is desirable.
`output_field` is not necessary.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
Turn (capitalize) add period. sql -> SQL
`"""Return the index at which the ordering expressions start."""`
Put the close ] on the next line.
`# Filter out fields contributed....` (chop comma before as)
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
Chop "Helper function that" "Return ..." (use PEP 257 verb style)
It seems like returning this string is the expected path through the function. So this should probably just be a return instead of raising an exception. Then the code that's calling it can decide to print it and reraise the exception that got us here if it chooses.
Okay... what about using `sys.stderr.write('%s\n' % ('\n'.join(errors)))` at the end of this function and then the calling function does sys.exit(1) afterwards? Almost the same result but it doesn't abuse the exception mechanism to make it happen :-) Probably should do something similar with the exception raised by the new connect method... maybe make the exception thrown there specific and then in this toplevel exception handler, print the exception message to stderr and sys.exit(1). The general rule is exceptions should be thrown and unhandled if there are programming errors. Error messages and sys.exit should be used when there are errors the user can correct without diving into the ansible code.
like diff = load_config(self._module, config_xml, [])
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
User of this inventory plugin should be able to use `v6_main_ip` for value of `ansible_host` (i don't know if it makes sens to be able to use `internal_ip`) or no `ansible_host` at all (meaning that `server['label']` will be used instead).
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
`Check the configuration files` seems vague, I propose: `Check inventory file and vultr configuration files`.
use a single loop? ~~~python for server in _retrieve_servers(api_key): server = Vultr.normalize_result(server, SCHEMA) .... ~~~ ~~~
When `hostname_preference` is equal to `name`, there is no need to define `ansible_host`.
this is a catchall, we normally frown on those. also note that for all of your `% e` they should be `% to_native(e)` to ensure proper character conversions
I think [`_extract_m3u8_formats`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L847) does the work better. If the existing method does not fit the need, feel free to modify it.
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
Above `tests` entries could drop the `[]` wrapping if you use `QuerySet.get` here ```suggestion self.assertEqual( queryset.values_list(lookup, flat=True).get(), expected ) ```
Similarly, ```if tc['skip'].get('i')```
Cases from lines 361 and 363 work with the previous implementation.
True, I mixed up sth when switching branches. Nevertheless they work on all databases except PostgreSQL, so we need to fix this.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
How about: ``` if storage_engine == 'InnoDB': return self.connection.mysql_version >= ( (10, 2, 2) if self.connection.mysql_is_mariadb else (5, 7, 5) ) return storage_engine in ('MyISAM', 'Aria') ```
This should be paginated
Can we emit a warning/info instead of silently failing
You can use `module.deprecate` to throw a deprecation warning.
FWIW using `monkeypatch` is preferable because it's pytest-native unless I need something like mocks or spies.
Nit: dowrade -> downgrade.
*nit* `/play.For/playbook. For /`
Given that 1.0 was released 2 years ago, I'd even go with 1.1 as the minimum version.
Do we want to support all these versions? I'd vote for only testing 1.0 and newer. 1.0 was released roughly 2 years ago.
If it doesn't add too much to the runtime, I think it would be good to include some more cases like you suggest.
Test failure is because this is missing `geography=True` on PostGIS.
This can use set comprehension ```suggestion reverse_fields = { ```
~if not keeping encryption, remove the __ansible_vault key, just assign the value directly~ nmvd, misread the if to the opposite ...
`output_field` is not necessary.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
I don't think extending `Subquery.__init__` to allow any `QuerySet` some method (e.g. `filter`, `order_by`, ...) is desirable.
Put the close ] on the next line.
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
Not sure if we need `normalize_interface` when fetching interface names from running-config.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
Chop "Helper function that" "Return ..." (use PEP 257 verb style)
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
`"""Return the index at which the ordering expressions start."""`
Put the close ] on the next line.
`# Filter out fields contributed....` (chop comma before as)
Won't this result in a confusing SQL-level error if you pass in `None` for `expression` or `pos` by accident? I'm assuming `length` is the only one we actually expect to possibly be `None`. If that's true, I think it would be better to do something like: ``` expressions = [expression, pos] if length is not None: expressions.append(length) ``` Or, if you prefer: `expressions = (expression, pos, length) if length is not None else (expression, pos)`
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Also need `m/alacarta`: ```suggestion _VALID_URL = r'https?://(?:www\.)?rtve\.es/(?P<kind>(?:playz?|(?:m/)?alacarta)/(?:audios|videos)|filmoteca)/[^/]+/[^/]+/(?P<id>\d+)' ```
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
The convention in other connection plugins seems to be to raise an `AnsibleError` instead of `IOError`.
Might want to use simple quote here.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
need to x2 check tmrow, but this should be an error, we don't allow for circular parenting
**Never ever** use `eval` on data you don't control.
In the module args, you can set certain arguments as mutually exclusive so users don't specify them together. For this module I think resource_url should be exclusive with the _name, _location, and _type options.
Use built-in method? Also, should it crash (raise) if `json.loads()` fails, or just return an empty dict again? ```suggestion try: return self._download_json( url, song_id, data=data.encode('ascii'), headers=headers) except ExtractorError as e: if type(e.cause) in (ValueError, TypeError): # JSON load failure raise except Exception: pass return {} ```
Although `operator.xor()` has the signature `(a, b)`, it might make sense to stick with `(x, y)` for consistency? ```suggestion def _sqlite_bitxor(x, y): if x is None or y is None: return None return x ^ y ```
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
When there is no `topicTitle`, `categories` will be `[None]` that does not make any sense.
Sorry, I had a typo in https://github.com/django/django/pull/12675#discussion_r418965162 -- what I ment to say was: keep `work factor` in safe summary (since it's for display and already translated) and `work_factor` in `decode`. This will also require less changes in the tests.
Should also update based on `block_size` and `parallelism`
Same as above, I'd revert to the simpler version. The main idea behind `decode` for me is a consistent interface where it makes sense, we don't have to bend backwards just to use it.
Alternatively, we can get rid of those lists by just matching on whether the test name ends with "join" (we match on table name elsewhere in this PR)
Ah, I didn't realize load takes that long, but in retrospect, it should have been obvious. Then, of course we shouldn't always load both, and my suggestion is just to match on the test name.
Would it be the acme of foolishness to suggest that we just always "load-two"? Then, we wouldn't have to maintain the extra lists of tests at the top of the file, and we wouldn't need this condition here. And also, we'd eliminate a difference among the benchmarks, since the brokers would be loaded with the same dataset in all cases (even if one of them is unused).
`self._test_scrypt_upgrade('parallelism', 'parallelism', 2)` fails, it seems we shouldn't take `parallelism` into account.
We confirmed that `parallelism` should be taken into account.
> My main worry here is: Is this correct and does it make sense to implement for such a complex hasher (notably we already have others where we argue it is simply not possible in a sensible way). > > Since scrypt can raise errors like this: > > > ValueError: Invalid parameter combination for n, r, p, maxmem. > > I am wondering if `must_update` couldn't also trigger this condition. Or can we always calculate `extra_iterations` and `extra_block` and be sure that the combinations are valid? You're right parameters may no be valid, e.g. ``` self.work_factor = 2 **14 decoded['work_factor'] = 2 ** 11 ``` both are a power of 2, however `extra_iterations = 14336` is not and raises `ValueError: n must be a power of 2`.
Should this be ``` python checked_list.sort(key=lambda x: sorted(list(x.items())) if isinstance(x, dict) else x) ``` tracebacks from test failures in CI make me suspect the view object from set(x.items()) fails because set() wants hashable items and the view object from x.items() is not. Does wrapping it in a list() or tuple() work? ``` 2017-06-26 16:43:33 The full traceback is: 2017-06-26 16:43:33 Traceback (most recent call last): 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_module_ecs_ecr.py", line 304, in run 2017-06-26 16:43:33 policy = sort_json_policy_dict(policy) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 603, in sort_json_policy_dict 2017-06-26 16:43:33 ordered_policy_dict[key] = value_is_list(value) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 595, in value_is_list 2017-06-26 16:43:33 checked_list.sort(key=lambda x: sorted(set(x.items())) if isinstance(x, dict) else x) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 595, in <lambda> 2017-06-26 16:43:33 checked_list.sort(key=lambda x: sorted(set(x.items())) if isinstance(x, dict) else x) 2017-06-26 16:43:33 TypeError: unhashable type: 'list' ```
Ah. lambda is prettier though :). maybe we can run pep8 with ignore lambda error.
This seems like it would break galaxy which needed expand_paths
Never mind, I now see the change to galaxy further down.
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
Could be nice to fix and test this typo separately.
Oh, I just meant to suggest the `value` to `values` (plural) renaming, not more...
I'd omit a blank line here.
It looks like both `__init__` method can be removed as they simply delegate to `AlterFooTogether.__init__`.
Should this be ``` python checked_list.sort(key=lambda x: sorted(list(x.items())) if isinstance(x, dict) else x) ``` tracebacks from test failures in CI make me suspect the view object from set(x.items()) fails because set() wants hashable items and the view object from x.items() is not. Does wrapping it in a list() or tuple() work? ``` 2017-06-26 16:43:33 The full traceback is: 2017-06-26 16:43:33 Traceback (most recent call last): 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_module_ecs_ecr.py", line 304, in run 2017-06-26 16:43:33 policy = sort_json_policy_dict(policy) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 603, in sort_json_policy_dict 2017-06-26 16:43:33 ordered_policy_dict[key] = value_is_list(value) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 595, in value_is_list 2017-06-26 16:43:33 checked_list.sort(key=lambda x: sorted(set(x.items())) if isinstance(x, dict) else x) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 595, in <lambda> 2017-06-26 16:43:33 checked_list.sort(key=lambda x: sorted(set(x.items())) if isinstance(x, dict) else x) 2017-06-26 16:43:33 TypeError: unhashable type: 'list' ```
Ah. lambda is prettier though :). maybe we can run pep8 with ignore lambda error.
This seems like it would break galaxy which needed expand_paths
Never mind, I now see the change to galaxy further down.
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
Could be nice to fix and test this typo separately.
Oh, I just meant to suggest the `value` to `values` (plural) renaming, not more...
I'd omit a blank line here.
It looks like both `__init__` method can be removed as they simply delegate to `AlterFooTogether.__init__`.
Should be ``self.weight``
Please don't make lines longer! There was nothing really wrong with this line before
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
(Same for the related options.)
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
Seems nicer to infer private_zone if vpc_id is set. But not a blocker
This should really be: ``` - Set custom DNS search domains. (Use I(dns_search) with C('') if you don't wish to set the search domain.) ```
Remove the two extra double-quotes here.
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
I'd recommend a more explicit import, ie ``` python from ansible.module_utils.basic import AnsibleModule' ``` And using regular imports for 're', 'shlex', 'os', 'json', 'tempfile'
We should avoid lambdas and filters most of the time. Use list comprehensions and generator expressions instead. It's both faster and more pythonic to use those idioms.
There's a few things that I'd change about this function. But I think the toplevel concern is that it's doing too much. It doesn't need to take req. It should just decide whether we're using the pycrypto or cryptography backend, format and return that one dependency. The calling code can then substitute the value.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I don't that we should do this. `BaseBackend` contains only basic methods.
... also we cannot use `User` in the `BaseBackend` so it will be hard to return something consistent.
Maybe rename this to substitute_crypto_req since it's replacing the one from requirements.txt with the one we autodetect rather than simply appending.
Nitpick, you can use a list comprehension here instead of calling list on a generator expression.
I think we should just let this exception propagate. We aren't adding any value by catching it here (and might be removing value if the traceback were useful). This isn't code in ansible itself so we don't have to create a nicer error message either.
Style note, I finally convinced people a year or so ago that ```{}``` is superior to ```dict()```. So new code should use the literal syntax.
Kind of a nitpick since this is setup.py... but API-wise I prefer returning a small dict with the dynamic values and letting the caller merge it/copy the static dict if necessary.
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
As discussed on IRC: no.
This can be single-lined: ```suggestion return (matched_tags or not tags) and not test_tags.intersection(exclude_tags) ```
I would chop `does_`: ```suggestion if test_match_tags(test, tags, exclude_tags) ```
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Sorry, I know it was in my changes, but I forgot to remove that lineÃ¢Â€Â¦ Changing the data seems a bit ugly here, could you remove this line and changing the next one by: ``` prefix = ipaddress.ip_network(data["prefix"] + "/" + data["prefix_length"]) ```
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
Appreciate what you've done with the tests but I think they're harder to comprehend now. As a Django dev I can jump in and read model classes, but the helpers are obscuring things a bit to me. Also you now have multiple tests per test method now - ideally there should just be a handful of classes per test and some assertions. I guess you can split based on overriding, non-overriding, etc.
I'm pretty sure you can make the abstract bases once in the test case class level: ``` class ...Tests(...): class SomeBase(models.Model): ... ``` Then reuse them appropriately in the individual test methods, which should only need to create the "bottom" classes? I grepped for `\btype\(` and found a few uses, but none are for "quick" building. I personally think the "tonnes of vertical scrolling" is a small concern compared to having to grok how `type()` works.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Use single quotes consistently.
I think `name.rsplit('-', 1)[-1]` is easier to read.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
Any problem with: ``` @property def media(self): ```
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
does not match docs
To me it looks the module does handle check mode (https://github.com/ansible/ansible/pull/20734/files#diff-672a20e0686da08f7554286ee4283346R423). So in my opinion `supports_check_mode` an be set to true
```suggestion Test that the returned value for timezone consists of only uppercase ```
I think `name.rsplit('-', 1)[-1]` is easier to read.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Any problem with: ``` @property def media(self): ```
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
Use another lookup instead of `epoch` e.g. `second`.
does not match docs
catchall exception, does not give us good idea of exact action that was being attempted on the module's side, unless the underlying API is very friendly/precise we normally prefer to do specific exception catching closer to the source of the exception so we can tell module user what was being attempted i.e `failed to change name of schedule: %s' % e`
To me it looks the module does handle check mode (https://github.com/ansible/ansible/pull/20734/files#diff-672a20e0686da08f7554286ee4283346R423). So in my opinion `supports_check_mode` an be set to true
ending module execution in an exception is a bug use fail_json, if you want to pass back traceback information use the 'exception' key in the returned json
I prefer putting the closing ) on the next line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
I think `enumerate` would work here
missing ini option for host
You can achieve this same result output with `module.exit_json(changed=True,**camel_dict_to_snake_dict(execution))` which can be imported from `ansible.module_utils.ec2`. That will automatically cover if AWS ever starts returning additional keys from this API and is generally easier to maintain.
```suggestion module.exit_json(changed=True,**camel_dict_to_snake_dict(res))) ```
I haven't actually used stepfunctions myself so I'm just going off the docs. AIUI, `ExecutionAlreadyExists` would only be returned on a running execution if the `execution_input` is different, not in a case where the input is the same. As an ansible user who is new to step functions, I feel like if I gave the same input and ran the playbook twice I would expect the 2nd time to be `changed=false` if the first execution is still running. But that doesn't mean that's how AWS users familiar with step functions will expect it.
`client.start_execution` is idempotent but `start_execution` always returns `changed=True` if `action=start`. We probably want to `client.list_executions` to see if the execution is already running before proceeding.
this seems like it should be shared with the main one rather than duplicated here
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
Please ignore, my suggestion is invalid syntax.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
I think `enumerate` would work here
This can instead be `continue` and let the `else` unnest.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
`del` is a builtin, not a function. These parens don't have to be here
prefer hanging indent style (allows longer lines, indentation always multiple of 4): ``` rasterheader = ( 1, 0, ... ) ```
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
Ah thanks, I checked the implementation again, it verifies this here: https://github.com/ansible/ansible/blob/f2dccb90e893df30c2b8bfc925dba80f6ceed6a7/lib/ansible/module_utils/basic.py#L1376
This gets called in check mode too and might change attributes on disk
No, `set_fs_attributes_if_different` respects `module.check`.
You need to make sure `tf` doesn't write a unicode string. tf.write(u("#!/bin/sh/\necho %s" % quote(self.passphrase)).encode("utf-8")) Ansible may have a utility function to do this.
In the "Generate privatekey6 - standard - with non-ASCII passphrase" test, this fails with `'ascii' codec can't decode byte 0xc3 in position 16: ordinal not in range(128)`. Stacktrace: ``` File "/tmp/ansible_openssh_keypair_payload_kCnMbD/__main__.py", line 193, in generate tf.write("#!/bin/sh\necho %s" % quote(self.passphrase)) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 706, in write return self.writer.write(data) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 369, in write data, consumed = self.encode(object, self.errors) ``` It seems to fail in all Python 2 tests; in Python 3 it seems to work.
You should use `module.add_cleanup_file()` (somewhere above) instead of manually trying to clean up.
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
I think [`_extract_m3u8_formats`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L847) does the work better. If the existing method does not fit the need, feel free to modify it.
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
You have some unmerged lines here
Minor but the `'%%Y-%%m-01 00:00:00'` could likely by appended to params as well.
I don't think boto_params will include access_key or secret_key if a profile is being used (which might be through passing `profile` parameter or with `AWS_PROFILE` set) Looking at https://github.com/jmenga/requests-aws-sign the way to do it is to use ``` session = session.Session() credentials = session.get_credentials() ``` but even there, we'd need to cope with the `profile` parameter.
not really wrong since we do not require things to be reproducible in that case, but I'd rather like to use context.reader().maxDoc() instead of context.docBase so that matches only depend on the current segment
``` for i, video_url in enumerate(video_urls): ```
I'm wondering why you decided to override this optional API. Is this impl expected to be faster than pulling the iterator and calling next in a loop? (this is what the default impl does)
See RandomScoreFunction (it does not depend on the order of visiting docs, and just substitute using the uid field with either docid or a user supplied field).
What is this supposed to achieve? Modules are cached on import, they'll be picked up from that cache regardless of the pointers being listed in `sys.modules`.
This check can be also moved to `module_utils`.
Is anything `required_together`, if not please remove this line
str? ```suggestion size=dict(type='str') ```
```python if state == 'present': if not volume: create_volume(module, array) elif volume: update_volume(module, array, volume) elif state == 'absent' and volume: delete_volume(module, array, volume) else: module.exit_json(changed=False) ```
`delete_volume` does not require size. This check will force user to specify size even if he/she is deleting the volume. I feel this is not intuitive UI/UX.
Unless there's a significant difference, a single RTL language seems fine... ðŸ¤”
This code is almost the same as in `replace_unnamed_groups()`, the only difference is that the beginning of non-capturing group is longer i.e. `'(?:'` instead of `'('`. We could add an internal hook and use it in both places, e.g. ```python def _find_groups(pattern, group_matcher): group_indices = [ (m.start(0), m.end()) for m in non_capturing_group_matcher.finditer(pattern) ] # Loop over the groups. for start, end in unnamed_group_indices: ... for idx, val in enumerate(pattern[end:]): ... if unmatched_open_brackets == 0: group_indices.append((start, end + idx + 1)) break # Remove unnamed group matches inside other unnamed capture groups. group_start_end_indices = [] prev_end = None for start, end in group_indices: if prev_end and start > prev_end or not prev_end: group_start_end_indices.append((start, end)) prev_end = end return group_start_end_indices ``` Moreover, with some boolean flags (e.g. `named=True/False`) this could also be reused in `replace_named_groups()` :thinking: .
+10k to what @sivel states above
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I don't see a need for string interpolation in cases like this.
```suggestion module.fail_json(msg="The following packages are absent in the currently booted rpm-ostree commit: %s" % ' '.join(pkgs)) ```
```suggestion type: list suboptions: ```
pass original exception as orig_exc so traceback can be shown with -vvv
not sure this debug is useful
Since it's the only plugin which does that, I would remove it. Either all plugins should do that, or none.
I'd recommend a more explicit import, ie ``` python from ansible.module_utils.basic import AnsibleModule' ``` And using regular imports for 're', 'shlex', 'os', 'json', 'tempfile'
This 'default_chains' doesn't seem to be used anywhere.
ipt_load_stderr doesnt appear to be defined anywhere.
Please use shorter name (less than 30 chars), e.g. `deferrable_pink_constraint`.
The hexdigest will always be a fixed length so this only happens if the provided suffix is too long, correct? In that case, I think it would be better to raise an error that the provided suffix is too long.
I don't see a need for string interpolation in cases like this.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
```suggestion module.fail_json(msg="The following packages are absent in the currently booted rpm-ostree commit: %s" % ' '.join(pkgs)) ```
```suggestion type: list suboptions: ```
Please don't make lines longer! There was nothing really wrong with this line before
This 'default_chains' doesn't seem to be used anywhere.
ipt_load_stderr doesnt appear to be defined anywhere.
I'd recommend a more explicit import, ie ``` python from ansible.module_utils.basic import AnsibleModule' ``` And using regular imports for 're', 'shlex', 'os', 'json', 'tempfile'
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I don't see a need for string interpolation in cases like this.
```suggestion type: list suboptions: ```
```suggestion module.fail_json(msg="The following packages are absent in the currently booted rpm-ostree commit: %s" % ' '.join(pkgs)) ```
pass original exception as orig_exc so traceback can be shown with -vvv
not sure this debug is useful
Since it's the only plugin which does that, I would remove it. Either all plugins should do that, or none.
I'd recommend a more explicit import, ie ``` python from ansible.module_utils.basic import AnsibleModule' ``` And using regular imports for 're', 'shlex', 'os', 'json', 'tempfile'
This 'default_chains' doesn't seem to be used anywhere.
ipt_load_stderr doesnt appear to be defined anywhere.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
Please don't make lines longer! There was nothing really wrong with this line before
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
[`get_param`](https://github.com/spotify/pyfg/blob/master/pyFG/forticonfig.py#L249) should not raise an exception. I guess you could check if `f.running_config[path]` contains `module.params['name']ÃŒÂ€`.
Don't use except without an exception type. What could be the exceptions here ? It would be better to check if `get_param` returns `None`.
You may want to consider using set operations here.
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
Parentheses around `e.message` are useless.
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
```suggestion raise AnsibleParserError("the field '%s' should be a list, but is a %s" % (value, type(value))) ```
[`get_param`](https://github.com/spotify/pyfg/blob/master/pyFG/forticonfig.py#L249) should not raise an exception. I guess you could check if `f.running_config[path]` contains `module.params['name']ÃŒÂ€`.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
Parentheses around `e.message` are useless.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
pass original exception as orig_exc so traceback can be shown with -vvv
Since it's the only plugin which does that, I would remove it. Either all plugins should do that, or none.
not sure this debug is useful
I'd recommend a more explicit import, ie ``` python from ansible.module_utils.basic import AnsibleModule' ``` And using regular imports for 're', 'shlex', 'os', 'json', 'tempfile'
Please use shorter name (less than 30 chars), e.g. `deferrable_pink_constraint`.
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
Please don't make lines longer! There was nothing really wrong with this line before
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
not sure this debug is useful
Since it's the only plugin which does that, I would remove it. Either all plugins should do that, or none.
This 'default_chains' doesn't seem to be used anywhere.
ipt_load_stderr doesnt appear to be defined anywhere.
I'd recommend a more explicit import, ie ``` python from ansible.module_utils.basic import AnsibleModule' ``` And using regular imports for 're', 'shlex', 'os', 'json', 'tempfile'
Minor but I'd move this control flow block after the `weights` one to match the args order.
Hmm, doesn't seem like this is correct. >version ['1', '0', '0-SNAPSHOT'] major_minor ['1', '0'] Extracting ['tar', 'xf', '/Users/ijuma/src/kafka/core/build/distributions/kafka_2.11-1.0.0-SNAPSHOT-site-docs.tgz', '--strip-components', '1'] Traceback (most recent call last): File "./release.py", line 235, in <module> command_stage_docs() File "./release.py", line 227, in command_stage_docs cmd('Extracting ', 'tar xf %s --strip-components 1' % docs_tar, cwd=os.path.join(kafka_site_repo_path, docs_version(version))) File "./release.py", line 108, in cmd output = subprocess.check_output(cmd, *args, stderr=subprocess.STDOUT, **kwargs) File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py", line 566, in check_output process = Popen(stdout=PIPE, *popenargs, **kwargs) File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py", line 710, in __init__ errread, errwrite) File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py", line 1335, in _execute_child raise child_exception OSError: [Errno 2] No such file or directory: '/Users/ijuma/src/kafka/../kafka-site/10'
Also, it seems like our convention for docs versions is quite bad. If we didn't want to use `.`, we could use something else like `_`.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
Please remove deprecated parameters as this is new module.
Minor but I'd move this control flow block after the `weights` one to match the args order.
There is no point to use `remove_start` since line is always a string.
>besides the test is there to make sure that breakage in this part of code will be detected That's a doubtful argument considering broken core tests at the beginning of this PR. The length of this string is const until one decides to refactor here something. Using 10 is a variation of code duplication since the length is already implicitly defined in the string literal itself. Also using 10 indicates no relation to the string literal so that one unfamiliar with code who decides to refactor it may forgot to change the number and may be unaware of the tests at all.
This is error prone, `len` of the actual string should be used instead.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Any problem with: ``` @property def media(self): ```
I think `name.rsplit('-', 1)[-1]` is easier to read.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Need a colon at the end here
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
I think `name.rsplit('-', 1)[-1]` is easier to read.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Any problem with: ``` @property def media(self): ```
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
It would have been more appropriate if `run_checks()` took an `Apps` instance as a parameter instead of a list of `AppConfig`s.
Can you use `assertContains()` here to prevent errors due to attribute order in the HTML tags.
Same here, can you use `assertContains()` here to prevent errors due to attribute order in the HTML tags.
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Any problem with: ``` @property def media(self): ```
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Need a colon at the end here
You probably should just exit here with `changed=False`
having `main()` call `run_module()` feels a bit redundant, I see no reason to not put all of the main logic in `main` (and splitting actions into functions where it makes sense)
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
the `and retries >= CONFIRM_UPDATE_MAX_RETRY` is redundant here. If the execution got here, it'll always be `True`
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Any problem with: ``` @property def media(self): ```
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Need a colon at the end here
You probably should just exit here with `changed=False`
having `main()` call `run_module()` feels a bit redundant, I see no reason to not put all of the main logic in `main` (and splitting actions into functions where it makes sense)
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
the `and retries >= CONFIRM_UPDATE_MAX_RETRY` is redundant here. If the execution got here, it'll always be `True`
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Any problem with: ``` @property def media(self): ```
move to finally
Need a colon at the end here
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I think this should be false (not a string)
No need to specify `choices`
happy new year ! yeah effectively finally this is required for python < 3.5 as you explain (I just tested it in an interactive session). I'm happy to open a PR with this specific change so.. should not be too long to come..
hmmm.. now I think I'm wrong, you want to only have the first partial func.. `func.func` shouldn't anyway be another partial or else that would be problematic I think, as views have to return something else.. Sorry for disturbance ;)
didn't knew about RegexURLPattern having the same method.. reading the docstring I'm thinking both should do this: ``` python # .. if isinstance(func, functools.partial): while isinstance(func, functools.partial): func = func.func # ... ``` shouldn't it ? again that's my wild guess here, I don't have read more about the context of how this method is used (nor the other one), etc.. So I could be simply wrong.
1: I'm definitely happy with this. I'm not sure why I didn't go for it myself. 2: Fine by me, I don't feel strongly.
Good catch, I will remove it before final squash.
Getting an error about an index when creating a constraint is confusing unless you understand the implementation details.
Thanks for these tests, they look great!
supporting snapmirror-label would affect this logic, are only one of the two could be present, or both of them.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Could use `bulk_create` ```suggestion Product.objects.bulk_create([ Product(name='p1'), Product(name='p1', color='red'), ]) ```
happy new year ! yeah effectively finally this is required for python < 3.5 as you explain (I just tested it in an interactive session). I'm happy to open a PR with this specific change so.. should not be too long to come..
hmmm.. now I think I'm wrong, you want to only have the first partial func.. `func.func` shouldn't anyway be another partial or else that would be problematic I think, as views have to return something else.. Sorry for disturbance ;)
didn't knew about RegexURLPattern having the same method.. reading the docstring I'm thinking both should do this: ``` python # .. if isinstance(func, functools.partial): while isinstance(func, functools.partial): func = func.func # ... ``` shouldn't it ? again that's my wild guess here, I don't have read more about the context of how this method is used (nor the other one), etc.. So I could be simply wrong.
1: I'm definitely happy with this. I'm not sure why I didn't go for it myself. 2: Fine by me, I don't feel strongly.
Good catch, I will remove it before final squash.
Getting an error about an index when creating a constraint is confusing unless you understand the implementation details.
Thanks for these tests, they look great!
supporting snapmirror-label would affect this logic, are only one of the two could be present, or both of them.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Could use `bulk_create` ```suggestion Product.objects.bulk_create([ Product(name='p1'), Product(name='p1', color='red'), ]) ```
happy new year ! yeah effectively finally this is required for python < 3.5 as you explain (I just tested it in an interactive session). I'm happy to open a PR with this specific change so.. should not be too long to come..
hmmm.. now I think I'm wrong, you want to only have the first partial func.. `func.func` shouldn't anyway be another partial or else that would be problematic I think, as views have to return something else.. Sorry for disturbance ;)
didn't knew about RegexURLPattern having the same method.. reading the docstring I'm thinking both should do this: ``` python # .. if isinstance(func, functools.partial): while isinstance(func, functools.partial): func = func.func # ... ``` shouldn't it ? again that's my wild guess here, I don't have read more about the context of how this method is used (nor the other one), etc.. So I could be simply wrong.
1: I'm definitely happy with this. I'm not sure why I didn't go for it myself. 2: Fine by me, I don't feel strongly.
Good catch, I will remove it before final squash.
Getting an error about an index when creating a constraint is confusing unless you understand the implementation details.
Thanks for these tests, they look great!
supporting snapmirror-label would affect this logic, are only one of the two could be present, or both of them.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Could use `bulk_create` ```suggestion Product.objects.bulk_create([ Product(name='p1'), Product(name='p1', color='red'), ]) ```
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
Chop "Helper function that" "Return ..." (use PEP 257 verb style)
Put the close ] on the next line.
`"""Return the index at which the ordering expressions start."""`
Need a colon at the end here
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
like diff = load_config(self._module, config_xml, [])
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
User of this inventory plugin should be able to use `v6_main_ip` for value of `ansible_host` (i don't know if it makes sens to be able to use `internal_ip`) or no `ansible_host` at all (meaning that `server['label']` will be used instead).
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
use a single loop? ~~~python for server in _retrieve_servers(api_key): server = Vultr.normalize_result(server, SCHEMA) .... ~~~ ~~~
Yeah, we still can't use dict comprehensions until 2.6 is formally dropped, sorry.
`Check the configuration files` seems vague, I propose: `Check inventory file and vultr configuration files`.
When `hostname_preference` is equal to `name`, there is no need to define `ansible_host`.
```suggestion vault_data(), ```
```suggestion assert ansible_json_encoder.default(test_input) == {'__ansible_vault': expected} ```
Chop "Helper function that" "Return ..." (use PEP 257 verb style)
Same as above: unnecessary fixture test.
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
```suggestion ({b'str'.encode(DEFAULT_ENCODING): b'str'.encode(DEFAULT_ENCODING)}, {u'str': u'str'}), ```
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Having a class for just one test method is unnecessary.
If this test won't be implemented it should be removed.
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
This one is a bit newer to CliBase, but also implemented verbatim in superclass
disconnect() is likewise handled in CliBase, and should be entirely unnecessary here.
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
self._connected is set by CliBase.connect(), shouldn't need to specify it here
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
This one is a bit newer to CliBase, but also implemented verbatim in superclass
disconnect() is likewise handled in CliBase, and should be entirely unnecessary here.
self._connected is set by CliBase.connect(), shouldn't need to specify it here
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
we want want -> we want
Need a colon at the end here
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
like diff = load_config(self._module, config_xml, [])
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
I didn't notice this before but you have eval here... What is that for? It's rather dangerous as it means if someone can get code into the config file for the foreman dynamic inventory, they can then execute whatever code they want as the user running ansible. We should get rid of this if at all possible.
All these methods can be clubbed into a single method that takes data and pattern string as arguments and returns the match else None
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
`Check the configuration files` seems vague, I propose: `Check inventory file and vultr configuration files`.
This one is a bit newer to CliBase, but also implemented verbatim in superclass
disconnect() is likewise handled in CliBase, and should be entirely unnecessary here.
self._connected is set by CliBase.connect(), shouldn't need to specify it here
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
`del` is a builtin, not a function. These parens don't have to be here
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
This one is a bit newer to CliBase, but also implemented verbatim in superclass
disconnect() is likewise handled in CliBase, and should be entirely unnecessary here.
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
self._connected is set by CliBase.connect(), shouldn't need to specify it here
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
`del` is a builtin, not a function. These parens don't have to be here
```suggestion assert ansible_json_encoder.default(test_input) == {'__ansible_vault': expected} ```
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
`del` is a builtin, not a function. These parens don't have to be here
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
Having a class for just one test method is unnecessary.
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
```suggestion assert ansible_json_encoder.default(test_input) == {'__ansible_vault': expected} ```
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
`del` is a builtin, not a function. These parens don't have to be here
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
Having a class for just one test method is unnecessary.
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
This one is a bit newer to CliBase, but also implemented verbatim in superclass
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
disconnect() is likewise handled in CliBase, and should be entirely unnecessary here.
self._connected is set by CliBase.connect(), shouldn't need to specify it here
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Need a colon at the end here
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
like diff = load_config(self._module, config_xml, [])
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
self._connected is set by CliBase.connect(), shouldn't need to specify it here
disconnect() is likewise handled in CliBase, and should be entirely unnecessary here.
This one is a bit newer to CliBase, but also implemented verbatim in superclass
*nit* `/play.For/playbook. For /`
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
If the minimum value is `1.17` as per module docs, shouldn't this be a float? ```suggestion video_memory_mb=dict(type='float'), ```
Ah, i'd expect a bool to always be True/False, if that is not the case (no time to test really right now :)), then indeed, this needs to stay (and this goes for my other bool remarks)
```suggestion module.fail_json(msg='Unable to find the specified virtual machine : %s' % (module.params.get('uuid') or module.params.get('name'))) ```
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
This one is a bit newer to CliBase, but also implemented verbatim in superclass
disconnect() is likewise handled in CliBase, and should be entirely unnecessary here.
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
self._connected is set by CliBase.connect(), shouldn't need to specify it here
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
`del` is a builtin, not a function. These parens don't have to be here
IIRC get_cols() was renamed to get_group_by_cols() some time ago - no need to have both.
No need to split the line.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
This whole block should be changed into: ```python module = AnsibleModule( argument_spec=dict( name=dict(type='str', required=True), remote=dict(type='str'), state=dict(type='str', default="present", choices=['absent', 'present']) ), supports_check_mode=True, ) ``` Assuming that **name** is always required, and **repo** is required when **state == 'present'**. - Put the type-option first, as this one is usually always added - Lines in Ansible can be 160 chars wide, so there's usually no need to split it a line
What if the flatpak name contained the string 'error' ? Seems a recipe for disaster.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
okay, but would be helpful to say _why_ we need to always return True.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Should have been module.fail_json
No need to split the line.
Various modules allow to specify the name of the executable in order to use a binary from a non-standard location (i.e. a binary not located in the PATH). Look at e.g. the isoextract on how to do this.
Also here, I don't know why you are splitting lines. It does not make it easier to read. Usually it is done out of necessity (e.g. PEP8 rules require it), but in this case there is no need to make this weird.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
This whole block should be changed into: ```python module = AnsibleModule( argument_spec=dict( name=dict(type='str', required=True), remote=dict(type='str'), state=dict(type='str', default="present", choices=['absent', 'present']) ), supports_check_mode=True, ) ``` Assuming that **name** is always required, and **repo** is required when **state == 'present'**. - Put the type-option first, as this one is usually always added - Lines in Ansible can be 160 chars wide, so there's usually no need to split it a line
Not sure if this piece is doing exactly what you expect in all cases.
What if the flatpak name contained the string 'error' ? Seems a recipe for disaster.
Since we expect the name to start with 'http://' or 'https://' I would so this: ```python if name.startswith('http://') or name.startswith('https://'): ... ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Should have been module.fail_json
Any problem with: ``` @property def media(self): ```
No need to split the line.
Various modules allow to specify the name of the executable in order to use a binary from a non-standard location (i.e. a binary not located in the PATH). Look at e.g. the isoextract on how to do this.
Also here, I don't know why you are splitting lines. It does not make it easier to read. Usually it is done out of necessity (e.g. PEP8 rules require it), but in this case there is no need to make this weird.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
This whole block should be changed into: ```python module = AnsibleModule( argument_spec=dict( name=dict(type='str', required=True), remote=dict(type='str'), state=dict(type='str', default="present", choices=['absent', 'present']) ), supports_check_mode=True, ) ``` Assuming that **name** is always required, and **repo** is required when **state == 'present'**. - Put the type-option first, as this one is usually always added - Lines in Ansible can be 160 chars wide, so there's usually no need to split it a line
Since we expect the name to start with 'http://' or 'https://' I would so this: ```python if name.startswith('http://') or name.startswith('https://'): ... ```
What if the flatpak name contained the string 'error' ? Seems a recipe for disaster.
okay, but would be helpful to say _why_ we need to always return True.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
No need to split the line.
Various modules allow to specify the name of the executable in order to use a binary from a non-standard location (i.e. a binary not located in the PATH). Look at e.g. the isoextract on how to do this.
Also here, I don't know why you are splitting lines. It does not make it easier to read. Usually it is done out of necessity (e.g. PEP8 rules require it), but in this case there is no need to make this weird.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
This whole block should be changed into: ```python module = AnsibleModule( argument_spec=dict( name=dict(type='str', required=True), remote=dict(type='str'), state=dict(type='str', default="present", choices=['absent', 'present']) ), supports_check_mode=True, ) ``` Assuming that **name** is always required, and **repo** is required when **state == 'present'**. - Put the type-option first, as this one is usually always added - Lines in Ansible can be 160 chars wide, so there's usually no need to split it a line
What if the flatpak name contained the string 'error' ? Seems a recipe for disaster.
Since we expect the name to start with 'http://' or 'https://' I would so this: ```python if name.startswith('http://') or name.startswith('https://'): ... ```
okay, but would be helpful to say _why_ we need to always return True.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Should have been module.fail_json
If changing to `None` from `''` in `.slice_expression()` above, then: ```suggestion if self.end is None: return f'{lhs}[%s:]', params + [self.start] else: return f'{lhs}[%s:%s]', params + [self.start, self.end] ```
this class has a bunch of details of WSGI environ so I think it more accurately belongs in `django.http.request`
You don't need the inner list comprehension: `tuple(i for i in x)` works just fine.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Do we need this check? All tests pass without it.
It's hard to assess the correctness of this change through visual inspection. I'll trust you (and the test suite) on this.
`form_class` is defined in `RangeField.formfield()` so this is redundant.
Use another lookup instead of `epoch` e.g. `second`.
this is not a 1.0 callback, its using 2.0 API
Please use a single quote.
Thanks for the patch @Giebisch, I think we should adjust the `expected_outputs` instead to be properly formatted in order to ensure formatting is actually used. https://github.com/django/django/blob/78657a201308a0b53661e2b267914407a9fd7fa1/tests/test_runner/test_debug_sql.py#L90-L111 https://github.com/django/django/blob/78657a201308a0b53661e2b267914407a9fd7fa1/tests/test_runner/test_debug_sql.py#L124-L133 The adjustments made make the tests pass with and without the proposed changes.
Also probably put that in the examples, for if users want that behavior but maybe don't know about `failed_when`.
I'm thinking these parameters shouldn't be filled in, since we're popping `stack_description` so if a user wanted a hard-fail for nonexistent stacks, they could do: ``` - cloudformation_facts: stack_name: not-real failed_when: not cloudformation['not-real'] ```
`test_jsonfield` looks good to me (since this is InspectDBTests, _introspection seems redundant).
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
You need to make sure `tf` doesn't write a unicode string. tf.write(u("#!/bin/sh/\necho %s" % quote(self.passphrase)).encode("utf-8")) Ansible may have a utility function to do this.
In the "Generate privatekey6 - standard - with non-ASCII passphrase" test, this fails with `'ascii' codec can't decode byte 0xc3 in position 16: ordinal not in range(128)`. Stacktrace: ``` File "/tmp/ansible_openssh_keypair_payload_kCnMbD/__main__.py", line 193, in generate tf.write("#!/bin/sh\necho %s" % quote(self.passphrase)) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 706, in write return self.writer.write(data) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 369, in write data, consumed = self.encode(object, self.errors) ``` It seems to fail in all Python 2 tests; in Python 3 it seems to work.
camel2snake should indeed handle NotificationARNs properly (#25105)
Ooh, this is a nasty bug - if this wasn't in this PR,I wouldn't have spotted this. If this code happens elsewhere, it'll break when 2.4 goes out (because a now valid key is overwritten by the content of a now missing key)
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
This is a "set" method called from redfish_config (which doesn't pass in the systems_uri param). So need to remove that param here and just use self.system_uris[0] below.
```suggestion assert expected == "exception" ```
```suggestion assert expected == "exception" ```
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
Should this be `type='bool'`, or is it more complex? If it's more complex then it might be worth expanding the description for this option.
You can achieve this same result output with `module.exit_json(changed=True,**camel_dict_to_snake_dict(execution))` which can be imported from `ansible.module_utils.ec2`. That will automatically cover if AWS ever starts returning additional keys from this API and is generally easier to maintain.
`msg="'%s' device group not found in Panorama. Is the name correct?" % devicegroup)`
```suggestion module.exit_json(changed=True,**camel_dict_to_snake_dict(res))) ```
I would add here fetch_nested=True, because we always want to return steps.
change 'type(job).__name__.lower()' to just 'job' no need to do it generic here
We confirmed that `parallelism` should be taken into account.
`self._test_scrypt_upgrade('parallelism', 'parallelism', 2)` fails, it seems we shouldn't take `parallelism` into account.
Matching empty string is senseless.
```suggestion assert expected == "exception" ```
Should also update based on `block_size` and `parallelism`
Subtests can be used here (with pairs `(key, expected)`).
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
```suggestion assert expected == "exception" ``` If `expected` is `False`, an exception should not be accepted.
Should this be `type='bool'`, or is it more complex? If it's more complex then it might be worth expanding the description for this option.
Same as above, I'd revert to the simpler version. The main idea behind `decode` for me is a consistent interface where it makes sense, we don't have to bend backwards just to use it.
The following indentation should pass pep8 (with the `==` starting under the `r` in `assert`): ``` assert constants.get_config(cfgparser, 'defaults', 'unknown', 'ANSIBLE_TEST_VAR', '~/local', value_type='path', expand_relative_paths=False) \ == os.path.join(user['home'], 'local') ```
I think so, btw please do `resolver.kwargs.copy()` to leave the original kwargs in place on the resolver object.
do we really need that `is not None` check? `check_token` should return `False` for None tokens.
please reuse the code, it's very same
So if I update some parameter+ change state to running, it won't start, IIUC
Title is mandatory.
It's usually better to use raw-strings for regexps: ```suggestion assert re.match(r'ansible [0-9.a-z]+ .*$', version_lines[0]), 'Incorrect ansible version line in "ansible --version" output' ``` (I'm pretty sure Python 3.6+ will emit warnings if you don't)
FWIW using `monkeypatch` is preferable because it's pytest-native unless I need something like mocks or spies.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Title is mandatory.
you could also write this as: h_item = search_obj_in_list(w_item['member'], h, key='member') or {}
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
Looks like this could be a single line.
the shell itself would have done it before. but might have done it slightly differently.
I don't think this is safe. If someone is calling this function without unsafe_shell they probably have not quoted the arguments to prevent this sort of expansion.
Okay, looking further down, I see that you're just moving this around though...
I would use `choices` in `labels` and `values` to simplify implementation, e.g. ```python @property def labels(cls): return [label for _, label in cls.choices] @property def values(cls): return [value for value, _ in cls.choices] ```
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
while purging all username we should preserve the username which is used to run the playbook otherwise we might hit connection timeout in middle and leave the box with partial configurations
I think this could use a generator expression instead of list comprehension to avoid additional temporary lists in memory. For example: ``` pairs = zip((chars.index(x) for x in nonce), (chars.index(x) for x in pad)) ``` (Same in `_unpad_cipher_token()`)
to be -> are
Is that check actually needed? This seems to be only needed if a token is `None` -- which would not happen if we return a token from `sanitize_token` instead of None :D
I would expect some explanation on why `quote_name` is used for some names and `geo_quote_name` for others.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
put the closing parenthesis on the next line
`%(expressions)s)` not `%(expression)s)`. You're missing the `s` at the end of `expressions`
Remove `self.function = 'CONCAT_WS'` and the following line which mutates `self.template` and instead: ``` return super(ConcatPair, self).as_sql( compiler, connection, function='CONCAT_WS', template="%(function)s('', %(expression)s)" ```
Is this check correct? A `Coalesce` can still result in a null value (if all of its arguments are null), so even if the expression is already a `Coalesce` ISTM it needs to be wrapped again (or have `Value('')` added to the end of its `source_expressions`, but just wrapping in another `Coalesce` seems cleaner).
to be -> are
If I'm not completely mistaken this will fail if the attacker requests a page where `csrfmiddlewaretoken` or `HTTP_X_CSRFTOKEN` is shorter than the `CSRF_KEY_LENGTH`.
Ah! Of course, sorry I missed that.
Is that check actually needed? This seems to be only needed if a token is `None` -- which would not happen if we return a token from `sanitize_token` instead of None :D
Won't this result in a confusing SQL-level error if you pass in `None` for `expression` or `pos` by accident? I'm assuming `length` is the only one we actually expect to possibly be `None`. If that's true, I think it would be better to do something like: ``` expressions = [expression, pos] if length is not None: expressions.append(length) ``` Or, if you prefer: `expressions = (expression, pos, length) if length is not None else (expression, pos)`
I don't really get this, why not just do. ```suggestion auth_str = "Signature" ```
Turn (capitalize) add period. sql -> SQL
Is that check actually needed? This seems to be only needed if a token is `None` -- which would not happen if we return a token from `sanitize_token` instead of None :D
`# Filter out fields contributed....` (chop comma before as)
We can register model without defining `OfficeAdminWithOrdering` with the same effect: ```python site.register(Office) ```
to be -> are
This class is unnecessary.
`"""Return the index at which the ordering expressions start."""`
Put the close ] on the next line.
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
Chop "Helper function that" "Return ..." (use PEP 257 verb style)
Dict literals are preferred (0d74c41981687598d3fa0a7eb9712ce4c387ca19).
This should be handled by the fields themselves, what you've written here is what a special cased `JSONFieldGinIndex.create_sql` implementation would look like.
There is no need to provide a value for `max_pages_num` - I think it should be calculated automatically: ```python (self.on_each_side + self.on_ends) * 2 ``` Otherwise developers can provide values that are incompatible with each other...
Remove `self.function = 'CONCAT_WS'` and the following line which mutates `self.template` and instead: ``` return super(ConcatPair, self).as_sql( compiler, connection, function='CONCAT_WS', template="%(function)s('', %(expression)s)" ```
`%(expressions)s)` not `%(expression)s)`. You're missing the `s` at the end of `expressions`
put the closing parenthesis on the next line
Is that check actually needed? This seems to be only needed if a token is `None` -- which would not happen if we return a token from `sanitize_token` instead of None :D
to be -> are
Is this check correct? A `Coalesce` can still result in a null value (if all of its arguments are null), so even if the expression is already a `Coalesce` ISTM it needs to be wrapped again (or have `Value('')` added to the end of its `source_expressions`, but just wrapping in another `Coalesce` seems cleaner).
Ah! Of course, sorry I missed that.
We can reuse existing objects.
CI failure due to trailing whitespace (PEP 8 check): ``` 2017-02-08 14:44:54 ERROR: PEP 8: lib/ansible/modules/cloud/amazon/ec2_win_password.py:167:10: W291 trailing whitespace (legacy) ```
Again: ```suggestion ['{0}={1}'.format(k, v if v is not None else 'undefined') ```
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
Should `template=None` be a local variable rather than an (unused) kwarg? I don't think the `as_vendor()` methods typically take any kwargs. Similarly for `function=None` for `as_postgresql/sqlite()`.
Use built-in method? Also, should it crash (raise) if `json.loads()` fails, or just return an empty dict again? ```suggestion try: return self._download_json( url, song_id, data=data.encode('ascii'), headers=headers) except ExtractorError as e: if type(e.cause) in (ValueError, TypeError): # JSON load failure raise except Exception: pass return {} ```
Again: ```suggestion return ('params={0}'.format(encrypted_params), headers) ```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
Making this change here doesn't work because we aren't guaranteed to have passlib installed. The crypt.crypt() method will require that we have a salt set. You could move salt generation into the conditional for ```not HAS_PASSLIB```.
to be -> are
It depends, but it can be public. You can solve metaclass conflicts manually creating class inhertiting from all of your metaclasses, or automatically by using this function (`metaclassmaker` or `six_with_metaclassmaker`), that would do exactly the same new metaclass for you.
start docstring on next line and "Create..."
Is that check actually needed? This seems to be only needed if a token is `None` -- which would not happen if we return a token from `sanitize_token` instead of None :D
Generally, I'd have put the static regex directly as the first argument of `re.search()`.
Use the Python 2 and low 3 Time Machine: `'url too short: %s' % (video_pre_parts, )` or: `'url too short: %(video_pre_parts)s' % {'video_pre_parts': video_pre_parts, }` or: `'url too short: {video_pre_parts}'.format(video_pre_parts=video_pre_parts)` or: `'url too short: {0}'.format(video_pre_parts)` No doubt there are other ways (eg `....format(**locals())`
@rondagostino Thanks for the PR. Couldn't we just use substring until the first "=" ? Something like this would be easier to read? ``` int index = urlSafeBase64EncodedUUID.indexOf('='); return index > 0 ? urlSafeBase64EncodedUUID.substring(0, index) : urlSafeBase64EncodedUUID; ```
```suggestion - If C(false) (NO CYCLE) is specified, any calls to nextval after the sequence ```
+1 for adding `name:` lines
I think this could use a generator expression instead of list comprehension to avoid additional temporary lists in memory. For example: ``` pairs = zip((chars.index(x) for x in nonce), (chars.index(x) for x in pad)) ``` (Same in `_unpad_cipher_token()`)
to be -> are
It depends, but it can be public. You can solve metaclass conflicts manually creating class inhertiting from all of your metaclasses, or automatically by using this function (`metaclassmaker` or `six_with_metaclassmaker`), that would do exactly the same new metaclass for you.
start docstring on next line and "Create..."
Is that check actually needed? This seems to be only needed if a token is `None` -- which would not happen if we return a token from `sanitize_token` instead of None :D
If I'm not completely mistaken this will fail if the attacker requests a page where `csrfmiddlewaretoken` or `HTTP_X_CSRFTOKEN` is shorter than the `CSRF_KEY_LENGTH`.
Generally, I'd have put the static regex directly as the first argument of `re.search()`.
Note to self: if this is not discussed elsewhere in the docs, we should add it.
Use the Python 2 and low 3 Time Machine: `'url too short: %s' % (video_pre_parts, )` or: `'url too short: %(video_pre_parts)s' % {'video_pre_parts': video_pre_parts, }` or: `'url too short: {video_pre_parts}'.format(video_pre_parts=video_pre_parts)` or: `'url too short: {0}'.format(video_pre_parts)` No doubt there are other ways (eg `....format(**locals())`
+1 for adding `name:` lines
Is that check actually needed? This seems to be only needed if a token is `None` -- which would not happen if we return a token from `sanitize_token` instead of None :D
Turn (capitalize) add period. sql -> SQL
to be -> are
`# Filter out fields contributed....` (chop comma before as)
`"""Return the index at which the ordering expressions start."""`
We can register model without defining `OfficeAdminWithOrdering` with the same effect: ```python site.register(Office) ```
Put the close ] on the next line.
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
IIRC get_cols() was renamed to get_group_by_cols() some time ago - no need to have both.
Chop "Helper function that" "Return ..." (use PEP 257 verb style)
Turn (capitalize) add period. sql -> SQL
Is that check actually needed? This seems to be only needed if a token is `None` -- which would not happen if we return a token from `sanitize_token` instead of None :D
`# Filter out fields contributed....` (chop comma before as)
We can register model without defining `OfficeAdminWithOrdering` with the same effect: ```python site.register(Office) ```
to be -> are
This class is unnecessary.
`"""Return the index at which the ordering expressions start."""`
Put the close ] on the next line.
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
Chop "Helper function that" "Return ..." (use PEP 257 verb style)
put the closing parenthesis on the next line
`%(expressions)s)` not `%(expression)s)`. You're missing the `s` at the end of `expressions`
Remove `self.function = 'CONCAT_WS'` and the following line which mutates `self.template` and instead: ``` return super(ConcatPair, self).as_sql( compiler, connection, function='CONCAT_WS', template="%(function)s('', %(expression)s)" ```
Is this check correct? A `Coalesce` can still result in a null value (if all of its arguments are null), so even if the expression is already a `Coalesce` ISTM it needs to be wrapped again (or have `Value('')` added to the end of its `source_expressions`, but just wrapping in another `Coalesce` seems cleaner).
to be -> are
If I'm not completely mistaken this will fail if the attacker requests a page where `csrfmiddlewaretoken` or `HTTP_X_CSRFTOKEN` is shorter than the `CSRF_KEY_LENGTH`.
Ah! Of course, sorry I missed that.
Is that check actually needed? This seems to be only needed if a token is `None` -- which would not happen if we return a token from `sanitize_token` instead of None :D
Won't this result in a confusing SQL-level error if you pass in `None` for `expression` or `pos` by accident? I'm assuming `length` is the only one we actually expect to possibly be `None`. If that's true, I think it would be better to do something like: ``` expressions = [expression, pos] if length is not None: expressions.append(length) ``` Or, if you prefer: `expressions = (expression, pos, length) if length is not None else (expression, pos)`
I don't really get this, why not just do. ```suggestion auth_str = "Signature" ```
@bcoca corrected me that _execute_module() takes care of delegation so this is fine. After discussion, though, we did decide that we should set the ansible_pkg_mgr fact when we return from this action plugin so that we don't end up having to discover it every time the actin plugin is run on the same host.
bcoca also mentioned that the facts end up being set on the host that's being processed in the task loop, not one that's being delegated_to. So we should only return the ansible_pkg_mgr fact if we are not delegating.
When delegating, I'd think that we'd need to run the facts module for the delegated host rather than for the inventory_hostname host....
Still need to add the ansible_pkg-mgr fact to results if: * We selected one of yum, yum4, or dnf * And we are not delegating.
So I guess that this is where we need to set the fact in results. This is the only place where we know that we've had to run the facts module in order to retrieve the package manager.
IMHO it's an antipattern when you catch everything wholesale. You mask future bugs this way.
I don't see a need for string interpolation in cases like this.
As you've noticed, we don't want to do this here because in this case the pkg_mgr is coming from a pre-existing fact.
```suggestion type: list suboptions: ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I don't see a need for string interpolation in cases like this.
Still need to add the ansible_pkg-mgr fact to results if: * We selected one of yum, yum4, or dnf * And we are not delegating.
@bcoca corrected me that _execute_module() takes care of delegation so this is fine. After discussion, though, we did decide that we should set the ansible_pkg_mgr fact when we return from this action plugin so that we don't end up having to discover it every time the actin plugin is run on the same host.
bcoca also mentioned that the facts end up being set on the host that's being processed in the task loop, not one that's being delegated_to. So we should only return the ansible_pkg_mgr fact if we are not delegating.
So I guess that this is where we need to set the fact in results. This is the only place where we know that we've had to run the facts module in order to retrieve the package manager.
When delegating, I'd think that we'd need to run the facts module for the delegated host rather than for the inventory_hostname host....
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
IMHO it's an antipattern when you catch everything wholesale. You mask future bugs this way.
As you've noticed, we don't want to do this here because in this case the pkg_mgr is coming from a pre-existing fact.
```suggestion type: list suboptions: ```
I don't see a need for string interpolation in cases like this.
Still need to add the ansible_pkg-mgr fact to results if: * We selected one of yum, yum4, or dnf * And we are not delegating.
@bcoca corrected me that _execute_module() takes care of delegation so this is fine. After discussion, though, we did decide that we should set the ansible_pkg_mgr fact when we return from this action plugin so that we don't end up having to discover it every time the actin plugin is run on the same host.
bcoca also mentioned that the facts end up being set on the host that's being processed in the task loop, not one that's being delegated_to. So we should only return the ansible_pkg_mgr fact if we are not delegating.
So I guess that this is where we need to set the fact in results. This is the only place where we know that we've had to run the facts module in order to retrieve the package manager.
When delegating, I'd think that we'd need to run the facts module for the delegated host rather than for the inventory_hostname host....
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
IMHO it's an antipattern when you catch everything wholesale. You mask future bugs this way.
As you've noticed, we don't want to do this here because in this case the pkg_mgr is coming from a pre-existing fact.
```suggestion type: list suboptions: ```
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
Make a method to determine constructing dict from which object will be confused
Fail here make the process exit, but we need to create it if the state is present
Duplicate with `get` method? This logic can be handled as `if get_traffic_manager_profile`
My bad, I thought the `upgradePhase-Starting a REBALANCE` message is used somewhere in verification and thought with the augmented log4j it can now be replaced, but now I see it is only for debugging purposes.
The properties' key-value and default/required can be defined in the spec.
excellent handling of congestion control
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
It's more readable to write this out into multiple if-then statements.
```suggestion updates.extend(line for line in set_commands if line not in config) ```
There's no need to wrap the strings like this. Our project lint settings accept up to 160 characters wide. ```suggestion result['warnings'].append('Some configuration commands were unmanaged, review unmanaged list') if result.get('invalid'): result['warnings'].append('Some configuration commands were invalid, review invalid list') ```
*nit* `/play.For/playbook. For /`
try/catch a ValueError here, in case the user try something funny (like '1gb500mb', or '1.5gb').
Yes, 'msg' key and value should always present on *_exit() calls
mandatory msg is missing
suggestion: `expected = disk[param]`
`def get_recommended_datastore(self, datastore_cluster_obj=None):` Ã¢Â†Â’ `def get_recommended_datastore(self, datastore_cluster_obj):`
Need a colon at the end here
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
```suggestion # Run masked_action when enabled param is not provided or service is supposed to be enabled/started. if not module.check_mode and module.params['enabled'] in [None, True]): ```
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
Please rewrite as ``` if __name__ == '__main__': main() ```
Same as above, I'd revert to the simpler version. The main idea behind `decode` for me is a consistent interface where it makes sense, we don't have to bend backwards just to use it.
Missing `=dict` on this and the next few lines
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Match the error message
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
here too: pysopenssl --> pyopenssl
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
```suggestion # Run masked_action when enabled param is not provided or service is supposed to be enabled/started. if not module.check_mode and module.params['enabled'] in [None, True]): ```
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
Please rewrite as ``` if __name__ == '__main__': main() ```
Same as above, I'd revert to the simpler version. The main idea behind `decode` for me is a consistent interface where it makes sense, we don't have to bend backwards just to use it.
Missing `=dict` on this and the next few lines
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Match the error message
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
here too: pysopenssl --> pyopenssl
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
For long query strings, it's better to use ```query``` parameter here.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
```not (foo is None)``` => ```foo is not None```
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
```not (foo is None)``` => ```foo is not None```
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
For long query strings, it's better to use ```query``` parameter here.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
```not (foo is None)``` => ```foo is not None```
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
```not (foo is None)``` => ```foo is not None```
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
For long query strings, it's better to use ```query``` parameter here.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
```not (foo is None)``` => ```foo is not None```
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
@chrisvanheuveln and I chatted about this. No further changes needed here since we avoid the 400 error with the order change and the command is blocking.
a simpler fix i was already working on: `+ display.display(json.dumps(text, cls=AnsibleJSONEncoder, sort_keys=True, indent=4))`
well, not die with unexpected exception .. tempted to say there is no real reason the type should be incorrect for any keys. So ending in an error should be fine, just not an unhandled one.
^ that seems to be an expression not really a data type issue (sorting keys, this is another known json issue), in any case, there is also an existing `jsonify` in module_utils.
docstring with example input/output would be really helpful
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
The pylint error is occurring because this `MockSelector` class shadows the one on line 59.
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
Please change this to `2.7`.
@chrisvanheuveln and I chatted about this. No further changes needed here since we avoid the 400 error with the order change and the command is blocking.
I think this option is deprecated - https://www.vmware.com/support/developer/converter-sdk/conv51_apireference/vim.vm.RelocateSpec.Transformation.html
It should be `if self._module.params.get('sparse') is not None`, because if `sparse` is `False` it won't send `sparse=False`
Use `required_if(['state', 'present', ['recipients', 'actions']])` in the `AnsibleModule` arguments
Yes, end users would only ever read this object, hence the lack of a friendly `Builder`. I think it's OK to leave as-is.
you dont need to include most of the defaults here (nor in docs), `default=None`, `type=str` and `required=false` are all redundant
as noted in docs, there are a bunch of 'binary choices fields' which should just be booleans.
This is not a good practice, use module.debug or module.log , you can even check module.verbosity to figure out the level you want.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Oh, so these modules have existed for a while, we are just upstreaming them now, that makes sense.
This is only ever called once. Do we need the default? (Same with SQL version)
This is only used once. Can we move it back to the `color` module? (That way `termcolors` is still only ever used by `color`)
prefer including a trailing comma in kwargs so if more items are added in the future we don't have to modify this line again
Minor consistency nit- our stuff seems to be using `[action]_[backend]()`, where these aliased imports have that reversed. I'd vote for `[action]_[backend]()` over this- makes the code a little easier to read...
Technically since we're doing this in a fixture, either `monkeypatch` should be used to insert these into `globals()`, or you should return `mod`, and then just use `mod.Template` and `mod.AnsibleNativeEnvironment` in your tests.
use ```from ansible.module_utils.vmware import find_obj```
Yes, end users would only ever read this object, hence the lack of a friendly `Builder`. I think it's OK to leave as-is.
This is unnecessary, AnsibleAWSModule handles it.
This whole connection block can be replaced with `conn = module.client('ssm')`
Simplify this by not adding required=False, and always starting with the type. Since every parameter has a type, starting with this adds consistency.
On all your parameters, if there are not required, there is no need to specify the `required=False`. But if they are, you should speficy `required=True`
width, height, and offset
```python mutually_exclusive=[ ['api_username', 'api_token'], ['api_password', 'api_token'], ], required_together=[ ['api_username', 'api_password'], ], required_one_of=[ ['api_username', 'api_token'] ], supports_check_mode=True, ```
It should be `if self._module.params.get('sparse') is not None`, because if `sparse` is `False` it won't send `sparse=False`
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
I find this error message very hard to understand. I don't have any experience with GitLab servers, though :) If the other gitlab_* modules have the same text, we should probably keep it here as well. Changing it could be done in another PR once this is merged.
Authentication should be handled here and not inside of the class
Please remove `no_log=True` from username
This regex does not make any sense.
no need for `.keys()`. `if property_id in vapp_properties_current:` is the same as `if property_id in vapp_properties_current.keys():`
I think a list comprehension would be more readable.
one more for the single line version
```suggestion assert expected == "exception" ```
is_vapp_changed = False
I think you didn't understand what I was suggesting. What you do : Add each of the property the user have asked for in new_vmconfig_spec.property, and if at least one of them modify the properties already attached to the VM Ã¢Â†Â’ apply new_vmconfig_spec What I propose : Only add to new_vmconfig_spec.property the property the user have asked for that actually modify (add/remove/edit) the properties already attached to the VM. And then, if there is some properties in new_vmconfig_spec.property Ã¢Â†Â’ apply new_vmconfig_spec.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
It'd be interesting to see a test case for something inheriting a `BaseException` too.
Should also include `block_size` and `parallelism`
```suggestion Test that the returned value for timezone consists of only uppercase ```
Should also update based on `block_size` and `parallelism`
We don't like these catch-all statements in Ansible. In this case I would expect the library to handle any exceptions caused by connect_to_api. `all_facts()` should be quite safe, if not the smaller pieces that may raise exceptions should be wrapped. `exit_json()` is not expected to raise exceptions. So I'd remove the whole try-block here altogether.
don't do this by default, set from cli `self._options` does not exist or is None
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
if datastore already exists
Do you really need to write it character by character. Seems to be quite inefficient when you could write in chunks. Also use `to_bytes(c, errors='surrogate_or_strict')` instead of `.encode()`
extra space after ,
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Why not also do this when `module.check_mode` is `True`? Then the check mode behavior is closer to the real thing.
I think that this can be replaced with the following as above: ```python @unittest.skipIf(geos_version_tuple() < (3, 7), 'GEOS >= 3.7.0 is required') ``` I think we only need to worry about testing the GEOS behaviour if it is actually supported; even if this is only testing the exception-raising behaviour.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Since the list is in order of most common use, the code detects incorrect passwords slightly faster if you preserve the order. :bikeshed:
Oh I missed that. Sorry!
Scanning a list will not be faster than a membership test of a set (unless the list of words is very small).
This would be better as a set rather than a list.
Not sure how much a difference it makes, but it seems better to store this in Python rather than having to read from a text file. Worth it to make the file location customizable? If so, it might be nice to make "common passwords" a separate package so we don't have to include that list in Django. I guess users might not care for the additional setup tasks though.
Should also include `block_size` and `parallelism`
Should also update based on `block_size` and `parallelism`
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
Use another lookup instead of `epoch` e.g. `second`.
extra space after ,
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
As stated below, this should become: ```python for store in host.datastore: _tmp = { 'name': store.summary.name, 'total': bytes_to_human(store.summary.capacity), 'free': bytes_to_human(store.summary.freeSpace), } facts['ansible_datastore'].append(_tmp) ```
A trailing comma is preferred.
if datastore already exists
We don't like these catch-all statements in Ansible. In this case I would expect the library to handle any exceptions caused by connect_to_api. `all_facts()` should be quite safe, if not the smaller pieces that may raise exceptions should be wrapped. `exit_json()` is not expected to raise exceptions. So I'd remove the whole try-block here altogether.
Please ignore, my suggestion is invalid syntax.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
Ditto. ```suggestion # Check empty result with non-execution optimization. with self.assertNumQueries(0): ```
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
This looks like a good change, however, for consistency with the rest of the code, I think we should use `self._play_context.ssh_executable` until we decide to switch the connection plugin in full over to using `get_option`.
Double checking the commit, this change, in this form leaks some state across migrations. Testing on CI right now. ``` python ERROR: test_alter_id_type_with_fk (migrations.test_executor.ExecutorTests) ---------------------------------------------------------------------- Traceback (most recent call last): File "/home/markus/Coding/django/django/db/backends/utils.py", line 64, in execute return self.cursor.execute(sql, params) psycopg2.ProgrammingError: foreign key constraint "book_app_book_author_id_93532c48_fk_author_app_author_id" cannot be implemented DETAIL: Key columns "author_id" and "id" are of incompatible types: integer and character varying. The above exception was the direct cause of the following exception: Traceback (most recent call last): File "/home/markus/Coding/django/django/test/utils.py", line 182, in inner return test_func(*args, **kwargs) File "/home/markus/Coding/django/tests/migrations/test_executor.py", line 401, in test_alter_id_type_with_fk executor.migrate([("author_app", "0002_alter_id")]) File "/home/markus/Coding/django/django/db/migrations/executor.py", line 94, in migrate self.apply_migration(states[migration], migration, fake=fake, fake_initial=fake_initial) File "/home/markus/Coding/django/django/db/migrations/executor.py", line 131, in apply_migration state = migration.apply(state, schema_editor) File "/home/markus/Coding/django/django/db/migrations/migration.py", line 118, in apply operation.database_forwards(self.app_label, schema_editor, old_state, project_state) File "/home/markus/Coding/django/django/db/migrations/operations/fields.py", line 201, in database_forwards schema_editor.alter_field(from_model, from_field, to_field) File "/home/markus/Coding/django/django/db/backends/base/schema.py", line 482, in alter_field old_db_params, new_db_params, strict) File "/home/markus/Coding/django/django/db/backends/base/schema.py", line 635, in _alter_field params, File "/home/markus/Coding/django/django/db/backends/base/schema.py", line 106, in execute cursor.execute(sql, params) File "/home/markus/Coding/django/django/db/backends/utils.py", line 79, in execute return super(CursorDebugWrapper, self).execute(sql, params) File "/home/markus/Coding/django/django/db/backends/utils.py", line 64, in execute return self.cursor.execute(sql, params) File "/home/markus/Coding/django/django/db/utils.py", line 95, in __exit__ six.reraise(dj_exc_type, dj_exc_value, traceback) File "/home/markus/Coding/django/django/utils/six.py", line 658, in reraise raise value.with_traceback(tb) File "/home/markus/Coding/django/django/db/backends/utils.py", line 64, in execute return self.cursor.execute(sql, params) django.db.utils.ProgrammingError: foreign key constraint "book_app_book_author_id_93532c48_fk_author_app_author_id" cannot be implemented DETAIL: Key columns "author_id" and "id" are of incompatible types: integer and character varying. ``` However, integrating this with the second commit on my PR fixes the issue. I thus squash those commits there and close your PR here.
I don't like how this entire if block looks. Is `plan` allowed to be `None` considering the check/assignment on line 73 and 74? I'd prefer to call out that it can not be forwards and backwards by doing something like: ``` if all_forwards == all_backards: raise.. # this may not be correct if `plan` can be `None` elif all_forwards: migrate_forwards() else: migrate_backwards() ``` I feel this highlights the invariant, and avoids potential problems of misreading the bool combinations in the original code.
That code is marginally longer, but I think it's clearer. I'll leave it up to what you think is best though.
```suggestion module.exit_json(changed=True,**camel_dict_to_snake_dict(res))) ```
You can achieve this same result output with `module.exit_json(changed=True,**camel_dict_to_snake_dict(execution))` which can be imported from `ansible.module_utils.ec2`. That will automatically cover if AWS ever starts returning additional keys from this API and is generally easier to maintain.
I haven't actually used stepfunctions myself so I'm just going off the docs. AIUI, `ExecutionAlreadyExists` would only be returned on a running execution if the `execution_input` is different, not in a case where the input is the same. As an ansible user who is new to step functions, I feel like if I gave the same input and ran the playbook twice I would expect the 2nd time to be `changed=false` if the first execution is still running. But that doesn't mean that's how AWS users familiar with step functions will expect it.
`client.start_execution` is idempotent but `start_execution` always returns `changed=True` if `action=start`. We probably want to `client.list_executions` to see if the execution is already running before proceeding.
This will only get the ip address for non fixed ip's? Another way of getting ip's from libvirt is ```python domain.interfaceAddresses(libvirt.VIR_DOMAIN_INTERFACE_ADDRESSES_SRC_AGENT, 0) {'lo': {'hwaddr': '00:00:00:00:00:00', 'addrs': [{'prefix': 8, 'type': 0, 'addr': '127.0.0.1'}, {'prefix': 128, 'type': 1, 'addr': '::1'}]}, 'eth0': {'hwaddr': '52:54:00:03:b1:0b', 'addrs': [{'prefix': 24, 'type': 0, 'addr': '192.168.122.2'}, {'prefix': 64, 'type': 1, 'addr': 'fe80::c2de:d88c:1c2f:21c5'}]}} ```
Please format this like this: ```python skip_lines = [ '+----------------------------------------------------------+', ' Available Repositories in /etc/yum.repos.d/redhat.repo' ] ```
If the issue is corrected. There will be some cases triggering this branch, please also update the user cases of the `raw_cloud_env` variable.
Same style as above.
I think this could use a generator expression instead of list comprehension to avoid additional temporary lists in memory. For example: ``` pairs = zip((chars.index(x) for x in nonce), (chars.index(x) for x in pad)) ``` (Same in `_unpad_cipher_token()`)
This seems wrong. Won't this end up being the equivalent of: ``` /bin/sh -c if [ x"test" = x"test" ] ; then printf "hi" ; fi ``` When what we really want is the former which is the equivalent of: ``` /bin/sh -c 'if [ x"test" = x"test" ] ; then printf "hi" ; fi' ```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Okay, looking further down, I see that you're just moving this around though...
the shell itself would have done it before. but might have done it slightly differently.
I don't think this is safe. If someone is calling this function without unsafe_shell they probably have not quoted the arguments to prevent this sort of expansion.
Can you combine these 2 tests using `subTest()`
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
The final command should probably return its stdout, stderr and rc back to the playbook.
There is a much easier way to do this, by using the index in the list. Something like: ```python unit = size[-1] units = list('b', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y') try: multiplier = 1024**units.index(unit) except ValueError:: e = get_exception() module.fail_json(msg="No valid size unit specified. %s" % e) ```
I don't think this is working correctly, did you test it for all possible inputs ? - First of all the size_unit is only correct if it is a known unit. - And if that's not true, and it's not a digit, you have to escalate. - If it is a digit, you likely won't have a unit and then you could assume bytes.
The mixup to me is that people using incorrect units will silently be using bytes, whereas it should report this as an error.
This means that if you'd be using 16m, it will be 16 bytes too. You will have to do a lot more to avoid mixups here.
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
The final command should probably return its stdout, stderr and rc back to the playbook.
This doesn't seem right, size is an integer at this point.
Same as above, I'd revert to the simpler version. The main idea behind `decode` for me is a consistent interface where it makes sense, we don't have to bend backwards just to use it.
Should also include `block_size` and `parallelism`
Sorry, I had a typo in https://github.com/django/django/pull/12675#discussion_r418965162 -- what I ment to say was: keep `work factor` in safe summary (since it's for display and already translated) and `work_factor` in `decode`. This will also require less changes in the tests.
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
We confirmed that `parallelism` should be taken into account.
`self._test_scrypt_upgrade('parallelism', 'parallelism', 2)` fails, it seems we shouldn't take `parallelism` into account.
I think this got copied from somewhere else by mistake and should be dropped (including the following line)
Matching empty string is senseless.
> My main worry here is: Is this correct and does it make sense to implement for such a complex hasher (notably we already have others where we argue it is simply not possible in a sensible way). > > Since scrypt can raise errors like this: > > > ValueError: Invalid parameter combination for n, r, p, maxmem. > > I am wondering if `must_update` couldn't also trigger this condition. Or can we always calculate `extra_iterations` and `extra_block` and be sure that the combinations are valid? You're right parameters may no be valid, e.g. ``` self.work_factor = 2 **14 decoded['work_factor'] = 2 ** 11 ``` both are a power of 2, however `extra_iterations = 14336` is not and raises `ValueError: n must be a power of 2`.
Should also update based on `block_size` and `parallelism`
If the issue is corrected. There will be some cases triggering this branch, please also update the user cases of the `raw_cloud_env` variable.
Same style as above.
I think this could use a generator expression instead of list comprehension to avoid additional temporary lists in memory. For example: ``` pairs = zip((chars.index(x) for x in nonce), (chars.index(x) for x in pad)) ``` (Same in `_unpad_cipher_token()`)
This seems wrong. Won't this end up being the equivalent of: ``` /bin/sh -c if [ x"test" = x"test" ] ; then printf "hi" ; fi ``` When what we really want is the former which is the equivalent of: ``` /bin/sh -c 'if [ x"test" = x"test" ] ; then printf "hi" ; fi' ```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Okay, looking further down, I see that you're just moving this around though...
the shell itself would have done it before. but might have done it slightly differently.
I don't think this is safe. If someone is calling this function without unsafe_shell they probably have not quoted the arguments to prevent this sort of expansion.
Can you combine these 2 tests using `subTest()`
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
If the issue is corrected. There will be some cases triggering this branch, please also update the user cases of the `raw_cloud_env` variable.
Same style as above.
I think this could use a generator expression instead of list comprehension to avoid additional temporary lists in memory. For example: ``` pairs = zip((chars.index(x) for x in nonce), (chars.index(x) for x in pad)) ``` (Same in `_unpad_cipher_token()`)
This seems wrong. Won't this end up being the equivalent of: ``` /bin/sh -c if [ x"test" = x"test" ] ; then printf "hi" ; fi ``` When what we really want is the former which is the equivalent of: ``` /bin/sh -c 'if [ x"test" = x"test" ] ; then printf "hi" ; fi' ```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Okay, looking further down, I see that you're just moving this around though...
the shell itself would have done it before. but might have done it slightly differently.
I don't think this is safe. If someone is calling this function without unsafe_shell they probably have not quoted the arguments to prevent this sort of expansion.
Can you combine these 2 tests using `subTest()`
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
If the issue is corrected. There will be some cases triggering this branch, please also update the user cases of the `raw_cloud_env` variable.
Same style as above.
I think this could use a generator expression instead of list comprehension to avoid additional temporary lists in memory. For example: ``` pairs = zip((chars.index(x) for x in nonce), (chars.index(x) for x in pad)) ``` (Same in `_unpad_cipher_token()`)
This seems wrong. Won't this end up being the equivalent of: ``` /bin/sh -c if [ x"test" = x"test" ] ; then printf "hi" ; fi ``` When what we really want is the former which is the equivalent of: ``` /bin/sh -c 'if [ x"test" = x"test" ] ; then printf "hi" ; fi' ```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Okay, looking further down, I see that you're just moving this around though...
the shell itself would have done it before. but might have done it slightly differently.
I don't think this is safe. If someone is calling this function without unsafe_shell they probably have not quoted the arguments to prevent this sort of expansion.
Can you combine these 2 tests using `subTest()`
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
Replace all `f''` expressions: * where possible as below * for an expression where the braced expressions aren't just local variables, replace each `{expr}` by `{n}` with n starting at 0 and incrementing, and append `.format(expr0, expr1, ...)` to the string literal, or rewrite using `%` formatting * if the braced expressions are all local variables, you can just add `.format(locals())` (possibly distasteful) * for format literals used to add or change URL query parameters, consider using `update_url_query()` instead. ```suggestion msg = 'Panopto said: ' + response.get('ErrorMessage') ```
`_search_regex`, `_parse_json`. Again: read coding conventions.
Sorry, I know it was in my changes, but I forgot to remove that lineÃ¢Â€Â¦ Changing the data seems a bit ugly here, could you remove this line and changing the next one by: ``` prefix = ipaddress.ip_network(data["prefix"] + "/" + data["prefix_length"]) ```
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
The final command should probably return its stdout, stderr and rc back to the playbook.
The final command should probably return its stdout, stderr and rc back to the playbook.
This doesn't seem right, size is an integer at this point.
It would be better to mention that in the release notes. :-)
Just use: ``` datetime.utcfromtimestamp(ts).replace(tzinfo=timezone.utc) ``` This will be more efficient. You don't need `make_aware` here because UTC doesn't have DST. Sure, this is a micro-optimization, but I like avoiding overhead at the lower levels ;-)
```suggestion vault_data(), ```
We're using PEP 257 verbs for new code "Return ..."
You can use `module.deprecate` to throw a deprecation warning.
```suggestion (datetime(2019, 6, 15, 14, 45, tzinfo=tz('Europe/Helsinki')), '2019-06-15T14:45:00+01:40'), ```
Could you please keep the same string quoting style across the module? ```suggestion (datetime(2019, 6, 15, 14, 45, tzinfo=tz('UTC')), '2019-06-15T14:45:00+00:00'), ```
I believe you can replace this whole block with: ``` tz = timezone.get_default_timezone() dt = timezone.make_aware(dt, tz) ```
It's a statement, not a function. ```suggestion assert ansible_json_encoder.default(test_input) == expected ```
I think it's nicer to have trailing commas: ```suggestion ], indirect=['mapping'], ```
It would be better to mention that in the release notes. :-)
Just use: ``` datetime.utcfromtimestamp(ts).replace(tzinfo=timezone.utc) ``` This will be more efficient. You don't need `make_aware` here because UTC doesn't have DST. Sure, this is a micro-optimization, but I like avoiding overhead at the lower levels ;-)
```suggestion vault_data(), ```
We're using PEP 257 verbs for new code "Return ..."
You can use `module.deprecate` to throw a deprecation warning.
```suggestion (datetime(2019, 6, 15, 14, 45, tzinfo=tz('Europe/Helsinki')), '2019-06-15T14:45:00+01:40'), ```
Could you please keep the same string quoting style across the module? ```suggestion (datetime(2019, 6, 15, 14, 45, tzinfo=tz('UTC')), '2019-06-15T14:45:00+00:00'), ```
I believe you can replace this whole block with: ``` tz = timezone.get_default_timezone() dt = timezone.make_aware(dt, tz) ```
It's a statement, not a function. ```suggestion assert ansible_json_encoder.default(test_input) == expected ```
I think it's nicer to have trailing commas: ```suggestion ], indirect=['mapping'], ```
shouldn't this line and the one below just not be here, and the loop be `for arg, version in self.DEFAULT_DEPRECATED_ARGS` (though those aren't really a default either, so `DEFAULT` is a bit of a misnomer)
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
I'd chop the intermediate variable
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
like diff = load_config(self._module, config_xml, [])
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
User of this inventory plugin should be able to use `v6_main_ip` for value of `ansible_host` (i don't know if it makes sens to be able to use `internal_ip`) or no `ansible_host` at all (meaning that `server['label']` will be used instead).
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
`e` isn't used: remove it or use it.
`id` isn't used, it is sufficient to iterate on keys.
`Check the configuration files` seems vague, I propose: `Check inventory file and vultr configuration files`.
use a single loop? ~~~python for server in _retrieve_servers(api_key): server = Vultr.normalize_result(server, SCHEMA) .... ~~~ ~~~
When `hostname_preference` is equal to `name`, there is no need to define `ansible_host`.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
To be consistent with the rest of the codebase, I'd import `from django.utils.six.moves import range` first.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
`del` is a builtin, not a function. These parens don't have to be here
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
need to x2 check tmrow, but this should be an error, we don't allow for circular parenting
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
`del` is a builtin, not a function. These parens don't have to be here
This can instead be `continue` and let the `else` unnest.
move to finally
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
need to x2 check tmrow, but this should be an error, we don't allow for circular parenting
okay, but would be helpful to say _why_ we need to always return True.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
IIRC get_cols() was renamed to get_group_by_cols() some time ago - no need to have both.
We could lowercase the vendored files, that would help at least for the `zh-*` variants.
This will probably break for `zh-CN` and others because Django returns lower cased language names. I do not have a good idea on how to fix that though.
Sorry, I know it was in my changes, but I forgot to remove that lineÃ¢Â€Â¦ Changing the data seems a bit ugly here, could you remove this line and changing the next one by: ``` prefix = ipaddress.ip_network(data["prefix"] + "/" + data["prefix_length"]) ```
This can be single-lined: ```suggestion return (matched_tags or not tags) and not test_tags.intersection(exclude_tags) ```
I would chop `does_`: ```suggestion if test_match_tags(test, tags, exclude_tags) ```
is the `any` required here - `if metrics_to_disable:` should suffice, I'd think
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
This should return {} instead of None so camel_dict_to_snake_dict works.
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
don't do this by default, set from cli `self._options` does not exist or is None
this whole section is not needed, just use `self._plugin_options[<option name>]`
its new in 2.4 so it doesn't apply to older versions
also need to check whether the workspace is name or resource id
Minor but I'd move this control flow block after the `weights` one to match the args order.
Please use [standard exception handling guidelines](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/amazon/GUIDELINES.md#boto3-2)
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
don't do this by default, set from cli `self._options` does not exist or is None
Chop "Helper function that" "Return ..." (use PEP 257 verb style)
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
`# Filter out fields contributed....` (chop comma before as)
Put the close ] on the next line.
`"""Return the index at which the ordering expressions start."""`
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
one more for the single line version
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Yes, from what I can see this doesn't work now. My `state: absent` is ignored when target is used by target group. Where is `else`? :)
This module only deregisters a module if `current_target_state` is `unused`. I would like to use it to deregister used targets. This fails silently now.
yeah, there's a missing chunk of code.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Why don't you just encode suffix like `u'-\u00c5\u00d1\u015a\u00cc\u03b2\u0141\u00c8'.encode('utf-8')` and for the no_temp_unicode override just do `suffix = b'-ansible'`? No need for this check which is arguably more confusing.
may be blind, but think this will never get fired if v is empty. you create an empty list, which will be skipped in the for loop.
For some reason this causes ``` {aliases: [attr]} {choices: [all, 'no', none, safe, urllib2, 'yes']} {default: [200]} ``` while non-sequence values for `default` yield ``` default: 30 ``` without the `{...}`.
FYI, in case you switch to working on whole config, `get_config.config` gives you the whole tree. (`get_config(:foo).config` is generally equivalent to `get_config.config[:foo]`.) Also, VMDB::Config is deprecated. May be better to use `::Settings.to_hash`.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
We don't need tags in this function, I think.
Missing conditional on execution of `main`: ```python if __name__ == '__main__': main() ```
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
Ah, that does work. :-)
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
I think this might now work when there is dict of dict in response? It might break idempotency in those cases. I think you need to call it recursively when there is dict of dict or dict of list of dict.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
For some reason this causes ``` {aliases: [attr]} {choices: [all, 'no', none, safe, urllib2, 'yes']} {default: [200]} ``` while non-sequence values for `default` yield ``` default: 30 ``` without the `{...}`.
FYI, in case you switch to working on whole config, `get_config.config` gives you the whole tree. (`get_config(:foo).config` is generally equivalent to `get_config.config[:foo]`.) Also, VMDB::Config is deprecated. May be better to use `::Settings.to_hash`.
This should be singular
Not sure if we need `normalize_interface` when fetching interface names from running-config.
Why don't you just encode suffix like `u'-\u00c5\u00d1\u015a\u00cc\u03b2\u0141\u00c8'.encode('utf-8')` and for the no_temp_unicode override just do `suffix = b'-ansible'`? No need for this check which is arguably more confusing.
Missing conditional on execution of `main`: ```python if __name__ == '__main__': main() ```
Maybe simplify with: ``` decoded_url.translate(None, '"\',').decode('utf-8') ``` Or in case there could be spaces in `['"cod", "ed ", "URL"]`: ``` re.sub(r'[\s"\',]', '', decoded_url).decode('utf-8') ```
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
`self._test_scrypt_upgrade('parallelism', 'parallelism', 2)` fails, it seems we shouldn't take `parallelism` into account.
> My main worry here is: Is this correct and does it make sense to implement for such a complex hasher (notably we already have others where we argue it is simply not possible in a sensible way). > > Since scrypt can raise errors like this: > > > ValueError: Invalid parameter combination for n, r, p, maxmem. > > I am wondering if `must_update` couldn't also trigger this condition. Or can we always calculate `extra_iterations` and `extra_block` and be sure that the combinations are valid? You're right parameters may no be valid, e.g. ``` self.work_factor = 2 **14 decoded['work_factor'] = 2 ** 11 ``` both are a power of 2, however `extra_iterations = 14336` is not and raises `ValueError: n must be a power of 2`.
Matching empty string is senseless.
Sorry, I had a typo in https://github.com/django/django/pull/12675#discussion_r418965162 -- what I ment to say was: keep `work factor` in safe summary (since it's for display and already translated) and `work_factor` in `decode`. This will also require less changes in the tests.
Subtests can be used here (with pairs `(key, expected)`).
Should also include `block_size` and `parallelism`
Should also update based on `block_size` and `parallelism`
Same as above, I'd revert to the simpler version. The main idea behind `decode` for me is a consistent interface where it makes sense, we don't have to bend backwards just to use it.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
the same for `streaming` key.
Nesting generator expressions and list comprehensions like this is bad style. It has the same problems as a run-on sentence in natural languages (Making it hard for other people to read and keep the entirety of the clause in their memory. Easy to misinterpret the meaning because of misreading one small piece of the grammar). In most cases, using a ```for``` loop with indentation for at least one of the loops is better. If I understand this correctly, You are trying to take input of this form: ``` names = ['one >1.0,<2.0', 'two', '>3.0', '<4.0'] ``` and flatten it so that it is in this form: ``` names = ['one', '>1.0', '<2.0', 'two', '>3.0', '<4.0'] ``` ? If so, it's not quite right as it will currently return ``` ['one >1.0', '<2.0', 'two', '>3.0', '<4.0'] ```
will fail if `streaming` value is `None`(`null`), same applies to `show` and `video` variables.
surround only the part that will threw the exception.
Need a colon at the end here
```suggestion for _ in range(3): ```
Having this check in each module will result in code duplication and an overhead for module writer. `module` param will be used if the check is moved to utils function
I think it is better to pass module as argument to build_xml() and handle lxml install check-in module_utils instead of having it in each module
`items = value.split(self.delimiter) if value else []` is slightly faster.
~~A list comprehension would be a little faster.~~ EDIT: Forgive me, I misread the code.
~~Maybe a list comprehension here too.~~
That's why I wrote "maybe" ;)
like diff = load_config(self._module, config_xml, [])
Here however, you shouldn't use a list comprehension as it makes you call `regex.match` two times instead of one. Plus a dict iterates on keys by default, so you don't need to write `.keys()`.
self._connected is set by CliBase.connect(), shouldn't need to specify it here
disconnect() is likewise handled in CliBase, and should be entirely unnecessary here.
This one is a bit newer to CliBase, but also implemented verbatim in superclass
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
Maybe simplify with: ``` decoded_url.translate(None, '"\',').decode('utf-8') ``` Or in case there could be spaces in `['"cod", "ed ", "URL"]`: ``` re.sub(r'[\s"\',]', '', decoded_url).decode('utf-8') ```
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
`self._test_scrypt_upgrade('parallelism', 'parallelism', 2)` fails, it seems we shouldn't take `parallelism` into account.
> My main worry here is: Is this correct and does it make sense to implement for such a complex hasher (notably we already have others where we argue it is simply not possible in a sensible way). > > Since scrypt can raise errors like this: > > > ValueError: Invalid parameter combination for n, r, p, maxmem. > > I am wondering if `must_update` couldn't also trigger this condition. Or can we always calculate `extra_iterations` and `extra_block` and be sure that the combinations are valid? You're right parameters may no be valid, e.g. ``` self.work_factor = 2 **14 decoded['work_factor'] = 2 ** 11 ``` both are a power of 2, however `extra_iterations = 14336` is not and raises `ValueError: n must be a power of 2`.
Matching empty string is senseless.
Sorry, I had a typo in https://github.com/django/django/pull/12675#discussion_r418965162 -- what I ment to say was: keep `work factor` in safe summary (since it's for display and already translated) and `work_factor` in `decode`. This will also require less changes in the tests.
Subtests can be used here (with pairs `(key, expected)`).
Should also include `block_size` and `parallelism`
Should also update based on `block_size` and `parallelism`
Same as above, I'd revert to the simpler version. The main idea behind `decode` for me is a consistent interface where it makes sense, we don't have to bend backwards just to use it.
`items = value.split(self.delimiter) if value else []` is slightly faster.
~~A list comprehension would be a little faster.~~ EDIT: Forgive me, I misread the code.
~~Maybe a list comprehension here too.~~
Forget it, I misread the double for-loop.
That's why I wrote "maybe" ;)
Here however, you shouldn't use a list comprehension as it makes you call `regex.match` two times instead of one. Plus a dict iterates on keys by default, so you don't need to write `.keys()`.
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
disconnect() is likewise handled in CliBase, and should be entirely unnecessary here.
This one is a bit newer to CliBase, but also implemented verbatim in superclass
surround only the part that will threw the exception.
set default `step_size=None` instead of `"any"` and only render that attribute if it's `not None`.
```python if step_size is not None: self.validators.append(validators.StepValueValidator(step_size)) ``` Use `django.core.validators.MinValueValidator` as a blueprint and create a `StepValueValidator`. Put the validation code from `to_python` in there.
Have a look at the `IntegerField` how it's done there. This has the advantage, that your `StepSizeValidator` can be reused. And it shall be reused, because `step` can also be applied to an `IntegerField`.
```suggestion msg = "'Ensure this value is a multiple of step size 0.02.'" with self.assertRaisesMessage(ValidationError, msg): ```
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
here you need just a 'steps' not whole module as well.
also please rename entity_id to job_id, no need to have it too generic here
You can achieve this same result output with `module.exit_json(changed=True,**camel_dict_to_snake_dict(execution))` which can be imported from `ansible.module_utils.ec2`. That will automatically cover if AWS ever starts returning additional keys from this API and is generally easier to maintain.
I would add here fetch_nested=True, because we always want to return steps.
change 'type(job).__name__.lower()' to just 'job' no need to do it generic here
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
```suggestion self, objs, fields, batch_size, on_conflict=None, update_fields=None, unique_fields=None, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
```suggestion item, fields=fields, using=self.db, ```
Please add a trailing comma: ```suggestion on_conflict=None, update_fields=None, unique_fields=None, ```
use a single line or use hanging indent (we avoid non-4 space indents)
single line as above
you can use `state` to avoid the 'or' to the user
You can remove the `?` - 2.4 is likely, and if not that can be updated :)
`items = value.split(self.delimiter) if value else []` is slightly faster.
Here however, you shouldn't use a list comprehension as it makes you call `regex.match` two times instead of one. Plus a dict iterates on keys by default, so you don't need to write `.keys()`.
The pylint error is occurring because this `MockSelector` class shadows the one on line 59.
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Need a colon at the end here
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
default=None is the default
required=False is the default
I'd like to remove `lambda_function_arn` in case anyone ever wants to implement other notification types
IMO this should not raise a warning.
No need to parametrize with just one case.
Both cases test the same thing, don't repeat yourself. Test things based on their properties. Here you'd probably want to test iterables, not just sets.
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
It probably makes sense to test that the exception reason also matches expectations
prefer including a trailing comma in kwargs so if more items are added in the future we don't have to modify this line again
Use _ (underline) instead of webpage if the value is not used.
Are all of these necessary? I think youtube-dl defaults suffice.
Use json.dumps instead
Should this be ``` python checked_list.sort(key=lambda x: sorted(list(x.items())) if isinstance(x, dict) else x) ``` tracebacks from test failures in CI make me suspect the view object from set(x.items()) fails because set() wants hashable items and the view object from x.items() is not. Does wrapping it in a list() or tuple() work? ``` 2017-06-26 16:43:33 The full traceback is: 2017-06-26 16:43:33 Traceback (most recent call last): 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_module_ecs_ecr.py", line 304, in run 2017-06-26 16:43:33 policy = sort_json_policy_dict(policy) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 603, in sort_json_policy_dict 2017-06-26 16:43:33 ordered_policy_dict[key] = value_is_list(value) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 595, in value_is_list 2017-06-26 16:43:33 checked_list.sort(key=lambda x: sorted(set(x.items())) if isinstance(x, dict) else x) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 595, in <lambda> 2017-06-26 16:43:33 checked_list.sort(key=lambda x: sorted(set(x.items())) if isinstance(x, dict) else x) 2017-06-26 16:43:33 TypeError: unhashable type: 'list' ```
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
As discussed on IRC: no.
I don't think this works? block_id defaults to the boolean False in the parameters to this function. So you probably need to check: ``` python if block_id is not False: ``` rather than checking against a string.
Thanks - I've replaced all uses of `filter` with comprehensions.
This will fail on python3 for a similar reason as the conditional check noted above. filter returns a generator. The generator can't be indexed. Use a list comprehension instead: ``` python block = [b for b in blocks if b.base_ip == base_ip] ```
the message here looks like a failure case rather than an exit case... Perhaps it should use module.fail_json or the message should be changed? Stylistically, exit_json() cases should retun back to the main function in the module and let the exit_json() occur there. It's more flexible to changing needs inside of the module than calling exit_json() here.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Should be "Can't detect any" rather than "none" I think.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Minor but I'd move this control flow block after the `weights` one to match the args order.
I believe you can just drop the `== 0` case here. Doing `DELETE FROM` on 0 rows should be harmless. No need to `SELECT COUNT(*)`. You can also find out if a table has >1000 rows without counting everything using ```sql SELECT COUNT(*) > 1000 FROM (SELECT * FROM table_name LIMIT 1001) SUBQUERY; ``` Which returns '1' (true) only if it does have >1000 rows. But I don't think we need that here for the time being, the approx row count should be fine as a heuristic.
If we're just testing broker compatibility I don't think we even need this part of the test.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Thanks - I've replaced all uses of `filter` with comprehensions.
This will fail on python3 for a similar reason as the conditional check noted above. filter returns a generator. The generator can't be indexed. Use a list comprehension instead: ``` python block = [b for b in blocks if b.base_ip == base_ip] ```
As discussed on IRC: no.
This is a bit confusing since it's assigning to the same name name as comes in. I think this section could be cleared up with the use of `set`. So something along the lines of: ``` for dead_tag in set(have_tag_keyvals).difference(want_tag_keyvals): dead_tags.append(..... and so on ...) ```
Should be "Can't detect any" rather than "none" I think.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
We tend to sort the imports alphabetically. It's a common thing in python.
This is also why shippable is failing.
```suggestion EPOCH_TS = 1594449296.124356 ```
Originally, there was `124356` microseconds in unit tests, instead of `123456`. At least two tests are rely on this figure.
Looks like values below in `DT` and `DT_UTC` are 124356, instead of 123456.
@samdoran then, I'd say that there's something wrong with the setup and TZ should be adjusted somewhere.
+1 it's better to just patch `time.time`
I think we don't need to patch `datetime.datetime.fromtimestamp()` and `datetime.datetime.utcfromtimestamp()` methods as they just return a datetime object from patched `time.time()`.
I've changes to `django.utils.inspect.func_supports_parameter()`.
A small oversight I noticed in an old Python 3.7.0 virtualenv: https://github.com/django/django/pull/13393
Minor but I'd move this control flow block after the `weights` one to match the args order.
can we factor the lentient handling part out in a single mehtod? ``` private Query rethrowUlessLentient(RuntimeException e) { if (settings.lenient()) { return null; } throw e; } ``` man I with we had support for annonymous functions here or macros even :)
[This is only an estimate on InnoDB tables](https://dev.mysql.com/doc/refman/5.7/en/tables-table.html) which is the default table engine and what's used on CI. > The number of rows. Some storage engines, such as MyISAM, store the exact count. For other storage engines, such as InnoDB, this value is an approximation, and may vary from the actual value by as much as 40% to 50%. In such cases, use SELECT COUNT(*) to obtain an accurate count. In short that means this value could report 0 while there's actually rows in the table and cause errors similar to the one you are experiencing.
I believe you can just drop the `== 0` case here. Doing `DELETE FROM` on 0 rows should be harmless. No need to `SELECT COUNT(*)`. You can also find out if a table has >1000 rows without counting everything using ```sql SELECT COUNT(*) > 1000 FROM (SELECT * FROM table_name LIMIT 1001) SUBQUERY; ``` Which returns '1' (true) only if it does have >1000 rows. But I don't think we need that here for the time being, the approx row count should be fine as a heuristic.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
Should this be ``` python checked_list.sort(key=lambda x: sorted(list(x.items())) if isinstance(x, dict) else x) ``` tracebacks from test failures in CI make me suspect the view object from set(x.items()) fails because set() wants hashable items and the view object from x.items() is not. Does wrapping it in a list() or tuple() work? ``` 2017-06-26 16:43:33 The full traceback is: 2017-06-26 16:43:33 Traceback (most recent call last): 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_module_ecs_ecr.py", line 304, in run 2017-06-26 16:43:33 policy = sort_json_policy_dict(policy) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 603, in sort_json_policy_dict 2017-06-26 16:43:33 ordered_policy_dict[key] = value_is_list(value) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 595, in value_is_list 2017-06-26 16:43:33 checked_list.sort(key=lambda x: sorted(set(x.items())) if isinstance(x, dict) else x) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 595, in <lambda> 2017-06-26 16:43:33 checked_list.sort(key=lambda x: sorted(set(x.items())) if isinstance(x, dict) else x) 2017-06-26 16:43:33 TypeError: unhashable type: 'list' ```
Minor but I'd move this control flow block after the `weights` one to match the args order.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
As discussed on IRC: no.
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Minor but I'd move this control flow block after the `weights` one to match the args order.
```suggestion for table_name, table_rows in rows: ```
I believe you can just drop the `== 0` case here. Doing `DELETE FROM` on 0 rows should be harmless. No need to `SELECT COUNT(*)`. You can also find out if a table has >1000 rows without counting everything using ```sql SELECT COUNT(*) > 1000 FROM (SELECT * FROM table_name LIMIT 1001) SUBQUERY; ``` Which returns '1' (true) only if it does have >1000 rows. But I don't think we need that here for the time being, the approx row count should be fine as a heuristic.
[This is only an estimate on InnoDB tables](https://dev.mysql.com/doc/refman/5.7/en/tables-table.html) which is the default table engine and what's used on CI. > The number of rows. Some storage engines, such as MyISAM, store the exact count. For other storage engines, such as InnoDB, this value is an approximation, and may vary from the actual value by as much as 40% to 50%. In such cases, use SELECT COUNT(*) to obtain an accurate count. In short that means this value could report 0 while there's actually rows in the table and cause errors similar to the one you are experiencing.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
A general remark: you should always use complete sentences. So this should end with a period.
(In general, I don't think modules should have such options.)
You should emphasize that the module can and will not do any idempotence checking for this.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
Please remove deprecated parameters as this is new module.
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
`# Filter out fields contributed....` (chop comma before as)
Chop "Helper function that" "Return ..." (use PEP 257 verb style)
Put the close ] on the next line.
`"""Return the index at which the ordering expressions start."""`
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Please remove deprecated parameters as this is new module.
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
We should pass `using` from the queryset ```suggestion ct_id = self.get_content_type(model=model_cls, using=ct_queryset.db).pk ```
returning `None` isn't perfectly equivalent to `continue`. I think it might be cleaner to move some or all of `_handle_object` back into `__iter__`. This allows us to use `continue` again.
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
while purging all username we should preserve the username which is used to run the playbook otherwise we might hit connection timeout in middle and leave the box with partial configurations
It's better to actually say that there's no file in place or it's inaccessible.
True, I mixed up sth when switching branches. Nevertheless they work on all databases except PostgreSQL, so we need to fix this.
Cases from lines 361 and 363 work with the previous implementation.
`check_args()` is a empty function. Is this call required? For other networks platforms `check_args()` is present for legacy reason.
We don't need to support different bases or other negation signs so I would expect a simplified implementation.
```suggestion return '-' + value if neg else value ```
Same. The `filter` doesn't make sense to me
```suggestion return '-' + value if neg else int(value) ```
Can we also test a conditional annotation? e.g. ``` .annotate(is_x_positive=Case(When(x__gt=0, then =True), default=False)).filter(is_active=~F('is_x_positive')) ```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Trailing commas: ```suggestion MAX_NUM_FORM_COUNT: self.max_num, }, renderer=self.renderer, ```
`self._test_scrypt_upgrade('parallelism', 'parallelism', 2)` fails, it seems we shouldn't take `parallelism` into account.
We confirmed that `parallelism` should be taken into account.
> My main worry here is: Is this correct and does it make sense to implement for such a complex hasher (notably we already have others where we argue it is simply not possible in a sensible way). > > Since scrypt can raise errors like this: > > > ValueError: Invalid parameter combination for n, r, p, maxmem. > > I am wondering if `must_update` couldn't also trigger this condition. Or can we always calculate `extra_iterations` and `extra_block` and be sure that the combinations are valid? You're right parameters may no be valid, e.g. ``` self.work_factor = 2 **14 decoded['work_factor'] = 2 ** 11 ``` both are a power of 2, however `extra_iterations = 14336` is not and raises `ValueError: n must be a power of 2`.
We should indicate that this is only a guess - the value may be smaller or larger than the actual size.
Should also include `block_size` and `parallelism`
yes, we've been wanting to change that for a while but were waiting until we made the architecture more pluggable to allow for old/new formats to be used transparently.
CTR mode doesn't actually require padding, so this is unnecessary. That said, I assume you're staying compatible with existing vault implementations which already do this. It's not a security thing, just a few wasted bytes/CPU cycles.
Should also update based on `block_size` and `parallelism`
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
> this test is checking the support for a possible object that is seekable but has no `.tell()`. > `tempfile.NamedTemporaryFile()` has `.tell()`. Just as an aside, whether there is any change here or not, but you could do: ```python with tempfile.NamedTemporaryFile() as tmp: del tmp.tell # Emulate seekable file handle without .tell(). ... ```
identical here for pod,
`not_data_actions` + tests (same as above)
Use a more clear name to better reflect what the function is actually doing. It doesn't just check availability but also mutates state. ```suggestion def try_import(self): ```
Again, this doesn't look like an availability check.
Not sure it makes a difference but before it looks like we got `form=None` in the context.
Ahhh, yes, thanks!
Using this directly is discouraged: https://docs.python.org/3/library/importlib.html#importlib.__import__ > Note Programmatic importing of modules should use import_module() instead of this function.
alright. let's keep it as is.
seems not used later in code
This error is raised when instantiating so we don't need to include a `route` in the message.
I would revert this change. We want to add a system check so this seems redundant.
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
As long as they both equals it shouldn't make a difference. By that I mean you should have a test asserts `Index('foo') == Index(F('foo'))` An alternative could be to defer that to `create_sql`.
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
Missing `=dict` on this and the next few lines
Please open a ticket to track the bug (all non-trivial changes should have a ticket).
hmmm.. now I think I'm wrong, you want to only have the first partial func.. `func.func` shouldn't anyway be another partial or else that would be problematic I think, as views have to return something else.. Sorry for disturbance ;)
didn't knew about RegexURLPattern having the same method.. reading the docstring I'm thinking both should do this: ``` python # .. if isinstance(func, functools.partial): while isinstance(func, functools.partial): func = func.func # ... ``` shouldn't it ? again that's my wild guess here, I don't have read more about the context of how this method is used (nor the other one), etc.. So I could be simply wrong.
happy new year ! yeah effectively finally this is required for python < 3.5 as you explain (I just tested it in an interactive session). I'm happy to open a PR with this specific change so.. should not be too long to come..
Please do not remove this trailing comma, this is our preference with dicts and lists to minimize future diffs when lines are changed.
This must be checked **before** any processing.
This must be checked **before** any processing.
Breaks if not `int`.
I don't see a reason we can't use Python's `hash()` builtin, which is even faster and cached on strings I also don't think we need a class here - a single function to do the shuffling would do.
is this test needed? I'd suspect it's already tested.
you could also write this as: h_item = search_obj_in_list(w_item['member'], h, key='member') or {}
under what circumstances do we need this? My system has 'UTF-8', so it's not very exciting as that's the default for these functions.
need to x2 check tmrow, but this should be an error, we don't allow for circular parenting
I think [`_extract_m3u8_formats`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L847) does the work better. If the existing method does not fit the need, feel free to modify it.
I think that this can be replaced with the following as above: ```python @unittest.skipIf(geos_version_tuple() < (3, 7), 'GEOS >= 3.7.0 is required') ``` I think we only need to worry about testing the GEOS behaviour if it is actually supported; even if this is only testing the exception-raising behaviour.
Unfortunately annotation names can contain LOOKUP_SEP - that is the reason why the ugly refs_aggregate method was added. For example qs.annotate(Max('id')) will create an annotation named max__id, and that can't be referred if the code checks for len(field_list) == 1.
should probably use `if stdout is not None:`
```suggestion Test that the returned value for timezone consists of only uppercase ```
extra space after ,
Line is too long.
Line is too long.
Line is too long.
Line is too long.
Too long line.
This doesn't support aurora snapshots. Besides that, this looks great.
```suggestion SubnetIds=module.params.get('subnet_ids'), ``` and here as well
```suggestion SubnetIds=module.params.get('subnet_ids'), ``` Here this var needs to stay as you had it originally - ansible prefers snake cased but boto typically needs camelcase. `instance_parameters` will be passed into the boto connection so needs to match what the API expects, both here and later when you access the returned parameters. https://docs.aws.amazon.com/dms/latest/APIReference/API_CreateReplicationSubnetGroup.html#API_CreateReplicationSubnetGroup_RequestSyntax
That is correct, argument spec is dynamic. If it is not dynamic, then any change to the API would render this module useless. Also, it would require me to write modules for every single API endpoint. There are some specific modules - https://github.com/willwagner602/ansible/tree/will-devel/lib/ansible/modules/network/fortios but there are a lot more of endpoints. So it made sense to let this core module build dynamic argument spec based on the data received from the API.
And just to clarify - currently the argument spec per API endpoint is being cached locally. So if API changes, the file has to be deleted and then it can be recreated again. Here's an example of this file after running a few playbooks: https://pastebin.com/CcPuMQCu
OK, I feel like it makes sense for the modules that are using screen scraping via SSH, but I see a lot of value of treating API calls as generic as possible. I'll hold off on making any changes to the module until the core members provide feedback. I'll be happy to attend the meeting to explain my point of view to the core developers.
I think that makes me even more concerned. You are saying that the argument spec that ansible will use becomes dynamic based on responses of APIs? I'm pretty sure that's not going to be allowed.
Due to this, at the moment I am denying acceptance of this PR. I'll ask to get additional feedback from some other core developers, but we have explicitly denied modules that attempted to address arbitrary API functionality like this in the past. The recommended way would be to write individual modules for individual parts of functionality.
I'm not fully sure what this is yet, but I would recommend renaming this function, as it is likely to cause confusion for devs/user due to ansible already calling something an "argument_spec"
We have historically not allowed generic API modules. We generally require a module to be a specific set of features that perform an action. This would be in opposition to that stand point.
2.6 or 2.7? Also you `requirements` listed here and the modules.
What kind of values are valid here? I would expect this to be `float` based on the `0.0` in the description.
A general remark: you should always use complete sentences. So this should end with a period.
Please use formatting like `C(<device-on-host>[:<device-on-container>][:<permissions>])`, and `(e.g. device C(/dev/sdc:/dev/xvdc:rwm))` in the line below.
I would remove this `device` here, and similar the option name in the examples for `device_read_bps`, `device_read_iops` etc.
(Same for the related options.)
I would remove all aliases if possible.
(In general, I don't think modules should have such options.)
You should emphasize that the module can and will not do any idempotence checking for this.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
`del` is a builtin, not a function. These parens don't have to be here
Unlike an "app", I wouldn't say that a model is "installed". Two solutions: - check if the app is installed and adjust the error message accordingly: "app X isn't installed" or "app X doesn't provide model Y" - just rephrase do "which doesn't exist"
It would have been more appropriate if `run_checks()` took an `Apps` instance as a parameter instead of a list of `AppConfig`s.
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
I don't think extending `Subquery.__init__` to allow any `QuerySet` some method (e.g. `filter`, `order_by`, ...) is desirable.
~if not keeping encryption, remove the __ansible_vault key, just assign the value directly~ nmvd, misread the if to the opposite ...
Test failure is because this is missing `geography=True` on PostGIS.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
colors should all be configurable
Put the close ] on the next line.
This can instead be `continue` and let the `else` unnest.
`SearchedCase` is no longer it's own type so this should be named `Case conditions must be lookups`. Alternatively, do this checking directly inside the `When` types, since users will be using these directly.
Why we don't use `super().deconstruct()`? ```python def deconstruct(self): path, args, kwargs = super().deconstruct() kwargs['expressions'] = self.expressions if self.condition is not None: kwargs['condition'] = self.condition if self.index_type != 'GIST': kwargs['index_type'] = self.index_type return path, args, kwargs ```
It will be `django.contrib.postgres.constraints.XXX` but I don't think that's an issue for core constraints we return `django.db.models.XXX`.
Also here: ```python if self.index_type.lower() != 'gist': ```
We can use `compiler = query.get_compiler(connection=schema_editor.connection)`
All tests work without it that's why I'm wondering if we need it.
Also make the comparison case-insensitive, i.e. ```python if self.index_type.lower() != 'gist': ```
I would move it to `_init__()`: ```python def __init__(self, *, name, expressions, index_type=None, condition=None): ... self.index_type = index_type or `GIST` self.condition = condition super().__init__(name=name) ``` and skip the default in `deconstruct()` and `__str__()`: ```python def deconstruct(self): ... if self.index_type != 'GIST': kwargs['index_type'] = self.index_type ```
Maybe `_parse_where()` -> `_get_condition_sql()`, I would return condition and add `WHERE` in `constraint_sql()`, e.g. ```python def _get_condition_sql(self, compiler, schema_editor, query): if self.condition is None: return None where = query.build_where(self.condition) sql, params = where.as_sql(compiler, schema_editor.connection) return sql % tuple(schema_editor.quote_value(p) for p in params) def constraint_sql(self, model, schema_editor): ... condition = self._get_condition_sql(compiler, schema_editor, query) return self.template % { ... 'where': ' WHERE (%s)' % condition if condition else '', } ```
IMO it will me more readable to move `base` outside, also we resolve `F()` expressions twice (line 28 & 29), so maybe, e.g. ```python if isinstance(expression, str): expression = F(expression) if isinstance(expression, F): expression = base.resolve_expression(query=query, simple_col=True) else: expression = expression.resolve_expression(query=query) ``` Moreover I think we don't need to iterate twice over the expressions: ```python for expression, operator in self.expressions: ... expressions.append('%s WITH %s' % (sql % params, operator) return expressions ```
Using `str('name=Hello%20G%C3%BCnter')` would also work here but using `six.PY2` could be a nice reminder to remove the `b'...'` branch.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
Could you keep `'MANIFEST.json'` in a var? I was hoping to extract https://github.com/ansible/ansible/blob/4a82e2c/lib/ansible/galaxy/dependency_resolution/dataclasses.py#L44 into a publicly exposed constant at some point...
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
I don't think you should re-number the existing tests.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
`del` is a builtin, not a function. These parens don't have to be here
```suggestion template_name = 'forms_tests/form_snippet.html' ```
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
You don't need this conditional, since Ansible enforces that these are the only choices.
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
please fail if required stuff is null
looks like there are two levels of indentation instead of one
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Missing space after the `for`
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: cannot create point collection with empty set of points at __randomizedtesting.SeedInfo.seed([9959A23819CF838B:6FE2182D9705AE]:0) at org.elasticsearch.common.geo.builders.ShapeBuilder.<init>(ShapeBuilder.java:97) at org.elasticsearch.common.geo.builders.MultiPointBuilder.<init>(MultiPointBuilder.java:46) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseMultiPoint(GeoWKTShapeParserTests.java:82) ... ```
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
nit: formatting, add some whitespaces
nit: formatting, add some whitespaces
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
please fail if required stuff is null
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
looks like there are two levels of indentation instead of one
`del` is a builtin, not a function. These parens don't have to be here
Missing space after the `for`
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: cannot create point collection with empty set of points at __randomizedtesting.SeedInfo.seed([9959A23819CF838B:6FE2182D9705AE]:0) at org.elasticsearch.common.geo.builders.ShapeBuilder.<init>(ShapeBuilder.java:97) at org.elasticsearch.common.geo.builders.MultiPointBuilder.<init>(MultiPointBuilder.java:46) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseMultiPoint(GeoWKTShapeParserTests.java:82) ... ```
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
nit: formatting, add some whitespaces
Again, error handling changed.
`Could not recursively set attri...`
Please convert this to just a list comprehension: ```suggestion groups = [e.name for e in entities if isinstance(e, Group)] ``` Additionally, just convert to a list of names here, since you only ever need that, and not the object later.
Use dict literals: ```suggestion return {} ```
I know you asked me about this but it might be a good idea to explain that PowerShell unpacks arrays if there's only a single entry so this is a defensive check on that.
```suggestion b_opath = os.path.realpath(to_bytes(os.path.join(path, group))) ```
Use a tuple as a the cache key instead of stringifying this. This will likely result in a performance problem later if not resolved now. ```suggestion key = tuple(groups) + (b_opath,) ```
The `to_bytes` should be on the `self._basedir` instead: ```suggestion b_opath = os.path.realpath(os.path.join(to_bytes(self._basedir), b'r_group_vars')) ```
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
Returns -> Return use period
needs a space before the quote here too
The log message makes no sense. You are trying to normalize (or rather kludge) something that should be a locale to be a language code/tag, to then convert to a locale. But you are saying the locale has been normalized to a language code/tag which is incorrect. A language code/tag is not a locale, and here we *should be providing a locale*, hence my misgivings already stated about this whole thing. Regardless, you probably want to do this: ```python def normalize_locale(original, stdout): """ Normalizes incorrect locale strings, e.g. zh-cn, zh_cn, ZH-CN are converted to zh_CN. """ corrected = to_locale(original.lower().replace('_', '-')) if original != corrected: stdout.write('Normalized %s to %s.' % (original, corrected)) return corrected ```
Oh yes, there definitely was some confusion. When looking at the code, I saw references to `obj.pk` and thought we were potentially dealing with a QuerySet (which I guess that _technically_ we _could_ be, but that wouldn't be the right way to use it). My apologies! Carry on. ðŸ˜‹
not a show stopper, but the code might be clearer if we just add the '-n' and '%s'/dir_arg in the `if/else` and just execute `run_command` at the end
you can use `state` to avoid the 'or' to the user
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
bump (it looks like this didn't get changed)
Might be nice for demonstration purposes if the two records actually have different keys. Maybe: ```suggestion aTopic.pipeInput(1, "999-alpha"); bTopic.pipeInput(999, "beta"); ```
Might want to use simple quote here.
Read also https://www.ianlewis.org/en/mixins-and-python
The code I've reviewed recently always has mixins on the left. I think it's easier to be consistent than have to think about it, but feel free to propose some different guidelines if you like.
Referring specifically to the kubeconfig definition, the remote file path will cover a lot of use cases but I was wondering if it would be useful to be able to specify it inline as well, which would open the door for keeping your authentication with your ansible playbooks/roles.
This should be: ```yaml type: bool ```
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
This isn't compatible with Python 2.4, which is causing it to fail the py24 tests: ``` 2016-08-11 20:25:51 + python2.4 -m compileall -fq -x 'module_utils/(a10|rax|openstack|ec2|gce|docker_common|azure_rm_common|vca|vmware|gcp|gcdns).py' lib/ansible/module_utils 2016-08-11 20:25:51 Compiling lib/ansible/module_utils/cloud.py ... 2016-08-11 20:25:51 File "lib/ansible/module_utils/cloud.py", line 86 2016-08-11 20:25:51 except Exception as e: 2016-08-11 20:25:51 ^ 2016-08-11 20:25:51 SyntaxError: invalid syntax ``` You can use this instead: ``` python except Exception: e = get_exception() ```
This should be `2.6` currently. If it is not merged by code freeze on Friday the 25th it will need to be `2.7`.
Hm, given those 15(?) other tests that failed on Python 3 in calls to syslog.syslog(), could we monkey-patch the rest of the tests so this is the only test that actually writes to the real syslog/journal? (EDIT: when I say "the rest of the tests", I mean in test_basic.py.)
The colon is causing the docs-build error.
Perhaps say that the repartitioning will happen automatically, i.e., user doesn't need to do anything.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Similarly, ```if tc['skip'].get('i')```
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This code is almost the same as in `replace_unnamed_groups()`, the only difference is that the beginning of non-capturing group is longer i.e. `'(?:'` instead of `'('`. We could add an internal hook and use it in both places, e.g. ```python def _find_groups(pattern, group_matcher): group_indices = [ (m.start(0), m.end()) for m in non_capturing_group_matcher.finditer(pattern) ] # Loop over the groups. for start, end in unnamed_group_indices: ... for idx, val in enumerate(pattern[end:]): ... if unmatched_open_brackets == 0: group_indices.append((start, end + idx + 1)) break # Remove unnamed group matches inside other unnamed capture groups. group_start_end_indices = [] prev_end = None for start, end in group_indices: if prev_end and start > prev_end or not prev_end: group_start_end_indices.append((start, end)) prev_end = end return group_start_end_indices ``` Moreover, with some boolean flags (e.g. `named=True/False`) this could also be reused in `replace_named_groups()` :thinking: .
What's the purpose of http://example.com/v.flv here? It always gives a 404 error and I think it's unrelated to iQiyi
need to x2 check tmrow, but this should be an error, we don't allow for circular parenting
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
Thanks :+1: , IMO a separate ticket is not necessary, we can `Refs #33348 -- ...`.
Can we deprecate passing `None` in `errors` in a separate PR? and do this in advance. There is already a lot of changes in this patch, that should make it more readable and easier to review. I can try to refactor this out if you don't have time to keep working on this.
no restructured text (:class:) in docstrings please
Is there a typo? I'm not sure what "hub" means in this sentence.
Can you name this a little more verbosely? I can't unsee "get best"
the `f.close()` is implied as part of the with block. Also, I don't think this should be in the module since it looks like a debug statement.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
This isn't needed, since we now only support 2.6+ anyway.
These parens aren't necessary for unpacking the return values.
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
2.6 or 2.7? Also you `requirements` listed here and the modules.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
okay, but would be helpful to say _why_ we need to always return True.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
This could be a bare `super()`.
a single file or a list ...
``` return files.getlist(name) if self.multiple else files.get(name) ```
Having said that, I'm immediately not even sure that we need the release note: `MultipleHiddenInput` is just broken no? (I guess it doesn't hurt...)
IMO we don't need a new parameter, the previous behavior was incorrect. We should use different `name`'s for all widgets (like `MultiWidget`). \cc @carltongibson
This one as well.
This and one above, replace `self.function = ..` with `function=...` in as_sql method call.
We might want to mention we use `super()` to avoid checking against `self.filter`.
Oh, I just meant to suggest the `value` to `values` (plural) renaming, not more...
I think that property name i.e. `order_by_expression`, `order_by_f_expression`, or `order_by_orderby_expression` is a sufficient description.
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
2.6 or 2.7? Also you `requirements` listed here and the modules.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
okay, but would be helpful to say _why_ we need to always return True.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
the `f.close()` is implied as part of the with block. Also, I don't think this should be in the module since it looks like a debug statement.
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
Can you name this a little more verbosely? I can't unsee "get best"
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
This isn't needed, since we now only support 2.6+ anyway.
These parens aren't necessary for unpacking the return values.
I would multiline: ``` field.attname for field in self.lookup_opts.fields if field.unique and not field.null ```
```python total_ordering_fields = {'pk'} | { field.attname for field in self.lookup_opts.fields if field.unique and not field.null } ```
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
``` # Transform minus sign prefixed strings into an OrderBy() expression. ordering = [ (OrderBy(F(o[1:]), descending=True) if isinstance(o, str) and o[0] == '-' else o) for o in ordering ] ```
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
`# Filter out fields contributed....` (chop comma before as)
Put the close ] on the next line.
`"""Return the index at which the ordering expressions start."""`
Turn (capitalize) add period. sql -> SQL
May as well do the following as a field name can only legally have a single `-` at the start: ```python field_name = part.lstrip('-') ```
@jrwdunham I think you can drop this if, yes
@jrwdunham I'm not an expert here, but from what I read the true assumption is that `SCRIPT_NAME` **never** ends with trailing slash. So in case you do not have subfolder, `SCRIPT_NAME` should be just empty/unset.
Does this check make sense now that we only handle it if it starts with a relative path? (ie if the prefix where there it would start with a slash anyways)
Can we deprecate passing `None` in `errors` in a separate PR? and do this in advance. There is already a lot of changes in this patch, that should make it more readable and easier to review. I can try to refactor this out if you don't have time to keep working on this.
Thanks :+1: , IMO a separate ticket is not necessary, we can `Refs #33348 -- ...`.
no restructured text (:class:) in docstrings please
Is there a typo? I'm not sure what "hub" means in this sentence.
Can you name this a little more verbosely? I can't unsee "get best"
This isn't needed, since we now only support 2.6+ anyway.
it seems this and other parameters are missing from docs
@jrwdunham I think you can drop this if, yes
@jrwdunham I'm not an expert here, but from what I read the true assumption is that `SCRIPT_NAME` **never** ends with trailing slash. So in case you do not have subfolder, `SCRIPT_NAME` should be just empty/unset.
Does this check make sense now that we only handle it if it starts with a relative path? (ie if the prefix where there it would start with a slash anyways)
Can we deprecate passing `None` in `errors` in a separate PR? and do this in advance. There is already a lot of changes in this patch, that should make it more readable and easier to review. I can try to refactor this out if you don't have time to keep working on this.
Thanks :+1: , IMO a separate ticket is not necessary, we can `Refs #33348 -- ...`.
no restructured text (:class:) in docstrings please
Is there a typo? I'm not sure what "hub" means in this sentence.
Can you name this a little more verbosely? I can't unsee "get best"
This isn't needed, since we now only support 2.6+ anyway.
it seems this and other parameters are missing from docs
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
FYI, in case you switch to working on whole config, `get_config.config` gives you the whole tree. (`get_config(:foo).config` is generally equivalent to `get_config.config[:foo]`.) Also, VMDB::Config is deprecated. May be better to use `::Settings.to_hash`.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
Same here, seems a ValueError would be cleaner.
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
I was thinking to assign the group permissions at the beginning of the test case so you can check all three together and not need the second round of tests along with setting the user back to `is_active=True`. Also, `codename='test_(user|group)'` would be helpful.
Can you name this a little more verbosely? I can't unsee "get best"
These parens aren't necessary for unpacking the return values.
Thanks :+1: , IMO a separate ticket is not necessary, we can `Refs #33348 -- ...`.
Can we deprecate passing `None` in `errors` in a separate PR? and do this in advance. There is already a lot of changes in this patch, that should make it more readable and easier to review. I can try to refactor this out if you don't have time to keep working on this.
no restructured text (:class:) in docstrings please
Is there a typo? I'm not sure what "hub" means in this sentence.
Can you name this a little more verbosely? I can't unsee "get best"
the `f.close()` is implied as part of the with block. Also, I don't think this should be in the module since it looks like a debug statement.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
This isn't needed, since we now only support 2.6+ anyway.
These parens aren't necessary for unpacking the return values.
you might want to get the module/action as the include can be given a name that does not match `include:`
with -> width
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
comma after tuple
flake8 complains about missing spaces around `*`
width, height, and offset
```suggestion raise AnsibleError('Invalid setting identifier, "%s" is not a string, it is a %s' % (term, type(term))) ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
```suggestion # the variable if it hasn't been set by the user already. ```
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
flake8 complains about missing spaces around `*`
I don't think boto_params will include access_key or secret_key if a profile is being used (which might be through passing `profile` parameter or with `AWS_PROFILE` set) Looking at https://github.com/jmenga/requests-aws-sign the way to do it is to use ``` session = session.Session() credentials = session.get_credentials() ``` but even there, we'd need to cope with the `profile` parameter.
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
Here, `self.count_upgrade` is an int, and `outdated` (as above) a `dict` resp. `list`.
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
Should be interesting to return data to the user, like variables `added`, `updated`, `removed
Here, `outdated` is a `dict` (or `list` in mheap/ansible#3).
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
2.6 or 2.7? Also you `requirements` listed here and the modules.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
This code is almost the same as in `replace_unnamed_groups()`, the only difference is that the beginning of non-capturing group is longer i.e. `'(?:'` instead of `'('`. We could add an internal hook and use it in both places, e.g. ```python def _find_groups(pattern, group_matcher): group_indices = [ (m.start(0), m.end()) for m in non_capturing_group_matcher.finditer(pattern) ] # Loop over the groups. for start, end in unnamed_group_indices: ... for idx, val in enumerate(pattern[end:]): ... if unmatched_open_brackets == 0: group_indices.append((start, end + idx + 1)) break # Remove unnamed group matches inside other unnamed capture groups. group_start_end_indices = [] prev_end = None for start, end in group_indices: if prev_end and start > prev_end or not prev_end: group_start_end_indices.append((start, end)) prev_end = end return group_start_end_indices ``` Moreover, with some boolean flags (e.g. `named=True/False`) this could also be reused in `replace_named_groups()` :thinking: .
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
This isn't needed, since we now only support 2.6+ anyway.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
Same here, seems a ValueError would be cleaner.
Can you name this a little more verbosely? I can't unsee "get best"
This isn't needed, since we now only support 2.6+ anyway.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
I was thinking to assign the group permissions at the beginning of the test case so you can check all three together and not need the second round of tests along with setting the user back to `is_active=True`. Also, `codename='test_(user|group)'` would be helpful.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
pass original exception as orig_exc so traceback can be shown with -vvv
2.6 or 2.7? Also you `requirements` listed here and the modules.
Since it's the only plugin which does that, I would remove it. Either all plugins should do that, or none.
not sure this debug is useful
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Should be interesting to return data to the user, like variables `added`, `updated`, `removed
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
Please follow best practice for this fail message ```python if not HAS_GITLAB_PACKAGE: module.fail_json(msg=missing_required_lib("python-gitlab"), exception=GITLAB_IMP_ERR) ```
It's better to actually say that there's no file in place or it's inaccessible.
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
comma after tuple
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
You don't need this conditional, since Ansible enforces that these are the only choices.
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
I would rather see ValueError instead of general exception
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
comma after tuple
with -> width
width, height, and offset
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
This isn't needed, since we now only support 2.6+ anyway.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
comma after tuple
with -> width
2.6 or 2.7? Also you `requirements` listed here and the modules.
width, height, and offset
Similarly, ```if tc['skip'].get('i')```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
I find this error message very hard to understand. I don't have any experience with GitLab servers, though :) If the other gitlab_* modules have the same text, we should probably keep it here as well. Changing it could be done in another PR once this is merged.
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
This code is almost the same as in `replace_unnamed_groups()`, the only difference is that the beginning of non-capturing group is longer i.e. `'(?:'` instead of `'('`. We could add an internal hook and use it in both places, e.g. ```python def _find_groups(pattern, group_matcher): group_indices = [ (m.start(0), m.end()) for m in non_capturing_group_matcher.finditer(pattern) ] # Loop over the groups. for start, end in unnamed_group_indices: ... for idx, val in enumerate(pattern[end:]): ... if unmatched_open_brackets == 0: group_indices.append((start, end + idx + 1)) break # Remove unnamed group matches inside other unnamed capture groups. group_start_end_indices = [] prev_end = None for start, end in group_indices: if prev_end and start > prev_end or not prev_end: group_start_end_indices.append((start, end)) prev_end = end return group_start_end_indices ``` Moreover, with some boolean flags (e.g. `named=True/False`) this could also be reused in `replace_named_groups()` :thinking: .
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
Why not also do this when `module.check_mode` is `True`? Then the check mode behavior is closer to the real thing.
I would also detect if the value is a bool and set it accordingly to `on`/`off`.
I would rather see ValueError instead of general exception
```suggestion Kwargs: ```
with -> width
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
width, height, and offset
comma after tuple
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
I would chop blank lines in this test.
Chop `Ensure that`.
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
Just in case it's probably a good idea to use `.get()` instead of dict access syntax in case the key is unset.
Similarly, ```if tc['skip'].get('i')```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
This code is almost the same as in `replace_unnamed_groups()`, the only difference is that the beginning of non-capturing group is longer i.e. `'(?:'` instead of `'('`. We could add an internal hook and use it in both places, e.g. ```python def _find_groups(pattern, group_matcher): group_indices = [ (m.start(0), m.end()) for m in non_capturing_group_matcher.finditer(pattern) ] # Loop over the groups. for start, end in unnamed_group_indices: ... for idx, val in enumerate(pattern[end:]): ... if unmatched_open_brackets == 0: group_indices.append((start, end + idx + 1)) break # Remove unnamed group matches inside other unnamed capture groups. group_start_end_indices = [] prev_end = None for start, end in group_indices: if prev_end and start > prev_end or not prev_end: group_start_end_indices.append((start, end)) prev_end = end return group_start_end_indices ``` Moreover, with some boolean flags (e.g. `named=True/False`) this could also be reused in `replace_named_groups()` :thinking: .
we want want -> we want
I would rather see ValueError instead of general exception
It's not actually a comprehension - this could just use a tuple literal.
@Ian-Foote thanks for the clarification I always mix up the two terms.
`if it encounter` => `if it encounters`
`mentionned` => `mentioned`
Missing `=dict` on this and the next few lines
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
missing space between `,` and `and`
It's not actually a comprehension - this could just use a tuple literal.
@Ian-Foote thanks for the clarification I always mix up the two terms.
`if it encounter` => `if it encounters`
`mentionned` => `mentioned`
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
For integers please: use `type='int` remove the `isdigit` check from `check_params`
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
missing space between `,` and `and`
This line is related to the `save` method, and should sit beneath it, rather than the new `get_updated_model` method.
please include the trailing comma on the last item in a dictionary so if more items are added we don't have to modify this line again
chop newline for consistency with other tests
I tried a similar approach while working on acfaec3db5ba39de52f6e607e74343dccf72fba1 and came to the conclusion that this approach can't work (due to something like module caching). As far as I know, you'll have to register the admin to a separate `AdminSite`.
I think there isn't much organization there. Using an existing site should be fine.
`parallell` => `parallel` `get_upodated_models` => `get_updated_models`
First we should verify this passes before we toggle `is_active` to False.
> Shall I also remove `test_target_model_is_unsaved_save` and `test_target_model_is_unsaved_bulk_create`? Yes, they are redundant.
Prefer this indent style: ``` response = self.client.post(reverse('...'), { .... }) ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Seems like it would result in less confusing code in the long run. If you do defer it and leave the TODO, I'd suggest to use your GitHub username instead of first name.
For clarity here, shouldn't we use `Func` rather than `Transform`, since they are equivalent and the latter is a back-compat-only name? It seems like using `Transform` might suggest to someone reading this code that there's something distinct about `Transform` as opposed to `Func`.
IIRC get_cols() was renamed to get_group_by_cols() some time ago - no need to have both.
This seems problematic. What happens if you do this: ``` reused_f = F('pk') qs1 = Model1.objects.filter(pk=reused_f) qs2 = Model2.objects.filter(pk=reused_f) ``` By my reading the reused_f will be first prepared for qs1, getting Model1.pk as col. Then the same object is reused and prepared for qs2, this time the reused_f's col is overwritten with Model2.pk. The ExpressionNodes must be either stateless, or they must be copied on prepare(). Doing the latter will be easier. A basic copy method could do the following: ``` def copy(self): copy = Empty() copy.__class__ = self.__class__ copy.__dict__ = self.__dict__.copy() return copy ```
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
I realize this predates your PR, but why is 'unused' not used? It appears to be the target field provided by the SQLUpdateCompiler during as_sql(). This seems like a really good spot to allow a backend to hook in and CAST the expression to a type that is allowed by the target column.
okay, but would be helpful to say _why_ we need to always return True.
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
Ah, good. Widgets... I think something like `formset_class=formset_class.__name__` would be clearer than the HTML string. Then at least you'd get this: ``` FAIL: test_formsets_with_order_custom_widget (tests.forms_tests.tests.test_formsets.FormsFormsetTestCase) [<object object at 0x10456f0a0>] (formset_class='OrderingMethodFormSet') ``` ... which clearly tells you which case went wrong.
~if not keeping encryption, remove the __ansible_vault key, just assign the value directly~ nmvd, misread the if to the opposite ...
We could keep tests more DRY and use loop (maybe with `subTest`) e.g. ```python def test_formsets_with_order_custom_widget(self): class OrderingAttributFormSet(BaseFormSet): ordering_widget = HiddenInput class OrderingMethodFormSet(BaseFormSet): def get_ordering_widget(self): return HiddenInput for formset in (OrderingAttributFormSet, OrderingMethodFormSet): ArticleFormSet = formset_factory(ArticleForm, formset=formset, can_order=True) ... ```
Use hanging indent as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style). ``` values = AggregateTestModel.objects.aggregate( arrayagg=ArrayAgg('char_field', ordering=ordering) ) ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
Chop "Helper function that" "Return ..." (use PEP 257 verb style)
`"""Return the index at which the ordering expressions start."""`
Put the close ] on the next line.
Ah, good. Widgets... I think something like `formset_class=formset_class.__name__` would be clearer than the HTML string. Then at least you'd get this: ``` FAIL: test_formsets_with_order_custom_widget (tests.forms_tests.tests.test_formsets.FormsFormsetTestCase) [<object object at 0x10456f0a0>] (formset_class='OrderingMethodFormSet') ``` ... which clearly tells you which case went wrong.
Please ignore, my suggestion is invalid syntax.
~if not keeping encryption, remove the __ansible_vault key, just assign the value directly~ nmvd, misread the if to the opposite ...
We could keep tests more DRY and use loop (maybe with `subTest`) e.g. ```python def test_formsets_with_order_custom_widget(self): class OrderingAttributFormSet(BaseFormSet): ordering_widget = HiddenInput class OrderingMethodFormSet(BaseFormSet): def get_ordering_widget(self): return HiddenInput for formset in (OrderingAttributFormSet, OrderingMethodFormSet): ArticleFormSet = formset_factory(ArticleForm, formset=formset, can_order=True) ... ```
`# Filter out fields contributed....` (chop comma before as)
Turn (capitalize) add period. sql -> SQL
perhaps `ordering_sql` is better name
return here -- no need for intermediate variables
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
super nit: the string is slightly malformatted. The script displays: Enter reviewers in the format of "name1 <email1>", "name2 <email2>: Also, Its not clear if I should actually type the quotes when entering reviewers.
Ah, good. Widgets... I think something like `formset_class=formset_class.__name__` would be clearer than the HTML string. Then at least you'd get this: ``` FAIL: test_formsets_with_order_custom_widget (tests.forms_tests.tests.test_formsets.FormsFormsetTestCase) [<object object at 0x10456f0a0>] (formset_class='OrderingMethodFormSet') ``` ... which clearly tells you which case went wrong.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
~if not keeping encryption, remove the __ansible_vault key, just assign the value directly~ nmvd, misread the if to the opposite ...
We could keep tests more DRY and use loop (maybe with `subTest`) e.g. ```python def test_formsets_with_order_custom_widget(self): class OrderingAttributFormSet(BaseFormSet): ordering_widget = HiddenInput class OrderingMethodFormSet(BaseFormSet): def get_ordering_widget(self): return HiddenInput for formset in (OrderingAttributFormSet, OrderingMethodFormSet): ArticleFormSet = formset_factory(ArticleForm, formset=formset, can_order=True) ... ```
Use hanging indent: ``` return [ e._output_field_or_none for ... ] ```
return here -- no need for intermediate variables
perhaps `ordering_sql` is better name
`# Filter out fields contributed....` (chop comma before as)
Put the close ] on the next line.
Turn (capitalize) add period. sql -> SQL
~if not keeping encryption, remove the __ansible_vault key, just assign the value directly~ nmvd, misread the if to the opposite ...
The `bool` type handles more cases than just `yes`. To assign the public_ip variable here to be true/false when provided or None by default, all you should need is to set the default to `None` as below, then assign the variable using: ``` module.params.get('assign_public_ip') ```
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
But it must be in this if.
might be better to have some kind of mapping here ``` convert_list = ['image_id', 'instance_type', 'instance_id', ...] camel_params = dict((k,v) for k, v in snake_dict_to_camel_dict(module.params).items() if v is not None and k is in convert_list) ``` and then special case any exceptions like IamInstanceProfile and InstanceMonitoring
No need to wrap.
use same indent style as previous item
No need to wrap.
`dict((k, v) for k, v in launch_configs[0].items() if k not in ...)` is probably a bit more readable.
what happens if it's not `> 0`? There will be a lot of `IndexError`s in the following code. We should set `launch_config = launch_configs[0]` if we know there is a result.
perhaps "if not region"? that keeps the standard flow from being in an "else", lets us bring the indenting back a level, etc. Otherwise this is fantastic. Thanks for pep8 and removing the stray code.
it means the same thing, it's just the flow.
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
For clarity, this should probably be ``` if module.check_mode: if not route_table: changed = changed or True else: result = ensure_tags(...) ``` to make it absolutely clear that you can't hit the `else` branch with `route_table` being `None`
We don't need to pass tags here.
This doesn't seem right... It's ignoring the push and pull mode parameter altogether so I think it will break that functionality. IIRC, ternary also isn't available in python 2.6... just write it out as an if-then-else, it will be easier to understand as well.
`get_random_string()` will never allow you to connect to the same memory database instance. You can probably use `self.connection.alias` for that so each database alias has it's own unique memory database. This also allows it to work with `threading.local`.
argument ordering should be reversed
If you like, something like this can be redone as `if protocol in ('tftp', 'ftp', 'sftp', 'scp'):`
need to catch BotoCoreError here too.
```suggestion pkg_version = line.partition(':')[-1].strip() ```
Is it possible ? For me it would mean that it exist a package without version.
It doesn't work if pkg_provides contain a version: ``` max@mde-oxalide % pacman -Qqei perl | grep Provide Provides : perl-archive-tar=2.30 perl-attribute-handlers=1.01 perl-autodie=2.29 perl-autoloader=5.74 perl-autouse=1.11 perl-b-debug=1.26 perl-base=2.27 perl-bignum=0.49 perl-carp=1.50 perl-compress-raw-bzip2=2.074 perl-compress-raw-zlib=2.076 perl-config-perl-v=0.29 perl-constant=1.33 perl-cpan-meta-requirements=2.140 perl-cpan-meta-yaml=0.018 perl-cpan-meta=2.150010 perl-cpan=2.20 perl-data-dumper=2.170 perl-db_file=1.840 perl-devel-ppport=3.40 perl-devel-selfstubber=1.06 perl-digest-md5=2.55 perl-digest-sha=6.01 perl-digest=1.17_01 perl-dumpvalue=1.18 perl-encode=2.97 perl-encoding-warnings=0.13 perl-env=1.04 perl-experimental=0.019 perl-exporter=5.73 perl-extutils-cbuilder=0.280230 perl-extutils-constant=0.25 perl-extutils-install=2.14 perl-extutils-makemaker=7.34 perl-extutils-manifest=1.70 perl-extutils-parsexs=3.39 perl-file-fetch=0.56 perl-file-path=2.15 perl-file-temp=0.2304 perl-filter-simple=0.95 perl-filter-util-call=1.58 perl-getopt-long=2.5 perl-http-tiny=0.070 perl-i18n-collate=1.02 perl-i18n-langtags=0.43 perl-if=0.0608 perl-io-compress=2.074 perl-io-socket-ip=0.39 perl-io-zlib=1.10 perl-io=1.39 perl-ipc-cmd=1.00 perl-ipc-sysv=2.07 perl-json-pp=2.97001 perl-lib=0.64 perl-libnet=3.11 perl-locale-codes=3.56 perl-locale-maketext-simple=0.21_01 perl-locale-maketext=1.29 perl-math-bigint-fastcalc=0.5006 perl-math-bigint=1.999811 perl-math-bigrat=0.2613 perl-math-complex=1.5901 perl-memoize=1.03_01 perl-mime-base64=3.15 perl-module-corelist=5.20180622 perl-module-load-conditional=0.68 perl-module-load=0.32 perl-module-loaded=0.08 perl-module-metadata=1.000033 perl-net-ping=2.62 perl-params-check=0.38 perl-parent=0.236 perl-pathtools=3.74 perl-perl-ostype=1.010 perl-perlfaq=5.021011 perl-perlio-via-quotedprint=0.08 perl-pod-checker=1.73 perl-pod-escapes=1.07 perl-pod-parser=1.63 perl-pod-perldoc=3.2801 perl-pod-simple=3.35 perl-pod-usage=1.69 perl-podlators=5.006 perl-safe=2.40 perl-scalar-list-utils=1.50 perl-search-dict=1.07 perl-selfloader=1.25 perl-socket=2.027 perl-storable=3.08 perl-sys-syslog=0.35 perl-term-ansicolor=4.06 perl-term-cap=1.17 perl-term-complete=1.403 perl-term-readline=1.17 perl-test-harness=3.42 perl-test-simple=1.302133 perl-test=1.31 perl-text-abbrev=1.02 perl-text-balanced=2.03 perl-text-parsewords=3.30 perl-text-tabs=2013.0523 perl-thread-queue=3.12 perl-thread-semaphore=2.13 perl-threads-shared=1.58 perl-threads=2.22 perl-tie-file=1.02 perl-tie-refhash=1.39 perl-time-hires=1.9759 perl-time-local=1.25 perl-time-piece=1.3204 perl-unicode-collate=1.25 perl-unicode-normalize=1.26 perl-version=0.9923 perl-xsloader=0.30 ```
Can you use `assertContains()` here to prevent errors due to attribute order in the HTML tags.
Same here, can you use `assertContains()` here to prevent errors due to attribute order in the HTML tags.
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
Maybe add "ansible versions below 2.10" or something so it's clear this is a one-time problem, not that they can never upgrade `ansible` again...
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
What is this supposed to achieve? Modules are cached on import, they'll be picked up from that cache regardless of the pointers being listed in `sys.modules`.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
What if `default` is not a constant but a field reference? e.g. `F('integer')`
You can use [`env_fallback`](https://github.com/ansible/ansible/blob/8f41270a010c00d058c70bdccdc611df8b454139/lib/ansible/module_utils/basic.py#L726)
Make `verify_cert` configurable. You can take a look at [this](https://github.com/ansible/ansible/blob/959395f4b40a4f9e44a4bce890f633f8364c43a6/lib/ansible/module_utils/vmware.py#L466)
Tests are missing for some validators, e.g. `DecimalValidator` and codes `invalid`, `max_digits`, `max_whole_digits`, and `max_decimal_places`.
```suggestion if self._module.check_mode: ``` You already checked for equality above.
Put the object creation out of this function in the `main` to be more readable. Then only pass the gitlab object to your function.
GitLab authentication should not be inside this class. It should be in the main function and then you init your class with the gitlab_instance object in parameter. Look at other GitLab module
Authentication should be handled here and not inside of the class
I find this error message very hard to understand. I don't have any experience with GitLab servers, though :) If the other gitlab_* modules have the same text, we should probably keep it here as well. Changing it could be done in another PR once this is merged.
2.6 or 2.7? Also you `requirements` listed here and the modules.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
No need to quote.
Like many other statements in the examples.
What are the default values for te below parameters if you do not specify it on creation ? - task_history_retention_limit - keep_old_snapshots - log_entries_for_slow_followers - heartbeat_tick - election_tick - dispatcher_heartbeat_period - node_cert_expiry - ca_force_rotate - autolock_managers These possibly require an additional entry in the description to state the defaults on creation. (So you can't add a real default value, because that may modify an existing entry)
So I noticed this long list of options without defaults or choices. Possibly this is exactly what is intended. However, in a lot of cases there is an implicit default that could be mentioned (even when it's not enforced when missing). If you initialize a new swarm I expect there are defaults set.
No need to use quotes. We tend to not quote when it's not needed, as this helps people understand the YAML rules better.
Can you name this a little more verbosely? I can't unsee "get best"
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
```suggestion pkg_version = line.partition(':')[-1].strip() ```
Is it possible ? For me it would mean that it exist a package without version.
It doesn't work if pkg_provides contain a version: ``` max@mde-oxalide % pacman -Qqei perl | grep Provide Provides : perl-archive-tar=2.30 perl-attribute-handlers=1.01 perl-autodie=2.29 perl-autoloader=5.74 perl-autouse=1.11 perl-b-debug=1.26 perl-base=2.27 perl-bignum=0.49 perl-carp=1.50 perl-compress-raw-bzip2=2.074 perl-compress-raw-zlib=2.076 perl-config-perl-v=0.29 perl-constant=1.33 perl-cpan-meta-requirements=2.140 perl-cpan-meta-yaml=0.018 perl-cpan-meta=2.150010 perl-cpan=2.20 perl-data-dumper=2.170 perl-db_file=1.840 perl-devel-ppport=3.40 perl-devel-selfstubber=1.06 perl-digest-md5=2.55 perl-digest-sha=6.01 perl-digest=1.17_01 perl-dumpvalue=1.18 perl-encode=2.97 perl-encoding-warnings=0.13 perl-env=1.04 perl-experimental=0.019 perl-exporter=5.73 perl-extutils-cbuilder=0.280230 perl-extutils-constant=0.25 perl-extutils-install=2.14 perl-extutils-makemaker=7.34 perl-extutils-manifest=1.70 perl-extutils-parsexs=3.39 perl-file-fetch=0.56 perl-file-path=2.15 perl-file-temp=0.2304 perl-filter-simple=0.95 perl-filter-util-call=1.58 perl-getopt-long=2.5 perl-http-tiny=0.070 perl-i18n-collate=1.02 perl-i18n-langtags=0.43 perl-if=0.0608 perl-io-compress=2.074 perl-io-socket-ip=0.39 perl-io-zlib=1.10 perl-io=1.39 perl-ipc-cmd=1.00 perl-ipc-sysv=2.07 perl-json-pp=2.97001 perl-lib=0.64 perl-libnet=3.11 perl-locale-codes=3.56 perl-locale-maketext-simple=0.21_01 perl-locale-maketext=1.29 perl-math-bigint-fastcalc=0.5006 perl-math-bigint=1.999811 perl-math-bigrat=0.2613 perl-math-complex=1.5901 perl-memoize=1.03_01 perl-mime-base64=3.15 perl-module-corelist=5.20180622 perl-module-load-conditional=0.68 perl-module-load=0.32 perl-module-loaded=0.08 perl-module-metadata=1.000033 perl-net-ping=2.62 perl-params-check=0.38 perl-parent=0.236 perl-pathtools=3.74 perl-perl-ostype=1.010 perl-perlfaq=5.021011 perl-perlio-via-quotedprint=0.08 perl-pod-checker=1.73 perl-pod-escapes=1.07 perl-pod-parser=1.63 perl-pod-perldoc=3.2801 perl-pod-simple=3.35 perl-pod-usage=1.69 perl-podlators=5.006 perl-safe=2.40 perl-scalar-list-utils=1.50 perl-search-dict=1.07 perl-selfloader=1.25 perl-socket=2.027 perl-storable=3.08 perl-sys-syslog=0.35 perl-term-ansicolor=4.06 perl-term-cap=1.17 perl-term-complete=1.403 perl-term-readline=1.17 perl-test-harness=3.42 perl-test-simple=1.302133 perl-test=1.31 perl-text-abbrev=1.02 perl-text-balanced=2.03 perl-text-parsewords=3.30 perl-text-tabs=2013.0523 perl-thread-queue=3.12 perl-thread-semaphore=2.13 perl-threads-shared=1.58 perl-threads=2.22 perl-tie-file=1.02 perl-tie-refhash=1.39 perl-time-hires=1.9759 perl-time-local=1.25 perl-time-piece=1.3204 perl-unicode-collate=1.25 perl-unicode-normalize=1.26 perl-version=0.9923 perl-xsloader=0.30 ```
Can you use `assertContains()` here to prevent errors due to attribute order in the HTML tags.
Same here, can you use `assertContains()` here to prevent errors due to attribute order in the HTML tags.
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
Maybe add "ansible versions below 2.10" or something so it's clear this is a one-time problem, not that they can never upgrade `ansible` again...
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
What is this supposed to achieve? Modules are cached on import, they'll be picked up from that cache regardless of the pointers being listed in `sys.modules`.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
oh I see, it makes sense then.
I would not as .format breaks in older versions and we are trying to still keep this kind of module working on older machines
I know this is a small one, but I can imagine % formatting will be decommissioned at some point, I would change this to .format() method if it's not too much of a hassle. To back-up this up, ansible is undergoing an effort to be Python3 compatible, as per Python docs the recommendation is to prefer string.format() method against string % formatting operator. [1][2] Also keep in mind that % might have undesired effects [3] [1] https://docs.python.org/2/library/stdtypes.html#str.format [2] https://www.python.org/dev/peps/pep-3101/ [3] http://stackoverflow.com/questions/5082452/python-string-formatting-vs-format
Is there any reason why this method accepts `level` and `md_device` as argument? IMHO, it would be natural to use `self.level` and `self.md_device` instead.
Please add check-mode support (and if possible also diff support).
also would be super cool if we would move this to the top of the file. ``` 178 Python Imports 179 ============== 180 181 To make it clear what a module is importing, imports should not be sprinkled throughout the code. ```
Nowadays we list all the functionality we use from a specific library explicitly. So: ```python from ansible.module_utils.basic import AnsibleModule ```
I think it should be of type "path".
Should also be "path"
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
@charettes thanks for the idea. I made PR #7755 with regression fix.
Maybe it will be better to move `force_text` to the return line ```python return force_text(query, self.charset), self._format_params(params) ``` instead of repeating it in each case.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
I would say `Deploy key has been updated` instead of `should have been updated`
Please use 'msg' for returned messages, this is a standardized return value.
Leave the trailing comma. This is explicitly allowed by python in order to make line-modifications (like moving the order of lines, or adding lines) without having to update unrelated lines.
So removed `required=False` and add `type='str' instead. Do this for the others as well.
Same as for the updated, I'd rather say `has been deleted`
Same here, default parameter is **validate_certs**, the aliases should be removed. We also simplify this by *not* adding `required=False`, and always starting with the type. Since every parameter has a type, starting with this adds consistency.
So add `type='str'` here too. And we tend to sort lists if the order is of no importance.
We already test this exact configuration in upgrade test (from 0.9 to 0.10, using both 0.9 producer and consumer, and default timestamp type). I would change timestamp_type of this test to LogAppendTime to make it different.
The last parenthesis should be moved to the next line due to hanging indentation, i.e.: ```python qs = WindowTestModel.objects.annotate(dense_rank=Window( expression=DenseRank(), partition_by=F('department'), order_by=[F('salary').desc(), F('name').asc()], )).order_by('department', 'dense_rank') ```
The trailing comma looks unneeded.
Remove the last comma: `...dense_rank))`. Please remove it also from other examples.
don't leave a file descriptor open. use a context manager (via `with` block)
You may try
refactor: ```python additional_kwargs = ( {'showAuthenticationRestrictions': True} if authentication_restrictions_supported else {} ) result = db.command( 'rolesInfo', role, showPrivileges=True, **additional_kwargs ) ```
The last parenthesis should be moved to the next line (as in the `test_dense_rank`).
True, I mixed up sth when switching branches. Nevertheless they work on all databases except PostgreSQL, so we need to fix this.
Cases from lines 361 and 363 work with the previous implementation.
`field_preference` must be a list or a tuple.
Code duplication 80-86, 89-94.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
Try except blocks should be as small as possible. In this case, if is only needed around the calls to modify and delete. Perhaps put the try except around the if state conditional to limit the statements that it surrounds
this seems like it should be shared with the main one rather than duplicated here
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
Must not be `None`.
Whether it has changed or not does not mean there should be a format with invalid URL.
There are multiple formats, some may have `file` some may not. If some new hq format is introduced with different rendition your approach will break downloading by default while proper URL handling will not. Finally read coding conventions.
@JunyiJ My previous suggestion was to use the `TAN` database function on Oracle, i.e. ```python def as_oracle(self, compiler, connection): return super().as_sql(compiler, connection, template='1 / TAN(%(expressions)s)') ```
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
In theory a locale dependant pattern could be provided and `locale.setlocale` be called but this is really unlikely.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
s/CONSTURCTOR/CONSTRUCTOR/ (and usages)
Right, but the order in which you're doing things would at a glance result in the not-uncommon case of, eg `foo (cr)(lf)` not being properly treated, since the break `sub` wouldn't see that, but the char drop would remove the (cr), leaving a line ending that wouldn't format the right way.
new versions still support --check for backwards compatibility
This import should likely be moved back where the original code was, so that it trips early, and other places that import `_json_compat` don't cause pre-mature exceptions that aren't handled
I tried to fix these in #12645 but clearly that hasn't carried over to docstrings ðŸ¤”
```suggestion for b_path in b_colldirs: ```
I don't have strong feelings about this. One possibility is to remove this, and if we get complaints about the removal during alpha or beta we can reintroduce it back. Another approach is to check how hard it is to write code that works both in 1.7 and 1.8 for those cases that use RelatedObject.
please alphabetize with the rest of the django imports
A small oversight I noticed in an old Python 3.7.0 virtualenv: https://github.com/django/django/pull/13393
What happens if it is missing? Will the module still work? (Right now, it never checks this variable again.)
You asked me about the `lru_cache` here; I don't think it matters one way or another :-)
Let's not block the merge on this. The current implementation matches the DEP which was largely discussed. I'm brainstorming to avoid future problems. Seeing the code sometimes gives new ideas.
Reading below, I see that Flask has an "any" converter that does something more complicated. Creating a converter with the same name but a different behavior doesn't sound good.
I'd use .objects.create
For the cases where I've wanted to use a nose test generator, I've ended up just making the test class inherit from object. You lose the TestCase methods (various self.assert*) but usually test generators methods have pretty simple assertions so not too much of a issue. To me it is simpler than injecting the support into a TestCase subclass. (an example is https://github.com/alikins/ansible/blob/6e8a3d0c25fdeaa349351c6636b062376c30b0a6/test/units/plugins/lookup/test_ini.py)
It doesn't make sense to turn a unicode string into a unicode string. It's just `str(str(''))`, which is probably not what you want. Maybe you wanted to turn bytes into text here? In such case check'd be against `binary_type` and the translation should use `to_text()` shim.
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
Do we want to support all these versions? I'd vote for only testing 1.0 and newer. 1.0 was released roughly 2 years ago.
Given that 1.0 was released 2 years ago, I'd even go with 1.1 as the minimum version.
If it doesn't add too much to the runtime, I think it would be good to include some more cases like you suggest.
I feel like this changes too much, for what could be achieved with less: ```diff diff --git a/lib/ansible/plugins/callback/json.py b/lib/ansible/plugins/callback/json.py index 3961a78aab..fd344a0cf0 100644 --- a/lib/ansible/plugins/callback/json.py +++ b/lib/ansible/plugins/callback/json.py @@ -101,14 +101,16 @@ class CallbackModule(CallbackBase): summary[h] = s custom_stats = {} + global_custom_stats = {} if self.get_option('show_custom_stats') and stats.custom: custom_stats.update(dict((self._convert_host_to_name(k), v) for k, v in stats.custom.items())) - custom_stats.pop('_run', None) + global_custom_stats.update(custom_stats.pop('_run', {})) output = { 'plays': self.results, 'stats': summary, 'custom_stats': custom_stats, + 'global_custom_stats': global_custom_stats, } self._display.display(json.dumps(output, indent=4, sort_keys=True)) ```
Please do not remove this trailing comma, this is our preference with dicts and lists to minimize future diffs when lines are changed.
```suggestion 'field_name': 'related_questions', ```
IMO this is not a proper fix because `CustomPK` doesn't appear in a `FOR UPDATE OF` statement, so this can cause a data loss. Both models should be included.
Docstrings should state the expected behavior and omit prefixes like "Tests that" since all tests test things.
Use hanging indent as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
(suggesting to delete this test anyway, however, this could fit on the previous line and if not, use hanging indent as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style)).
It doesn't make sense to turn a unicode string into a unicode string. It's just `str(str(''))`, which is probably not what you want. Maybe you wanted to turn bytes into text here? In such case check'd be against `binary_type` and the translation should use `to_text()` shim.
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
I'm not sure if there are lookups where it's not the case, but comparisons such as `Choice.objects.filter(votes__gte='2')` seem to work fine with the value as a string so the "transform" stuff seems unnecessary, at this for the first version of this.
I feel like this changes too much, for what could be achieved with less: ```diff diff --git a/lib/ansible/plugins/callback/json.py b/lib/ansible/plugins/callback/json.py index 3961a78aab..fd344a0cf0 100644 --- a/lib/ansible/plugins/callback/json.py +++ b/lib/ansible/plugins/callback/json.py @@ -101,14 +101,16 @@ class CallbackModule(CallbackBase): summary[h] = s custom_stats = {} + global_custom_stats = {} if self.get_option('show_custom_stats') and stats.custom: custom_stats.update(dict((self._convert_host_to_name(k), v) for k, v in stats.custom.items())) - custom_stats.pop('_run', None) + global_custom_stats.update(custom_stats.pop('_run', {})) output = { 'plays': self.results, 'stats': summary, 'custom_stats': custom_stats, + 'global_custom_stats': global_custom_stats, } self._display.display(json.dumps(output, indent=4, sort_keys=True)) ```
Please do not remove this trailing comma, this is our preference with dicts and lists to minimize future diffs when lines are changed.
```suggestion 'field_name': 'related_questions', ```
IMO this is not a proper fix because `CustomPK` doesn't appear in a `FOR UPDATE OF` statement, so this can cause a data loss. Both models should be included.
Docstrings should state the expected behavior and omit prefixes like "Tests that" since all tests test things.
Use hanging indent as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
(suggesting to delete this test anyway, however, this could fit on the previous line and if not, use hanging indent as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style)).
It doesn't make sense to turn a unicode string into a unicode string. It's just `str(str(''))`, which is probably not what you want. Maybe you wanted to turn bytes into text here? In such case check'd be against `binary_type` and the translation should use `to_text()` shim.
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
I'm not sure if there are lookups where it's not the case, but comparisons such as `Choice.objects.filter(votes__gte='2')` seem to work fine with the value as a string so the "transform" stuff seems unnecessary, at this for the first version of this.
Use hanging indent as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
(suggesting to delete this test anyway, however, this could fit on the previous line and if not, use hanging indent as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style)).
Docstrings should state the expected behavior and omit prefixes like "Tests that" since all tests test things.
```suggestion 'field_name': 'related_questions', ```
IMO this is not a proper fix because `CustomPK` doesn't appear in a `FOR UPDATE OF` statement, so this can cause a data loss. Both models should be included.
It doesn't make sense to turn a unicode string into a unicode string. It's just `str(str(''))`, which is probably not what you want. Maybe you wanted to turn bytes into text here? In such case check'd be against `binary_type` and the translation should use `to_text()` shim.
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
I think this might explode if `obj_type` is not one of `functions`, `indexes` or `tables`, maybe it would be safer to do something like? ```suggestion obj_func = self.obj_func_mapping.get(obj_type.strip()) if obj_func is not None: obj_func() else: module.warn("Unknown filter option '{}'".filter(obj_type.strip()) ```
Not something that you have to change but I think this could be simplified to ```python for obj_func in self.obj_func_mapping.values(): obj_func() ```
I'm not sure if there are lookups where it's not the case, but comparisons such as `Choice.objects.filter(votes__gte='2')` seem to work fine with the value as a string so the "transform" stuff seems unnecessary, at this for the first version of this.
"Always clear..." would be sufficient
Fair, but we are still in a critical section of code performance-wise.
While my tests suggest that iterating a set is ~8% slower in this case, it is a negligible difference once you look at the whole `_expire_cache` function.
For what I've read about performance, sets perform better when testing membership and lists perform better when iterating over (which seems the case here). To be checked...
It doesn't make sense to turn a unicode string into a unicode string. It's just `str(str(''))`, which is probably not what you want. Maybe you wanted to turn bytes into text here? In such case check'd be against `binary_type` and the translation should use `to_text()` shim.
Given that 1.0 was released 2 years ago, I'd even go with 1.1 as the minimum version.
Do we want to support all these versions? I'd vote for only testing 1.0 and newer. 1.0 was released roughly 2 years ago.
If it doesn't add too much to the runtime, I think it would be good to include some more cases like you suggest.
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
Nit: dowrade -> downgrade.
You: `[0-9|a-f]`. Stackoverflow: `[0-9a-f]`. Difference? You have to learn to distinguish `(...)` and `[...]`.
I'd use .objects.create
For the cases where I've wanted to use a nose test generator, I've ended up just making the test class inherit from object. You lose the TestCase methods (various self.assert*) but usually test generators methods have pretty simple assertions so not too much of a issue. To me it is simpler than injecting the support into a TestCase subclass. (an example is https://github.com/alikins/ansible/blob/6e8a3d0c25fdeaa349351c6636b062376c30b0a6/test/units/plugins/lookup/test_ini.py)
It doesn't make sense to turn a unicode string into a unicode string. It's just `str(str(''))`, which is probably not what you want. Maybe you wanted to turn bytes into text here? In such case check'd be against `binary_type` and the translation should use `to_text()` shim.
Nit: space missing before `timestamp_type`.
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
Do we want to support all these versions? I'd vote for only testing 1.0 and newer. 1.0 was released roughly 2 years ago.
Given that 1.0 was released 2 years ago, I'd even go with 1.1 as the minimum version.
If it doesn't add too much to the runtime, I think it would be good to include some more cases like you suggest.
Nit: dowrade -> downgrade.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
If rc != 0 it is always err. You don't need out or err.
This will throw an exception every time when a server is down. When glusterfsd is down the output looks like this: Brick 10.70.43.200:/mnt/engine Status: Transport endpoint is not connected Number of entries: - And you'll be trying to do int('-') which will throw ValueError. And the module throws error: fatal: [10.70.42.25]: FAILED! => {"changed": false, "msg": "Invalid heal status option."} in the function main.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Can you move this function above main() as per ansible guildelines: " Ansible follows C-style code flow where the caller functions/methods are towards the bottom of the file and the callee implementations are above them. "
lines can be longer
`del` is a builtin, not a function. These parens don't have to be here
This can instead be `continue` and let the `else` unnest.
Add here that the `key_alias` or `key_arn` are both ways to provide it.
You don't need this conditional, since Ansible enforces that these are the only choices.
immediatelly -> immediately
extra space after [
~~ use the shared open_url function, it takes care of many issues with python's ssl ~~
ignore it then, I stopped reading at import ssl, did not realize it is an encrypted tcp socket connection and assumed http/s
`yield from` is not allowed in async functions.
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
can be ignored
Not sure it makes a difference but before it looks like we got `form=None` in the context.
This logic seems ignore the use case of removing all tags.
I'm not a huge fan of this, we should be able to build the result set from module.params and the response.
This is lifted from Python 3.10 so we should attribute appropriately and also ensure that we remove it when no longer needed: ```suggestion if PY310: from contextlib import aclosing else: from contextlib import AbstractAsyncContextManager # Backport of contextlib.aclosing() from Python 3.10. # Copyright (C) 2020 Python Software Foundation (see LICENSE.python). class aclosing(AbstractAsyncContextManager): """ Async context manager for safely finalizing an asynchronously cleaned-up resource such as an async generator, calling its ``aclose()`` method. """ def __init__(self, thing): self.thing = thing async def __aenter__(self): return self.thing async def __aexit__(self, *exc_info): await self.thing.aclose() ```
Does this need to be a separate method? Seems unnecessary to me.
chop trailing newline (check code with flake8 to see a couple other minor formatting things)
Single quotes please.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
Not sure it makes a difference but before it looks like we got `form=None` in the context.
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
immediatelly -> immediately
For the cases where I've wanted to use a nose test generator, I've ended up just making the test class inherit from object. You lose the TestCase methods (various self.assert*) but usually test generators methods have pretty simple assertions so not too much of a issue. To me it is simpler than injecting the support into a TestCase subclass. (an example is https://github.com/alikins/ansible/blob/6e8a3d0c25fdeaa349351c6636b062376c30b0a6/test/units/plugins/lookup/test_ini.py)
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
single line looks more readable here
immediatelly -> immediately
Single quotes please.
Returns -> Return use period
Not sure it makes a difference but before it looks like we got `form=None` in the context.
nit: add a newline here too.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
remove "0" in {0}
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Line 41. By the way, it would make the patch a bit easier to review and see what has changed if you didn't reorder methods (e.g. set is moved above _set) and delete above _discard). Perhaps the reordering could be done in a separate commit afterwards if it's needed.
I take this back and have a deeper look at the tests.
immediatelly -> immediately
single line looks more readable here
Not sure it makes a difference but before it looks like we got `form=None` in the context.
Single quotes please.
For the cases where I've wanted to use a nose test generator, I've ended up just making the test class inherit from object. You lose the TestCase methods (various self.assert*) but usually test generators methods have pretty simple assertions so not too much of a issue. To me it is simpler than injecting the support into a TestCase subclass. (an example is https://github.com/alikins/ansible/blob/6e8a3d0c25fdeaa349351c6636b062376c30b0a6/test/units/plugins/lookup/test_ini.py)
remove "0" in {0}
Again, error handling changed.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Thanks. This PR is assigned to the 2.12 milestone, which we'll be going through after creating the stable-2.11 branch (which will be done when 2.11 RC1 comes out).
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
Use another lookup instead of `epoch` e.g. `second`.
No worries :)
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
Please use a single quote.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Ah, I think this ought to be `local_size == remote_size`, since this conditional causes the file to be skipped for this strategy. That explains the odd behavior I saw with `date_size`
It looks like this is just a serial upload, how is this faster than the current S3 module? I definitely see the benefit of the glob & sync strategies, but it seems like this would be just as fast.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Please ignore, my suggestion is invalid syntax.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
You can also add a deprecation warning for usages like `from django.forms.extras.widgets import SelectDateWidget`.
I don't think this is working correctly, did you test it for all possible inputs ? - First of all the size_unit is only correct if it is a known unit. - And if that's not true, and it's not a digit, you have to escalate. - If it is a digit, you likely won't have a unit and then you could assume bytes.
This means that if you'd be using 16m, it will be 16 bytes too. You will have to do a lot more to avoid mixups here.
The mixup to me is that people using incorrect units will silently be using bytes, whereas it should report this as an error.
You could make the unit mandatory (which is what I would do), or you could allow no unit (which means in bytes) and in that case you will have to consider that case as well. (i.e.if unit is a digit)
That's up to you. There is no requirement to have a 1:1 mapping between the command and the Ansible interface. And I personally prefer to always have units (e.g. the AIX LV interface assumes no unit to mean MB, not MiB so...)
There is a much easier way to do this, by using the index in the list. Something like: ```python unit = size[-1] units = list('b', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y') try: multiplier = 1024**units.index(unit) except ValueError:: e = get_exception() module.fail_json(msg="No valid size unit specified. %s" % e) ```
The final command should probably return its stdout, stderr and rc back to the playbook.
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
I would change this to a `NotSupportedError`, e.g. ```python raise NotSupportedError( 'Bulk create a multi-table inherited model is not supported ' 'on this database backend.' ) ```
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
Maybe the following will be more explicit: ```python first_author_books = result[0].books.all() self.assertEqual([self.book1], [first_author_books]) self.assertEqual([self.reader], [first_author_books[0].read_by.all()])) ```
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
This doesn't seem right, size is an integer at this point.
The final command should probably return its stdout, stderr and rc back to the playbook.
The final command should probably return its stdout, stderr and rc back to the playbook.
The grow and shrink booleans are used for allowing to grow and shrink the image. (That is why they default resp. to true and false). So you have to compare the original size against the wanted size, and if it shrinks and shrinking is allowed, only then should it resize (otherwise it needs to escalate). Similar for the grow-case.
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
This means that if you'd be using 16m, it will be 16 bytes too. You will have to do a lot more to avoid mixups here.
The mixup to me is that people using incorrect units will silently be using bytes, whereas it should report this as an error.
I don't think this is working correctly, did you test it for all possible inputs ? - First of all the size_unit is only correct if it is a known unit. - And if that's not true, and it's not a digit, you have to escalate. - If it is a digit, you likely won't have a unit and then you could assume bytes.
```suggestion content = app_path.joinpath("apps.py").read_text(encoding="utf8") ```
`else` is unnecessary, I think we can leave: ```python if self.ignorenonexistent: continue raise ```
`field_names` are used only when `self.ignorenonexistent is True`, so we can optimize this part, e.g.: ```python field_names = set() if self.ignorenonexistent: if Model not in field_names_cache: self.field_names_cache[Model] = {f.name for f in Model._meta.get_fields()} field_names = self.field_names_cache[Model] ```
I don't see a need for string interpolation in cases like this.
Appreciate what you've done with the tests but I think they're harder to comprehend now. As a Django dev I can jump in and read model classes, but the helpers are obscuring things a bit to me. Also you now have multiple tests per test method now - ideally there should just be a handful of classes per test and some assertions. I guess you can split based on overriding, non-overriding, etc.
I'm pretty sure you can make the abstract bases once in the test case class level: ``` class ...Tests(...): class SomeBase(models.Model): ... ``` Then reuse them appropriately in the individual test methods, which should only need to create the "bottom" classes? I grepped for `\btype\(` and found a few uses, but none are for "quick" building. I personally think the "tonnes of vertical scrolling" is a small concern compared to having to grok how `type()` works.
```suggestion type: list suboptions: ```
In `_handle_m2m_field()` and `_handle_foreign_key_field()` we can avoid of temporary variables (`value`) and return directly, e.g. ```python def _handle_foreign_key_field(self, field, field_value): return base.deserialize_fk_value(field, field_value, self.using, self.handle_forward_references) ```
We can start using f-string ```suggestion raise base.DeserializationError( f"Invalid model identifier: {model_identifier!r}" ) ```
Perhaps: ```suggestion def esxi_version_at_least(self, version): """ Check that the ESXi Host is at least a specific version number. Inputs: - version (tuple): a version tuple, for example (6, 7, 1) Returns: bool """ ``` Suggest moving into module_utils/vmware.py and providing a unit test.
As you've noticed, we don't want to do this here because in this case the pkg_mgr is coming from a pre-existing fact.
IMHO it's an antipattern when you catch everything wholesale. You mask future bugs this way.
```suggestion module.fail_json(msg="The following packages are absent in the currently booted rpm-ostree commit: %s" % ' '.join(pkgs)) ```
I don't see a need for string interpolation in cases like this.
@bcoca corrected me that _execute_module() takes care of delegation so this is fine. After discussion, though, we did decide that we should set the ansible_pkg_mgr fact when we return from this action plugin so that we don't end up having to discover it every time the actin plugin is run on the same host.
bcoca also mentioned that the facts end up being set on the host that's being processed in the task loop, not one that's being delegated_to. So we should only return the ansible_pkg_mgr fact if we are not delegating.
So I guess that this is where we need to set the fact in results. This is the only place where we know that we've had to run the facts module in order to retrieve the package manager.
When delegating, I'd think that we'd need to run the facts module for the delegated host rather than for the inventory_hostname host....
Still need to add the ansible_pkg-mgr fact to results if: * We selected one of yum, yum4, or dnf * And we are not delegating.
```suggestion type: list suboptions: ```
As you've noticed, we don't want to do this here because in this case the pkg_mgr is coming from a pre-existing fact.
IMHO it's an antipattern when you catch everything wholesale. You mask future bugs this way.
we should also return if we both delegate executions and delegate_facts
bcoca also mentioned that the facts end up being set on the host that's being processed in the task loop, not one that's being delegated_to. So we should only return the ansible_pkg_mgr fact if we are not delegating.
@bcoca corrected me that _execute_module() takes care of delegation so this is fine. After discussion, though, we did decide that we should set the ansible_pkg_mgr fact when we return from this action plugin so that we don't end up having to discover it every time the actin plugin is run on the same host.
Still need to add the ansible_pkg-mgr fact to results if: * We selected one of yum, yum4, or dnf * And we are not delegating.
When delegating, I'd think that we'd need to run the facts module for the delegated host rather than for the inventory_hostname host....
So I guess that this is where we need to set the fact in results. This is the only place where we know that we've had to run the facts module in order to retrieve the package manager.
I don't see a need for string interpolation in cases like this.
```suggestion type: list suboptions: ```
```suggestion type: list suboptions: ```
I don't see a need for string interpolation in cases like this.
```suggestion content = app_path.joinpath("apps.py").read_text(encoding="utf8") ```
@bcoca corrected me that _execute_module() takes care of delegation so this is fine. After discussion, though, we did decide that we should set the ansible_pkg_mgr fact when we return from this action plugin so that we don't end up having to discover it every time the actin plugin is run on the same host.
bcoca also mentioned that the facts end up being set on the host that's being processed in the task loop, not one that's being delegated_to. So we should only return the ansible_pkg_mgr fact if we are not delegating.
When delegating, I'd think that we'd need to run the facts module for the delegated host rather than for the inventory_hostname host....
So I guess that this is where we need to set the fact in results. This is the only place where we know that we've had to run the facts module in order to retrieve the package manager.
Under what circumstances would the `or {}`? An empty entry point inside the `argument_specs.yml` file? ```yaml main: short_description: My role does amazing things other_entry: ```
Perhaps: ```suggestion def esxi_version_at_least(self, version): """ Check that the ESXi Host is at least a specific version number. Inputs: - version (tuple): a version tuple, for example (6, 7, 1) Returns: bool """ ``` Suggest moving into module_utils/vmware.py and providing a unit test.
```suggestion except OSError as err: ```
Might be worth renaming this to `self.root_queryset`.
These lines will be unnecessary with the new version of `resolve_model_field_relations()` (see #14781) because it uses `self.real_apps` and handles `concretes`.
Much better IMO, I haven't managed to fail it but let's see what @aaugustin thinks. I didn't find any other usages of `get_app_paths()`. Edit: I probably should have posted that globally since it's not directly connected to this line of code.
This doesn't feel very efficient - there will be a query per loop. You should be able to fetch all objects in a single query, grouping by the field.
You could put `text = format_html('<a href="{}">{}</a>', change_url, text)` in the "else" block, `pass` in the `except` block and then a single return statement at the end `return format_html(label, text)`.
IMO it's more readable without `get(model, {})` I would also collect `ID`'s in the first step, e.g. ```python def clear_restricted_objects_from_queryset(self, model, qs): if model in self.restricted_objects: ids = [ obj.pk for objs in self.restricted_objects[model].values() for obj in objs ] self.restricted_objects[model] = { field: items - set(qs.filter(pk__in=ids)) for field, items in self.restricted_objects[model].items() } ```
Use single quotes consistently.
You should use `module.add_cleanup_file()` (somewhere above) instead of manually trying to clean up.
You need to make sure `tf` doesn't write a unicode string. tf.write(u("#!/bin/sh/\necho %s" % quote(self.passphrase)).encode("utf-8")) Ansible may have a utility function to do this.
In the "Generate privatekey6 - standard - with non-ASCII passphrase" test, this fails with `'ascii' codec can't decode byte 0xc3 in position 16: ordinal not in range(128)`. Stacktrace: ``` File "/tmp/ansible_openssh_keypair_payload_kCnMbD/__main__.py", line 193, in generate tf.write("#!/bin/sh\necho %s" % quote(self.passphrase)) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 706, in write return self.writer.write(data) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 369, in write data, consumed = self.encode(object, self.errors) ``` It seems to fail in all Python 2 tests; in Python 3 it seems to work.
Use a single line. Our [style guide](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style) allows lines up to 119 characters if it helps readability.
Code duplication. Keep the original `_write_thumbnails` code and generalize item part. `param_name` is a bad name for something that describes an item's kind.
we probably want to move this 'adhoc list' into constants.py anyways
this probably needs updating to ansible.legacy.X see constants.py for function that deals with the multiple possible names
makedirs_safe already does this, just use that function
Download archive behavior must not change, it must only take place after success of the actual download and post processing.
this is a non atomic operation and can lead to file corruption, write to a temp file and use shared 'atomic_move' to put the file into place
```suggestion self._servers[index], **self._client_kwargs, ```
I would not recommend any alternatives: ```suggestion f"Cannot update applied migration {migration}." ```
set type to 'path'
As you've noticed, we don't want to do this here because in this case the pkg_mgr is coming from a pre-existing fact.
IMHO it's an antipattern when you catch everything wholesale. You mask future bugs this way.
```suggestion module.fail_json(msg="The following packages are absent in the currently booted rpm-ostree commit: %s" % ' '.join(pkgs)) ```
I don't see a need for string interpolation in cases like this.
@bcoca corrected me that _execute_module() takes care of delegation so this is fine. After discussion, though, we did decide that we should set the ansible_pkg_mgr fact when we return from this action plugin so that we don't end up having to discover it every time the actin plugin is run on the same host.
bcoca also mentioned that the facts end up being set on the host that's being processed in the task loop, not one that's being delegated_to. So we should only return the ansible_pkg_mgr fact if we are not delegating.
So I guess that this is where we need to set the fact in results. This is the only place where we know that we've had to run the facts module in order to retrieve the package manager.
When delegating, I'd think that we'd need to run the facts module for the delegated host rather than for the inventory_hostname host....
Still need to add the ansible_pkg-mgr fact to results if: * We selected one of yum, yum4, or dnf * And we are not delegating.
```suggestion type: list suboptions: ```
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
prefer hanging indent style with 1 arg per line
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
I think `enumerate` would work here
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
I think it is better to pass module as argument to build_xml() and handle lxml install check-in module_utils instead of having it in each module
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
This would be more readable and consistent with our indentation style with something like: ``` foo_constraints = [ name for name, details in constraints.items() if details['columns'] == ['name'] and details['unique'] and name != custom_constraint_name] ] self.assertEqual(len(foo_constraints), 1) ``` (choosing a different name than "foo"
I think we'd want more details about why this hack is needed.
This pattern has a small issue where it never guarantees the assertion actually runs. It could be refactored so that the assertion is outside the loop, after the desired constraint is assigned to some variable.
`UniqueConstraint` not `Index`.
Good catch, I will remove it before final squash.
happy new year ! yeah effectively finally this is required for python < 3.5 as you explain (I just tested it in an interactive session). I'm happy to open a PR with this specific change so.. should not be too long to come..
didn't knew about RegexURLPattern having the same method.. reading the docstring I'm thinking both should do this: ``` python # .. if isinstance(func, functools.partial): while isinstance(func, functools.partial): func = func.func # ... ``` shouldn't it ? again that's my wild guess here, I don't have read more about the context of how this method is used (nor the other one), etc.. So I could be simply wrong.
hmmm.. now I think I'm wrong, you want to only have the first partial func.. `func.func` shouldn't anyway be another partial or else that would be problematic I think, as views have to return something else.. Sorry for disturbance ;)
Please open a ticket to track the bug (all non-trivial changes should have a ticket).
```suggestion description: Returns a dictionary for every extension OID ```
1: I'm definitely happy with this. I'm not sure why I didn't go for it myself. 2: Fine by me, I don't feel strongly.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
Getting an error about an index when creating a constraint is confusing unless you understand the implementation details.
This message is different on PostgreSQL 13+, it should be enough to check that a constraint name is in the message.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
redundant, remove ```suggestion ```
> Is there any specific reason why we would prefer using the operation in this case? Yes, because we have it. Using a RAW SQL is the last option, we're developing the ORM in order not to use them.
Why we're adding a constraint manually and not with `AddConstraintNotValid()`? Also, please use hanging indentation.
Could use `bulk_create` ```suggestion Product.objects.bulk_create([ Product(name='p1'), Product(name='p1', color='red'), ]) ```
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
prefer hanging indent style with 1 arg per line
`band_input`, you don't get much by saving one char :-)
put closing parenthesis on the next line
I think `enumerate` would work here
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
Need a colon at the end here
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
Omit these lines please.
I don't think boto_params will include access_key or secret_key if a profile is being used (which might be through passing `profile` parameter or with `AWS_PROFILE` set) Looking at https://github.com/jmenga/requests-aws-sign the way to do it is to use ``` session = session.Session() credentials = session.get_credentials() ``` but even there, we'd need to cope with the `profile` parameter.
2.6 or 2.7? Also you `requirements` listed here and the modules.
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Can you name this a little more verbosely? I can't unsee "get best"
These parens aren't necessary for unpacking the return values.
No need to assign to `self.widgets` ```python if isinstance(widgets, dict): self.widgets_names = [ ('_%s' % name) if name else '' for name in widgets ] widgets = widgets.values() else: self.widgets_names = ['_%s' % i for i in range(len(widgets))] self.widgets = [w() if isinstance(w, type) else w for w in widgets] ```
This code is almost the same as in `replace_unnamed_groups()`, the only difference is that the beginning of non-capturing group is longer i.e. `'(?:'` instead of `'('`. We could add an internal hook and use it in both places, e.g. ```python def _find_groups(pattern, group_matcher): group_indices = [ (m.start(0), m.end()) for m in non_capturing_group_matcher.finditer(pattern) ] # Loop over the groups. for start, end in unnamed_group_indices: ... for idx, val in enumerate(pattern[end:]): ... if unmatched_open_brackets == 0: group_indices.append((start, end + idx + 1)) break # Remove unnamed group matches inside other unnamed capture groups. group_start_end_indices = [] prev_end = None for start, end in group_indices: if prev_end and start > prev_end or not prev_end: group_start_end_indices.append((start, end)) prev_end = end return group_start_end_indices ``` Moreover, with some boolean flags (e.g. `named=True/False`) this could also be reused in `replace_named_groups()` :thinking: .
So if I update some parameter+ change state to running, it won't start, IIUC
please reuse the code, it's very same
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
You can use iteritems as below ``` from ansible.module_utils.six import iteritems iteritems(parsed) ```
Please don't make lines longer! There was nothing really wrong with this line before
`check_args()` is a empty function. Is this call required? For other networks platforms `check_args()` is present for legacy reason.
Why `dict(required=True)` is added here? It is already part of the spec.
Is `address` option required as part of this module? I think address can be configured as part of `vyos_l3_interface` module.
No need to assign to `self.widgets` ```python if isinstance(widgets, dict): self.widgets_names = [ ('_%s' % name) if name else '' for name in widgets ] widgets = widgets.values() else: self.widgets_names = ['_%s' % i for i in range(len(widgets))] self.widgets = [w() if isinstance(w, type) else w for w in widgets] ```
This code is almost the same as in `replace_unnamed_groups()`, the only difference is that the beginning of non-capturing group is longer i.e. `'(?:'` instead of `'('`. We could add an internal hook and use it in both places, e.g. ```python def _find_groups(pattern, group_matcher): group_indices = [ (m.start(0), m.end()) for m in non_capturing_group_matcher.finditer(pattern) ] # Loop over the groups. for start, end in unnamed_group_indices: ... for idx, val in enumerate(pattern[end:]): ... if unmatched_open_brackets == 0: group_indices.append((start, end + idx + 1)) break # Remove unnamed group matches inside other unnamed capture groups. group_start_end_indices = [] prev_end = None for start, end in group_indices: if prev_end and start > prev_end or not prev_end: group_start_end_indices.append((start, end)) prev_end = end return group_start_end_indices ``` Moreover, with some boolean flags (e.g. `named=True/False`) this could also be reused in `replace_named_groups()` :thinking: .
please reuse the code, it's very same
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Can this be added in agrspec as choices as well? Probably by having this list as a global variable.
You can use iteritems as below ``` from ansible.module_utils.six import iteritems iteritems(parsed) ```
Why `dict(required=True)` is added here? It is already part of the spec.
`check_args()` is a empty function. Is this call required? For other networks platforms `check_args()` is present for legacy reason.
type='str' is a default value not required to mention in separately.
Is `address` option required as part of this module? I think address can be configured as part of `vyos_l3_interface` module.
Good catch, I will remove it before final squash.
To catch this as a `IntegrityError` on Oracle we need to add `ORA-00001` to the wrapper: ```diff --git a/django/db/backends/oracle/base.py b/django/db/backends/oracle/base.py index 4d2f5b51f1..3908aebda1 100644 --- a/django/db/backends/oracle/base.py +++ b/django/db/backends/oracle/base.py @@ -73,7 +73,7 @@ def wrap_oracle_errors(): # _C00102056) violated - parent key not found' # Convert that case to Django's IntegrityError exception. x = e.args[0] - if hasattr(x, 'code') and hasattr(x, 'message') and x.code == 2091 and 'ORA-02291' in x.message: + if hasattr(x, 'code') and hasattr(x, 'message') and x.code == 2091 and ('ORA-02291' in x.message or 'ORA-00001' in x.message): raise IntegrityError(*tuple(e.args)) raise ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Please use shorter name (less than 30 chars), e.g. `deferrable_pink_constraint`.
Should be ``self.weight``
Seems nicer to infer private_zone if vpc_id is set. But not a blocker
Remove the two extra double-quotes here.
Please don't make lines longer! There was nothing really wrong with this line before
I was able to fix this locally by changing to `if r == self.payload:`. No idea if there's some case this doesn't work for though. At scale ansible might not be the right tool. However I have no desire to deal with adding another tool just for updating a single record.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Should be ``self.weight``
To catch this as a `IntegrityError` on Oracle we need to add `ORA-00001` to the wrapper: ```diff --git a/django/db/backends/oracle/base.py b/django/db/backends/oracle/base.py index 4d2f5b51f1..3908aebda1 100644 --- a/django/db/backends/oracle/base.py +++ b/django/db/backends/oracle/base.py @@ -73,7 +73,7 @@ def wrap_oracle_errors(): # _C00102056) violated - parent key not found' # Convert that case to Django's IntegrityError exception. x = e.args[0] - if hasattr(x, 'code') and hasattr(x, 'message') and x.code == 2091 and 'ORA-02291' in x.message: + if hasattr(x, 'code') and hasattr(x, 'message') and x.code == 2091 and ('ORA-02291' in x.message or 'ORA-00001' in x.message): raise IntegrityError(*tuple(e.args)) raise ```
Good catch, I will remove it before final squash.
Please use shorter name (less than 30 chars), e.g. `deferrable_pink_constraint`.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Seems nicer to infer private_zone if vpc_id is set. But not a blocker
Remove the two extra double-quotes here.
Please don't make lines longer! There was nothing really wrong with this line before
I was able to fix this locally by changing to `if r == self.payload:`. No idea if there's some case this doesn't work for though. At scale ansible might not be the right tool. However I have no desire to deal with adding another tool just for updating a single record.
Too long line.
Line is too long.
Too long line.
This line is too long. Max line length allowed in Ansible is 120 characters.
Line is too long.
Line is too long.
it might not be relevant in this particular case but be aware to better handle LANG when searching in output. use `module.run_command_environ_update = dict(LANG='C', LC_ALL='C', LC_MESSAGES='C', LC_CTYPE='C')` early after `module` has been instantiated to get that covered.
Line is too long.
use python bool for the default `default=False`
Default type is `str` so you don't have to set it explicitly. Just use `size=dict(),` instead. The same bellow.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
looks like there are two levels of indentation instead of one
colors should all be configurable
nit: formatting, add some whitespaces
nit: formatting, add some whitespaces
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
please fail if required stuff is null
++ thanks for changing this :)
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
```suggestion assert ansible_json_encoder.default(test_input) == {'__ansible_vault': expected} ```
nit: formatting, add some whitespaces
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
please fail if required stuff is null
++ thanks for changing this :)
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
looks like there are two levels of indentation instead of one
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: cannot create point collection with empty set of points at __randomizedtesting.SeedInfo.seed([9959A23819CF838B:6FE2182D9705AE]:0) at org.elasticsearch.common.geo.builders.ShapeBuilder.<init>(ShapeBuilder.java:97) at org.elasticsearch.common.geo.builders.MultiPointBuilder.<init>(MultiPointBuilder.java:46) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseMultiPoint(GeoWKTShapeParserTests.java:82) ... ```
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
nit: formatting, add some whitespaces
`band_input`, you don't get much by saving one char :-)
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
I think `enumerate` would work here
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
prefer hanging indent style with 1 arg per line
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
I prefer putting the closing ) on the next line
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
Similarly, ```if tc['skip'].get('i')```
Add trailing comma.
argument ordering should be reversed
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Use hanging indentation (the same in the second test).
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
Oh I see :)
not a blocker, I would probably not error out here, instead you could print out a warning message, up to your decision: ~~~python module.warn(warning='Cannot change type of an existing volume.') ~~~
This doesn't seem right, size is an integer at this point.
Okay I think this makes sense, let's just follow this pattern then.
please use a variable for this string so that if it changes, we don't have to update it below as well
@chouseknecht thanks a bunch, it makes sense to me. FWIW, I'm happy to help with the maintenance of the openshift client too. I'd like to help keeping the ansible module and the openshift client aligned with upstream kubernetes.
This change looks unrelated.
I don't see a need for string interpolation in cases like this.
I meant for the entire string here to be a constant; otherwise looks good to me.
remove extra newline
Running the entire test suite I could reproduce the error, but not if only running `django.contrib.gis`. Not sure exactly the cause but this code seems suspicious as we leave a different model admin registered for other tests.
Both cases test the same thing, don't repeat yourself. Test things based on their properties. Here you'd probably want to test iterables, not just sets.
No need to parametrize with just one case.
An example of using the `mocker` fixture. ```suggestion def mock_NamedTemporaryFile(mocker, **args): mock_ntf = mocker.MagicMock() mock_ntf.write = mocker.MagicMock() mock_ntf.close = mocker.MagicMock() ```
`constraint_name` should also be quoted.
WDYT about adding `or self.title` here? ``` python self.title = title or self.title or self.lookup_title ``` This would add support for the following: ``` python class MyStaffFilter(BooleanFieldListFilter): title = "By can haz admin" # no effect :( overwritten in __init__ @admin.register(User) class UserAdmin(UserAdminBase): list_filter = ( ("is_staff", MyStaffFilter), ) ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
Use another lookup instead of `epoch` e.g. `second`.
Yes, fine with me.
Please use a single quote.
I believe you would need to add a custom `save_form` method to a `ModelAdmin` and somehow incorporate the `change` flag in it -- perhaps modify the form's cleaned_data to assign the field to a model field before save.
Please check test coverage carefully. I didn't spot a test for this change.
docstring with example input/output would be really helpful
a simpler fix i was already working on: `+ display.display(json.dumps(text, cls=AnsibleJSONEncoder, sort_keys=True, indent=4))`
^ that seems to be an expression not really a data type issue (sorting keys, this is another known json issue), in any case, there is also an existing `jsonify` in module_utils.
well, not die with unexpected exception .. tempted to say there is no real reason the type should be incorrect for any keys. So ending in an error should be fine, just not an unhandled one.
For these 3, add expected_type `int`, eg: ```py 'width': try_get(item, lambda x: x['video']['width'], int), ``` (equivalent in effect to wrapping in `int_or_none()`).
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
Please ignore, my suggestion is invalid syntax.
Should contain `quality` key.
`enumerate` on for range.
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Am not sure all this is necessary as the same object is referenced... Just add the one line to call the method that wraps `close()`.
That was the thing I was saying earlier- checking for/failing on undefined inputs upfront would probably be what most people would expect, but that'd be a breaking change. Even if we chose that breaking change, we'd probably need a way for filters that *want* to allow undefined inputs to do so (transparently to the caller of the filter). And yeah, we probably need an analogue for tests, since the same problems exist.
Another option might be to build it the way we wish it were, then start a deprecation warning cycle on this bridging behavior (eg, do something like this and/or input inspection with an explicit optin/optout in plugin metadata, decorator, or whatever, then issue a dep warning on the exception catch or undefined inputs for plugins that haven't explicitly opted into receiving them). :shrug:
You can join this and a couple of other lines. We have a max line length of 119 chars.
Can you use hanging indents and a trailing comma, please: ``` python foo( 1, 2, ) ```
"The unused `extra_context` parameter for `logout_and_login` is deprecated." Also, you can move this into the view rather than creating a decorator since it's only used once.
I'd keep `dispatch()` the first method on the view and sort the others by call stack.
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Am not sure all this is necessary as the same object is referenced... Just add the one line to call the method that wraps `close()`.
Another option might be to build it the way we wish it were, then start a deprecation warning cycle on this bridging behavior (eg, do something like this and/or input inspection with an explicit optin/optout in plugin metadata, decorator, or whatever, then issue a dep warning on the exception catch or undefined inputs for plugins that haven't explicitly opted into receiving them). :shrug:
That was the thing I was saying earlier- checking for/failing on undefined inputs upfront would probably be what most people would expect, but that'd be a breaking change. Even if we chose that breaking change, we'd probably need a way for filters that *want* to allow undefined inputs to do so (transparently to the caller of the filter). And yeah, we probably need an analogue for tests, since the same problems exist.
You can join this and a couple of other lines. We have a max line length of 119 chars.
Can you use hanging indents and a trailing comma, please: ``` python foo( 1, 2, ) ```
"The unused `extra_context` parameter for `logout_and_login` is deprecated." Also, you can move this into the view rather than creating a decorator since it's only used once.
I'd keep `dispatch()` the first method on the view and sort the others by call stack.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
You might append this to the `using` sql passed to `super()` to fix the test failure on Jenkins (due to usage of TABLESPACE).
Can you re-warp this block to 79 chars? (First line is too short.)
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
> Or should I just check that those settings doesn't exist outside the context? I think that'd work.
@slafs non-existing setting names, e.g. `TEST_SETTING=True`
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
Am not sure all this is necessary as the same object is referenced... Just add the one line to call the method that wraps `close()`.
That was the thing I was saying earlier- checking for/failing on undefined inputs upfront would probably be what most people would expect, but that'd be a breaking change. Even if we chose that breaking change, we'd probably need a way for filters that *want* to allow undefined inputs to do so (transparently to the caller of the filter). And yeah, we probably need an analogue for tests, since the same problems exist.
Another option might be to build it the way we wish it were, then start a deprecation warning cycle on this bridging behavior (eg, do something like this and/or input inspection with an explicit optin/optout in plugin metadata, decorator, or whatever, then issue a dep warning on the exception catch or undefined inputs for plugins that haven't explicitly opted into receiving them). :shrug:
You can join this and a couple of other lines. We have a max line length of 119 chars.
"The unused `extra_context` parameter for `logout_and_login` is deprecated." Also, you can move this into the view rather than creating a decorator since it's only used once.
Can you use hanging indents and a trailing comma, please: ``` python foo( 1, 2, ) ```
I'd keep `dispatch()` the first method on the view and sort the others by call stack.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
This variable is never assigned ```suggestion api.get_collection_versions('namespace', 'collection') ```
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
I think it may be cleaner to check this right away ```suggestion actual_versions = api.get_collection_versions('namespace', 'collection') assert actual_versions == [u'1.0.0', u'1.0.1', u'1.0.2', u'1.0.3', u'1.0.4', u'1.0.5'] ``` and this would simplify the check at the end of this test.
nit: ```suggestion cached_server = final_cache['galaxy.server.com:'] cached_collection = cached_server['/api/v2/collections/namespace/collection/versions/'] cached_versions = [r['version'] for r in cached_collection['results']] ```
Per https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/ these could use `assertIs(..., True)`. I've done this for existing `.check_token()` tests in #12380.
excellent handling of congestion control
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
Am not sure all this is necessary as the same object is referenced... Just add the one line to call the method that wraps `close()`.
That was the thing I was saying earlier- checking for/failing on undefined inputs upfront would probably be what most people would expect, but that'd be a breaking change. Even if we chose that breaking change, we'd probably need a way for filters that *want* to allow undefined inputs to do so (transparently to the caller of the filter). And yeah, we probably need an analogue for tests, since the same problems exist.
Another option might be to build it the way we wish it were, then start a deprecation warning cycle on this bridging behavior (eg, do something like this and/or input inspection with an explicit optin/optout in plugin metadata, decorator, or whatever, then issue a dep warning on the exception catch or undefined inputs for plugins that haven't explicitly opted into receiving them). :shrug:
You can join this and a couple of other lines. We have a max line length of 119 chars.
"The unused `extra_context` parameter for `logout_and_login` is deprecated." Also, you can move this into the view rather than creating a decorator since it's only used once.
Can you use hanging indents and a trailing comma, please: ``` python foo( 1, 2, ) ```
I'd keep `dispatch()` the first method on the view and sort the others by call stack.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
You might append this to the `using` sql passed to `super()` to fix the test failure on Jenkins (due to usage of TABLESPACE).
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
Right, this was more a of suggestion open to discussion given the composite nature of `(-180.0, -90.0, 180.0, 90.0)`. I kind of wish `Field.deconstruct` was more smart wrt to `__init__` defaults by relying on `inspect` reflection in the first place.
I wonder if it's worth adding two class attributes constants for these defaults.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Can you re-warp this block to 79 chars? (First line is too short.)
Unlike an "app", I wouldn't say that a model is "installed". Two solutions: - check if the app is installed and adjust the error message accordingly: "app X isn't installed" or "app X doesn't provide model Y" - just rephrase do "which doesn't exist"
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
It would have been more appropriate if `run_checks()` took an `Apps` instance as a parameter instead of a list of `AppConfig`s.
remove "0" in {0}
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
"The unused `extra_context` parameter for `logout_and_login` is deprecated." Also, you can move this into the view rather than creating a decorator since it's only used once.
Can you re-warp this block to 79 chars? (First line is too short.)
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
@slafs non-existing setting names, e.g. `TEST_SETTING=True`
> Or should I just check that those settings doesn't exist outside the context? I think that'd work.
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
"The unused `extra_context` parameter for `logout_and_login` is deprecated." Also, you can move this into the view rather than creating a decorator since it's only used once.
Can you re-warp this block to 79 chars? (First line is too short.)
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
@slafs non-existing setting names, e.g. `TEST_SETTING=True`
> Or should I just check that those settings doesn't exist outside the context? I think that'd work.
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
You might append this to the `using` sql passed to `super()` to fix the test failure on Jenkins (due to usage of TABLESPACE).
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
Can you re-warp this block to 79 chars? (First line is too short.)
> Or should I just check that those settings doesn't exist outside the context? I think that'd work.
@slafs non-existing setting names, e.g. `TEST_SETTING=True`
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
You might append this to the `using` sql passed to `super()` to fix the test failure on Jenkins (due to usage of TABLESPACE).
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
Can you re-warp this block to 79 chars? (First line is too short.)
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
@slafs non-existing setting names, e.g. `TEST_SETTING=True`
> Or should I just check that those settings doesn't exist outside the context? I think that'd work.
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
If we `from itertools import chain` we can save the overhead of attribute access here.
I think there's no reason that 'template' and 'arg_joiner' need to go into `data` -- they aren't meant to be interpolated into `template`, correct? Instead of the `data.get()` lines below, wouldn't this work: ``` python template = template or self.template arg_joiner = arg_joiner or self.arg_joiner ```
Ah. Yes, quite right. So this could be simplified to: ```python return None if None in args else func(*args, **kwargs) ```
I don't think that SQLite can pass kwargs, so I think that this can be replaced by `if None in args`.
Would there be any utility to sorted() on files/sub_folders? AFAICT, os.walk will use the os.listdir() results directly for filenames. os.listdir() is doc'ed as 'arbitrary order' (iirc, more or less up to fs). Wondering if sorting those results would make for more reproducible behavior when trying to replicate errors. It shouldn't matter at all if there are no bugs, but it could make tracking down odd failures easier.
Can you please rename this one to `ModelManagerSerializer` and let it inherit from `DeconstructableSerializer`.
And all the blank lines in this test.
Can you please rename it to `ModelManagerSerializer`. I think you just missed it.
`value` could be used instead of `argv`. `argk` is only required when `argv is not None` and could be moved in the `if` block.
Gotcha! When you replace the `IterableSerializer` with the `TupleSerializer` in the factory you end up with this failing test. I think we can leave it as as. ``` python Traceback (most recent call last): File "/home/markus/Coding/django/tests/migrations/test_writer.py", line 362, in test_serialize_settings ("((0, 0), (1, 1), (2, 4))", set()) File "/home/markus/Coding/django/tests/migrations/test_writer.py", line 187, in assertSerializedResultEqual self.assertEqual(MigrationWriter.serialize(value), target) File "/home/markus/Coding/django/django/db/migrations/writer.py", line 308, in serialize return serializer_factory(value).serialize() File "/home/markus/Coding/django/django/db/migrations/serializer.py", line 46, in serialize format = self._format() File "/home/markus/Coding/django/django/db/migrations/serializer.py", line 303, in _format return "(%s)" if len(self.value) != 1 else "(%s,)" TypeError: object of type 'generator' has no len() ```
If we `from itertools import chain` we can save the overhead of attribute access here.
I think there's no reason that 'template' and 'arg_joiner' need to go into `data` -- they aren't meant to be interpolated into `template`, correct? Instead of the `data.get()` lines below, wouldn't this work: ``` python template = template or self.template arg_joiner = arg_joiner or self.arg_joiner ```
Ah. Yes, quite right. So this could be simplified to: ```python return None if None in args else func(*args, **kwargs) ```
I don't think that SQLite can pass kwargs, so I think that this can be replaced by `if None in args`.
Would there be any utility to sorted() on files/sub_folders? AFAICT, os.walk will use the os.listdir() results directly for filenames. os.listdir() is doc'ed as 'arbitrary order' (iirc, more or less up to fs). Wondering if sorting those results would make for more reproducible behavior when trying to replicate errors. It shouldn't matter at all if there are no bugs, but it could make tracking down odd failures easier.
Can you please rename this one to `ModelManagerSerializer` and let it inherit from `DeconstructableSerializer`.
And all the blank lines in this test.
Can you please rename it to `ModelManagerSerializer`. I think you just missed it.
`value` could be used instead of `argv`. `argk` is only required when `argv is not None` and could be moved in the `if` block.
Gotcha! When you replace the `IterableSerializer` with the `TupleSerializer` in the factory you end up with this failing test. I think we can leave it as as. ``` python Traceback (most recent call last): File "/home/markus/Coding/django/tests/migrations/test_writer.py", line 362, in test_serialize_settings ("((0, 0), (1, 1), (2, 4))", set()) File "/home/markus/Coding/django/tests/migrations/test_writer.py", line 187, in assertSerializedResultEqual self.assertEqual(MigrationWriter.serialize(value), target) File "/home/markus/Coding/django/django/db/migrations/writer.py", line 308, in serialize return serializer_factory(value).serialize() File "/home/markus/Coding/django/django/db/migrations/serializer.py", line 46, in serialize format = self._format() File "/home/markus/Coding/django/django/db/migrations/serializer.py", line 303, in _format return "(%s)" if len(self.value) != 1 else "(%s,)" TypeError: object of type 'generator' has no len() ```
If we `from itertools import chain` we can save the overhead of attribute access here.
I think there's no reason that 'template' and 'arg_joiner' need to go into `data` -- they aren't meant to be interpolated into `template`, correct? Instead of the `data.get()` lines below, wouldn't this work: ``` python template = template or self.template arg_joiner = arg_joiner or self.arg_joiner ```
Ah. Yes, quite right. So this could be simplified to: ```python return None if None in args else func(*args, **kwargs) ```
I don't think that SQLite can pass kwargs, so I think that this can be replaced by `if None in args`.
Would there be any utility to sorted() on files/sub_folders? AFAICT, os.walk will use the os.listdir() results directly for filenames. os.listdir() is doc'ed as 'arbitrary order' (iirc, more or less up to fs). Wondering if sorting those results would make for more reproducible behavior when trying to replicate errors. It shouldn't matter at all if there are no bugs, but it could make tracking down odd failures easier.
Can you please rename this one to `ModelManagerSerializer` and let it inherit from `DeconstructableSerializer`.
And all the blank lines in this test.
Can you please rename it to `ModelManagerSerializer`. I think you just missed it.
`value` could be used instead of `argv`. `argk` is only required when `argv is not None` and could be moved in the `if` block.
Gotcha! When you replace the `IterableSerializer` with the `TupleSerializer` in the factory you end up with this failing test. I think we can leave it as as. ``` python Traceback (most recent call last): File "/home/markus/Coding/django/tests/migrations/test_writer.py", line 362, in test_serialize_settings ("((0, 0), (1, 1), (2, 4))", set()) File "/home/markus/Coding/django/tests/migrations/test_writer.py", line 187, in assertSerializedResultEqual self.assertEqual(MigrationWriter.serialize(value), target) File "/home/markus/Coding/django/django/db/migrations/writer.py", line 308, in serialize return serializer_factory(value).serialize() File "/home/markus/Coding/django/django/db/migrations/serializer.py", line 46, in serialize format = self._format() File "/home/markus/Coding/django/django/db/migrations/serializer.py", line 303, in _format return "(%s)" if len(self.value) != 1 else "(%s,)" TypeError: object of type 'generator' has no len() ```
If we `from itertools import chain` we can save the overhead of attribute access here.
I think there's no reason that 'template' and 'arg_joiner' need to go into `data` -- they aren't meant to be interpolated into `template`, correct? Instead of the `data.get()` lines below, wouldn't this work: ``` python template = template or self.template arg_joiner = arg_joiner or self.arg_joiner ```
Ah. Yes, quite right. So this could be simplified to: ```python return None if None in args else func(*args, **kwargs) ```
I don't think that SQLite can pass kwargs, so I think that this can be replaced by `if None in args`.
Would there be any utility to sorted() on files/sub_folders? AFAICT, os.walk will use the os.listdir() results directly for filenames. os.listdir() is doc'ed as 'arbitrary order' (iirc, more or less up to fs). Wondering if sorting those results would make for more reproducible behavior when trying to replicate errors. It shouldn't matter at all if there are no bugs, but it could make tracking down odd failures easier.
Can you please rename this one to `ModelManagerSerializer` and let it inherit from `DeconstructableSerializer`.
And all the blank lines in this test.
Can you please rename it to `ModelManagerSerializer`. I think you just missed it.
`value` could be used instead of `argv`. `argk` is only required when `argv is not None` and could be moved in the `if` block.
Gotcha! When you replace the `IterableSerializer` with the `TupleSerializer` in the factory you end up with this failing test. I think we can leave it as as. ``` python Traceback (most recent call last): File "/home/markus/Coding/django/tests/migrations/test_writer.py", line 362, in test_serialize_settings ("((0, 0), (1, 1), (2, 4))", set()) File "/home/markus/Coding/django/tests/migrations/test_writer.py", line 187, in assertSerializedResultEqual self.assertEqual(MigrationWriter.serialize(value), target) File "/home/markus/Coding/django/django/db/migrations/writer.py", line 308, in serialize return serializer_factory(value).serialize() File "/home/markus/Coding/django/django/db/migrations/serializer.py", line 46, in serialize format = self._format() File "/home/markus/Coding/django/django/db/migrations/serializer.py", line 303, in _format return "(%s)" if len(self.value) != 1 else "(%s,)" TypeError: object of type 'generator' has no len() ```
We can reuse existing objects.
I guess `get_admin_readonly_field()` could take `response` instead of `response.context['adminform']`.
Missing asserts here? Also, please split the test into smaller parts.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Could you sort those in reverse order (to be applied first: top ; to be applied last: bottom), please. Same for the other cases in this test.
I don't think this assertion is necessary -- if it's None we'll get an error in the next assertion which should be just as easy to debug.
I think this test can be entirely removed if `test_nested_subquery_outer_ref_with_function` covers the usage case appropriately.
Actually `assertContains` checks the `status_code` too, so the other assertion is redundant.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
We could also check `response.context`. Something like (needs to be cleaned up a bit): ``` >>> field_line = [field_line for field_line in [fieldset for fieldset in response.context['adminform']][0]][-1] >>> for f in field_line: print(f.contents()) Brand New Plot ``` This is easier to debug when it fails than `assertContains`.
We can reuse existing objects.
I would use: ``` self.assertRaisesMessage(AttributeError, "'ProxyModel' has no attribute 'test_objects'"):` TestModel.test_objects ```
Maybe: ```suggestion try: self.assertInHTML('<th>1</th>', f.render()) except RecursionError: self.fail('Cyclic reference in BoundField.render().') ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
We don't need to test multiple cases because we want to ignore only locales with hyphens.
Would it be enough to check `form.fields`? This might make the test a bit easier to follow instead of having to parse the HTML to see what's expected..
Missing asserts here? Also, please split the test into smaller parts.
```suggestion '<tr class="row-form-errors"><td colspan="3">' '<ul class="errorlist nonfield"><li>A non-field error</li></ul></td></tr>', ```
```suggestion '<thead><tr><th class="original"></th>' '<th class="column-name required">Name</th>' '<th class="column-position required hidden">Position</th>' '<th>Delete?</th></tr></thead>', ```
You might append this to the `using` sql passed to `super()` to fix the test failure on Jenkins (due to usage of TABLESPACE).
I think you meant `get_ellipsized_page_range` here. Use `number` instead of `page_num` which matches the argument name used by `Paginator.get_page()`.
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
Can you re-warp this block to 79 chars? (First line is too short.)
@slafs non-existing setting names, e.g. `TEST_SETTING=True`
I'd use generic setting names to make it clear what's going to raise the exception and to avoid adjusting these settings if we ever get rid/deprecate one of these settings.
> Or should I just check that those settings doesn't exist outside the context? I think that'd work.
I wonder if it's worth adding two class attributes constants for these defaults.
Right, this was more a of suggestion open to discussion given the composite nature of `(-180.0, -90.0, 180.0, 90.0)`. I kind of wish `Field.deconstruct` was more smart wrt to `__init__` defaults by relying on `inspect` reflection in the first place.
It would have been more appropriate if `run_checks()` took an `Apps` instance as a parameter instead of a list of `AppConfig`s.
This module will be rewritten into 'exos_lldp_interfaces' using Resource Module Builder.
```suggestion - Exactly one of I(name) or I(group_id) must be provided. ```
```suggestion - Exactly one of I(name) or I(group_id) must be provided. ```
type should be `int`
```suggestion - Controls the state of interface. Up or down. ```
```suggestion - Enable or disable multicast for the interface. ```
Name as per convention can be `lag_itnerfaces_facts`
The name as per conventions can be `existing_lag_interfaces_facts` and `get_lag_interfaces_facts`
Same as above and applicable at other places as well
like diff = load_config(self._module, config_xml, [])
Another option might be to build it the way we wish it were, then start a deprecation warning cycle on this bridging behavior (eg, do something like this and/or input inspection with an explicit optin/optout in plugin metadata, decorator, or whatever, then issue a dep warning on the exception catch or undefined inputs for plugins that haven't explicitly opted into receiving them). :shrug:
That was the thing I was saying earlier- checking for/failing on undefined inputs upfront would probably be what most people would expect, but that'd be a breaking change. Even if we chose that breaking change, we'd probably need a way for filters that *want* to allow undefined inputs to do so (transparently to the caller of the filter). And yeah, we probably need an analogue for tests, since the same problems exist.
Am not sure all this is necessary as the same object is referenced... Just add the one line to call the method that wraps `close()`.
You can join this and a couple of other lines. We have a max line length of 119 chars.
"The unused `extra_context` parameter for `logout_and_login` is deprecated." Also, you can move this into the view rather than creating a decorator since it's only used once.
Can you use hanging indents and a trailing comma, please: ``` python foo( 1, 2, ) ```
I'd keep `dispatch()` the first method on the view and sort the others by call stack.
s/strng or or/string or/
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Am not sure all this is necessary as the same object is referenced... Just add the one line to call the method that wraps `close()`.
s/strng or or/string or/
Can you use hanging indents and a trailing comma, please: ``` python foo( 1, 2, ) ```
You can join this and a couple of other lines. We have a max line length of 119 chars.
That was the thing I was saying earlier- checking for/failing on undefined inputs upfront would probably be what most people would expect, but that'd be a breaking change. Even if we chose that breaking change, we'd probably need a way for filters that *want* to allow undefined inputs to do so (transparently to the caller of the filter). And yeah, we probably need an analogue for tests, since the same problems exist.
Another option might be to build it the way we wish it were, then start a deprecation warning cycle on this bridging behavior (eg, do something like this and/or input inspection with an explicit optin/optout in plugin metadata, decorator, or whatever, then issue a dep warning on the exception catch or undefined inputs for plugins that haven't explicitly opted into receiving them). :shrug:
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
I'd keep `dispatch()` the first method on the view and sort the others by call stack.
Am not sure all this is necessary as the same object is referenced... Just add the one line to call the method that wraps `close()`.
s/strng or or/string or/
Can you use hanging indents and a trailing comma, please: ``` python foo( 1, 2, ) ```
You can join this and a couple of other lines. We have a max line length of 119 chars.
That was the thing I was saying earlier- checking for/failing on undefined inputs upfront would probably be what most people would expect, but that'd be a breaking change. Even if we chose that breaking change, we'd probably need a way for filters that *want* to allow undefined inputs to do so (transparently to the caller of the filter). And yeah, we probably need an analogue for tests, since the same problems exist.
Another option might be to build it the way we wish it were, then start a deprecation warning cycle on this bridging behavior (eg, do something like this and/or input inspection with an explicit optin/optout in plugin metadata, decorator, or whatever, then issue a dep warning on the exception catch or undefined inputs for plugins that haven't explicitly opted into receiving them). :shrug:
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
I'd keep `dispatch()` the first method on the view and sort the others by call stack.
Another option might be to build it the way we wish it were, then start a deprecation warning cycle on this bridging behavior (eg, do something like this and/or input inspection with an explicit optin/optout in plugin metadata, decorator, or whatever, then issue a dep warning on the exception catch or undefined inputs for plugins that haven't explicitly opted into receiving them). :shrug:
That was the thing I was saying earlier- checking for/failing on undefined inputs upfront would probably be what most people would expect, but that'd be a breaking change. Even if we chose that breaking change, we'd probably need a way for filters that *want* to allow undefined inputs to do so (transparently to the caller of the filter). And yeah, we probably need an analogue for tests, since the same problems exist.
Am not sure all this is necessary as the same object is referenced... Just add the one line to call the method that wraps `close()`.
You can join this and a couple of other lines. We have a max line length of 119 chars.
"The unused `extra_context` parameter for `logout_and_login` is deprecated." Also, you can move this into the view rather than creating a decorator since it's only used once.
Can you use hanging indents and a trailing comma, please: ``` python foo( 1, 2, ) ```
I'd keep `dispatch()` the first method on the view and sort the others by call stack.
s/strng or or/string or/
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Hah. Had the same thought before I got here. See the caveats mentioned above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
This will still result in a command like `git config --unset foo ''`. According to the git config man page, that extra argument is a "value_regex" and its presence means only those values matching this regex will be unset. Luckily the empty string is a regex that matches everything, so it all works out fine in the end.
This variable is never assigned ```suggestion api.get_collection_versions('namespace', 'collection') ```
I think it may be cleaner to check this right away ```suggestion actual_versions = api.get_collection_versions('namespace', 'collection') assert actual_versions == [u'1.0.0', u'1.0.1', u'1.0.2', u'1.0.3', u'1.0.4', u'1.0.5'] ``` and this would simplify the check at the end of this test.
nit: ```suggestion cached_server = final_cache['galaxy.server.com:'] cached_collection = cached_server['/api/v2/collections/namespace/collection/versions/'] cached_versions = [r['version'] for r in cached_collection['results']] ```
include trailing commas
I'm curious why it's necessary to pass the loader in this case (and the others in thise PR)? I was under the impression that `shared_loader_obj` was no longer needed.
Meh, what you have is fine for now- they're basically singletons already, so that constructor arg is probably superfluous, but I don't think there are any active plans to kill it... Just IIUC whatever reason it existed for in the first place doesn't exist anymore. So this looks good to merge to me.
Per https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/ these could use `assertIs(..., True)`. I've done this for existing `.check_token()` tests in #12380.
both are know beforehand, so there is no need to use `urljoin`.
Please "rebase" to pickup the changes in 44c0ecdd9226d039a8c666b36ae320af2046a1c1.
```suggestion version_added: '2.9' ```
alright. let's keep it as is.
use python bool for the default `default=False`
Default type is `str` so you don't have to set it explicitly. Just use `size=dict(),` instead. The same bellow.
Line is too long.
Line is too long.
it might not be relevant in this particular case but be aware to better handle LANG when searching in output. use `module.run_command_environ_update = dict(LANG='C', LC_ALL='C', LC_MESSAGES='C', LC_CTYPE='C')` early after `module` has been instantiated to get that covered.
Line is too long.
Line is too long.
This line is too long. Max line length allowed in Ansible is 120 characters.
We could do that. I had planned to simply fill in the Python 3 message in a few months when Python 2 support is dropped in master.
I would be in favor of including the full exception text for the two cases with `if PY2`, etc. It's more cryptic than it needs to be. And when switching to Django 2, the `PY2` block can simply be deleted rather than having to remember to fill in the fuller message.
Would it maybe be better to only check the beginning of the query? Assume that the query is `SELECT * FROM USER_UPDATES`; in that case, because `INSERT` appears in `q.upper()` and `cursor.rowcount > 0` (assuming the table isn't empty), the module will say something was changed.
there is a helper in `AnsibleModule` for mutually exclusive params: ~~~diff module = AnsibleModule( argument_spec=argument_spec, + mutually_exclusive=(('positional_args', 'named_args'),), supports_check_mode=True, ) ~~~
How about ```suggestion q = q.lstrip()[0:max_keyword_len].upper() ``` and removing the `.upper()` calls below? Will be more efficient :)
Scratch that, the public API is `django.template.Library`, not `django.template.base.Library`
`Library` is a public API. In order to move it, you must include a dummy subclass whose `__init__` raises a deprecation warning. See `django.contrib.gis.db.models.manager.GeoManager` for an example of how to do this. Add the relevant information to 1.9.txt and deprecation.txt.
alright. let's keep it as is.
```suggestion query=dict(type='list', elements='str'), ```
Please remove this, `AnsibleModule` already prevents this.
This inline doc formatting is odd to me. Are you trying to do sphinx format maybe? ``` :param binary: ... :param subsystem: ... :param other_args: ... ```
```suggestion - Whether passed queries run in a single transaction or commit (C(yes)) them one-by-one (C(no)). ```
I'd name this argument with a simpler name, maybe something like `file` or `path`, in opposition to `query`, or at least giving an alias making it easier to use.
Minor, please append `type: str` here as well
Yeah, so none of the tests failed if I forced either the multiple or single argument form of the function. That can't be right.
Would it maybe be better to only check the beginning of the query? Assume that the query is `SELECT * FROM USER_UPDATES`; in that case, because `INSERT` appears in `q.upper()` and `cursor.rowcount > 0` (assuming the table isn't empty), the module will say something was changed.
How about ```suggestion q = q.lstrip()[0:max_keyword_len].upper() ``` and removing the `.upper()` calls below? Will be more efficient :)
there is a helper in `AnsibleModule` for mutually exclusive params: ~~~diff module = AnsibleModule( argument_spec=argument_spec, + mutually_exclusive=(('positional_args', 'named_args'),), supports_check_mode=True, ) ~~~
```suggestion query=dict(type='list', elements='str'), ```
Please remove this, `AnsibleModule` already prevents this.
We could do that. I had planned to simply fill in the Python 3 message in a few months when Python 2 support is dropped in master.
Per https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/ these could use `assertIs(..., True)`. I've done this for existing `.check_token()` tests in #12380.
This should be set only for values.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Please remove this, `AnsibleModule` already prevents this.
This doesn't cover `TextField`.
I think you meant `get_ellipsized_page_range` here. Use `number` instead of `page_num` which matches the argument name used by `Paginator.get_page()`.
There is no need to provide a value for `max_pages_num` - I think it should be calculated automatically: ```python (self.on_each_side + self.on_ends) * 2 ``` Otherwise developers can provide values that are incompatible with each other...
Single quotes for all of these as well.
You can omit the tuple and just put this on the previous line.
`'Big serial'` (`BigIntegerField` has `'Big (8 byte) integer'`, but I think we can keep this description simple.)
I understand that this is the extra query that @codingjoe is trying to get rid of before trying to merge this is; however, if this block of code does end up being used, "pg_get_serial_sequence" should be used in place using of the implicit Postgres sequence name to enable compatibility with DB migrations.
I saw that you're now handling this at the database level. It makes more sense to me.
Wouldn't be required if you subclasses `IntegerField`.
This check is also redundant.
You might append this to the `using` sql passed to `super()` to fix the test failure on Jenkins (due to usage of TABLESPACE).
This shouldn't be needed once you add the custom `deconstruct()`.
Single quotes for all of these as well.
I saw that you're now handling this at the database level. It makes more sense to me.
I understand that this is the extra query that @codingjoe is trying to get rid of before trying to merge this is; however, if this block of code does end up being used, "pg_get_serial_sequence" should be used in place using of the implicit Postgres sequence name to enable compatibility with DB migrations.
This is already tested in `test_args_kwargs_request_on_self()`, I'm going to remove these assertions.
`'Big serial'` (`BigIntegerField` has `'Big (8 byte) integer'`, but I think we can keep this description simple.)
And this one.
This test is not related with the patch, so I'll move it to a separate commit.
And all the blank lines in this test.
The docs talk about URL pattern names a lot, so I guess it's clear.
We could do that. I had planned to simply fill in the Python 3 message in a few months when Python 2 support is dropped in master.
Per https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/ these could use `assertIs(..., True)`. I've done this for existing `.check_token()` tests in #12380.
This should be set only for values.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Please remove this, `AnsibleModule` already prevents this.
This doesn't cover `TextField`.
The code I've reviewed recently always has mixins on the left. I think it's easier to be consistent than have to think about it, but feel free to propose some different guidelines if you like.
I second Tim. Unless a very specific use case, mixins should be on the left.
Read also https://www.ianlewis.org/en/mixins-and-python
I'd change this check with `not isinstance(to, (list, tuple))`. You can pass a `set()`(and almost any iterable except a string) to `EmailMessage` with using the current check.
It would be great to keep the API simple and consistent with the Python standard library and the ecosystem. So, `not isinstance(to, (list, tuple))` would be enough in my opinion.
IMO it's unpythonic to type check any more strictly than necessary. I would definitely not require this argument to be a list or tuple, when any iterable should work.
The common CPython message format for `TypeError` is `'to' argument must be a list or tuple, not 'foo'`.
I'd rather raise an exception to account for the fact that assertions are ignored with `python -O` unless there's some reason to prefer the assert.
chop "one of" add comma before "or"
put closing parenthesis on the next line
This is not necessary.
These doesn't use hooks from `OperationTestBase`.
IMO it's enough to test that `CreateExtension` honor `allow_migrate()`, creating extension is already tested in `postgres_tests`.
Should use `assertRaisesMessage()` to verify the string also.
Here you pass the variable `label`. You have never defined it, though (you defined `ch_label` -- maybe you meant that one?). That's the first of the sanity checks which fails. You might already get that output by running `flake8` on your module.
I'm curious why it's necessary to pass the loader in this case (and the others in thise PR)? I was under the impression that `shared_loader_obj` was no longer needed.
Meh, what you have is fine for now- they're basically singletons already, so that constructor arg is probably superfluous, but I don't think there are any active plans to kill it... Just IIUC whatever reason it existed for in the first place doesn't exist anymore. So this looks good to merge to me.
Here also `required=False` is not needed for parameters that are not required. Guideline dictates to leave it out.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I think a brief sentence is worthy. Nothing is worse than trying something only to figure out the target API on the server isn't compatible. We should try to remember to ask this question of all new modules and substantial changes.
I think this could be increased to 10 just in case the system is really slow. The important thing is that it doesn't loop forever.
Single quotes please.
Does this need to be a separate method? Seems unnecessary to me.
You can use `self.fail('Cyclic reference in Exception Reporter.get_traceback_frames()')`
immediatelly -> immediately
There is no guarantee something was changed here. So this would mean the module can only either fail or report changes.
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
@romgar If you find the time that would be great!
It's probably better to loop over the items and check the value before changing, so you can report back if there was a change or not.
Not sure it makes a difference but before it looks like we got `form=None` in the context.
``` python # the following time is equivalent to UTC 2014-03-13 05:34:23.24000 ```
I think this could be increased to 10 just in case the system is really slow. The important thing is that it doesn't loop forever.
Single quotes please.
You can use `self.fail('Cyclic reference in Exception Reporter.get_traceback_frames()')`
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
Guess it's better to use `self.assertGreater(len(para), 0)` instead
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
Same here, not following order `(value, expected)`
Docstring shouldn't have a trailing space. Docstring sentences should end with a period.
Same here, not following order `(value, expected)`
Not sure it makes a difference but before it looks like we got `form=None` in the context.
use the `flatten_dict` method from module_utils.network.nxos.utils.utils.py
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
There is no guarantee something was changed here. So this would mean the module can only either fail or report changes.
no blank line needed
It's probably better to loop over the items and check the value before changing, so you can report back if there was a change or not.
Unless I am mistaken, this doesnt actually do anything. I do not believe it's possible to have an 'absent' flag to remove an entire stanza. `chsec` can only remove given attrs/options from a given stanza, it cannot remove the entire stanza. This code block is just 'unsetting' all of the given attrs/options and ignoring the value assigned to them. These two YAML examples behave exactly the same way even though one has state=present and the other state=absent: ```yaml - name: Remove LDAP user stanzas aix_chsec: path: /etc/security/user stanza: ldapuser options: SYSTEM=LDAP,registry=LDAP state: absent ``` ```yaml - name: Remove LDAP user stanzas aix_chsec: path: /etc/security/user stanza: ldapuser options: SYSTEM=,registry= state: present ``` HOWEVER, if there are other attrs set on that user stanza, the *stanza will still exist*. You need to specify every single user attr/option on that stanza to remove the stanza. Adding a 'true' state=absent will mean going through the given `file`, finding the stanza, getting all of the key:value pairs in that stanza, and then running chsec key=null to every one of those. That's a loooot more work.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Here you pass the variable `label`. You have never defined it, though (you defined `ch_label` -- maybe you meant that one?). That's the first of the sanity checks which fails. You might already get that output by running `flake8` on your module.
This should be extracted in the first place.
But this code is never going to be hit if the argument_spec is set to required=True, unless someone set `url: ""`, but it's difficult to test for every bad input format - someone could equally pass `url: "not_a_protocol://thisisnonsense"`
Basically @gundalow's point is that you don't need to repeat the work that Ansible is already doing enforcing required parameters
`field_preference` must be `list` or `tuple`. There is no need to touch this usually since default sorting works fine.
```suggestion query=dict(type='list', elements='str'), ```
You should use `module.add_cleanup_file()` (somewhere above) instead of manually trying to clean up.
Please remove this, `AnsibleModule` already prevents this.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Python (capital letter when referring go the software)
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
comma after tuple
flake8 complains about missing spaces around `*`
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
++ thanks for changing this :)
It should be `if self._module.params.get('sparse') is not None`, because if `sparse` is `False` it won't send `sparse=False`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
please fail if required stuff is null
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Ahh true, sorry for the noise. No changes are required.
I removed it.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
This is not fixed.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
I prefer putting the closing ) on the next line
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I think `enumerate` would work here
prefer hanging indent style with 1 arg per line
Try except blocks should be as small as possible. In this case, if is only needed around the calls to modify and delete. Perhaps put the try except around the if state conditional to limit the statements that it surrounds
You could append the `name` parameter here (using something like `in module.params['member'] + [module.params['name']]`).
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
``` if 'autoselect_datastore' in disk: self.module.fail_json(Ã¢Â€Â¦) elif 'autoselect_datastore' not in disk: Ã¢Â€Â¦ ``` Ã¢Â†Â’ ``` if 'autoselect_datastore' in disk: self.module.fail_json(Ã¢Â€Â¦) Ã¢Â€Â¦ ```
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
Could you replace "create model" and "add field" with "CreateModel" and "AddField" respectively, please.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
non existing -> nonexistent
You could append the `name` parameter here (using something like `in module.params['member'] + [module.params['name']]`).
``` if 'autoselect_datastore' in disk: self.module.fail_json(Ã¢Â€Â¦) elif 'autoselect_datastore' not in disk: Ã¢Â€Â¦ ``` Ã¢Â†Â’ ``` if 'autoselect_datastore' in disk: self.module.fail_json(Ã¢Â€Â¦) Ã¢Â€Â¦ ```
It would be better avoiding changing method signature, you can pass the config and state as a dictionary to the method and unpack the them inside the render_config.
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
I prefer putting the closing ) on the next line
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I think `enumerate` would work here
prefer hanging indent style with 1 arg per line
Try except blocks should be as small as possible. In this case, if is only needed around the calls to modify and delete. Perhaps put the try except around the if state conditional to limit the statements that it surrounds
You could append the `name` parameter here (using something like `in module.params['member'] + [module.params['name']]`).
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
``` if 'autoselect_datastore' in disk: self.module.fail_json(Ã¢Â€Â¦) elif 'autoselect_datastore' not in disk: Ã¢Â€Â¦ ``` Ã¢Â†Â’ ``` if 'autoselect_datastore' in disk: self.module.fail_json(Ã¢Â€Â¦) Ã¢Â€Â¦ ```
Could you replace "create model" and "add field" with "CreateModel" and "AddField" respectively, please.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
Since the same result is expected as above, wouldn't it be clearer to save the output in a variable? Something like: ``` expected = ( "[ ] author_app.0001_initial\n" "[ ] book_app.0001_initial\n" "[ ] author_app.0002_alter_id\n" "[ ] mutate_state_b.0001_initial\n" "[ ] mutate_state_b.0002_add_field\n" ) ```
Yes and no. Keeping the output explicit feels easier to read.
Chop "Makes sure" prefix in favor of stating the expected behavior as described in https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/.
Copy / paste? I would have expected the ordering to change for the `duth1` and `duth2` arguments.
Swap the apps in `call_command()` but leave them sorted in the error message.
non existing -> nonexistent
Why's that? It's non-obvious at first glance.
It would help readability to use a name like "nonexistent_app" rather than "duth..".
This could be final? Also, applies to parameters in the other models
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
yeah I think it would be worthwhile to at least test a single JOIN scenario.
Might want to use simple quote here.
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: cannot create point collection with empty set of points at __randomizedtesting.SeedInfo.seed([3BC798163FFF812D:918E10886BC43EC1]:0) at org.elasticsearch.common.geo.builders.ShapeBuilder.<init>(ShapeBuilder.java:97) at org.elasticsearch.common.geo.builders.LineStringBuilder.<init>(LineStringBuilder.java:49) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseMultiLineString(GeoWKTShapeParserTests.java:112) ... ```
+1 on making configurable as well (it may very well change from one use case to the other.. this is a good default set however)
nit: formatting, add some whitespaces
Would be nice to add this as part of this PR unless something big blocks it.
This could be final? Also, applies to parameters in the other models
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
yeah I think it would be worthwhile to at least test a single JOIN scenario.
Might want to use simple quote here.
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: cannot create point collection with empty set of points at __randomizedtesting.SeedInfo.seed([3BC798163FFF812D:918E10886BC43EC1]:0) at org.elasticsearch.common.geo.builders.ShapeBuilder.<init>(ShapeBuilder.java:97) at org.elasticsearch.common.geo.builders.LineStringBuilder.<init>(LineStringBuilder.java:49) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseMultiLineString(GeoWKTShapeParserTests.java:112) ... ```
+1 on making configurable as well (it may very well change from one use case to the other.. this is a good default set however)
nit: formatting, add some whitespaces
Would be nice to add this as part of this PR unless something big blocks it.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
use the `flatten_dict` method from module_utils.network.nxos.utils.utils.py
```suggestion pkg_version = line.partition(':')[-1].strip() ```
Is it possible ? For me it would mean that it exist a package without version.
It doesn't work if pkg_provides contain a version: ``` max@mde-oxalide % pacman -Qqei perl | grep Provide Provides : perl-archive-tar=2.30 perl-attribute-handlers=1.01 perl-autodie=2.29 perl-autoloader=5.74 perl-autouse=1.11 perl-b-debug=1.26 perl-base=2.27 perl-bignum=0.49 perl-carp=1.50 perl-compress-raw-bzip2=2.074 perl-compress-raw-zlib=2.076 perl-config-perl-v=0.29 perl-constant=1.33 perl-cpan-meta-requirements=2.140 perl-cpan-meta-yaml=0.018 perl-cpan-meta=2.150010 perl-cpan=2.20 perl-data-dumper=2.170 perl-db_file=1.840 perl-devel-ppport=3.40 perl-devel-selfstubber=1.06 perl-digest-md5=2.55 perl-digest-sha=6.01 perl-digest=1.17_01 perl-dumpvalue=1.18 perl-encode=2.97 perl-encoding-warnings=0.13 perl-env=1.04 perl-experimental=0.019 perl-exporter=5.73 perl-extutils-cbuilder=0.280230 perl-extutils-constant=0.25 perl-extutils-install=2.14 perl-extutils-makemaker=7.34 perl-extutils-manifest=1.70 perl-extutils-parsexs=3.39 perl-file-fetch=0.56 perl-file-path=2.15 perl-file-temp=0.2304 perl-filter-simple=0.95 perl-filter-util-call=1.58 perl-getopt-long=2.5 perl-http-tiny=0.070 perl-i18n-collate=1.02 perl-i18n-langtags=0.43 perl-if=0.0608 perl-io-compress=2.074 perl-io-socket-ip=0.39 perl-io-zlib=1.10 perl-io=1.39 perl-ipc-cmd=1.00 perl-ipc-sysv=2.07 perl-json-pp=2.97001 perl-lib=0.64 perl-libnet=3.11 perl-locale-codes=3.56 perl-locale-maketext-simple=0.21_01 perl-locale-maketext=1.29 perl-math-bigint-fastcalc=0.5006 perl-math-bigint=1.999811 perl-math-bigrat=0.2613 perl-math-complex=1.5901 perl-memoize=1.03_01 perl-mime-base64=3.15 perl-module-corelist=5.20180622 perl-module-load-conditional=0.68 perl-module-load=0.32 perl-module-loaded=0.08 perl-module-metadata=1.000033 perl-net-ping=2.62 perl-params-check=0.38 perl-parent=0.236 perl-pathtools=3.74 perl-perl-ostype=1.010 perl-perlfaq=5.021011 perl-perlio-via-quotedprint=0.08 perl-pod-checker=1.73 perl-pod-escapes=1.07 perl-pod-parser=1.63 perl-pod-perldoc=3.2801 perl-pod-simple=3.35 perl-pod-usage=1.69 perl-podlators=5.006 perl-safe=2.40 perl-scalar-list-utils=1.50 perl-search-dict=1.07 perl-selfloader=1.25 perl-socket=2.027 perl-storable=3.08 perl-sys-syslog=0.35 perl-term-ansicolor=4.06 perl-term-cap=1.17 perl-term-complete=1.403 perl-term-readline=1.17 perl-test-harness=3.42 perl-test-simple=1.302133 perl-test=1.31 perl-text-abbrev=1.02 perl-text-balanced=2.03 perl-text-parsewords=3.30 perl-text-tabs=2013.0523 perl-thread-queue=3.12 perl-thread-semaphore=2.13 perl-threads-shared=1.58 perl-threads=2.22 perl-tie-file=1.02 perl-tie-refhash=1.39 perl-time-hires=1.9759 perl-time-local=1.25 perl-time-piece=1.3204 perl-unicode-collate=1.25 perl-unicode-normalize=1.26 perl-version=0.9923 perl-xsloader=0.30 ```
Unfortunately annotation names can contain LOOKUP_SEP - that is the reason why the ugly refs_aggregate method was added. For example qs.annotate(Max('id')) will create an annotation named max__id, and that can't be referred if the code checks for len(field_list) == 1.
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
Since this isn't implemented, perhaps lets not mention it? I found it confusing
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
use the `flatten_dict` method from module_utils.network.nxos.utils.utils.py
```suggestion pkg_version = line.partition(':')[-1].strip() ```
Is it possible ? For me it would mean that it exist a package without version.
It doesn't work if pkg_provides contain a version: ``` max@mde-oxalide % pacman -Qqei perl | grep Provide Provides : perl-archive-tar=2.30 perl-attribute-handlers=1.01 perl-autodie=2.29 perl-autoloader=5.74 perl-autouse=1.11 perl-b-debug=1.26 perl-base=2.27 perl-bignum=0.49 perl-carp=1.50 perl-compress-raw-bzip2=2.074 perl-compress-raw-zlib=2.076 perl-config-perl-v=0.29 perl-constant=1.33 perl-cpan-meta-requirements=2.140 perl-cpan-meta-yaml=0.018 perl-cpan-meta=2.150010 perl-cpan=2.20 perl-data-dumper=2.170 perl-db_file=1.840 perl-devel-ppport=3.40 perl-devel-selfstubber=1.06 perl-digest-md5=2.55 perl-digest-sha=6.01 perl-digest=1.17_01 perl-dumpvalue=1.18 perl-encode=2.97 perl-encoding-warnings=0.13 perl-env=1.04 perl-experimental=0.019 perl-exporter=5.73 perl-extutils-cbuilder=0.280230 perl-extutils-constant=0.25 perl-extutils-install=2.14 perl-extutils-makemaker=7.34 perl-extutils-manifest=1.70 perl-extutils-parsexs=3.39 perl-file-fetch=0.56 perl-file-path=2.15 perl-file-temp=0.2304 perl-filter-simple=0.95 perl-filter-util-call=1.58 perl-getopt-long=2.5 perl-http-tiny=0.070 perl-i18n-collate=1.02 perl-i18n-langtags=0.43 perl-if=0.0608 perl-io-compress=2.074 perl-io-socket-ip=0.39 perl-io-zlib=1.10 perl-io=1.39 perl-ipc-cmd=1.00 perl-ipc-sysv=2.07 perl-json-pp=2.97001 perl-lib=0.64 perl-libnet=3.11 perl-locale-codes=3.56 perl-locale-maketext-simple=0.21_01 perl-locale-maketext=1.29 perl-math-bigint-fastcalc=0.5006 perl-math-bigint=1.999811 perl-math-bigrat=0.2613 perl-math-complex=1.5901 perl-memoize=1.03_01 perl-mime-base64=3.15 perl-module-corelist=5.20180622 perl-module-load-conditional=0.68 perl-module-load=0.32 perl-module-loaded=0.08 perl-module-metadata=1.000033 perl-net-ping=2.62 perl-params-check=0.38 perl-parent=0.236 perl-pathtools=3.74 perl-perl-ostype=1.010 perl-perlfaq=5.021011 perl-perlio-via-quotedprint=0.08 perl-pod-checker=1.73 perl-pod-escapes=1.07 perl-pod-parser=1.63 perl-pod-perldoc=3.2801 perl-pod-simple=3.35 perl-pod-usage=1.69 perl-podlators=5.006 perl-safe=2.40 perl-scalar-list-utils=1.50 perl-search-dict=1.07 perl-selfloader=1.25 perl-socket=2.027 perl-storable=3.08 perl-sys-syslog=0.35 perl-term-ansicolor=4.06 perl-term-cap=1.17 perl-term-complete=1.403 perl-term-readline=1.17 perl-test-harness=3.42 perl-test-simple=1.302133 perl-test=1.31 perl-text-abbrev=1.02 perl-text-balanced=2.03 perl-text-parsewords=3.30 perl-text-tabs=2013.0523 perl-thread-queue=3.12 perl-thread-semaphore=2.13 perl-threads-shared=1.58 perl-threads=2.22 perl-tie-file=1.02 perl-tie-refhash=1.39 perl-time-hires=1.9759 perl-time-local=1.25 perl-time-piece=1.3204 perl-unicode-collate=1.25 perl-unicode-normalize=1.26 perl-version=0.9923 perl-xsloader=0.30 ```
Unfortunately annotation names can contain LOOKUP_SEP - that is the reason why the ugly refs_aggregate method was added. For example qs.annotate(Max('id')) will create an annotation named max__id, and that can't be referred if the code checks for len(field_list) == 1.
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Minor but I'd move this control flow block after the `weights` one to match the args order.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
It looks like just setting `instance_monitoring` to `true` now won't really do anything, since basic monitoring is on by default and the value of `advanced_instance_monitoring` would default to `False`. This seems confusing.
Running the legacy ec2_asg integration tests that use this module, I think this line should be `if block_device_mapping:` because it doesn't look like it can be None or an empty list. The error: ``` An exception occurred during task execution. To see the full traceback, use -vvv. The error was: Missing required parameter in BlockDeviceMappings[0]: "DeviceName" fatal: [localhost]: FAILED! => {"changed": false, "failed": true, "module_stderr": "Traceback (most recent call last):\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 436, in <module>\n main()\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 430, in main\n create_launch_config(connection, module)\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 326, in create_launch_config\n connection.create_launch_configuration(**launch_config)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 253, in _api_call\n return self._make_api_call(operation_name, kwargs)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 517, in _make_api_call\n api_params, operation_model, context=request_context)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 572, in _convert_to_request_dict\n api_params, operation_model)\n File \"/Library/Python/2.7/site-packages/botocore/validate.py\", line 270, in serialize_to_request\n raise ParamValidationError(report=report.generate_report())\nbotocore.exceptions.ParamValidationError: Parameter validation failed:\nMissing required parameter in BlockDeviceMappings[0]: \"DeviceName\"\n", "module_stdout": "", "msg": "MODULE FAILURE", "rc": 0} ```
It would be good to wrap this in a try/except botocore.exceptions.ClientError as e
not a blocker, I would probably not error out here, instead you could print out a warning message, up to your decision: ~~~python module.warn(warning='Cannot change type of an existing volume.') ~~~
The CamelCase exception .response attribute is particular to boto3's ClientError. IOError doesn't have .response so you can remove the `**camel_dict_to_snake_dict(e.response)` bit of this.
You may want to import to_text() from ansible.module_utils._text and use that instead of str() to be sure of python3 compatibility. (But having trouble testing that since boto3 isn't returning the created time for me...)
If we get into this else block the lack of an exception is going to throw a traceback. I'm not really sure about this if/else (could it take a few moments to successfully create the launch config?). But you can just fail with the module.fail_json(msg="helpful message") here since there isn't a traceback or an exception to have a .response.
Use `aliases` if you want to make these two synonymous.
Use dict literals: ```suggestion return {} ```
```suggestion b_opath = os.path.realpath(to_bytes(os.path.join(path, group))) ```
We should probably do more here than just reraise the exception with a different type. Add a message here so it gives context about the failure. The same with the next one too.
Please convert this to just a list comprehension: ```suggestion groups = [e.name for e in entities if isinstance(e, Group)] ``` Additionally, just convert to a list of names here, since you only ever need that, and not the object later.
Don't use except without an exception type. What could be the exceptions here ? It would be better to check if `get_param` returns `None`.
[`get_param`](https://github.com/spotify/pyfg/blob/master/pyFG/forticonfig.py#L249) should not raise an exception. I guess you could check if `f.running_config[path]` contains `module.params['name']ÃŒÂ€`.
You may want to consider using set operations here.
I prefer `if not group_members`.
Parentheses around `e.message` are useless.
I feel like this should be moved to `else:`
Minor but I'd move this control flow block after the `weights` one to match the args order.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
As you've noticed, we don't want to do this here because in this case the pkg_mgr is coming from a pre-existing fact.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
```suggestion type: list suboptions: ```
This doesn't appear to support the use parameter but probably should.
Running the legacy ec2_asg integration tests that use this module, I think this line should be `if block_device_mapping:` because it doesn't look like it can be None or an empty list. The error: ``` An exception occurred during task execution. To see the full traceback, use -vvv. The error was: Missing required parameter in BlockDeviceMappings[0]: "DeviceName" fatal: [localhost]: FAILED! => {"changed": false, "failed": true, "module_stderr": "Traceback (most recent call last):\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 436, in <module>\n main()\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 430, in main\n create_launch_config(connection, module)\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 326, in create_launch_config\n connection.create_launch_configuration(**launch_config)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 253, in _api_call\n return self._make_api_call(operation_name, kwargs)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 517, in _make_api_call\n api_params, operation_model, context=request_context)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 572, in _convert_to_request_dict\n api_params, operation_model)\n File \"/Library/Python/2.7/site-packages/botocore/validate.py\", line 270, in serialize_to_request\n raise ParamValidationError(report=report.generate_report())\nbotocore.exceptions.ParamValidationError: Parameter validation failed:\nMissing required parameter in BlockDeviceMappings[0]: \"DeviceName\"\n", "module_stdout": "", "msg": "MODULE FAILURE", "rc": 0} ```
Minor but I'd move this control flow block after the `weights` one to match the args order.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Use `aliases` if you want to make these two synonymous.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
It looks like just setting `instance_monitoring` to `true` now won't really do anything, since basic monitoring is on by default and the value of `advanced_instance_monitoring` would default to `False`. This seems confusing.
Minor but I'd move this control flow block after the `weights` one to match the args order.
As you've noticed, we don't want to do this here because in this case the pkg_mgr is coming from a pre-existing fact.
```suggestion type: list suboptions: ```
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
This doesn't appear to support the use parameter but probably should.
Same as above, I'd revert to the simpler version. The main idea behind `decode` for me is a consistent interface where it makes sense, we don't have to bend backwards just to use it.
`if it encounter` => `if it encounters`
`mentionned` => `mentioned`
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
Missing `=dict` on this and the next few lines
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
missing space between `,` and `and`
Please rewrite as ``` if __name__ == '__main__': main() ```
I'd do this unconditionally.
The method name with "sane" suggests quite a broad meaning. It's better to be more specific about what that method does. Also, does it need to be a method? I'd have a pure function instead.
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
I'm curious if this should be a `BaseException` so that it'd work on `Ctrl+C` (`SIGINT`) or `SystemExit` (`SIGTERM`).
maybe just ```suggestion part_boundary, b"--", ```
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
Why do the clone here and not use the clone/fetch/... code that already exists? The archive option in this way works only with a local checkout, which the module already does, so I'd rather use the existing checkout setup and run archive afterwards.
For public APIs we should be accepting text strings and internally convert to bytes where needed. If we really must accept bytes (which in this case I don't see a reason to) we should prefix the arg with `b_` to indicate that. The `b_` prefix may go away as we start to add type annotations but so far the behaviour is the standard we usually follow in ansible.
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
`default=None` is the default, it's not required.
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
Why do the clone here and not use the clone/fetch/... code that already exists? The archive option in this way works only with a local checkout, which the module already does, so I'd rather use the existing checkout setup and run archive afterwards.
Seems like this should either be ```b_missing.append(b_path)``` or ```missing.append(path)```. I don't see missing being compared or combined with any other strings later, so it may be that the latter is fine. Then again, it may be confusing to have a single variable which contains non-byte paths so it might be more readable to use the former.
I just filed https://github.com/ansible/ansible/issues/34119 which, since you're already making changes to this line, you could address.
Just add a backslash before `.tbz2`! easy peasy
Probably a typo here? ```suggestion shutil.rmtree(b_path) ```
Will this command work on all supported platforms? I tried this patch on mac and it failed. Example I used: ``` - git: repo: 'https://github.com/ganeshrn/ansible.git' dest: /var/tmp/ansible archive: yes ``` ``` TASK [git] ************************************************************************************************************************************************ fatal: [localhost]: FAILED! => {"changed": false, "failed": true, "msg": "Failed to perform archive operation"} ```
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
Probably a typo here? ```suggestion shutil.rmtree(b_path) ```
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
Seems like this should either be ```b_missing.append(b_path)``` or ```missing.append(path)```. I don't see missing being compared or combined with any other strings later, so it may be that the latter is fine. Then again, it may be confusing to have a single variable which contains non-byte paths so it might be more readable to use the former.
I'd do this unconditionally.
Why do the clone here and not use the clone/fetch/... code that already exists? The archive option in this way works only with a local checkout, which the module already does, so I'd rather use the existing checkout setup and run archive afterwards.
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
maybe just ```suggestion part_boundary, b"--", ```
note, if expanded paths is large, this might be slow. It's faster to do it like this, if so: ```suggestion expanded_paths=to_native(b', '.join(b_expanded_paths), errors='surrogate_or_strict') ```
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
I'd do this unconditionally.
The method name with "sane" suggests quite a broad meaning. It's better to be more specific about what that method does. Also, does it need to be a method? I'd have a pure function instead.
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
I'm curious if this should be a `BaseException` so that it'd work on `Ctrl+C` (`SIGINT`) or `SystemExit` (`SIGTERM`).
maybe just ```suggestion part_boundary, b"--", ```
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
Why do the clone here and not use the clone/fetch/... code that already exists? The archive option in this way works only with a local checkout, which the module already does, so I'd rather use the existing checkout setup and run archive afterwards.
For public APIs we should be accepting text strings and internally convert to bytes where needed. If we really must accept bytes (which in this case I don't see a reason to) we should prefix the arg with `b_` to indicate that. The `b_` prefix may go away as we start to add type annotations but so far the behaviour is the standard we usually follow in ansible.
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
I'd do this unconditionally.
The method name with "sane" suggests quite a broad meaning. It's better to be more specific about what that method does. Also, does it need to be a method? I'd have a pure function instead.
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
I'm curious if this should be a `BaseException` so that it'd work on `Ctrl+C` (`SIGINT`) or `SystemExit` (`SIGTERM`).
maybe just ```suggestion part_boundary, b"--", ```
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
Why do the clone here and not use the clone/fetch/... code that already exists? The archive option in this way works only with a local checkout, which the module already does, so I'd rather use the existing checkout setup and run archive afterwards.
For public APIs we should be accepting text strings and internally convert to bytes where needed. If we really must accept bytes (which in this case I don't see a reason to) we should prefix the arg with `b_` to indicate that. The `b_` prefix may go away as we start to add type annotations but so far the behaviour is the standard we usually follow in ansible.
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
It seems like you could simplify things if you had an operator map and do smth like: ```suggestion import operator ... op_map = { '!=': operator.ne, '==': operator.eq, '>=': operator.ge, '>': operator.gt, ... '*': operator.eq, } ... pos = 2 if req[1] == '=' else 1 op_map(req[:pos])(LooseVersion(version), LooseVersion(req[pos:])) ... ```
can we rename this to make clear that this has nothing to do with git, but is only about s3? `--upload-to-s3` seems a bit long
Seems like this should either be ```b_missing.append(b_path)``` or ```missing.append(path)```. I don't see missing being compared or combined with any other strings later, so it may be that the latter is fine. Then again, it may be confusing to have a single variable which contains non-byte paths so it might be more readable to use the former.
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
maybe just ```suggestion part_boundary, b"--", ```
Why do the clone here and not use the clone/fetch/... code that already exists? The archive option in this way works only with a local checkout, which the module already does, so I'd rather use the existing checkout setup and run archive afterwards.
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
For public APIs we should be accepting text strings and internally convert to bytes where needed. If we really must accept bytes (which in this case I don't see a reason to) we should prefix the arg with `b_` to indicate that. The `b_` prefix may go away as we start to add type annotations but so far the behaviour is the standard we usually follow in ansible.
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
Probably a typo here? ```suggestion shutil.rmtree(b_path) ```
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
Seems like this should either be ```b_missing.append(b_path)``` or ```missing.append(path)```. I don't see missing being compared or combined with any other strings later, so it may be that the latter is fine. Then again, it may be confusing to have a single variable which contains non-byte paths so it might be more readable to use the former.
I'd do this unconditionally.
Why do the clone here and not use the clone/fetch/... code that already exists? The archive option in this way works only with a local checkout, which the module already does, so I'd rather use the existing checkout setup and run archive afterwards.
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
maybe just ```suggestion part_boundary, b"--", ```
note, if expanded paths is large, this might be slow. It's faster to do it like this, if so: ```suggestion expanded_paths=to_native(b', '.join(b_expanded_paths), errors='surrogate_or_strict') ```
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
You should at least add real values for `datastore`, `source`, and `destination`.
You should add real values: https://github.com/ansible/ansible/pull/47271#discussion_r226405450
All modules in VMware space uses Jinja variables like the current implementation. I would stick to this naming scheme as this will make all modules same and readable. Adding arbitary values does not make sense to me.
+1 for this.
Use https://github.com/ansible/ansible/blob/cd988f645aaf774c55a98a90e3ef42cc5b1a1563/lib/ansible/module_utils/urls.py#L1147 instead of requests.get
Make `verify` configurable via parameter.
https://github.com/ansible/ansible/blob/cd988f645aaf774c55a98a90e3ef42cc5b1a1563/lib/ansible/module_utils/urls.py#L1147 gives all options.
I'd do this unconditionally.
maybe just ```suggestion part_boundary, b"--", ```
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
This is useless. `filepath` must be required to be a valid path. This must be asserted.
This will break unicode strings under python 2.
I don't think boto_params will include access_key or secret_key if a profile is being used (which might be through passing `profile` parameter or with `AWS_PROFILE` set) Looking at https://github.com/jmenga/requests-aws-sign the way to do it is to use ``` session = session.Session() credentials = session.get_credentials() ``` but even there, we'd need to cope with the `profile` parameter.
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
I would remove `else` since it is unnecessary after `raise`.
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
Here you have a choice between `bytes` and `bytearray`. I ran this crude benchmark (Python 3.7, Linux, glibc): ```python import time CHUNK = b'\x00' * 1000 NUM_ITERS = 1000000 print('num_chunks,bytes_time,bytearray_time') for num_chunks in range(1, 21): start = time.monotonic() for i in range(NUM_ITERS): body = b'' for i in range(num_chunks): body += CHUNK finish = time.monotonic() bytes_time = time.monotonic() - start start = time.monotonic() for i in range(NUM_ITERS): body = bytearray() for i in range(num_chunks): body += CHUNK body = bytes(body) bytearray_time = time.monotonic() - start print(f'{num_chunks},{bytes_time},{bytearray_time}') ``` The result is: ![bench](https://user-images.githubusercontent.com/1223550/56269664-d69ab900-60fc-11e9-9393-ba4fd3998b87.png) At least with the parameters I used, bytearray is slower for a small number of chunks, but looks linear, while bytes looks quadratic.
rquest -> request
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
It seems like you could simplify things if you had an operator map and do smth like: ```suggestion import operator ... op_map = { '!=': operator.ne, '==': operator.eq, '>=': operator.ge, '>': operator.gt, ... '*': operator.eq, } ... pos = 2 if req[1] == '=' else 1 op_map(req[:pos])(LooseVersion(version), LooseVersion(req[pos:])) ... ```
In what case is this branch reached? Should there be a test for it? I guess you meant `raise CommandError(e)`.
I'd do this unconditionally.
maybe just ```suggestion part_boundary, b"--", ```
Maybe you could use a CM here as well.
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
Make `verify` configurable via parameter.
https://github.com/ansible/ansible/blob/cd988f645aaf774c55a98a90e3ef42cc5b1a1563/lib/ansible/module_utils/urls.py#L1147 gives all options.
Use https://github.com/ansible/ansible/blob/cd988f645aaf774c55a98a90e3ef42cc5b1a1563/lib/ansible/module_utils/urls.py#L1147 instead of requests.get
can we rename this to make clear that this has nothing to do with git, but is only about s3? `--upload-to-s3` seems a bit long
For public APIs we should be accepting text strings and internally convert to bytes where needed. If we really must accept bytes (which in this case I don't see a reason to) we should prefix the arg with `b_` to indicate that. The `b_` prefix may go away as we start to add type annotations but so far the behaviour is the standard we usually follow in ansible.
It seems like you could simplify things if you had an operator map and do smth like: ```suggestion import operator ... op_map = { '!=': operator.ne, '==': operator.eq, '>=': operator.ge, '>': operator.gt, ... '*': operator.eq, } ... pos = 2 if req[1] == '=' else 1 op_map(req[:pos])(LooseVersion(version), LooseVersion(req[pos:])) ... ```
can we rename this to make clear that this has nothing to do with git, but is only about s3? `--upload-to-s3` seems a bit long
Seems like this should either be ```b_missing.append(b_path)``` or ```missing.append(path)```. I don't see missing being compared or combined with any other strings later, so it may be that the latter is fine. Then again, it may be confusing to have a single variable which contains non-byte paths so it might be more readable to use the former.
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
maybe just ```suggestion part_boundary, b"--", ```
Why do the clone here and not use the clone/fetch/... code that already exists? The archive option in this way works only with a local checkout, which the module already does, so I'd rather use the existing checkout setup and run archive afterwards.
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
For public APIs we should be accepting text strings and internally convert to bytes where needed. If we really must accept bytes (which in this case I don't see a reason to) we should prefix the arg with `b_` to indicate that. The `b_` prefix may go away as we start to add type annotations but so far the behaviour is the standard we usually follow in ansible.
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
It seems like you could simplify things if you had an operator map and do smth like: ```suggestion import operator ... op_map = { '!=': operator.ne, '==': operator.eq, '>=': operator.ge, '>': operator.gt, ... '*': operator.eq, } ... pos = 2 if req[1] == '=' else 1 op_map(req[:pos])(LooseVersion(version), LooseVersion(req[pos:])) ... ```
How about: ```suggestion if not os.path.exists(file_path): continue ```
can we rename this to make clear that this has nothing to do with git, but is only about s3? `--upload-to-s3` seems a bit long
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
maybe just ```suggestion part_boundary, b"--", ```
Why do the clone here and not use the clone/fetch/... code that already exists? The archive option in this way works only with a local checkout, which the module already does, so I'd rather use the existing checkout setup and run archive afterwards.
For public APIs we should be accepting text strings and internally convert to bytes where needed. If we really must accept bytes (which in this case I don't see a reason to) we should prefix the arg with `b_` to indicate that. The `b_` prefix may go away as we start to add type annotations but so far the behaviour is the standard we usually follow in ansible.
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
Probably a typo here? ```suggestion shutil.rmtree(b_path) ```
I'm curious if this should be a `BaseException` so that it'd work on `Ctrl+C` (`SIGINT`) or `SystemExit` (`SIGTERM`).
The method name with "sane" suggests quite a broad meaning. It's better to be more specific about what that method does. Also, does it need to be a method? I'd have a pure function instead.
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
Seems like this should either be ```b_missing.append(b_path)``` or ```missing.append(path)```. I don't see missing being compared or combined with any other strings later, so it may be that the latter is fine. Then again, it may be confusing to have a single variable which contains non-byte paths so it might be more readable to use the former.
Why do the clone here and not use the clone/fetch/... code that already exists? The archive option in this way works only with a local checkout, which the module already does, so I'd rather use the existing checkout setup and run archive afterwards.
maybe just ```suggestion part_boundary, b"--", ```
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
I'm curious if this should be a `BaseException` so that it'd work on `Ctrl+C` (`SIGINT`) or `SystemExit` (`SIGTERM`).
The method name with "sane" suggests quite a broad meaning. It's better to be more specific about what that method does. Also, does it need to be a method? I'd have a pure function instead.
Probably a typo here? ```suggestion shutil.rmtree(b_path) ```
Seems like this should either be ```b_missing.append(b_path)``` or ```missing.append(path)```. I don't see missing being compared or combined with any other strings later, so it may be that the latter is fine. Then again, it may be confusing to have a single variable which contains non-byte paths so it might be more readable to use the former.
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
maybe just ```suggestion part_boundary, b"--", ```
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
Why do the clone here and not use the clone/fetch/... code that already exists? The archive option in this way works only with a local checkout, which the module already does, so I'd rather use the existing checkout setup and run archive afterwards.
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
I'd do ```suggestion if not ignore_errors: raise ```
It's actually missing `--ignore-certs` CLI arg that implies `validate_certs=True` according to the code I saw...
How about: ```suggestion if not os.path.exists(file_path): continue ```
On python 2 `root` is byte string and `file_path` is unicode. When `root` contains non-ASCII `os.path.join` fails when trying to decode with default ascii encoding: ``` C:\temp\Ã‘Â‚ÃÂµÃ‘ÂÃ‘Â‚\youtube-dl>youtube-dl.exe -vs test:youtube [debug] System config: [] [debug] User config: [] [debug] Command-line args: [u'-vs', u'test:youtube'] [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251 [debug] youtube-dl version 2015.12.18 [debug] Python version 2.7.10 - Windows-7-6.1.7601-SP1 [debug] exe versions: ffmpeg N-73189-g7728d23, ffprobe N-73189-g7728d23, rtmpdump 2.4 [debug] Proxy map: {'http': 'http://127.0.0.1:8888', 'https': 'https://127.0.0.1:8888'} Traceback (most recent call last): File "__main__.py", line 19, in <module> File "youtube_dl\__init__.pyo", line 414, in main File "youtube_dl\__init__.pyo", line 404, in _real_main File "youtube_dl\YoutubeDL.pyo", line 1676, in download File "youtube_dl\YoutubeDL.pyo", line 664, in extract_info File "youtube_dl\extractor\common.pyo", line 291, in extract File "youtube_dl\extractor\testurl.pyo", line 65, in _real_extract File "youtube_dl\utils.pyo", line 2556, in translate File "youtube_dl\utils.pyo", line 2604, in find_file_in_root File "ntpath.pyo", line 85, in join UnicodeDecodeError: 'ascii' codec can't decode byte 0xf2 in position 6: ordinal not in range(128) ``` `root` should be decoded with proper encoding: ``` python full_path = os.path.join(decodeFilename(root), file_path) ```
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
Typo? ```suggestion files = info.get('files', {}) ```
You can save some indentation by reverting this clause: ```suggestion if not os.path.isdir(collection_path): continue ```
I'd avoid such double negation + you can save some indentation here as well: ```suggestion if no_deps: return dependency_map ```
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
This will break unicode strings under python 2.
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
Why do the clone here and not use the clone/fetch/... code that already exists? The archive option in this way works only with a local checkout, which the module already does, so I'd rather use the existing checkout setup and run archive afterwards.
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
I would remove `else` since it is unnecessary after `raise`.
Here you have a choice between `bytes` and `bytearray`. I ran this crude benchmark (Python 3.7, Linux, glibc): ```python import time CHUNK = b'\x00' * 1000 NUM_ITERS = 1000000 print('num_chunks,bytes_time,bytearray_time') for num_chunks in range(1, 21): start = time.monotonic() for i in range(NUM_ITERS): body = b'' for i in range(num_chunks): body += CHUNK finish = time.monotonic() bytes_time = time.monotonic() - start start = time.monotonic() for i in range(NUM_ITERS): body = bytearray() for i in range(num_chunks): body += CHUNK body = bytes(body) bytearray_time = time.monotonic() - start print(f'{num_chunks},{bytes_time},{bytearray_time}') ``` The result is: ![bench](https://user-images.githubusercontent.com/1223550/56269664-d69ab900-60fc-11e9-9393-ba4fd3998b87.png) At least with the parameters I used, bytearray is slower for a small number of chunks, but looks linear, while bytes looks quadratic.
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
rquest -> request
It seems like you could simplify things if you had an operator map and do smth like: ```suggestion import operator ... op_map = { '!=': operator.ne, '==': operator.eq, '>=': operator.ge, '>': operator.gt, ... '*': operator.eq, } ... pos = 2 if req[1] == '=' else 1 op_map(req[:pos])(LooseVersion(version), LooseVersion(req[pos:])) ... ```
How about: ```suggestion if not os.path.exists(file_path): continue ```
Probably a typo here? ```suggestion shutil.rmtree(b_path) ```
I'm curious if this should be a `BaseException` so that it'd work on `Ctrl+C` (`SIGINT`) or `SystemExit` (`SIGTERM`).
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
Seems like this should either be ```b_missing.append(b_path)``` or ```missing.append(path)```. I don't see missing being compared or combined with any other strings later, so it may be that the latter is fine. Then again, it may be confusing to have a single variable which contains non-byte paths so it might be more readable to use the former.
Why do the clone here and not use the clone/fetch/... code that already exists? The archive option in this way works only with a local checkout, which the module already does, so I'd rather use the existing checkout setup and run archive afterwards.
maybe just ```suggestion part_boundary, b"--", ```
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
note, if expanded paths is large, this might be slow. It's faster to do it like this, if so: ```suggestion expanded_paths=to_native(b', '.join(b_expanded_paths), errors='surrogate_or_strict') ```
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
Probably a typo here? ```suggestion shutil.rmtree(b_path) ```
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
Seems like this should either be ```b_missing.append(b_path)``` or ```missing.append(path)```. I don't see missing being compared or combined with any other strings later, so it may be that the latter is fine. Then again, it may be confusing to have a single variable which contains non-byte paths so it might be more readable to use the former.
I'd do this unconditionally.
Why do the clone here and not use the clone/fetch/... code that already exists? The archive option in this way works only with a local checkout, which the module already does, so I'd rather use the existing checkout setup and run archive afterwards.
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
maybe just ```suggestion part_boundary, b"--", ```
note, if expanded paths is large, this might be slow. It's faster to do it like this, if so: ```suggestion expanded_paths=to_native(b', '.join(b_expanded_paths), errors='surrogate_or_strict') ```
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
I'd do this unconditionally.
The method name with "sane" suggests quite a broad meaning. It's better to be more specific about what that method does. Also, does it need to be a method? I'd have a pure function instead.
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
I'm curious if this should be a `BaseException` so that it'd work on `Ctrl+C` (`SIGINT`) or `SystemExit` (`SIGTERM`).
maybe just ```suggestion part_boundary, b"--", ```
Is this the only way this can fail? If not, might be nice to provide the out, err in the message, if this is the only way this can fail this is fine though.
Why do the clone here and not use the clone/fetch/... code that already exists? The archive option in this way works only with a local checkout, which the module already does, so I'd rather use the existing checkout setup and run archive afterwards.
For public APIs we should be accepting text strings and internally convert to bytes where needed. If we really must accept bytes (which in this case I don't see a reason to) we should prefix the arg with `b_` to indicate that. The `b_` prefix may go away as we start to add type annotations but so far the behaviour is the standard we usually follow in ansible.
replace this with `return`, the actual `exit_json` call happens later and there is some ssh_wrapper code in the main routine that should run. Where do you remove tempdir/new_archive? Maybe that code got lost in one of the last refactorings. Remove the temp part should also happen before the `exit`.
The remove is in the wrong place and tempdir is not removed. Better: ``` tempdir = tempfile.mkdtemp() new_archive = os.path.join(tempdir, 'archive.' + archive_fmt) git_archive(git_path, module, dest, new_archive, archive_fmt, version) archive_unchanged = filecmp.cmp(new_archive, archive) shutil.rmtree(tempdir) if archive_unchanged: result.update(changed=False) else: ... ```
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
Omit expected type.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
`enumerate` on for range.
Lack of data is denoted by `None` not 0.
Breaks. Read coding conventions.
Useless with timestamp available.
Don't capture groups you don't use. Unused captured group.
No brackets needed.
No such meta field.
`acodec == 'none'`.
Use built-in method? Also, should it crash (raise) if `json.loads()` fails, or just return an empty dict again? ```suggestion try: return self._download_json( url, song_id, data=data.encode('ascii'), headers=headers) except ExtractorError as e: if type(e.cause) in (ValueError, TypeError): # JSON load failure raise except Exception: pass return {} ```
`field_preference` must be `list` or `tuple`. There is no need to touch this usually since default sorting works fine.
Useless with timestamp available.
Don't capture groups you don't use. Unused captured group.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
No brackets needed.
This is no longer actual.
This is useless at the end.
`enumerate` on for range.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Don't capture groups you don't use. Unused captured group.
This is useless at the end.
This is no longer actual.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
No brackets needed.
Useless with timestamp available.
Referring `url` from `url` looks like nonsense. Provide rationale.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
docstring with example input/output would be really helpful
I needed to add a `set_source_expressions` method which sets `self.subquery = source_expressions` here to get relabelling to work.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
Please ignore, my suggestion is invalid syntax.
I don't think this is safe. If someone is calling this function without unsafe_shell they probably have not quoted the arguments to prevent this sort of expansion.
Okay, looking further down, I see that you're just moving this around though...
the shell itself would have done it before. but might have done it slightly differently.
I had to change this to `template_params['subquery'], sql_params = self.subquery.query.get_compiler(connection=connection).as_sql()` in order to compile correctly when using a database other than the `DEFAULT_DB_ALIAS`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
```python if step_size is not None: self.validators.append(validators.StepValueValidator(step_size)) ``` Use `django.core.validators.MinValueValidator` as a blueprint and create a `StepValueValidator`. Put the validation code from `to_python` in there.
Have a look at the `IntegerField` how it's done there. This has the advantage, that your `StepSizeValidator` can be reused. And it shall be reused, because `step` can also be applied to an `IntegerField`.
And this can be reverted.
`min` and `max` attrs should only be added if `self.localized is False`
What if the prepared value contains unicode? (As I said before, you should use a list comprehension instead of a generator, it'll be about 2 times faster here)
`items = value.split(self.delimiter) if value else []` is slightly faster.
Ditto about the `for`/`else` construct.
Simply return `validators`.
Tests are missing for some validators, e.g. `DecimalValidator` and codes `invalid`, `max_digits`, `max_whole_digits`, and `max_decimal_places`.
consider assertRaisesMessage to make the test a bit more specific.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
`if it encounter` => `if it encounters`
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
`mentionned` => `mentioned`
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
`call_command()` raises an exception so these lines are unreachable.
Similarly, ```if tc['skip'].get('i')```
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
Can we detect if this is not a json and fallback to print it directly
Code duplication. Keep the original `_write_thumbnails` code and generalize item part. `param_name` is a bad name for something that describes an item's kind.
It's not obvious to me that the template approach is the best solution for readability as opposed to just creating another test settings file.
The purpose of this class isn't obvious to me. Is this code adapted your use case? Maybe you can add a bit more explanation to the ticket. I'm not sure what "custom default settings" are.
I guess more of the preparation activities could be moved outside making the tests cleaner: ```suggestion DOCKER_IMAGES_CALLS = [ call( '', ['images', 'quay.io/ansible/centos7-test-container', '--format', '{{json .}}'], capture=True, always=True), call( '', ['images', 'quay.io/ansible/centos7-test-container', '--format', 'json'], capture=True, always=True), ] @pytest.fixture def docker_command_patch_kwargs(docker_images, mocker, request): patch_kwargs = request.param if not patch_kwargs: return mocker.patch( 'ansible_test._internal.docker_util.docker_command', **patch_kwargs ) @pytest.mark.parametrize( ('returned_items_count', 'patched_dc_stdout', dc_calls_num), ( (3, {'return_value': (DOCKER_OUTPUT_MULTIPLE, '')}), (2, {'return_value': (PODMAN_OUTPUT, '')}), (0, {'return_value': ('', '')}), ), indirect=('ansible_module_args', ), ids=('docker JSONL', 'podman JSON sequence', 'empty output'), ) @pytest.mark.usefixtures('docker_command_patch_kwargs') def test_docker_images(returned_items_count, patched_dc_stdout, dc_calls_num, ansible_test): ret = docker_images('', 'quay.io/ansible/centos7-test-container') assert len(ret) == returned_items_count ansible_test._internal.docker_util.docker_command.assert_has_calls( DOCKER_IMAGES_CALLS[:1], ) def test_podman_fallback(ansible_test, docker_images, subprocess_error, mocker): '''Test podman >2 && <2.2 fallback''' cmd = ['docker', 'images', 'quay.io/ansible/centos7-test-container', '--format', '{{json .}}'] docker_command_results = [ subprocess_error(cmd, status=1, stderr='function "json" not defined'), (PODMAN_OUTPUT, ''), ] mocker.patch( 'ansible_test._internal.docker_util.docker_command', side_effect=docker_command_results) ret = docker_images('', 'quay.io/ansible/centos7-test-container') ansible_test._internal.docker_util.docker_command.assert_has_calls(DOCKER_IMAGES_CALLS) assert len(ret) == 2 ```
An example of using the `mocker` fixture. ```suggestion def mock_NamedTemporaryFile(mocker, **args): mock_ntf = mocker.MagicMock() mock_ntf.write = mocker.MagicMock() mock_ntf.close = mocker.MagicMock() ```
I'd rename this and the associated CLI option to `artifacts`, since not all artifacts are coverage artifacts.
This is performing a job match against the artifact name. In order to filter artifacts based on jobs you need to use the timeline results and match the artifact `source` against the timeline `id` for a given job.
I learned recently that you can use actual separate literals to improve readability: ```suggestion @pytest.mark.parametrize(['url', 'expected'], [ ```
Use module_utils/urls.py instead.
You can also add a deprecation warning for usages like `from django.forms.extras.widgets import SelectDateWidget`.
You don't modify ignore_when_null in this function so it's probably harmless to use [] as its default value but it's a bad habit to get into. You should try to always use a immutable as a default value. In this case, you can do: ```ignore_when_null=tuple()```.
If you're unfamiliar with why that is, you should probably google it. It has to do with python processing the function declaration once when the function is declared and therefore there's only one copy of the default value which is used every time the function is called. If you have a mutable container as a default value, it will not be recreated between invocations so it may not be empty the second time you call the function.
There's a lot of copying going on here as well. Both on this line and on line 456. Copying is slow so you want to eliminate any that aren't needed.
You probably want to compare this to None specifically instead of just doing a check of whether ```item[ignored_key]``` is a false value. Otherwise you'll also catch 0, False, and empty containers here. Comparing to None would look like this: ``` if ignored_key in item and item[ignored_key] is None: ```
This will fail if ```updated_list``` has items which are not present in ```original_list```. You probably want something like this instead: ``` python merged_items[item_key] = items_map.get(item_key, {}) merged_items[item_key].update(item) ```
Better to do this like this: ``` python return list(merged_items.values()) ``` Using list there will make sure that it is a list on python 3 (rather than a DictView).
In python, this is probably better represented as a global dictionary in a separate module (as outlined above). Something like file serviceprovider.py: SPKEYS = { 'ID': 'id', 'NAME': 'name', [..] } ```
also need to check whether the workspace is name or resource id
It doesn't look like you need an OrderedDict here. A regular dict will do (and be faster and take up less memory).
```suggestion self, objs, fields, batch_size, on_conflict=None, update_fields=None, unique_fields=None, ```
test failure says this should be `(None, None)`
Do we need to call `list(fields)` here? :thinking:
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
```suggestion item, fields=fields, using=self.db, ```
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Please add a trailing comma: ```suggestion on_conflict=None, update_fields=None, unique_fields=None, ```
```suggestion self, objs, fields, batch_size, on_conflict=None, update_fields=None, unique_fields=None, ```
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
Ditto for `[]` â†’ `None` and `ON_CONFLICTS_NONE` â†’ `None`.
Code duplication. Keep the original `_write_thumbnails` code and generalize item part. `param_name` is a bad name for something that describes an item's kind.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
```suggestion item, fields=fields, using=self.db, ```
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
Module argument specification ensures this cannot be true. The field is required.
Use module_utils/urls.py instead.
Please add a trailing comma: ```suggestion on_conflict=None, update_fields=None, unique_fields=None, ```
```suggestion self, objs, fields, batch_size, on_conflict=None, update_fields=None, unique_fields=None, ```
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
Ditto for `[]` â†’ `None` and `ON_CONFLICTS_NONE` â†’ `None`.
Code duplication. Keep the original `_write_thumbnails` code and generalize item part. `param_name` is a bad name for something that describes an item's kind.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
```suggestion item, fields=fields, using=self.db, ```
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
Module argument specification ensures this cannot be true. The field is required.
Use module_utils/urls.py instead.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
This sentence is too long, maybe: ```python choice = self._choice_input( f"It is impossible to add the field '{field_name}' with " f"'auto_now_add=True' to {model_name} without providing a " f"default. This is because the database needs something to " f"populate existing rows.\n", [ ... ```
I would revert this change, the previous version is clearer to me.
Period ```suggestion "Quit and manually define a default value in models.py.", ```
Maybe a little too familiar wording? Also a suggestion of what one may want to do would be helpful, for instance: `(e.g. deal with existing NULL using a separate RunSQL or RunPython operation)`.
`return None` doesn't appear to be necessary -- that's the default.
Also message above doesn't have quotes around the model name.
Once (field_name, model_name) expand I doubt it'll fit in 80chars even above. Good catch on the quote inconsistency.
Think the position of the newline should be altered -- above it seems to be so that the line fits in a standard size terminal window -- here the first line is too long -- should break after "non-nullable" instead.
"operation in the new migration file before the AlterField operation" (to give better guidance?) need newlines in the this message
```suggestion the I(verification_method) will be updated and validation data (if applicable) will be returned. ```
```suggestion - If the domain is already in the validation process but the I(verification_method) specified is different than the current I(verification_method), ```
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
```suggestion - There is a small delay (typically about 5 seconds, but can be as long as 30 seconds) before obtaining the random values when requesting a validation. ```
boto3_conn() now handles NoRegionError and ClientError so you can remove that here.
```suggestion - There is a small delay (typically about 5 seconds, but can be as long as 60 seconds) before obtaining the random values when requesting a validation ```
Sometimes you have a trailing dot (here), sometimes not (previous one). I guess you should pick one style and stick to it :)
BotoCoreError should be caught here as well and all other places catching ClientError https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/amazon/GUIDELINES.md#boto3-1 BotoCoreError does not have a .response, so using AnsibleAWSModule will cut down on exception handling logic as I suggested above.
You will need to remove these and below as we are moving away from setting the invocation parameters.
This will remove check for datacenter from line 215 ```suggestion dc_obj = self.find_datacenter_by_name(datacenter_name=self.params['datacenter']) if not dc_obj: self.module.fail_json(msg="Failed to find the datacenter %s" % self.params['datacenter']) objects = get_all_objs(content, vimtype, folder=dc_obj.networkFolder) ```
docstring with example input/output would be really helpful
Please ignore, my suggestion is invalid syntax.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
Oh, I just meant to suggest the `value` to `values` (plural) renaming, not more...
last `%version` should be `%major_minor_version`
If we're just testing broker compatibility I don't think we even need this part of the test.
boto3_conn() now handles NoRegionError and ClientError so you can remove that here.
BotoCoreError should be caught here as well and all other places catching ClientError https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/amazon/GUIDELINES.md#boto3-1 BotoCoreError does not have a .response, so using AnsibleAWSModule will cut down on exception handling logic as I suggested above.
Use a tuple as a the cache key instead of stringifying this. This will likely result in a performance problem later if not resolved now. ```suggestion key = tuple(groups) + (b_opath,) ```
The `to_bytes` should be on the `self._basedir` instead: ```suggestion b_opath = os.path.realpath(os.path.join(to_bytes(self._basedir), b'r_group_vars')) ```
Might be worth adding a `note:` to say that only the hash of the file is used for comparisons, not any filemode/permissions/etc.
I know you asked me about this but it might be a good idea to explain that PowerShell unpacks arrays if there's only a single entry so this is a defensive check on that.
chop "should" (just state the behavior)
maybe just ```suggestion part_boundary, b"--", ```
Returns -> Return use period
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
This fails for me: ``` AssertionError: <MagicMock name='WatchmanReloader()' id='140033332456584'> is not an instance of <class 'django.utils.autoreload.StatReloader'> ``` perhaps that's the failure that Jenkins encountered.
Beware that the fourth positional argument has shifted, so if people used this interface using positional arguments it may break. (I don't think there's a risk, as we can safely assume you're the only user of the interface...)
no restructured text (:class:) in docstrings please
Is there a typo? I'm not sure what "hub" means in this sentence.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
it should also check if it can write there
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
`copy()` in unnecessary.
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
Please wrap: ``` # If true, uniqueness validation checks will consider this a new, unsaved # object. Necessary for correct validation of new instances of objects with # explicit (non-auto) PKs. This impacts validation only; it has no effect # on the actual ```
Thanks both :+1: I pushed edits.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
These don't appear to be "public" attributes, so we should stay consistent with indicating their privateness by prepending their names with an `_` character.
Looking at this, this one picks first source as self.source. Above self.col is picked from last target. Should we just throw an error for multicolumn expressions? I bet they don't work currently in any sane way, so lets not pretend they work.
This loop seems strange to say the least - all but last result are overwritten by the looping.
We can remove this check after fixing the `Field.slice_expression()`.
Chop blank line.
consider assertRaisesMessage to make the test a bit more specific.
"if all childs refer same children", I'm not too sure what it means. Also maybe the `num_childs` var could be changed to `num_children`.
chop newline for consistency with other tests
please include the trailing comma on the last item in a dictionary so if more items are added we don't have to modify this line again
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
I think there isn't much organization there. Using an existing site should be fine.
I tried a similar approach while working on acfaec3db5ba39de52f6e607e74343dccf72fba1 and came to the conclusion that this approach can't work (due to something like module caching). As far as I know, you'll have to register the admin to a separate `AdminSite`.
Prefer this indent style: ``` response = self.client.post(reverse('...'), { .... }) ```
`assertNotContains` is a bit fragile; instead I'd like if you could check `response.context` variables.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Use a tuple as a the cache key instead of stringifying this. This will likely result in a performance problem later if not resolved now. ```suggestion key = tuple(groups) + (b_opath,) ```
The `to_bytes` should be on the `self._basedir` instead: ```suggestion b_opath = os.path.realpath(os.path.join(to_bytes(self._basedir), b'r_group_vars')) ```
Might be worth adding a `note:` to say that only the hash of the file is used for comparisons, not any filemode/permissions/etc.
I know you asked me about this but it might be a good idea to explain that PowerShell unpacks arrays if there's only a single entry so this is a defensive check on that.
chop "should" (just state the behavior)
maybe just ```suggestion part_boundary, b"--", ```
Returns -> Return use period
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
This fails for me: ``` AssertionError: <MagicMock name='WatchmanReloader()' id='140033332456584'> is not an instance of <class 'django.utils.autoreload.StatReloader'> ``` perhaps that's the failure that Jenkins encountered.
Use a tuple as a the cache key instead of stringifying this. This will likely result in a performance problem later if not resolved now. ```suggestion key = tuple(groups) + (b_opath,) ```
The `to_bytes` should be on the `self._basedir` instead: ```suggestion b_opath = os.path.realpath(os.path.join(to_bytes(self._basedir), b'r_group_vars')) ```
Might be worth adding a `note:` to say that only the hash of the file is used for comparisons, not any filemode/permissions/etc.
I know you asked me about this but it might be a good idea to explain that PowerShell unpacks arrays if there's only a single entry so this is a defensive check on that.
chop "should" (just state the behavior)
maybe just ```suggestion part_boundary, b"--", ```
Returns -> Return use period
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
This fails for me: ``` AssertionError: <MagicMock name='WatchmanReloader()' id='140033332456584'> is not an instance of <class 'django.utils.autoreload.StatReloader'> ``` perhaps that's the failure that Jenkins encountered.
First we should verify this passes before we toggle `is_active` to False.
check the group permissions before `is_active = False` too.
Actually I guess the ideal way to test this would be something like: ``` backend = ModelBackend() backend.get_user_permissions(user) ```
Can you use `assertContains()` here to prevent errors due to attribute order in the HTML tags.
Same here, can you use `assertContains()` here to prevent errors due to attribute order in the HTML tags.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
okay, but would be helpful to say _why_ we need to always return True.
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
@wrouesnel CI failure due to PEP 8 issue: ``` 2017-01-28 07:26:38 ERROR: PEP 8: lib/ansible/modules/cloud/amazon/ec2.py:930:1: W293 blank line contains whitespace (legacy) ```
```suggestion # Nested coalesce ```
if min_count <=
According to pep8, there's a missing space after `+` in here.
`xrange` has been removed in Python 3. Simply use `range` instead.
Don't use floating point math for calculations that depend on precise results! Instead, you can simply calculate `((videos_count + self.PAGINATED - 1) // self.PAGINATED) + 1`
I've just tested, and ordering by count is fine, provided you switch the order of expected to: ``` {'rating': 4.0, 'count': 1}, {'rating': 4.0, 'count': 2}, ``` To remain consistent, let's order the first test by `count` also.
@timgraham is ordering by the result of an aggregate allowed without subqueries? If ordering by count does not error, then it should be safe to use that ordering. If not, introducing a different field into the orderby will affect the grouping (not that you suggested that), so we'll need to look at comparing the queryset out of order if there's another assert method available that does that. I'm not able to check either of these things at the moment, but I can take a look in about 8 hours if it's not resolved.
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
You could use `subtest()` for the loop.
According to pep8, there's a missing space after `+` in here.
`xrange` has been removed in Python 3. Simply use `range` instead.
Don't use floating point math for calculations that depend on precise results! Instead, you can simply calculate `((videos_count + self.PAGINATED - 1) // self.PAGINATED) + 1`
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
Given that `model` also exists, it is weird that `field` refers to `original_model`. (Also where is `original_model` defined -- if nowhere then certainly tests seems to be missing). Maybe also rename to source & target model
You are leaking information about whether somebody has access or something doesn't exist.
Raising a 404 with the same message as in the previous check would mask the issue. Then again I think we already leak a lot like that in other admin pages, will have to double check.
according to doc spec, there should be an alias: ~~~diff - ip=dict(), + ip=dict(aliases=['network']), ~~~
I think something like this will be better (please test it to ensure you do get an error if the param/ENV isn't set) `api_token=dict(fallback=(env_fallback, ['CLOUDSCALE_API_TOKEN']), no_log=True, required=True),` Then you can delete ``` api_token = module.params['api_token'] or os.environ.get('CLOUDSCALE_API_TOKEN') if not api_token: module.fail_json... ```
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
@wrouesnel CI failure due to PEP 8 issue: ``` 2017-01-28 07:26:38 ERROR: PEP 8: lib/ansible/modules/cloud/amazon/ec2.py:930:1: W293 blank line contains whitespace (legacy) ```
```suggestion # Nested coalesce ```
if min_count <=
According to pep8, there's a missing space after `+` in here.
`xrange` has been removed in Python 3. Simply use `range` instead.
Don't use floating point math for calculations that depend on precise results! Instead, you can simply calculate `((videos_count + self.PAGINATED - 1) // self.PAGINATED) + 1`
I've just tested, and ordering by count is fine, provided you switch the order of expected to: ``` {'rating': 4.0, 'count': 1}, {'rating': 4.0, 'count': 2}, ``` To remain consistent, let's order the first test by `count` also.
@timgraham is ordering by the result of an aggregate allowed without subqueries? If ordering by count does not error, then it should be safe to use that ordering. If not, introducing a different field into the orderby will affect the grouping (not that you suggested that), so we'll need to look at comparing the queryset out of order if there's another assert method available that does that. I'm not able to check either of these things at the moment, but I can take a look in about 8 hours if it's not resolved.
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
@wrouesnel CI failure due to PEP 8 issue: ``` 2017-01-28 07:26:38 ERROR: PEP 8: lib/ansible/modules/cloud/amazon/ec2.py:930:1: W293 blank line contains whitespace (legacy) ```
```suggestion # Nested coalesce ```
if min_count <=
According to pep8, there's a missing space after `+` in here.
`xrange` has been removed in Python 3. Simply use `range` instead.
Don't use floating point math for calculations that depend on precise results! Instead, you can simply calculate `((videos_count + self.PAGINATED - 1) // self.PAGINATED) + 1`
I've just tested, and ordering by count is fine, provided you switch the order of expected to: ``` {'rating': 4.0, 'count': 1}, {'rating': 4.0, 'count': 2}, ``` To remain consistent, let's order the first test by `count` also.
@timgraham is ordering by the result of an aggregate allowed without subqueries? If ordering by count does not error, then it should be safe to use that ordering. If not, introducing a different field into the orderby will affect the grouping (not that you suggested that), so we'll need to look at comparing the queryset out of order if there's another assert method available that does that. I'm not able to check either of these things at the moment, but I can take a look in about 8 hours if it's not resolved.
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Any problem with: ``` @property def media(self): ```
Perhaps "Cannot convert DateField '%s' to DateTimeField." (I'm not certain it's an issue here, but we're favoring '%s' over %r until we drop Python 2 support to avoid possible u'' prefixes.)
Put the ending `)` on a new line.
okay, but would be helpful to say _why_ we need to always return True.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
@wrouesnel CI failure due to PEP 8 issue: ``` 2017-01-28 07:26:38 ERROR: PEP 8: lib/ansible/modules/cloud/amazon/ec2.py:930:1: W293 blank line contains whitespace (legacy) ```
```suggestion # Nested coalesce ```
if min_count <=
According to pep8, there's a missing space after `+` in here.
`xrange` has been removed in Python 3. Simply use `range` instead.
Don't use floating point math for calculations that depend on precise results! Instead, you can simply calculate `((videos_count + self.PAGINATED - 1) // self.PAGINATED) + 1`
I've just tested, and ordering by count is fine, provided you switch the order of expected to: ``` {'rating': 4.0, 'count': 1}, {'rating': 4.0, 'count': 2}, ``` To remain consistent, let's order the first test by `count` also.
@timgraham is ordering by the result of an aggregate allowed without subqueries? If ordering by count does not error, then it should be safe to use that ordering. If not, introducing a different field into the orderby will affect the grouping (not that you suggested that), so we'll need to look at comparing the queryset out of order if there's another assert method available that does that. I'm not able to check either of these things at the moment, but I can take a look in about 8 hours if it's not resolved.
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
okay, but would be helpful to say _why_ we need to always return True.
Put the ending `)` on a new line.
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
Does `mask` require `addr`, and does `addr` require `mask`, if so you may wish to add: ``` required_together ```
For integers please: use `type='int'` remove the `isdigit` check from `check_params`
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` required_together ```
For integers please: use `type='int'` remove the `isdigit` check from `check_params`
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` required_together ```
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
I'd omit a blank line after each docstring.
Can you re-warp this block to 79 chars? (First line is too short.)
You are leaking information about whether somebody has access or something doesn't exist.
Raising a 404 with the same message as in the previous check would mask the issue. Then again I think we already leak a lot like that in other admin pages, will have to double check.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
Given that `model` also exists, it is weird that `field` refers to `original_model`. (Also where is `original_model` defined -- if nowhere then certainly tests seems to be missing). Maybe also rename to source & target model
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Any problem with: ``` @property def media(self): ```
Perhaps "Cannot convert DateField '%s' to DateTimeField." (I'm not certain it's an issue here, but we're favoring '%s' over %r until we drop Python 2 support to avoid possible u'' prefixes.)
Put the ending `)` on a new line.
okay, but would be helpful to say _why_ we need to always return True.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Any problem with: ``` @property def media(self): ```
Perhaps "Cannot convert DateField '%s' to DateTimeField." (I'm not certain it's an issue here, but we're favoring '%s' over %r until we drop Python 2 support to avoid possible u'' prefixes.)
Put the ending `)` on a new line.
okay, but would be helpful to say _why_ we need to always return True.
put closing parenthesis on the next line
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Any problem with: ``` @property def media(self): ```
Perhaps "Cannot convert DateField '%s' to DateTimeField." (I'm not certain it's an issue here, but we're favoring '%s' over %r until we drop Python 2 support to avoid possible u'' prefixes.)
Put the ending `)` on a new line.
okay, but would be helpful to say _why_ we need to always return True.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Any problem with: ``` @property def media(self): ```
Perhaps "Cannot convert DateField '%s' to DateTimeField." (I'm not certain it's an issue here, but we're favoring '%s' over %r until we drop Python 2 support to avoid possible u'' prefixes.)
Put the ending `)` on a new line.
okay, but would be helpful to say _why_ we need to always return True.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
I'd omit a blank line after each docstring.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
Here, `outdated` is a `dict` (or `list` in mheap/ansible#3).
Given that `model` also exists, it is weird that `field` refers to `original_model`. (Also where is `original_model` defined -- if nowhere then certainly tests seems to be missing). Maybe also rename to source & target model
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
I'd omit a blank line after each docstring.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
Here, `outdated` is a `dict` (or `list` in mheap/ansible#3).
Given that `model` also exists, it is weird that `field` refers to `original_model`. (Also where is `original_model` defined -- if nowhere then certainly tests seems to be missing). Maybe also rename to source & target model
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
I'd omit a blank line after each docstring.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
Here, `outdated` is a `dict` (or `list` in mheap/ansible#3).
Given that `model` also exists, it is weird that `field` refers to `original_model`. (Also where is `original_model` defined -- if nowhere then certainly tests seems to be missing). Maybe also rename to source & target model
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Any problem with: ``` @property def media(self): ```
Perhaps "Cannot convert DateField '%s' to DateTimeField." (I'm not certain it's an issue here, but we're favoring '%s' over %r until we drop Python 2 support to avoid possible u'' prefixes.)
Put the ending `)` on a new line.
okay, but would be helpful to say _why_ we need to always return True.
put closing parenthesis on the next line
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
Passing title here makes no sense for non playlists since it will be overridden by delegated extractor. If you want to pass metadata that won't be overridden you should return info dict of [`url_transparent` type](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/makertv.py#L28-L31).
``` for i, video_url in enumerate(video_urls): ```
`check_args()` is a empty function. Is this call required? For other networks platforms `check_args()` is present for legacy reason.
Why `dict(required=True)` is added here? It is already part of the spec.
You can use iteritems as below ``` from ansible.module_utils.six import iteritems iteritems(parsed) ```
Should this handle for space within interface name? Something similar to https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/network/eos/eos_vrf.py#L224 to avid idempotency related issues.
Maybe output the bad value here as well, to help the user find out which one was wrong.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Is `address` option required as part of this module? I think address can be configured as part of `vyos_l3_interface` module.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Why not also do this when `module.check_mode` is `True`? Then the check mode behavior is closer to the real thing.
ipt_load_stderr doesnt appear to be defined anywhere.
```suggestion Kwargs: ```
I'd recommend a more explicit import, ie ``` python from ansible.module_utils.basic import AnsibleModule' ``` And using regular imports for 're', 'shlex', 'os', 'json', 'tempfile'
Why not pass `check_mode` into `exec_sql` and let it (if set to `True`) simply not execute the command? Then you don't have to repeat the pattern `if not check_mode: / exec_sql(...) / else: / self.executed_queries.append(...)` all over the module.
In case this pattern is used in other modules as well, moving it into `exec_sql` is probably a good idea. Also, adding a new argument to `exec_sql` with a default value shouldn't affect the other users.
one more for the single line version
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
This is lifted from Python 3.10 so we should attribute appropriately and also ensure that we remove it when no longer needed: ```suggestion if PY310: from contextlib import aclosing else: from contextlib import AbstractAsyncContextManager # Backport of contextlib.aclosing() from Python 3.10. # Copyright (C) 2020 Python Software Foundation (see LICENSE.python). class aclosing(AbstractAsyncContextManager): """ Async context manager for safely finalizing an asynchronously cleaned-up resource such as an async generator, calling its ``aclose()`` method. """ def __init__(self, thing): self.thing = thing async def __aenter__(self): return self.thing async def __aexit__(self, *exc_info): await self.thing.aclose() ```
Does this need to be a separate method? Seems unnecessary to me.
`always_text` is gone.
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
```suggestion to iterate use a C(with_) directive. ```
No `else` needed since we used `return` for both other cases. For the exception, I think we can just throw `ClassCastException` since `IllegalStateException` doesn't fit very well for this case. I would also make the message a bit more generic to avoid it going stale when we add more `Records` subtypes. For example: ```java "The record type is " + partition.records().getClass().getSimpleName() + ", which is not a subtype of " + Records.class.getSimpleName() + ". This method is only safe to call if the `FetchResponse` was deserialized from bytes."
One more thing, let's call this `recordsOrFail` to make it clear that the operation is not necessarily safe.
Good catch. It's probably too late to change that though.
+1 for consistency
```suggestion Test that the returned value for timezone consists of only uppercase ```
This seems like it will make for a hard API to use because it will fail when the lock_file is owned by another user (so playbooks run by different users or async with tasks that become different users will raise Permission denied errors). It seems like problems opening the lock_file should be part of the timeout.
For a gzip file, I believe this will cause the browser to unzip it. ``` $ python3 >>> import mimetypes >>> mimetypes.guess_type('f.tar.gz') ('application/x-tar', 'gzip') ``` But I'm guessing the user wants the mime type to be `application/gzip` (or similar) with no encoding. [MDN Docs](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Encoding) > The Content-Encoding entity header is used to compress the media-type. When present, its value indicates which encodings were applied to the entity-body. It lets the client know, how to decode in order to obtain the media-type referenced by the Content-Type header. I know of at least one third party Django app that has this issue: https://github.com/johnsensible/django-sendfile/issues/38
I think we are missing the `call_command()` here.
~~why stat and return the data when you are dropping it on caller?~~ 2nd caller does use
BlockingIOError is not available in Python2. Probably need to use OSError and check that it's the particular OSErrors that we care about.
it should also check if it can write there
And `\r` or `\n` -- I dunno if Django protects from header injection
this creates race condition. there is a time between remove and move that the file is unavailable. I see original code did same, but we should just allow move to work as it will be an atomic operation
to_text and u prefix on string.
Display should use text strings, will need a to_text in the format and u prefix the string so we don't have any ASCII encoding errors on Python 2.
`always_text` is gone.
Should have a default set to `present`.
Should become parameter `username` (with a backward-compatible alias). (See #20160 and #25398)
Should become parameter `password` (with a backward-compatible alias). (See #20160 and #25398)
I would still leave one example without the implicit `state: present`
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
Since we have a line for `version_added`, I'd remove the "(added in Ansible 2.0)".
+1 for adding `name:` lines
Note to self: if this is not discussed elsewhere in the docs, we should add it.
Typo: a reboot
Prefer this indent style: ``` response = self.client.post(reverse('...'), { .... }) ```
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Use single quotes consistently.
Any problem with: ``` @property def media(self): ```
I think `name.rsplit('-', 1)[-1]` is easier to read.
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
Instead, you can use `return data.get('name')` and it will return None if name is undefined.
I would use a list comprehension rather than `list(map())`
add trailing comma
include trailing ,
nit: since the error message says "the elements" shouldn't this check all elements? ```suggestion if not all(val): ``` alternatively, we can leave the if-statement as-is and instead change the error to something like "the first key in the list must not be empty"
We do not have to overdo it, but `if not all(val)` seems easier and brings over the intent better than `if not val[0]`
If `getattr()` is not necessary, do we need this round trip at all? ```python if val is None: ... val = data[field_name] data[field_name] = val ``` It looks that we can remove [this line](https://github.com/django/django/pull/13036/files#diff-1e7fc0d7d1b36358e371fab97bd1ddb1R152) :thinking: , e.g. ```python if val is None: instance.refresh_from_db(fields=[field_name]) else: data[field_name] = val ```
`# If a list of input formats from one of the format_modules was retrieved, make sure...`
construct a local dict in this method and add result to to this dict, finally return it. DON'T modified a complicated global variable in different methods, this makes the code hard to read and maintain.
IMO, there is no sense to add an additional method call for `self.fetch`, putting the logic to `self.run` is enough. Then the fetch should maintain a local dict rather than changing a global variable frequently
Replace this method with `fetch`, with return value.
You should follow standard [Ansible AWS Exception handling guidelines](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/amazon/GUIDELINES.md#exception-handling-for-boto3-and-botocore)
You can also add a deprecation warning for usages like `from django.forms.extras.widgets import SelectDateWidget`.
I would also detect if the value is a bool and set it accordingly to `on`/`off`.
```suggestion self, objs, fields, batch_size, on_conflict=None, update_fields=None, unique_fields=None, ```
It would be more readable to raise an error explicitly (like previously), e.g. ```python db_features = connections[self.db].features if ignore_conflicts and not db_features.supports_ignore_conflicts: raise NotSupportedError('This database backend does not support ignoring conflicts.') if update_conflicts: if not db_feature.supports_update_conflicts: raise NotSupportedError( 'This database backend does not support updating conflicts.' ) if unique_fields and not db_features.supports_update_conflicts_with_target: raise NotSupportedError( 'This database backend does not support updating conflicts with ' 'specifying unique fields that will trigger the upsert.' ) ``` (I used new feature flags.)
Ditto for `[]` â†’ `None` and `ON_CONFLICTS_NONE` â†’ `None`.
Please add a trailing comma: ```suggestion on_conflict=None, update_fields=None, unique_fields=None, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
Use another lookup instead of `epoch` e.g. `second`.
Please use a single quote.
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
Instead, you can use `return data.get('name')` and it will return None if name is undefined.
with -> width
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
comma after tuple
width, height, and offset
```suggestion - For tracking function statistics the PostgreSQL C(track_functions) parameter must be enabled. ```
I just dag deeper into the code and I don't understand why we don't support check mode in all situations where possible.
Why can't we just use `SHOW` to check if the parameter is set to the correct value? Not supporting check mode is a big limitation for this module which for me it would make it useless. Postgres is usually a very I important so I should know what changes are going to be made on it, before they are made.
I think this might explode if `obj_type` is not one of `functions`, `indexes` or `tables`, maybe it would be safer to do something like? ```suggestion obj_func = self.obj_func_mapping.get(obj_type.strip()) if obj_func is not None: obj_func() else: module.warn("Unknown filter option '{}'".filter(obj_type.strip()) ```
Not something that you have to change but I think this could be simplified to ```python for obj_func in self.obj_func_mapping.values(): obj_func() ```
Right, this was more a of suggestion open to discussion given the composite nature of `(-180.0, -90.0, 180.0, 90.0)`. I kind of wish `Field.deconstruct` was more smart wrt to `__init__` defaults by relying on `inspect` reflection in the first place.
I wonder if it's worth adding two class attributes constants for these defaults.
We can register model without defining `OfficeAdminWithOrdering` with the same effect: ```python site.register(Office) ```
This class is unnecessary.
This can be single-lined.
That does make sense. Thanks.
Given that `model` also exists, it is weird that `field` refers to `original_model`. (Also where is `original_model` defined -- if nowhere then certainly tests seems to be missing). Maybe also rename to source & target model
(In general, I don't think modules should have such options.)
You should emphasize that the module can and will not do any idempotence checking for this.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
Keep the `r`
is_vapp_changed = False
I think you didn't understand what I was suggesting. What you do : Add each of the property the user have asked for in new_vmconfig_spec.property, and if at least one of them modify the properties already attached to the VM Ã¢Â†Â’ apply new_vmconfig_spec What I propose : Only add to new_vmconfig_spec.property the property the user have asked for that actually modify (add/remove/edit) the properties already attached to the VM. And then, if there is some properties in new_vmconfig_spec.property Ã¢Â†Â’ apply new_vmconfig_spec.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
I would chop blank lines in this test.
Chop `Ensure that`.
Same question for dropping lambda here as well.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
In theory a locale dependant pattern could be provided and `locale.setlocale` be called but this is really unlikely.
no need for `.keys()`. `if property_id in vapp_properties_current:` is the same as `if property_id in vapp_properties_current.keys():`
I think you didn't understand what I was suggesting. What you do : Add each of the property the user have asked for in new_vmconfig_spec.property, and if at least one of them modify the properties already attached to the VM Ã¢Â†Â’ apply new_vmconfig_spec What I propose : Only add to new_vmconfig_spec.property the property the user have asked for that actually modify (add/remove/edit) the properties already attached to the VM. And then, if there is some properties in new_vmconfig_spec.property Ã¢Â†Â’ apply new_vmconfig_spec.
is_vapp_changed = False
I would chop blank lines in this test.
Chop `Ensure that`.
Same question for dropping lambda here as well.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
In theory a locale dependant pattern could be provided and `locale.setlocale` be called but this is really unlikely.
Hah. Had the same thought before I got here. See the caveats mentioned above.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
no need for `.keys()`. `if property_id in vapp_properties_current:` is the same as `if property_id in vapp_properties_current.keys():`
I think you didn't understand what I was suggesting. What you do : Add each of the property the user have asked for in new_vmconfig_spec.property, and if at least one of them modify the properties already attached to the VM Ã¢Â†Â’ apply new_vmconfig_spec What I propose : Only add to new_vmconfig_spec.property the property the user have asked for that actually modify (add/remove/edit) the properties already attached to the VM. And then, if there is some properties in new_vmconfig_spec.property Ã¢Â†Â’ apply new_vmconfig_spec.
is_vapp_changed = False
I would chop blank lines in this test.
Chop `Ensure that`.
Same question for dropping lambda here as well.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
I would chop blank lines in this test.
Chop `Ensure that`.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
while purging all username we should preserve the username which is used to run the playbook otherwise we might hit connection timeout in middle and leave the box with partial configurations
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'd be great to assert the permission and content types were appropriately created as well!
Silenced the output. The `create_default_site` method deserves a more basic test, but there's more to it than the output, could be another ticket I guess (this one is pretty convolved as is). Here's a [sketch of a test](https://github.com/wrwrwr/django/commit/8927a52a4271ab8208783ce4c2af31814314a6c8).
preferred format is "#15346, #15573 - Issue description"
`value` could be used instead of `argv`. `argk` is only required when `argv is not None` and could be moved in the `if` block.
it's a separate item, but I wonder if we could patch override_settings to handle DATABASE_ROUTERS like is done below
Instead of ignoring these arguments, I would flag them as mutually exclusive (Proposal: https://github.com/ansible/ansible/pull/39809/commits/c984053884c1866b5bf8cd42a01e671352f4142e from #39809).
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
Interesting thought. But if a key is compromised one would switch from one key in the list to still one key (the new one) because you wouldn't want to keep the compromised key active. So in the case of a compromise I'd always expect the list to stay constant in length because the offending key would be replace with a new one (independent of other keys probably). Either way for the majority of cases (ie under normal operation) I'd expect just one key in there (or always two if one rotates a key every $x weeks)
Actually it needs to be `AttributeError` to keep everything happy (which the import machinery will turn into an `ImportError` if it was sourced by an import)
If I'm not completely mistaken this will fail if the attacker requests a page where `csrfmiddlewaretoken` or `HTTP_X_CSRFTOKEN` is shorter than the `CSRF_KEY_LENGTH`.
I think we should catch here only `core.signing.BadSignature`.
with -> width
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
comma after tuple
Could use `assertSequenceEqual` to avoid the `itemgetter`
width, height, and offset
I would chop blank lines in this test.
Chop `Ensure that`.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
Same question for dropping lambda here as well.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
This function is too complex. Could you please factor and avoid: - more than 2 level of condition - inner loop. You can probably break down the function in several small functions.
with -> width
width, height, and offset
comma after tuple
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
Never mind, I now see the change to galaxy further down.
This seems like it would break galaxy which needed expand_paths
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
As discussed on IRC: no.
I would chop blank lines in this test.
Chop `Ensure that`.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
Same question for dropping lambda here as well.
This is not fixed.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
This function is too complex. Could you please factor and avoid: - more than 2 level of condition - inner loop. You can probably break down the function in several small functions.
We should state clearly what to use when deprecation ends, there is a risk that we'll remove logging of suspicious session: ```python def decode(self, session_data): try: return signing.loads(session_data, salt=self.key_salt, serializer=self.serializer) # RemovedInDjango40Warning: when the deprecation ends, replace with: # except signing.BadSignature: # logger = logging.getLogger('django.security.SuspiciousSession') # logger.warning('Session data corrupted') # except Exception: # # ValueError, unpickling exceptions. If any of these happen, # # return an empty dictionary (an empty session). # pass # return {} except Exception: return self._legacy_decode(session_data) ```
with -> width
width, height, and offset
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
comma after tuple
```suggestion - For tracking function statistics the PostgreSQL C(track_functions) parameter must be enabled. ```
I don't understand why we have methods with a double underscores prefix which are copies from `SessionBase`, e.g. `__hash()`, `__legacy_encode()`, `__legacy_decode()` :thinking:
I assume you meant to say `Unable to decrypt value using KMS`
We can also write a single line return statement. Please see if it makes sense. ```suggestion return (uid is not None and path_stat.st_uid == uid) or (gid is not None and path_stat.st_gid == gid) or (uid is None and gid is None) ```
no need for this line as None is already a False value
your rigth ! when I developped this module, there was no `uid` in Grafana, so i've removed the `uid` same as was doing for the `id` field before when adding the Grafana 5 compatibility.
OK, so we can override the `uid` of the json with the uid provided in ansible tasks. Good ! LGTM
It's possible we need to guard this code with `if settings.USE_I18N` is some way. Also, I'd like to make sure we are not interfering with the `Trans/_trans` black magic this module performs by putting this signal receiver here.
I think it's best to always provide **template_destpath**, and if it wasn't defined return `jinja.StrictUndefined` or possible `None`.
I fixed this issue in fe0ddbc84e4d8836ce8d27a1218d360c5482c2be.
I think this should use the `request['path']`.
`AnsibleConnectionFailure` is a generic error, it does not mean the resource is missing. It should be more like an err 500.
Not sure it makes a difference but before it looks like we got `form=None` in the context.
Does the order matter? If yes, it's probably better to use ```suggestion return '/'.join(sorted(priv_list)) ```
This function is too complex. Could you please factor and avoid: - more than 2 level of condition - inner loop. You can probably break down the function in several small functions.
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
with -> width
width, height, and offset
I don't understand why we have methods with a double underscores prefix which are copies from `SessionBase`, e.g. `__hash()`, `__legacy_encode()`, `__legacy_decode()` :thinking:
comma after tuple
```suggestion - For tracking function statistics the PostgreSQL C(track_functions) parameter must be enabled. ```
I assume you meant to say `Unable to decrypt value using KMS`
What if `default` is not a constant but a field reference? e.g. `F('integer')`
```suggestion pkg_version = line.partition(':')[-1].strip() ```
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Good catch, I will remove it before final squash.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
Please don't make lines longer! There was nothing really wrong with this line before
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
I would remove all aliases if possible.
```suggestion Each host name will be added to the container's ``/etc/hosts`` file. ```
This should really be: ``` - Set custom DNS search domains. (Use I(dns_search) with C('') if you don't wish to set the search domain.) ```
I would remove this `device` here, and similar the option name in the examples for `device_read_bps`, `device_read_iops` etc.
(Same for the related options.)
Minor but I'd move this control flow block after the `weights` one to match the args order.
No need to wrap.
might be better to have some kind of mapping here ``` convert_list = ['image_id', 'instance_type', 'instance_id', ...] camel_params = dict((k,v) for k, v in snake_dict_to_camel_dict(module.params).items() if v is not None and k is in convert_list) ``` and then special case any exceptions like IamInstanceProfile and InstanceMonitoring
No need to wrap.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
`dict((k, v) for k, v in launch_configs[0].items() if k not in ...)` is probably a bit more readable.
Running the legacy ec2_asg integration tests that use this module, I think this line should be `if block_device_mapping:` because it doesn't look like it can be None or an empty list. The error: ``` An exception occurred during task execution. To see the full traceback, use -vvv. The error was: Missing required parameter in BlockDeviceMappings[0]: "DeviceName" fatal: [localhost]: FAILED! => {"changed": false, "failed": true, "module_stderr": "Traceback (most recent call last):\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 436, in <module>\n main()\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 430, in main\n create_launch_config(connection, module)\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 326, in create_launch_config\n connection.create_launch_configuration(**launch_config)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 253, in _api_call\n return self._make_api_call(operation_name, kwargs)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 517, in _make_api_call\n api_params, operation_model, context=request_context)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 572, in _convert_to_request_dict\n api_params, operation_model)\n File \"/Library/Python/2.7/site-packages/botocore/validate.py\", line 270, in serialize_to_request\n raise ParamValidationError(report=report.generate_report())\nbotocore.exceptions.ParamValidationError: Parameter validation failed:\nMissing required parameter in BlockDeviceMappings[0]: \"DeviceName\"\n", "module_stdout": "", "msg": "MODULE FAILURE", "rc": 0} ```
what happens if it's not `> 0`? There will be a lot of `IndexError`s in the following code. We should set `launch_config = launch_configs[0]` if we know there is a result.
It looks like just setting `instance_monitoring` to `true` now won't really do anything, since basic monitoring is on by default and the value of `advanced_instance_monitoring` would default to `False`. This seems confusing.
It would be good to wrap this in a try/except botocore.exceptions.ClientError as e
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
For python 2.6 compatibility, this and the following four strings have to look like `cmd = "ping {0} {1}".format(vrf, dest)`
I don't think the default is string. The default is to use whatever YAML makes of it IMO (that's the only backward compatible way to add type-support when we did). This also makes it possible to have a parameter that accepts both e.g. a list or dict and let the module figure it out (like it used to be in the old days ;-)) There are modules that use YAML as input, so in this case it doesn't matter whether it is a string, dict or list. (e.g. the xml module) and there is no type-indication for those parameters.
Mea culpa. I told them to take the defaults out for brevity. I'll keep your preference in mind in the future.
I prefer explicit types for all the parameters (unless they are multi-type parameters)
I prefer that the module would check the connection itself as well, without actually sending the message. If the API supports stub messages (or empty messages?), use that. Otherwise just test the authentication/connection some other way.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
`if it encounter` => `if it encounters`
`mentionned` => `mentioned`
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
missing space between `,` and `and`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
`if it encounter` => `if it encounters`
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
`mentionned` => `mentioned`
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
missing space between `,` and `and`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
Combine line 99 with this line: ```suggestion - May not be used with C(backrefs) or C(insertafter). ```
Why change the example docs? The yaml dict style is the preferred format for EXAMPLES
Please also fix this on the original method (line 1739)
```suggestion * Furthermore, via {@link org.apache.kafka.streams.processor.Punctuator#punctuate()} the processing progress can ```
```suggestion * is an empty {@link java.lang.Iterable Iterable} or {@code null}, no records are emitted. ``` Please also fix this on the original method
We should also mention somewhere that we do not support concurrent transactions.
So I would get rid of these.
Please don't add `default: null`, that's implicit and adds no value. (Especially since it is a required parameter)
Empty aliases is not needed. Keep it simple.
I see double backslashes here, that is not needed and actually unwanted. Also fix this for the other examples.
Why change the example docs? The yaml dict style is the preferred format for EXAMPLES
The point of `RenameMethodsBase` is to insure that if someone overloads the method then the custom method still gets called. But that's only useful if the implementation of the underlying method hasn't changed. I believe using `RenameMethodsBase` doesn't buy us much here.
You could use RenameMethodsBase.
`try / finally`
It's not actually a comprehension - this could just use a tuple literal.
@Ian-Foote thanks for the clarification I always mix up the two terms.
`if it encounter` => `if it encounters`
Missing `=dict` on this and the next few lines
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
The point of `RenameMethodsBase` is to insure that if someone overloads the method then the custom method still gets called. But that's only useful if the implementation of the underlying method hasn't changed. I believe using `RenameMethodsBase` doesn't buy us much here.
You could use RenameMethodsBase.
`if it encounter` => `if it encounters`
Missing `=dict` on this and the next few lines
`mentionned` => `mentioned`
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
missing space between `,` and `and`
prefer hanging indent style with 1 arg per line
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
I prefer putting the closing ) on the next line
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
I think `enumerate` would work here
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
`mentionned` => `mentioned`
Please rewrite as ``` if __name__ == '__main__': main() ```
`elif` might be clearer (I understand it's not necessary)
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
You could update the verbs tense to the convention used elsewhere: "Handle", "set", etc. rather than the "ing" versions.
prefer hanging indent style with 1 arg per line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
I prefer putting the closing ) on the next line
I think `enumerate` would work here
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
Should this be 6 instead of 9? When I try `Instant instant = Instant.from(formatter.parse("0.0000001"));` in the tests I get an `java.lang.ArithmeticException: Rounding necessary`
`if it encounter` => `if it encounters`
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
`mentionned` => `mentioned`
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
missing space between `,` and `and`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
`if it encounter` => `if it encounters`
`mentionned` => `mentioned`
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
missing space between `,` and `and`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
is this the same as "validate_certs" in other modules? if so I would prefer to standardize the naming to that
non required fields should show default, even if just null
`while` seems to be missing
It's on the agenda for London, so I am confident that once we have decided about the plan, we can collaborate quickly on the implementation. Exciting times !
Please don't add `default: null`, that's implicit and adds no value. (Especially since it is a required parameter)
So I would get rid of these.
Please don't add `default: null`, that's implicit and adds no value. (Especially since it is a required parameter)
Empty aliases is not needed. Keep it simple.
Add double quotes to Author name.
I see double backslashes here, that is not needed and actually unwanted. Also fix this for the other examples.
use of one the styles in 04de4369325097472d7ad036dac262555002ba88
This test name mentions multi-table inheritance but the body of the test has nothing to do with it.
I would use: ``` self.assertRaisesMessage(AttributeError, "'ProxyModel' has no attribute 'test_objects'"):` TestModel.test_objects ```
Do we need to use a dict? It seems unnecessary complicated. Model classes that we pass in the keys must match the base models from querysets. We also don't protect against incorrect values e.g. ```python queryset={ Animal: Bookmark.objects.all() } ``` I would use a list/tuple instead and raise an error when a queryset for the specific model is already resolved, e.g. ```python for qs in querysets: ct_id = self.get_content_type(model=qs.query.model, using=qs.db).pk if ct_id in custom_queryset_dict: raise ValueError(...) custom_queryset_dict[ct_id] = qs ``` We should also add a new argument (maybe `querysets`) because it's misleading to pass list of querysets in the argument called `queryset`.
We should pass `using` from the queryset ```suggestion ct_id = self.get_content_type(model=model_cls, using=ct_queryset.db).pk ```
can simply use "a" rather than "a#" in this, the next, and some of the other tests.
We can simplify this with `captured_stdout()`, e.g. ```diff diff --git a/tests/auth_tests/test_migrations.py b/tests/auth_tests/test_migrations.py index 4b4fe4bda2..56494d58d1 100644 --- a/tests/auth_tests/test_migrations.py +++ b/tests/auth_tests/test_migrations.py @@ -6,6 +6,7 @@ from django.apps import apps from django.contrib.auth.models import Permission, User from django.contrib.contenttypes.models import ContentType from django.test import TestCase +from django.test.utils import captured_stdout from .models import Proxy, UserProxy @@ -178,12 +179,6 @@ class ProxyModelWithSameAppLabelTests(TestCase): codename='display_proxys', name='May display proxys information', ) - - stream = io.StringIO() - old_stdout = sys.stdout - sys.stdout = stream - - update_proxy_permissions.update_proxy_model_permissions(apps, None) - - sys.stdout = old_stdout - self.assertIn('A problem arose migrating proxy model permissions', stream.getvalue()) + with captured_stdout() as stdout: + update_proxy_permissions.update_proxy_model_permissions(apps, None) + self.assertIn('A problem arose migrating proxy model permissions', stdout.getvalue()) ```
I'd omit the `shortcut_url` variable and put this directly in the `get()`.
True, I mixed up sth when switching branches. Nevertheless they work on all databases except PostgreSQL, so we need to fix this.
Cases from lines 361 and 363 work with the previous implementation.
You can join this line with the previous.
I think there is, just so folks have a little less to learn between the two.
Better yet: `empty_label or VERBOSE_BLANK_CHOICE_DASH`
nit: it will be hard to keep these arguments up-to-date- with those from `AnsibleModule` if they ever change. Perhaps we could just have `*args, **kwargs` here and pass it to the `AnsibleModule` init call. If unsupported params are passed through, the `AnsibleModule` will fail to initialize.
Remove all pointless changes.
this seems to assume a single level of sub specs, in some cases we might want 2 or 3 so making it recursive with the top options seems like the best approach
I would suggest changing the code so you're able to use the simpler `faulthandler=True` as the argument name. This will also simplify the code below that reads `if not no_faulthandler`. (You can still keep `--no-faulthandler` as the exposed option though.)
`no_faulthandler=False` -> `enable_faulthandler=True`
This regex does not make any sense.
You could do this setup in Python. `self.school.students.add(...)`
Referring specifically to the kubeconfig definition, the remote file path will cover a lot of use cases but I was wondering if it would be useful to be able to specify it inline as well, which would open the door for keeping your authentication with your ansible playbooks/roles.
Can we use instead `assertEqual()` and `assertIsNot()`? ```suggestion self.assertEqual(clone, source) self.assertIsNot(clone, source) ```
```suggestion description: Can be used to request certificates from ECS, with C(provider=entrust). ```
Should we make this message a bit more user friendly? I'm not sure may users will know what a tangential point is
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
prefer hanging indent style (allows longer lines, indentation always multiple of 4): ``` rasterheader = ( 1, 0, ... ) ```
Would it be the acme of foolishness to suggest that we just always "load-two"? Then, we wouldn't have to maintain the extra lists of tests at the top of the file, and we wouldn't need this condition here. And also, we'd eliminate a difference among the benchmarks, since the brokers would be loaded with the same dataset in all cases (even if one of them is unused).
Ah, I didn't realize load takes that long, but in retrospect, it should have been obvious. Then, of course we shouldn't always load both, and my suggestion is just to match on the test name.
Alternatively, we can get rid of those lists by just matching on whether the test name ends with "join" (we match on table name elsewhere in this PR)
You could do this setup in Python. `self.school.students.add(...)`
``` change_url = reverse('admin:admin_widgets_school_change', args=(self.school.id,) self.selenium.get(self.live_server_url + change_url) ```
This regex does not make any sense.
use `from ansible.module_utils.six import iteritems`
This doesn't seem to be necessary to reproduce the bug. Just refreshing the page without making any changes caused the select options to clear.
Remove all pointless changes.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
```suggestion for key, value in self.parameters.plugin_options.items(): ```
Please don't use lists for tracking differences, but `DifferenceTracker`. That produces a much better output.
For these 3, add expected_type `int`, eg: ```py 'width': try_get(item, lambda x: x['video']['width'], int), ``` (equivalent in effect to wrapping in `int_or_none()`).
Sarmonise with yt-dlp pt7: ```suggestion # Stripchat declares the RTA meta-tag, but in an non-standard format so _rta_search() can't be used 'age_limit': 18, } ```
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
prefer hanging indent style (allows longer lines, indentation always multiple of 4): ``` rasterheader = ( 1, 0, ... ) ```
Prefer `post.get()` for these two.
More robust: ```py entries.extend([self.url_result('https://www.douyin.com/video/%s' % aweme_id, ie=DouyinVideoIE.ie_key(), video_id=aweme_id) for aweme_id in filter(None, (aweme.get('aweme_id') for aweme in aweme_list if isinstance(aweme, dict)))]) ```
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
I would call `split()` once only. ``` entry = x.split() if len(entry) > 0: ldp = {} ldp['neighbor'] = entry[1] ldp['source'] = entry[3] ```
Please add `type='str'`
* use the resulting match object * avoid excessive indentation * `r'\s'` includes any whitespace * simplify `clean_html()` expressions ```suggestion href = extract_attributes(html[mobj.start(0):mobj.start('content')]).get('href') if not href: continue mobj1 = re.search(r'/(?P<s_id>\d+)\.html', href) if mobj1 and mobj1.group('s_id') == series_id: series_title = clean_html(re.sub(r'\s+', ' ', mobj.group('content'))) title = clean_html(re.sub(r'\s+', ' ', html)) break ```
It should always return a list.
Fix test: ```suggestion 'ext': 'mp4', ```
Fix test: ```suggestion 'ext': 'mp4', ```
Use the function defined earlier: ```suggestion mobj = _get_element_by_tag_and_attrib(html, tag='a') ```
Simpler: ```suggestion series_id, season_id, episode_id = video_id.split('-') ```
Simplify: ```suggestion series_title, title = None, None for html in get_elements_by_class('title', webpage_html): ```
You're checking two separate properties here. This should be in a separate test.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
I think we should drop the space after "function" through this file (can be a separate commit).
Referring specifically to the kubeconfig definition, the remote file path will cover a lot of use cases but I was wondering if it would be useful to be able to specify it inline as well, which would open the door for keeping your authentication with your ansible playbooks/roles.
Can we use instead `assertEqual()` and `assertIsNot()`? ```suggestion self.assertEqual(clone, source) self.assertIsNot(clone, source) ```
```suggestion description: Can be used to request certificates from ECS, with C(provider=entrust). ```
Should we make this message a bit more user friendly? I'm not sure may users will know what a tangential point is
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
prefer hanging indent style (allows longer lines, indentation always multiple of 4): ``` rasterheader = ( 1, 0, ... ) ```
Would it be the acme of foolishness to suggest that we just always "load-two"? Then, we wouldn't have to maintain the extra lists of tests at the top of the file, and we wouldn't need this condition here. And also, we'd eliminate a difference among the benchmarks, since the brokers would be loaded with the same dataset in all cases (even if one of them is unused).
Ah, I didn't realize load takes that long, but in retrospect, it should have been obvious. Then, of course we shouldn't always load both, and my suggestion is just to match on the test name.
Alternatively, we can get rid of those lists by just matching on whether the test name ends with "join" (we match on table name elsewhere in this PR)
prefer hanging indent style (allows longer lines, indentation always multiple of 4): ``` rasterheader = ( 1, 0, ... ) ```
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
You're checking two separate properties here. This should be in a separate test.
I would call `split()` once only. ``` entry = x.split() if len(entry) > 0: ldp = {} ldp['neighbor'] = entry[1] ldp['source'] = entry[3] ```
ACk. Just FYI OS discovery can be done after CLICONF is implemented.
Please add `type='str'`
Why shouldn't a try/except block not be used ? I'm intrigued.
I would change this whole block with something like: ```python units = list('b', 'kb', 'mb', 'gb', 'tb', 'pb', 'eb', 'zb', 'yb') try: multiplier = 1000**units.index(unit) except ValueError: units = list(None, 'kib', 'mib', 'gib', 'tib', 'pib', 'eib', 'zib', 'yib') try: multiplier = 1024**units.index(unit) except ValueError: e = get_exception() module.fail_json(msg="No valid size unit specified. %s" % e) ```
Both are fine IMO, there's nothing wrong with doing presence-checks either. I'll let you decide.
No need to define another attribute. The form class should be accessible as `self.TestForm`.
Ah, ok so maybe we can check both in one call, i.e. ```python self.assertEqual(check_language_settings_consistent(None), [ Error( ... ), Warning( ... ), ]) ```
No need for the `u` prefix, we're already importing `unicode_literals`.
this is not a safe across py2 and py3, use the provided `string_types` instead ... but better yet, jsut define the option as type: list and this will be automatically handled for you. Single element, comma separated string and 'actual list' will all be normalized so you always get 'a list'.
again, code not needed as this is already 'required'
I maybe wrong but we are getting user input for `yield_on_poll` but you are not using here.
```suggestion self.assertFormsetError(response, 'formset', 0, 'field', 'invalid value') ```
Maybe the following will be more explicit: ```python first_author_books = result[0].books.all() self.assertEqual([self.book1], [first_author_books]) self.assertEqual([self.reader], [first_author_books[0].read_by.all()])) ```
you should not need to checked disabled, as the plugin itself wont be called at all if true
Shouldn't the edit operation only be set if we want to make the actual change (inside the `if disk['size'] != disk_spec.device.capacityInKB` part)
No need for the `u` prefix, we're already importing `unicode_literals`.
```suggestion self.assertFormError(response, 'form', 'field', 'invalid value') ```
this is not a safe across py2 and py3, use the provided `string_types` instead ... but better yet, jsut define the option as type: list and this will be automatically handled for you. Single element, comma separated string and 'actual list' will all be normalized so you always get 'a list'.
Yes, this seems fine then.
@rajinisivaram Sorry should have caught this before -- instead of `time.sleep` here, using `wait_until` on the checks for errors below and then moving the assertion for `self.producer.num_acked == 0` to below those `wait_until`'s might make the test more robust. We'll still have a timeout on the `wait_until` calls, but it can be a lot more conservative and the test may be able to finish a lot faster.
again, code not needed as this is already 'required'
```suggestion self.assertFormsetError(response, 'formset', 0, 'field', 'invalid value') ```
you should not need to checked disabled, as the plugin itself wont be called at all if true
this check is not needed, the default is `False` if you should never get a `None` at this point
`required=False` is the default, and no not required. Same for `type=str`
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
```suggestion item, fields=fields, using=self.db, ```
Maybe the following will be more explicit: ```python first_author_books = result[0].books.all() self.assertEqual([self.book1], [first_author_books]) self.assertEqual([self.reader], [first_author_books[0].read_by.all()])) ```
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
Yeah, it's fine.
it should also check if it can write there
this is not a safe across py2 and py3, use the provided `string_types` instead ... but better yet, jsut define the option as type: list and this will be automatically handled for you. Single element, comma separated string and 'actual list' will all be normalized so you always get 'a list'.
again, code not needed as this is already 'required'
you should not need to checked disabled, as the plugin itself wont be called at all if true
this check is not needed, the default is `False` if you should never get a `None` at this point
set this as the default, then you dont need to do checks yourself
check is redundant as you already flagged as 'required'
again, this is not required, since the key IS REQUIRED you never get to this point
no need to split if you define the type as 'list', user can then supply a list, comma separated string or single element, you always get a list
Use standard boto3 exception handling: - https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/amazon/GUIDELINES.md#boto3-2 I've just noticed there's a missing `c` at the end of `traceback.format_exc()`, so just be careful there :) - I've added #23168 to fix that
it should also check if it can write there
this is not a safe across py2 and py3, use the provided `string_types` instead ... but better yet, jsut define the option as type: list and this will be automatically handled for you. Single element, comma separated string and 'actual list' will all be normalized so you always get 'a list'.
this is not a 1.0 callback, its using 2.0 API
again, code not needed as this is already 'required'
you should not need to checked disabled, as the plugin itself wont be called at all if true
this check is not needed, the default is `False` if you should never get a `None` at this point
set this as the default, then you dont need to do checks yourself
check is redundant as you already flagged as 'required'
again, this is not required, since the key IS REQUIRED you never get to this point
no need to split if you define the type as 'list', user can then supply a list, comma separated string or single element, you always get a list
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
> ... so I made it a required parameter. Sorry, I think we're not understanding each other. ðŸ¤” * The `on_bind` parameter is defined as `on_bind=None`, so it's optional. * Exactly when `on_bind=None` that `server_bind` is only declared conditionally with lead to a `... is referenced before assignment` problem. If looks like this: ``` >>> on_bind = None >>> if on_bind is not None: ... a = "I won't be defined" ... >>> a Traceback (most recent call last): File "<stdin>", line 1, in <module> NameError: name 'a' is not defined ```
Maybe _"The 'no_color' and 'force_color' cannot be used together."_.
I'd use `cannot` rather than `can not` here and below
I think `get_internal_type` is better to use.
Wouldn't this approach fix the slicable fields to just these types? A mixin along with `slicable = True/False` on `F` may be an approach. Besides, I believe the policy is to keep `django.contrib.postgres` as separate as possible.
I think a mixin is a bit complicated. The attribute sounds simpler - it could be a function instead of just a boolean, then it could return the transform if applicable, else `None` for non-sliceable fields (default implementation).
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
Use another lookup instead of `epoch` e.g. `second`.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
2.0 is what you want here
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
Use another lookup instead of `epoch` e.g. `second`.
this is not a safe across py2 and py3, use the provided `string_types` instead ... but better yet, jsut define the option as type: list and this will be automatically handled for you. Single element, comma separated string and 'actual list' will all be normalized so you always get 'a list'.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
again, code not needed as this is already 'required'
this check is not needed, the default is `False` if you should never get a `None` at this point
you should not need to checked disabled, as the plugin itself wont be called at all if true
```suggestion Test that the returned value for timezone consists of only uppercase ```
For python 2.6 compatibility, this and the following four strings have to look like `cmd = "ping {0} {1}".format(vrf, dest)`
What I meant was that you could stick the following in the top of `_destruct_iterable_mapping_values()` to make it more DRY: ```python if isinstance(data, Mapping): yield from data.items() return ``` I don't see why you think we'd need an extra for-loop...
There are only two uses of `_destruct_iterable_mapping_values()` and we use this same pattern exactly. I think that you could push the `isinstance(..., Mapping)` check into that function.
As a separate commit, I think that it is also probably worth moving that global function to be a `@staticmethod` on the `CaseInsensitiveMapping` class. It is closely associated with this class and would avoid the need to import it separately in `django.http.response`.
For some reason this causes ``` {aliases: [attr]} {choices: [all, 'no', none, safe, urllib2, 'yes']} {default: [200]} ``` while non-sequence values for `default` yield ``` default: 30 ``` without the `{...}`.
I think a mixin is a bit complicated. The attribute sounds simpler - it could be a function instead of just a boolean, then it could return the transform if applicable, else `None` for non-sliceable fields (default implementation).
Wouldn't this approach fix the slicable fields to just these types? A mixin along with `slicable = True/False` on `F` may be an approach. Besides, I believe the policy is to keep `django.contrib.postgres` as separate as possible.
I think `get_internal_type` is better to use.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
If you use to_text(xxx, errors='surrogate_or_strict') it won't throw exceptions.
Minor but I'd move this control flow block after the `weights` one to match the args order.
we should also return if we both delegate executions and delegate_facts
```suggestion type: list suboptions: ```
As you've noticed, we don't want to do this here because in this case the pkg_mgr is coming from a pre-existing fact.
This should be extracted in the first place.
`acodec == 'none'`.
`field_preference` must be `list` or `tuple`. There is no need to touch this usually since default sorting works fine.
If so, I think a separate commit would be better.
This is ready for commit. But... don't you think code this could be more readable moving some code to a new helper function? The existing code is already a bit convoluted, and this fix adds lines and indent levels that make it worth some refactoring.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Minor but I'd move this control flow block after the `weights` one to match the args order.
As you've noticed, we don't want to do this here because in this case the pkg_mgr is coming from a pre-existing fact.
Here you pass the variable `label`. You have never defined it, though (you defined `ch_label` -- maybe you meant that one?). That's the first of the sanity checks which fails. You might already get that output by running `flake8` on your module.
we should also return if we both delegate executions and delegate_facts
If so, I think a separate commit would be better.
This is ready for commit. But... don't you think code this could be more readable moving some code to a new helper function? The existing code is already a bit convoluted, and this fix adds lines and indent levels that make it worth some refactoring.
```suggestion type: list suboptions: ```
`field_preference` must be `list` or `tuple`. There is no need to touch this usually since default sorting works fine.
`acodec == 'none'`.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Minor but I'd move this control flow block after the `weights` one to match the args order.
Nitpick, you can use a list comprehension here instead of calling list on a generator expression.
We should avoid lambdas and filters most of the time. Use list comprehensions and generator expressions instead. It's both faster and more pythonic to use those idioms.
Style note, I finally convinced people a year or so ago that ```{}``` is superior to ```dict()```. So new code should use the literal syntax.
Kind of a nitpick since this is setup.py... but API-wise I prefer returning a small dict with the dynamic values and letting the caller merge it/copy the static dict if necessary.
I think we should just let this exception propagate. We aren't adding any value by catching it here (and might be removing value if the traceback were useful). This isn't code in ansible itself so we don't have to create a nicer error message either.
Maybe rename this to substitute_crypto_req since it's replacing the one from requirements.txt with the one we autodetect rather than simply appending.
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
Do we need to call `list(fields)` here? :thinking:
`try/finally` should be a bit smaller. If there's an exception before `form.save()` the photo won't exist so there's no need to cleanup. I'll make this change and merge it.
We want to save file with the `symlink.txt` name so this assertion doesn't have much value IMO. I would check that the new file exists.
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
I think this should be false (not a string)
No need to specify `choices`
We've been using "Take / apply" verb-style in new docstrings.
chop trailing newline (check code with flake8 to see a couple other minor formatting things)
For consistency path pruning should be done the same as in the boilerplate test: https://github.com/ansible/ansible/blob/60a24bbdaae3a17bf20613810f347bfb8283f306/test/sanity/code-smell/boilerplate.py#L13-L14 https://github.com/ansible/ansible/blob/60a24bbdaae3a17bf20613810f347bfb8283f306/test/sanity/code-smell/boilerplate.py#L41-L42
set the safe
This is probably also broken for the same reason as the one above.
```suggestion to iterate use a C(with_) directive. ```
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
Can you re-warp this block to 79 chars? (First line is too short.)
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
I think we should add an `allow_overwrite` or similar param.
We should indicate that this is only a guess - the value may be smaller or larger than the actual size.
No need to wrap.
`{}` doesn't work in python 2.6.
No need to wrap.
I missed that, then you should either use [itertools.islice](https://docs.python.org/2/library/itertools.html#itertools.islice) or ``` python for i, track in enumerate(tracks): if i >= n: break ```
Usually we use syntaxes like 'soundcloud said: ' but not brackets in error messages.
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
The regexp can be problematic when the json payload contains ";", for example the anime title can be "Dat Girl;Ã¯Â¼Â‰". In this case the matched result is not a valid json string.
what happens if it's not `> 0`? There will be a lot of `IndexError`s in the following code. We should set `launch_config = launch_configs[0]` if we know there is a result.
We should indicate that this is only a guess - the value may be smaller or larger than the actual size.
The locale should be set to C if we do string matching.
```suggestion - For tracking function statistics the PostgreSQL C(track_functions) parameter must be enabled. ```
And `\r` or `\n` -- I dunno if Django protects from header injection
I think this might explode if `obj_type` is not one of `functions`, `indexes` or `tables`, maybe it would be safer to do something like? ```suggestion obj_func = self.obj_func_mapping.get(obj_type.strip()) if obj_func is not None: obj_func() else: module.warn("Unknown filter option '{}'".filter(obj_type.strip()) ```
Not something that you have to change but I think this could be simplified to ```python for obj_func in self.obj_func_mapping.values(): obj_func() ```
The regexp can be problematic when the json payload contains ";", for example the anime title can be "Dat Girl;Ã¯Â¼Â‰". In this case the matched result is not a valid json string.
Same here (and further below).
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
You should use `module.get_bin_path('ssh-keygen', True)`.
I'm not sure whether it should be considered part of a separate clean up across all cases of this in the code base or whether we can handle this here for all of the adjacent lines, but it would be nice to fix the indentation: ```python self.management_form_html + ( '<tr><th>Choice:</th><td><input type="text" name="choices-0-choice" value="Calexico"></td></tr>' '<tr><th>Votes:</th><td><input type="number" name="choices-0-votes" value="100"></td></tr>' ) ```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
I prefer putting the closing ) on the next line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
, keeping a reference to the cyptes object so that the vsimem file...
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
`band_input`, you don't get much by saving one char :-)
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
I think `enumerate` would work here
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
, keeping a reference to the cyptes object so that the vsimem file...
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
`items = value.split(self.delimiter) if value else []` is slightly faster.
What if the prepared value contains unicode? (As I said before, you should use a list comprehension instead of a generator, it'll be about 2 times faster here)
~~A list comprehension would be a little faster.~~ EDIT: Forgive me, I misread the code.
point -> points
consider assertRaisesMessage to make the test a bit more specific.
While we're at it: please add a trailing comma, that makes it easier to add another option (if we ever need to) :)
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
excellent handling of congestion control
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
`items = value.split(self.delimiter) if value else []` is slightly faster.
What if the prepared value contains unicode? (As I said before, you should use a list comprehension instead of a generator, it'll be about 2 times faster here)
~~A list comprehension would be a little faster.~~ EDIT: Forgive me, I misread the code.
point -> points
consider assertRaisesMessage to make the test a bit more specific.
While we're at it: please add a trailing comma, that makes it easier to add another option (if we ever need to) :)
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
excellent handling of congestion control
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
I needed to add a `set_source_expressions` method which sets `self.subquery = source_expressions` here to get relabelling to work.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
I had to change this to `template_params['subquery'], sql_params = self.subquery.query.get_compiler(connection=connection).as_sql()` in order to compile correctly when using a database other than the `DEFAULT_DB_ALIAS`.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Usage of `template=None, **extra_context` params seems untested. Not sure if it's important.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
it should also check if it can write there
`DigitalOcean droplet size facts` would be clear to the end user.
So yeah, the callers that I see are fine. You should be able to simply remove to_text() here.
Please follow best practice for this fail message ```python if not HAS_GITLAB_PACKAGE: module.fail_json(msg=missing_required_lib("python-gitlab"), exception=GITLAB_IMP_ERR) ```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
Should be interesting to return data to the user, like variables `added`, `updated`, `removed
I don't think you should re-number the existing tests.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
pass original exception as orig_exc so traceback can be shown with -vvv
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
It's better to actually say that there's no file in place or it's inaccessible.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
It's better to actually say that there's no file in place or it's inaccessible.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
This isn't needed, since we now only support 2.6+ anyway.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
It's better to actually say that there's no file in place or it's inaccessible.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
This isn't needed, since we now only support 2.6+ anyway.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
It's better to actually say that there's no file in place or it's inaccessible.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
This isn't needed, since we now only support 2.6+ anyway.
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
It's better to actually say that there's no file in place or it's inaccessible.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
This isn't needed, since we now only support 2.6+ anyway.
`field_preference` must be a list or a tuple.
Code duplication 80-86, 89-94.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
I prefer putting the closing ) on the next line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
this seems like it should be shared with the main one rather than duplicated here
Must not be `None`.
Whether it has changed or not does not mean there should be a format with invalid URL.
There are multiple formats, some may have `file` some may not. If some new hq format is introduced with different rendition your approach will break downloading by default while proper URL handling will not. Finally read coding conventions.
missing ini option for host
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
It's better to actually say that there's no file in place or it's inaccessible.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
This isn't needed, since we now only support 2.6+ anyway.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
If I'm not completely mistaken this will fail if the attacker requests a page where `csrfmiddlewaretoken` or `HTTP_X_CSRFTOKEN` is shorter than the `CSRF_KEY_LENGTH`.
Ah. lambda is prettier though :). maybe we can run pep8 with ignore lambda error.
```suggestion pkg_version = line.partition(':')[-1].strip() ```
Is it possible ? For me it would mean that it exist a package without version.
It doesn't work if pkg_provides contain a version: ``` max@mde-oxalide % pacman -Qqei perl | grep Provide Provides : perl-archive-tar=2.30 perl-attribute-handlers=1.01 perl-autodie=2.29 perl-autoloader=5.74 perl-autouse=1.11 perl-b-debug=1.26 perl-base=2.27 perl-bignum=0.49 perl-carp=1.50 perl-compress-raw-bzip2=2.074 perl-compress-raw-zlib=2.076 perl-config-perl-v=0.29 perl-constant=1.33 perl-cpan-meta-requirements=2.140 perl-cpan-meta-yaml=0.018 perl-cpan-meta=2.150010 perl-cpan=2.20 perl-data-dumper=2.170 perl-db_file=1.840 perl-devel-ppport=3.40 perl-devel-selfstubber=1.06 perl-digest-md5=2.55 perl-digest-sha=6.01 perl-digest=1.17_01 perl-dumpvalue=1.18 perl-encode=2.97 perl-encoding-warnings=0.13 perl-env=1.04 perl-experimental=0.019 perl-exporter=5.73 perl-extutils-cbuilder=0.280230 perl-extutils-constant=0.25 perl-extutils-install=2.14 perl-extutils-makemaker=7.34 perl-extutils-manifest=1.70 perl-extutils-parsexs=3.39 perl-file-fetch=0.56 perl-file-path=2.15 perl-file-temp=0.2304 perl-filter-simple=0.95 perl-filter-util-call=1.58 perl-getopt-long=2.5 perl-http-tiny=0.070 perl-i18n-collate=1.02 perl-i18n-langtags=0.43 perl-if=0.0608 perl-io-compress=2.074 perl-io-socket-ip=0.39 perl-io-zlib=1.10 perl-io=1.39 perl-ipc-cmd=1.00 perl-ipc-sysv=2.07 perl-json-pp=2.97001 perl-lib=0.64 perl-libnet=3.11 perl-locale-codes=3.56 perl-locale-maketext-simple=0.21_01 perl-locale-maketext=1.29 perl-math-bigint-fastcalc=0.5006 perl-math-bigint=1.999811 perl-math-bigrat=0.2613 perl-math-complex=1.5901 perl-memoize=1.03_01 perl-mime-base64=3.15 perl-module-corelist=5.20180622 perl-module-load-conditional=0.68 perl-module-load=0.32 perl-module-loaded=0.08 perl-module-metadata=1.000033 perl-net-ping=2.62 perl-params-check=0.38 perl-parent=0.236 perl-pathtools=3.74 perl-perl-ostype=1.010 perl-perlfaq=5.021011 perl-perlio-via-quotedprint=0.08 perl-pod-checker=1.73 perl-pod-escapes=1.07 perl-pod-parser=1.63 perl-pod-perldoc=3.2801 perl-pod-simple=3.35 perl-pod-usage=1.69 perl-podlators=5.006 perl-safe=2.40 perl-scalar-list-utils=1.50 perl-search-dict=1.07 perl-selfloader=1.25 perl-socket=2.027 perl-storable=3.08 perl-sys-syslog=0.35 perl-term-ansicolor=4.06 perl-term-cap=1.17 perl-term-complete=1.403 perl-term-readline=1.17 perl-test-harness=3.42 perl-test-simple=1.302133 perl-test=1.31 perl-text-abbrev=1.02 perl-text-balanced=2.03 perl-text-parsewords=3.30 perl-text-tabs=2013.0523 perl-thread-queue=3.12 perl-thread-semaphore=2.13 perl-threads-shared=1.58 perl-threads=2.22 perl-tie-file=1.02 perl-tie-refhash=1.39 perl-time-hires=1.9759 perl-time-local=1.25 perl-time-piece=1.3204 perl-unicode-collate=1.25 perl-unicode-normalize=1.26 perl-version=0.9923 perl-xsloader=0.30 ```
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
It's better to actually say that there's no file in place or it's inaccessible.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
comma after tuple
If I'm not completely mistaken this will fail if the attacker requests a page where `csrfmiddlewaretoken` or `HTTP_X_CSRFTOKEN` is shorter than the `CSRF_KEY_LENGTH`.
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
It's better to actually say that there's no file in place or it's inaccessible.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
This isn't needed, since we now only support 2.6+ anyway.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
It's better to actually say that there's no file in place or it's inaccessible.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
This isn't needed, since we now only support 2.6+ anyway.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
`field_preference` must be a list or a tuple.
Code duplication 80-86, 89-94.
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
I prefer putting the closing ) on the next line
this seems like it should be shared with the main one rather than duplicated here
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
Must not be `None`.
There are multiple formats, some may have `file` some may not. If some new hq format is introduced with different rendition your approach will break downloading by default while proper URL handling will not. Finally read coding conventions.
Whether it has changed or not does not mean there should be a format with invalid URL.
missing ini option for host
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Similarly, ```if tc['skip'].get('i')```
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
`@requires_tz_support` skips the test on Windows. Actually these tests work on Windows as long as pytz in installed (tested with SQLite and PostgreSQL). Without pytz installed on Windows: SQLite: `ImproperlyConfigured: This query requires pytz, but it isn't installed.` (as on Linux) PostgreSQL: `DataError: time zone "Eastern Standard Time" not recognized` (this situation works on Linux) MySQL: `ValueError: Database returned an invalid datetime value. Are time zone definitions for your database and pytz installed?` for trunc tests and assertion errors like `(datetime.datetime(2015, 6, 15, 18, 30, 50, 321, tzinfo=<UTC>), None) != (datetime.datetime(2015, 6, 15, 14, 30, 50, 321, tzinfo=<django.utils.timezone.LocalTimezone object at 0x03C73A10>), 15)` for the extract tests. (again, no problem on Linux)
Rather than writing some more complex skip condition, I think I'm inclined to skip these tests if pytz isn't installed.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Similarly, ```if tc['skip'].get('i')```
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
`@requires_tz_support` skips the test on Windows. Actually these tests work on Windows as long as pytz in installed (tested with SQLite and PostgreSQL). Without pytz installed on Windows: SQLite: `ImproperlyConfigured: This query requires pytz, but it isn't installed.` (as on Linux) PostgreSQL: `DataError: time zone "Eastern Standard Time" not recognized` (this situation works on Linux) MySQL: `ValueError: Database returned an invalid datetime value. Are time zone definitions for your database and pytz installed?` for trunc tests and assertion errors like `(datetime.datetime(2015, 6, 15, 18, 30, 50, 321, tzinfo=<UTC>), None) != (datetime.datetime(2015, 6, 15, 14, 30, 50, 321, tzinfo=<django.utils.timezone.LocalTimezone object at 0x03C73A10>), 15)` for the extract tests. (again, no problem on Linux)
Rather than writing some more complex skip condition, I think I'm inclined to skip these tests if pytz isn't installed.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Similarly, ```if tc['skip'].get('i')```
To be consistent with other modules each of the options should be on a single line (unless they have many choices)
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Rather than writing some more complex skip condition, I think I'm inclined to skip these tests if pytz isn't installed.
`@requires_tz_support` skips the test on Windows. Actually these tests work on Windows as long as pytz in installed (tested with SQLite and PostgreSQL). Without pytz installed on Windows: SQLite: `ImproperlyConfigured: This query requires pytz, but it isn't installed.` (as on Linux) PostgreSQL: `DataError: time zone "Eastern Standard Time" not recognized` (this situation works on Linux) MySQL: `ValueError: Database returned an invalid datetime value. Are time zone definitions for your database and pytz installed?` for trunc tests and assertion errors like `(datetime.datetime(2015, 6, 15, 18, 30, 50, 321, tzinfo=<UTC>), None) != (datetime.datetime(2015, 6, 15, 14, 30, 50, 321, tzinfo=<django.utils.timezone.LocalTimezone object at 0x03C73A10>), 15)` for the extract tests. (again, no problem on Linux)
Please add `type='str'`
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
This is only ever called once. Do we need the default? (Same with SQL version)
That's a really interesting piece of information I did not know.
IMO, we can use `assertEqual` instead of `assertAlmostEqual` in all tests.
An example of using the `mocker` fixture. ```suggestion def mock_NamedTemporaryFile(mocker, **args): mock_ntf = mocker.MagicMock() mock_ntf.write = mocker.MagicMock() mock_ntf.close = mocker.MagicMock() ```
If you turn this into ```suggestion if not os.path.exists(collection_output): _build_collection_tar( collection_path, collection_output, collection_manifest, file_manifest, ) return ``` you could reduce some nesting which improves readability.
This doesn't match because your test uses `http://` instead of `https://`.
Oh, and `args[0]` is the module, so you want to look at `args[1]` for the url.
To apply the side effect, use this: ```suggestion mocker.patch('ansible.module_utils.network.meraki.meraki.fetch_url', side_effect=mocked_fetch_url) ``` You'll also need to update `mocked_fetch_url` to accept args, like: ```python def mocked_fetch_url(*args, **kwargs): ```
Remove this since it doesn't do anything and `Mock` isn't defined.
This is only used once. Can we move it back to the `color` module? (That way `termcolors` is still only ever used by `color`)
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
and please format it with indent, so it's more clear
add this condition also to lin 2063
But it must be in this if.
My next question was going to be if the random boot ID is widely available. I guess I'll have to do some research to see when it was added. I created a WIP PR (#45656) exploring what it would look like to add platform and dist to the plugin. I'm not terribly happy with it, but it's a start (and it accounts for Alpine, which is particularly annoying since it lacks both `who` and `uptime` for no good reason (IMO)). That PR also changes to using the random boot ID by default rather than `who -b`. That has the nice side effect of being able to remove the `uptime -s` command for systems that lack a RTC, but at the cost of all the added complexity of platfrom and dist checking.
I would aggregate both errors here, both 'uptime' and 'cat /proc..'
use same indent style as previous item
I'd move this into an example
Not sure if this will get rendered correctly, from the root of your checkout please do `make webdocs` Then check the HTML that's generated
Please delete this line
Please delete this line
No such meta field.
No such meta field.
No such meta field.
No such meta field.
Useless with timestamp available.
This is no longer actual.
No brackets needed.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
Don't capture groups you don't use. Unused captured group.
Here you pass the variable `label`. You have never defined it, though (you defined `ch_label` -- maybe you meant that one?). That's the first of the sanity checks which fails. You might already get that output by running `flake8` on your module.
`check_rc` is false by default, no need to pass.
Please replace with `type: false`
Ok then we should have a message in the docs then to warn users that this module/option will not be idempotent, so that it doesn't catch them unawares.
This will not be idempotent, is that intended? We should try to support `present`, or if this is not possible warn users that there is no idempotency.
I don't like the `new` state too. This will be confusing for the users since it's a rather non-standard behaviour. And worst, this breaks the idem-potency of the module. May I suggest something like: ```yaml - name: Set the disk of the VM vmware_guest_controller: name: test_VM controllers: - state: present type: sata - state: present type: sata - state: present type: nvme - state: present type: usb3 ```
Your module is correct
also please rename entity_id to job_id, no need to have it too generic here
here you need just a 'steps' not whole module as well.
pass here just description, as it's the only parameter you need.
Function name can be different, IMHO this is returning boolean if policy is enabled or not. You can rename as `is_policy_enabled` or something.
I think this should probably be `'auto'` instead of `None` ... unless I missed something (which I'm always open to the possibility that I have): ```python use_backend = self._task.args.get('use', self._task.args.get('use_backend', 'auto')) ```
I'd go for `try/except IOError` in order to make a point that variable initialized above is in fact a fallback.
Nitpick, you can use a list comprehension here instead of calling list on a generator expression.
We should avoid lambdas and filters most of the time. Use list comprehensions and generator expressions instead. It's both faster and more pythonic to use those idioms.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
Maybe rename this to substitute_crypto_req since it's replacing the one from requirements.txt with the one we autodetect rather than simply appending.
```suggestion sample: 3 ```
ContainerInstance doesn't support profile, it's why it doesn't have a `models` method
You can avoid inspect with ```python if hasattr(client, 'DEFAULT_API_VERSION') ``` Will not work for azure-mgmt-resource before 1.3.0 if you want multi-api support there as well.
I'm thinking these parameters shouldn't be filled in, since we're popping `stack_description` so if a user wanted a hard-fail for nonexistent stacks, they could do: ``` - cloudformation_facts: stack_name: not-real failed_when: not cloudformation['not-real'] ```
This whole connection block can be replaced with `conn = module.client('ssm')`
This is unnecessary, AnsibleAWSModule handles it.
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
What do you need this function for? Ansible already handles aliases, there's no need to do this manually.
You can avoid inspect with ```python if hasattr(client, 'DEFAULT_PROFILE') ```
I'm a little concerned about the loss of `constant_time_compare()` here which sounds like it was added as a potential mitigation against timing attacks.
You can avoid inspect with ```python if hasattr(client, 'DEFAULT_API_VERSION') ``` Will not work for azure-mgmt-resource before 1.3.0 if you want multi-api support there as well.
Positional arguments cannot follow keyword arguments.
I don't understand why we have methods with a double underscores prefix which are copies from `SessionBase`, e.g. `__hash()`, `__legacy_encode()`, `__legacy_decode()` :thinking:
ContainerInstance doesn't support profile, it's why it doesn't have a `models` method
`max_age` is not being passed into `signing.loads()`, nor is `self.serializer`. `session_dict` should be `session_data`.
For optional fields like `duration`, `feed['feed']['duration']` should be `feed.get('feed', {}).get('duration')`. All fields other than `id``,``title``and``formats`` are optional.
immediatelly -> immediately
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
could move this to the previous line
Or ```if not hasattr(filename, 'write')``` or ```if isinstance(filename, compat_str)```? Checking for an exact type makes the codes less flexible.
Should be better with ```isinstanceof(filename, BytesIO)```
In what case is this branch reached? Should there be a test for it? I guess you meant `raise CommandError(e)`.
I guess `filename` can also be included.
We should indicate that this is only a guess - the value may be smaller or larger than the actual size.
Thanks for finding out the bug. However, progress hooks should always be called; otherwise third party applications using youtube-dl's Python API will be broken. In this case `report_progress` should be fixed instead.
Ah I see, okay then.
Perhaps: ```suggestion def esxi_version_at_least(self, version): """ Check that the ESXi Host is at least a specific version number. Inputs: - version (tuple): a version tuple, for example (6, 7, 1) Returns: bool """ ``` Suggest moving into module_utils/vmware.py and providing a unit test.
```suggestion except OSError as err: ```
These 2 `if` statements could probably be combined. To `pop` when if it doesn't start with `_` or not equal to `msg`.
Thanks for the patch @Giebisch, I think we should adjust the `expected_outputs` instead to be properly formatted in order to ensure formatting is actually used. https://github.com/django/django/blob/78657a201308a0b53661e2b267914407a9fd7fa1/tests/test_runner/test_debug_sql.py#L90-L111 https://github.com/django/django/blob/78657a201308a0b53661e2b267914407a9fd7fa1/tests/test_runner/test_debug_sql.py#L124-L133 The adjustments made make the tests pass with and without the proposed changes.
Meanwhile, I merged a conflicting patch. I think you just need to remove the quotes from around the star.
flake8 doesn't like the hanging indent here.
`test_jsonfield` looks good to me (since this is InspectDBTests, _introspection seems redundant).
This shouldn't be in `response.keys()`, it's in `response['ins_api'].keys()`. I would rather `if response['ins_api'].get('outputs'):`
I'm thinking these parameters shouldn't be filled in, since we're popping `stack_description` so if a user wanted a hard-fail for nonexistent stacks, they could do: ``` - cloudformation_facts: stack_name: not-real failed_when: not cloudformation['not-real'] ```
Also probably put that in the examples, for if users want that behavior but maybe don't know about `failed_when`.
self.get_stack_info returns a string. I don't think this will work as expected. You probably want it to return a list.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
flake8 doesn't like the hanging indent here.
Meanwhile, I merged a conflicting patch. I think you just need to remove the quotes from around the star.
`test_jsonfield` looks good to me (since this is InspectDBTests, _introspection seems redundant).
This shouldn't be in `response.keys()`, it's in `response['ins_api'].keys()`. I would rather `if response['ins_api'].get('outputs'):`
I'm thinking these parameters shouldn't be filled in, since we're popping `stack_description` so if a user wanted a hard-fail for nonexistent stacks, they could do: ``` - cloudformation_facts: stack_name: not-real failed_when: not cloudformation['not-real'] ```
Also probably put that in the examples, for if users want that behavior but maybe don't know about `failed_when`.
Just checked, ec2_asg_facts is the only other module that references 'ar_ns', and that has code that copes either way.
Ooh, this is a nasty bug - if this wasn't in this PR,I wouldn't have spotted this. If this code happens elsewhere, it'll break when 2.4 goes out (because a now valid key is overwritten by the content of a now missing key)
camel2snake should indeed handle NotificationARNs properly (#25105)
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
We should pass `using` from the queryset ```suggestion ct_id = self.get_content_type(model=model_cls, using=ct_queryset.db).pk ```
could switch to single quotes for consistency
Cases from lines 361 and 363 work with the previous implementation.
True, I mixed up sth when switching branches. Nevertheless they work on all databases except PostgreSQL, so we need to fix this.
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
add trailing comma
include trailing ,
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
I would use a list comprehension rather than `list(map())`
Such an extensive docstring is not necessary, IMO.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
No need to use quotes. We tend to not quote when it's not needed, as this helps people understand the YAML rules better.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
This isn't needed, since we now only support 2.6+ anyway.
Both cases test the same thing, don't repeat yourself. Test things based on their properties. Here you'd probably want to test iterables, not just sets.
(Similarly, mark any string that you are going to call decode on with as a b"string".)
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
I would chop blank lines in this test.
Chop `Ensure that`.
You should probably expect unicode strings
It probably makes sense to test that the exception reason also matches expectations
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Chop `Ensure that`.
I would chop blank lines in this test.
```suggestion msg = 'Slice stop must be greater than slice start.' ```
FWIW using `monkeypatch` is preferable because it's pytest-native unless I need something like mocks or spies.
```suggestion with self.subTest(s=s), with self.assertRaisesMessage(ValueError, msg): F('name')[s] ```
Perhaps extend this for a wider range of field types, e.g. `BooleanField`, `IntegerField`, `FloatField`, etc.
I don't think it's necessary. In all cases we test the same (default) implementation. A single assertion should be enough, e.g. ```python def test_invalid_fields_in_slicing_f_expressions(self): msg = 'This field does not support slicing.' with self.assertRaisesMessage(NotSupportedError, msg): Company.objects.update(num_chairs=F('num_chairs')[:4]) ```
(And round-tripping of the messages is already tested in other tests)
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Any problem with: ``` @property def media(self): ```
I think `name.rsplit('-', 1)[-1]` is easier to read.
No indentation is needed: ```suggestion "Optimizing from %d operations to %d operations." % ```
That's not true, `return` is to avoid setting new migrations.
I thought you wanted to remove `return`. Nevertheless I'd also leave the `else` as it increases readability.
You added the return statement to the above if the condition which means no need else statement. You should remove it to make it easier to read.
I prefer the following one rather than the above one ```py def greet(): condition = False if condition: return "Hi" return "Hello" ``` Feel free whether follow the things I point out.
Do you think else statement is required in the following example? ```py def greet(): condition = False if condition: return "Hi" else: return "Hello" ```
```suggestion sys.exit(1) ```
Do we need an indentation in the message? ```suggestion self.stdout.write("No optimizations possible.") ``` We can also leave an indentation and add a heading: ```python if self.verbosity > 0: self.stdout.write(self.style.MIGRATE_HEADING("Optimizing...")) optimizer = MigrationOptimizer() new_operations = optimizer.optimize(migration.operations, migration.app_label) if len(migration.operations) == len(new_operations): if verbosity > 0: self.stdout.write(" No optimizations possible.") ```
I would chop blank lines in this test.
Chop `Ensure that`.
```suggestion msg = 'Slice stop must be greater than slice start.' ```
Perhaps extend this for a wider range of field types, e.g. `BooleanField`, `IntegerField`, `FloatField`, etc.
I don't think it's necessary. In all cases we test the same (default) implementation. A single assertion should be enough, e.g. ```python def test_invalid_fields_in_slicing_f_expressions(self): msg = 'This field does not support slicing.' with self.assertRaisesMessage(NotSupportedError, msg): Company.objects.update(num_chairs=F('num_chairs')[:4]) ```
```suggestion with self.subTest(s=s), with self.assertRaisesMessage(ValueError, msg): F('name')[s] ```
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
Both cases test the same thing, don't repeat yourself. Test things based on their properties. Here you'd probably want to test iterables, not just sets.
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
(Similarly, mark any string that you are going to call decode on with as a b"string".)
I would chop blank lines in this test.
Chop `Ensure that`.
You should probably expect unicode strings
It probably makes sense to test that the exception reason also matches expectations
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
(In general, I don't think modules should have such options.)
You should emphasize that the module can and will not do any idempotence checking for this.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
This command has always left a trailing `,`. You could potentially omit the commands after the `cut` and just do a split/join in python that will give exactly what we want. Also, not sure if it was intentional or not, but this command seems to elide the alphabetical sorting that's in the command on the wiki.
This should go to `announce@apache.org` as well, that's actually the most critical one as that makes it "official".
I double people will update this script correctly. We can only hope, that release managers verify this before sending the email... As an alternative, we can also wildcard this, and let release manger insert those manually. Similar to `<DETAILS OF THE CHANGES>` above.
This can be single-lined: ```suggestion return (matched_tags or not tags) and not test_tags.intersection(exclude_tags) ```
I would chop `does_`: ```suggestion if test_match_tags(test, tags, exclude_tags) ```
Not sure what "This one go down into the class tree" means. I know it's from the original method, but see if you can improve this language so it's more clear.
Looking for outside counsel on this one ;-)
I understand what you're trying to do here, but managing both parent as child objects in a single module is going to be problematic IMO. This can only work if your childs are always fully listed (and replaced/updated).
In the metric system, 'kilo' is abbreviated as 'k'. I guess this is wrong in the UCS interface though.
I expected that much.
Should this be ``` python checked_list.sort(key=lambda x: sorted(list(x.items())) if isinstance(x, dict) else x) ``` tracebacks from test failures in CI make me suspect the view object from set(x.items()) fails because set() wants hashable items and the view object from x.items() is not. Does wrapping it in a list() or tuple() work? ``` 2017-06-26 16:43:33 The full traceback is: 2017-06-26 16:43:33 Traceback (most recent call last): 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_module_ecs_ecr.py", line 304, in run 2017-06-26 16:43:33 policy = sort_json_policy_dict(policy) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 603, in sort_json_policy_dict 2017-06-26 16:43:33 ordered_policy_dict[key] = value_is_list(value) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 595, in value_is_list 2017-06-26 16:43:33 checked_list.sort(key=lambda x: sorted(set(x.items())) if isinstance(x, dict) else x) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 595, in <lambda> 2017-06-26 16:43:33 checked_list.sort(key=lambda x: sorted(set(x.items())) if isinstance(x, dict) else x) 2017-06-26 16:43:33 TypeError: unhashable type: 'list' ```
This code isn't correct. (2, 7) is the check you want, and then with >= if I am not mistaken.
Shouldn't this be `return isinstance(x, collections.Iterator)` instead? Actually we should replace all reference to this function by a `collection.Iterator` instance check.
```suggestion pkg_version = line.partition(':')[-1].strip() ```
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
I think we can use `in cls` instead of `in cls.__members__.values()` (here and in `names`).
Ah. lambda is prettier though :). maybe we can run pep8 with ignore lambda error.
This is missing other likely candidates: `y` and `yes` I'd also argue it should be simplified to be case-insensitive so that `TRUE` == `true` (which requires you actually assert the incoming `val` is actuall stringy enough to have `.lower()`, rather than simply doing an equality match against any of the values)
I typically use something like the following: ```python return str(val).lower() in {'1', 't', 'y', 'on', 'true', 'yes'} ``` This works whether you pass a boolean, integer or string, although we are only expecting a string from the environment anyway. If we wanted to be strict, we should reject invalid values: ```python if str(val).lower() in {'1', 't', 'y', 'on', 'true', 'yes'}: return True if str(val).lower() in {'0', 'f', 'n', 'off', 'false', 'no'}: return False raise ValueError('Non-boolean string provided.') ```
Should this be ``` python checked_list.sort(key=lambda x: sorted(list(x.items())) if isinstance(x, dict) else x) ``` tracebacks from test failures in CI make me suspect the view object from set(x.items()) fails because set() wants hashable items and the view object from x.items() is not. Does wrapping it in a list() or tuple() work? ``` 2017-06-26 16:43:33 The full traceback is: 2017-06-26 16:43:33 Traceback (most recent call last): 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_module_ecs_ecr.py", line 304, in run 2017-06-26 16:43:33 policy = sort_json_policy_dict(policy) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 603, in sort_json_policy_dict 2017-06-26 16:43:33 ordered_policy_dict[key] = value_is_list(value) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 595, in value_is_list 2017-06-26 16:43:33 checked_list.sort(key=lambda x: sorted(set(x.items())) if isinstance(x, dict) else x) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 595, in <lambda> 2017-06-26 16:43:33 checked_list.sort(key=lambda x: sorted(set(x.items())) if isinstance(x, dict) else x) 2017-06-26 16:43:33 TypeError: unhashable type: 'list' ```
This is an identity check: ```suggestion if default is NoDefaultValue: ```
Shouldn't this be `return isinstance(x, collections.Iterator)` instead? Actually we should replace all reference to this function by a `collection.Iterator` instance check.
This code isn't correct. (2, 7) is the check you want, and then with >= if I am not mistaken.
I typically use something like the following: ```python return str(val).lower() in {'1', 't', 'y', 'on', 'true', 'yes'} ``` This works whether you pass a boolean, integer or string, although we are only expecting a string from the environment anyway. If we wanted to be strict, we should reject invalid values: ```python if str(val).lower() in {'1', 't', 'y', 'on', 'true', 'yes'}: return True if str(val).lower() in {'0', 'f', 'n', 'off', 'false', 'no'}: return False raise ValueError('Non-boolean string provided.') ```
This is missing other likely candidates: `y` and `yes` I'd also argue it should be simplified to be case-insensitive so that `TRUE` == `true` (which requires you actually assert the incoming `val` is actuall stringy enough to have `.lower()`, rather than simply doing an equality match against any of the values)
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
2.6 or 2.7? Also you `requirements` listed here and the modules.
Ah. lambda is prettier though :). maybe we can run pep8 with ignore lambda error.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
okay, but would be helpful to say _why_ we need to always return True.
This isn't needed, since we now only support 2.6+ anyway.
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
`del` is a builtin, not a function. These parens don't have to be here
This can instead be `continue` and let the `else` unnest.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Similarly, ```if tc['skip'].get('i')```
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
You don't need this conditional, since Ansible enforces that these are the only choices.
`del` is a builtin, not a function. These parens don't have to be here
This can instead be `continue` and let the `else` unnest.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
Similarly, ```if tc['skip'].get('i')```
This will overwrite an explicitly given message if you use ``` python validator = DomainNameValidator(accept_idna=True, message='Only IDNA domain allowed') ```
It looks like there will be a SQL syntax error due to a trailing comma if gin_pending_list_limit is used without fastupdate. Maybe `with_params` should be a list and joined with `', '`.
The only place I can vaguely remember `repr` being used is during the migrations. If you have the `AddIndex/RemoveIndex` operation in your migrations file, it shows this representation when the migrations are run. Since it is very common that a dev might want to create multiple gin indexes in the same table, it is necessary to have the `fields` of the index as well to distinguish the representation of these indexes. So, my decision would be based on how commonly devs have two gin indexes in the same model with the same fields but with different values of `fastupdate` or `gin_pending_list_limit`. If it is a very common case we might want to keep them in `repr`.
Right, I think we could use model fields as a parallel -- we don't include all options a field is initialized with in the `__repr__()`. Here's the implementation: ``` def __repr__(self): """Display the module, class, and name of the field.""" path = '%s.%s' % (self.__class__.__module__, self.__class__.__name__) name = getattr(self, 'name', None) if name is not None: return '<%s: %s>' % (path, name) return '<%s>' % path ``` So I'm +1 to not implementing a custom `__repr__()` for each subclass unless someone can present a counter argument. \cc @akki I created https://github.com/django/django/pull/8643 to remove `BrinIndex.__repr__()`.
Do you think we're adding value with such complicated `__repr__` methods? I've thought maybe we should ditch worrying about including of Index's params in them and just let `Index.__repr__` show the basics. Does the repr should up anywhere important? I can't think of anywhere.
What do you think about ```python def __repr__(self): template = "<%(name)s: fields='%(fields)s'%(fastupdate)s%(gin_pending_list_limit)s>" template_values = { 'name': self.__class__.__name__, 'fields': ', '.join(self.fields), 'fastupdate': '', 'gin_pending_list_limit': '', } if self.fastupdate is not None: template_values['fastupdate'] = ', fastupdate=%s' % ('True' if self.fastupdate else 'False') if self.gin_pending_list_limit is not None: template_values['gin_pending_list_limit'] = ', gin_pending_list_limit=%d' % self.gin_pending_list_limit return template % template_values```
Tests for this method seem missing. It seems like we need a better way to build these reprs that's not so complicated and repetitive for each index.
Maybe @felixxm or @carltongibson can guide, but I believe it'd be good practice to use a `warnings.warn` in `__init__`, although a deprecation timeline has not been determined for `django.contrib.postgres.field.JSONField`.
using hanging indent: ```` template_values = { '...': a, '...': b, }
`Backend` supports negative precision, `SQLite` does not: ```suggestion raise ValueError('SQLite does not support negative precision.') ```
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
`field_preference` must be a list or a tuple.
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
I prefer putting the closing ) on the next line
Code duplication 80-86, 89-94.
Must not be `None`.
Whether it has changed or not does not mean there should be a format with invalid URL.
There are multiple formats, some may have `file` some may not. If some new hq format is introduced with different rendition your approach will break downloading by default while proper URL handling will not. Finally read coding conventions.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
shorten line (rule of thumb I'm using is ~120 characters so I don't have scroll horizontally in github review)
`field_preference` must be a list or a tuple.
Code duplication 80-86, 89-94.
Whether it has changed or not does not mean there should be a format with invalid URL.
There are multiple formats, some may have `file` some may not. If some new hq format is introduced with different rendition your approach will break downloading by default while proper URL handling will not. Finally read coding conventions.
Must not be `None`.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
This is not introduced in this PR: we can try to get the record collector in the constructor and cache it instead of trying to access it every time. I checked all the access patterns of this field and they should not be modified dynamically.
No need to assign to `self.widgets` ```python if isinstance(widgets, dict): self.widgets_names = [ ('_%s' % name) if name else '' for name in widgets ] widgets = widgets.values() else: self.widgets_names = ['_%s' % i for i in range(len(widgets))] self.widgets = [w() if isinstance(w, type) else w for w in widgets] ```
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
You can use iteritems as below ``` from ansible.module_utils.six import iteritems iteritems(parsed) ```
type='str' is a default value not required to mention in separately.
`check_args()` is a empty function. Is this call required? For other networks platforms `check_args()` is present for legacy reason.
This code is almost the same as in `replace_unnamed_groups()`, the only difference is that the beginning of non-capturing group is longer i.e. `'(?:'` instead of `'('`. We could add an internal hook and use it in both places, e.g. ```python def _find_groups(pattern, group_matcher): group_indices = [ (m.start(0), m.end()) for m in non_capturing_group_matcher.finditer(pattern) ] # Loop over the groups. for start, end in unnamed_group_indices: ... for idx, val in enumerate(pattern[end:]): ... if unmatched_open_brackets == 0: group_indices.append((start, end + idx + 1)) break # Remove unnamed group matches inside other unnamed capture groups. group_start_end_indices = [] prev_end = None for start, end in group_indices: if prev_end and start > prev_end or not prev_end: group_start_end_indices.append((start, end)) prev_end = end return group_start_end_indices ``` Moreover, with some boolean flags (e.g. `named=True/False`) this could also be reused in `replace_named_groups()` :thinking: .
Can this be added in agrspec as choices as well? Probably by having this list as a global variable.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
`field_preference` must be a list or a tuple.
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
I prefer putting the closing ) on the next line
Code duplication 80-86, 89-94.
Must not be `None`.
Whether it has changed or not does not mean there should be a format with invalid URL.
There are multiple formats, some may have `file` some may not. If some new hq format is introduced with different rendition your approach will break downloading by default while proper URL handling will not. Finally read coding conventions.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
as a tuple
prefer hanging indent style (allows longer lines, indentation always multiple of 4): ``` rasterheader = ( 1, 0, ... ) ```
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
okay, but would be helpful to say _why_ we need to always return True.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
Use `aliases` if you want to make these two synonymous.
Running the legacy ec2_asg integration tests that use this module, I think this line should be `if block_device_mapping:` because it doesn't look like it can be None or an empty list. The error: ``` An exception occurred during task execution. To see the full traceback, use -vvv. The error was: Missing required parameter in BlockDeviceMappings[0]: "DeviceName" fatal: [localhost]: FAILED! => {"changed": false, "failed": true, "module_stderr": "Traceback (most recent call last):\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 436, in <module>\n main()\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 430, in main\n create_launch_config(connection, module)\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 326, in create_launch_config\n connection.create_launch_configuration(**launch_config)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 253, in _api_call\n return self._make_api_call(operation_name, kwargs)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 517, in _make_api_call\n api_params, operation_model, context=request_context)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 572, in _convert_to_request_dict\n api_params, operation_model)\n File \"/Library/Python/2.7/site-packages/botocore/validate.py\", line 270, in serialize_to_request\n raise ParamValidationError(report=report.generate_report())\nbotocore.exceptions.ParamValidationError: Parameter validation failed:\nMissing required parameter in BlockDeviceMappings[0]: \"DeviceName\"\n", "module_stdout": "", "msg": "MODULE FAILURE", "rc": 0} ```
It looks like just setting `instance_monitoring` to `true` now won't really do anything, since basic monitoring is on by default and the value of `advanced_instance_monitoring` would default to `False`. This seems confusing.
You could update the verbs tense to the convention used elsewhere: "Handle", "set", etc. rather than the "ing" versions.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
I prefer putting the closing ) on the next line
I think `enumerate` would work here
prefer hanging indent style with 1 arg per line
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
What do you need this function for? Ansible already handles aliases, there's no need to do this manually.
This must be checked **before** any processing.
This must be checked **before** any processing.
This must be checked **before** any processing.
This can be single-lined: ```suggestion return (matched_tags or not tags) and not test_tags.intersection(exclude_tags) ```
I would chop `does_`: ```suggestion if test_match_tags(test, tags, exclude_tags) ```
Also, it seems like our convention for docs versions is quite bad. If we didn't want to use `.`, we could use something else like `_`.
Hmm, doesn't seem like this is correct. >version ['1', '0', '0-SNAPSHOT'] major_minor ['1', '0'] Extracting ['tar', 'xf', '/Users/ijuma/src/kafka/core/build/distributions/kafka_2.11-1.0.0-SNAPSHOT-site-docs.tgz', '--strip-components', '1'] Traceback (most recent call last): File "./release.py", line 235, in <module> command_stage_docs() File "./release.py", line 227, in command_stage_docs cmd('Extracting ', 'tar xf %s --strip-components 1' % docs_tar, cwd=os.path.join(kafka_site_repo_path, docs_version(version))) File "./release.py", line 108, in cmd output = subprocess.check_output(cmd, *args, stderr=subprocess.STDOUT, **kwargs) File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py", line 566, in check_output process = Popen(stdout=PIPE, *popenargs, **kwargs) File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py", line 710, in __init__ errread, errwrite) File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py", line 1335, in _execute_child raise child_exception OSError: [Errno 2] No such file or directory: '/Users/ijuma/src/kafka/../kafka-site/10'
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Any problem with: ``` @property def media(self): ```
okay, but would be helpful to say _why_ we need to always return True.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
If it is not required, you don't have to add `'required: False`.
If it is not required, you don't have to add `'required: False`.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
Running the legacy ec2_asg integration tests that use this module, I think this line should be `if block_device_mapping:` because it doesn't look like it can be None or an empty list. The error: ``` An exception occurred during task execution. To see the full traceback, use -vvv. The error was: Missing required parameter in BlockDeviceMappings[0]: "DeviceName" fatal: [localhost]: FAILED! => {"changed": false, "failed": true, "module_stderr": "Traceback (most recent call last):\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 436, in <module>\n main()\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 430, in main\n create_launch_config(connection, module)\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 326, in create_launch_config\n connection.create_launch_configuration(**launch_config)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 253, in _api_call\n return self._make_api_call(operation_name, kwargs)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 517, in _make_api_call\n api_params, operation_model, context=request_context)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 572, in _convert_to_request_dict\n api_params, operation_model)\n File \"/Library/Python/2.7/site-packages/botocore/validate.py\", line 270, in serialize_to_request\n raise ParamValidationError(report=report.generate_report())\nbotocore.exceptions.ParamValidationError: Parameter validation failed:\nMissing required parameter in BlockDeviceMappings[0]: \"DeviceName\"\n", "module_stdout": "", "msg": "MODULE FAILURE", "rc": 0} ```
It looks like just setting `instance_monitoring` to `true` now won't really do anything, since basic monitoring is on by default and the value of `advanced_instance_monitoring` would default to `False`. This seems confusing.
`del` is a builtin, not a function. These parens don't have to be here
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Useless with timestamp available.
Don't capture groups you don't use. Unused captured group.
No brackets needed.
Add here that the `key_alias` or `key_arn` are both ways to provide it.
Need a colon at the end here
You don't need this conditional, since Ansible enforces that these are the only choices.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
This is superfluous, the extension can be extracted automatically.
Eliminate all methods that is only used once.
You will add it when there will be a playlist support. For now it's completely useless.
No point in base class.
No `id` extracted.
I've already pointed out: use `display_id` until you get real id.
Useless with timestamp available.
Don't capture groups you don't use. Unused captured group.
No brackets needed.
This is no longer actual.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
point -> points
Useless with timestamp available.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Don't capture groups you don't use. Unused captured group.
No brackets needed.
This is useless at the end.
Mandatory field: `video_detail.['name']`, or add `or self._generic_title(url)` (not generally done).
This is no longer actual.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Useless with timestamp available.
Don't capture groups you don't use. Unused captured group.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
point -> points
No brackets needed.
This is no longer actual.
Add here that the `key_alias` or `key_arn` are both ways to provide it.
You don't need this conditional, since Ansible enforces that these are the only choices.
Useless with timestamp available.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
point -> points
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
Add here that the `key_alias` or `key_arn` are both ways to provide it.
You don't need this conditional, since Ansible enforces that these are the only choices.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
`del` is a builtin, not a function. These parens don't have to be here
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
Add here that the `key_alias` or `key_arn` are both ways to provide it.
You don't need this conditional, since Ansible enforces that these are the only choices.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
`del` is a builtin, not a function. These parens don't have to be here
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
Add here that the `key_alias` or `key_arn` are both ways to provide it.
You don't need this conditional, since Ansible enforces that these are the only choices.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
`del` is a builtin, not a function. These parens don't have to be here
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Hah. Had the same thought before I got here. See the caveats mentioned above.
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
Add here that the `key_alias` or `key_arn` are both ways to provide it.
You don't need this conditional, since Ansible enforces that these are the only choices.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
`del` is a builtin, not a function. These parens don't have to be here
This can instead be `continue` and let the `else` unnest.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Similarly, ```if tc['skip'].get('i')```
This message is different on PostgreSQL 13+, it should be enough to check that a constraint name is in the message.
redundant, remove ```suggestion ```
Good catch, I will remove it before final squash.
I think these should be `is True` since the `.assertTrue` being replaced would fail if on something like `"foo"` which is truthy. So the `assert "foo"` would pass where `self.assertTrue("foo")` wouldn't.
```suggestion with self.assertRaises((OperationalError, ProgrammingError)): ```
I'm not sure why these tests raises a `ProgrammingError` :thinking:
```suggestion with self.assertRaises((OperationalError, ProgrammingError)): ```
```suggestion with self.assertRaises((OperationalError, ProgrammingError)): ```
Why we're adding a constraint manually and not with `AddConstraintNotValid()`? Also, please use hanging indentation.
> Is there any specific reason why we would prefer using the operation in this case? Yes, because we have it. Using a RAW SQL is the last option, we're developing the ORM in order not to use them.
`del` is a builtin, not a function. These parens don't have to be here
This can instead be `continue` and let the `else` unnest.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
This is not fixed.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Similarly, ```if tc['skip'].get('i')```
Use built-in method? Also, should it crash (raise) if `json.loads()` fails, or just return an empty dict again? ```suggestion try: return self._download_json( url, song_id, data=data.encode('ascii'), headers=headers) except ExtractorError as e: if type(e.cause) in (ValueError, TypeError): # JSON load failure raise except Exception: pass return {} ```
`enumerate` on for range.
Again: ```suggestion return ('params={0}'.format(encrypted_params), headers) ```
Useless with timestamp available.
This is useless at the end.
This is no longer actual.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
No brackets needed.
Don't capture groups you don't use. Unused captured group.
Referring `url` from `url` looks like nonsense. Provide rationale.
This needs to incorporate `name` somehow else it will break when form prefixes are involved or on top level field names collisions.
All `Col` should have an `alias`.
put closing parenthesis on the next line
chop "one of" add comma before "or"
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
prefer hanging indent style with 1 arg per line
`band_input`, you don't get much by saving one char :-)
I think `enumerate` would work here
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
This needs to incorporate `name` somehow else it will break when form prefixes are involved or on top level field names collisions.
You could zip here as well ```python for i, (widget_name, widget) in enumerate(self.widget_names, self.widgets): if input_type is not None: widget.input_type = input_type widget_name = name + self.widgets_names[i] ```
This whole logic can be simplified to ```diff diff --git a/django/db/models/sql/query.py b/django/db/models/sql/query.py index 4e120d2741..0d39671f0b 100644 --- a/django/db/models/sql/query.py +++ b/django/db/models/sql/query.py @@ -1620,17 +1620,22 @@ class Query(BaseExpression): _resolve_cols(self.annotations[name].get_source_expressions()) return set(cols) + @classmethod + def _gen_col_aliases(cls, exprs): + for expr in exprs: + if isinstance(expr, Col): + yield expr.alias + else: + yield from cls._gen_col_aliases(expr.get_source_expressions()) + def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False, simple_col=False): if not allow_joins and LOOKUP_SEP in name: raise FieldError("Joined field references are not permitted in this query") - if name in self.annotations: - if not allow_joins and self.annotations[name].contains_column_references: - for alias in ( - {getattr(self.annotations[name], 'alias', None)} - if isinstance(self.annotations[name], Col) else - self.resolve_cols(name) - ): - if alias and isinstance(self.alias_map[alias], Join): + annotation = self.annotations.get(name) + if annotation is not None: + if not allow_joins: + for alias in self._gen_col_aliases([annotation]): + if isinstance(self.alias_map[alias], Join): raise FieldError('Joined field references are not permitted in this query') if summarize: # Summarize currently means we are doing an aggregate() query @@ -1639,7 +1644,7 @@ class Query(BaseExpression): # that case we need to return a Ref to the subquery's annotation. return Ref(name, self.annotation_select[name]) else: - return self.annotations[name] + return annotation else: field_list = name.split(LOOKUP_SEP) join_info = self.setup_joins(field_list, self.get_meta(), self.get_initial_alias(), can_reuse=reuse) ```
All `Col` should have an `alias`.
You can drop the `.contains_column_references` as it's wasteful in this case because of the way it walks the expression tree given we'll have to walk it anyway below.
I liked generator approach more: it's more readable in terms of separating a distinct logical step from a higher-level flow.
`band_input`, you don't get much by saving one char :-)
I think `enumerate` would work here
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
This check is only necessary in `URLResolver._populate()`, since `URLPattern._populate()` can never be called recursively.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
Any problem with: ``` @property def media(self): ```
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
Would it maybe be better to only check the beginning of the query? Assume that the query is `SELECT * FROM USER_UPDATES`; in that case, because `INSERT` appears in `q.upper()` and `cursor.rowcount > 0` (assuming the table isn't empty), the module will say something was changed.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
You probably should just exit here with `changed=False`
How about ```suggestion q = q.lstrip()[0:max_keyword_len].upper() ``` and removing the `.upper()` calls below? Will be more efficient :)
the `and retries >= CONFIRM_UPDATE_MAX_RETRY` is redundant here. If the execution got here, it'll always be `True`
FYI, in case you switch to working on whole config, `get_config.config` gives you the whole tree. (`get_config(:foo).config` is generally equivalent to `get_config.config[:foo]`.) Also, VMDB::Config is deprecated. May be better to use `::Settings.to_hash`.
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
Same here, default parameter is **validate_certs**, the aliases should be removed. We also simplify this by *not* adding `required=False`, and always starting with the type. Since every parameter has a type, starting with this adds consistency.
So add `type='str'` here too. And we tend to sort lists if the order is of no importance.
So removed `required=False` and add `type='str' instead. Do this for the others as well.
Please use `msg` instead of `result`. Also the standard casing for both parameters as well as return values is snail_case. So it would become `deploy_key` instead of `deployKey`.
Same as for the updated, I'd rather say `has been deleted`
Leave the trailing comma. This is explicitly allowed by python in order to make line-modifications (like moving the order of lines, or adding lines) without having to update unrelated lines.
Use `missing_required_lib` from `ansible.module_utils.basic`
use the `missing_required_lib` function from `ansible.module_utils.basic`
Use collections.Mapping for dict
Use `missing_required_lib` from `ansible.module_utils.basic`
It would be useful to tell the user which `key` is invalid.
hmm, true, not sure how this was passing the tests before ...
Sure, a separate PR sounds good.
you can move it to before `if` as just `docs = {}` line, this should read better.
Noticed inconsistency between ``add_host_to_inventory`` and ``add_host_to_group`` with regard to return value.
I would be consistent and pick one of them. Although, returning after in place modification doesn't add a value, it only makes sense if you will do a deepcopy of the dict and never modify the passed one.
FWIW using `monkeypatch` is preferable because it's pytest-native unless I need something like mocks or spies.
Return ``None`` is also valid? Ã°ÂŸÂ˜Â‰
I think it would be better to use ``params`` with ``requests.get`` rather than building url query.
I suggest handling the response failure would yield better error messages. ``.json()`` will always raise a ``JSONDecodeError`` Example: ```python r = requests.get(...) r.raise_for_status() # if 404, 401, 500 etc ... hosts_list = r.json() ```
Sure, a separate PR sounds good.
you can move it to before `if` as just `docs = {}` line, this should read better.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
Noticed inconsistency between ``add_host_to_inventory`` and ``add_host_to_group`` with regard to return value.
I would be consistent and pick one of them. Although, returning after in place modification doesn't add a value, it only makes sense if you will do a deepcopy of the dict and never modify the passed one.
I think it would be better to use ``params`` with ``requests.get`` rather than building url query.
I suggest handling the response failure would yield better error messages. ``.json()`` will always raise a ``JSONDecodeError`` Example: ```python r = requests.get(...) r.raise_for_status() # if 404, 401, 500 etc ... hosts_list = r.json() ```
Return ``None`` is also valid? Ã°ÂŸÂ˜Â‰
FYI, in case you switch to working on whole config, `get_config.config` gives you the whole tree. (`get_config(:foo).config` is generally equivalent to `get_config.config[:foo]`.) Also, VMDB::Config is deprecated. May be better to use `::Settings.to_hash`.
I think a better wording would be: "Backwards compatibility for storage backends without support for `max_length` will be removed in Django 2.0."
I'm omit the intermediate extra variable in favor of: ``` getattr(logger, level)( message, *args, extra={...}, exc_info=exc_info, ) ```
May as well do the following as a field name can only legally have a single `-` at the start: ```python field_name = part.lstrip('-') ```
This can be single-lined: ```suggestion return (matched_tags or not tags) and not test_tags.intersection(exclude_tags) ```
As above, wrap the format in quotes. "Your URL pattern '{}' has .."
As above, wrap the format in quotes. "Your URL pattern '{}' has .."
I would chop `does_`: ```suggestion if test_match_tags(test, tags, exclude_tags) ```
I'd split this line in two
In the "Generate privatekey6 - standard - with non-ASCII passphrase" test, this fails with `'ascii' codec can't decode byte 0xc3 in position 16: ordinal not in range(128)`. Stacktrace: ``` File "/tmp/ansible_openssh_keypair_payload_kCnMbD/__main__.py", line 193, in generate tf.write("#!/bin/sh\necho %s" % quote(self.passphrase)) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 706, in write return self.writer.write(data) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 369, in write data, consumed = self.encode(object, self.errors) ``` It seems to fail in all Python 2 tests; in Python 3 it seems to work.
You need to make sure `tf` doesn't write a unicode string. tf.write(u("#!/bin/sh/\necho %s" % quote(self.passphrase)).encode("utf-8")) Ansible may have a utility function to do this.
Unlike an "app", I wouldn't say that a model is "installed". Two solutions: - check if the app is installed and adjust the error message accordingly: "app X isn't installed" or "app X doesn't provide model Y" - just rephrase do "which doesn't exist"
@graingert Probably it doesn't worth it, but still doable :-) ``` In [85]: class desc: ...: def __init__(self, f): ...: self.name = f.__name__ ...: ...: def __get__(self, instance, cls): ...: if instance is None: ...: return self ...: ...: for cls in cls.__mro__: ...: name = '_%s%s' % (cls.__name__, self.name) ...: if cls.__dict__.get(name) is self: ...: return name ...: ...: class A: ...: @desc ...: def __x(self): ...: pass ...: ...: ...: class B(A): ...: pass ...: In [86]: a = A() ...: a._A__x ...: Out[86]: '_A__x' In [87]: b = B() ...: b._A__x ...: Out[87]: '_A__x' ```
I think it's the right class: ``` In [38]: class desc: ...: def __get__(self, instance, cls): ...: if instance is None: ...: return self ...: return instance.__dict__['_%s__mangled' % cls.__name__] In [39]: class A: ...: d = desc() ...: ...: def __init__(self): ...: self.__mangled = 42 ...: In [40]: A().d Out[40]: 42 ```
@graingert `cls` is passed here.
I'd say `on Python < 3.6`
Yeah, so none of the tests failed if I forced either the multiple or single argument form of the function. That can't be right.
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
It would have been more appropriate if `run_checks()` took an `Apps` instance as a parameter instead of a list of `AppConfig`s.
`import ansible.module_utils.parsing.convert_bool import BOOLEANS` and use that constant
This should just force strict and leave unchanged on typeerror ```diff --git a/lib/ansible/plugins/action/set_fact.py b/lib/ansible/plugins/action/set_fact.py index d7fe573c1a..934245d07c 100644 --- a/lib/ansible/plugins/action/set_fact.py +++ b/lib/ansible/plugins/action/set_fact.py @@ -51,8 +51,11 @@ class ActionModule(ActionBase): "letters, numbers and underscores." % k) return result - if not C.DEFAULT_JINJA2_NATIVE and isinstance(v, string_types) and v.lower() in ('true', 'false', 'yes', 'no'): - v = boolean(v, strict=False) + if not C.DEFAULT_JINJA2_NATIVE and isinstance(v, string_types): + try: + v = boolean(v, strict=True) + except TypeError: + pass # not valid bool string facts[k] = v result['changed'] = False```
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
```not (foo is None)``` => ```foo is not None```
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
I think that we only want to remove a single `'g'` from the end of the string and continue to allow things like `'gg'`, `'1gg'`, `'g2'`, and `'g3g'` to raise an error as they would have before. ```suggestion if arg.endswith('g'): arg = arg[:-1] force_grouping = True ```
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
"any other input"
You could update the verbs tense to the convention used elsewhere: "Handle", "set", etc. rather than the "ing" versions.
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
This can be single lined: ```python '%s\n%s' % (combined_list[prev], combined_list[k]), ```
Please use a single quote.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
```suggestion template_name = 'forms_tests/form_snippet.html' ```
Unless there's a significant difference, a single RTL language seems fine... ðŸ¤”
Should also include `block_size` and `parallelism`
```suggestion return ''.join([ self.handle_word(word) for word in words ]) ``` I changed `handle_word` to return `word` in remaining cases.
Is there a good reason to order the data like this? I'd personally expect the hash to be at the end, so it could include a `$` .
Good point, updated :+1:
Should also update based on `block_size` and `parallelism`
Passing `lead` and `trail` to the `trim_punctuation()` looks unnecessary. This method is called only here so we always pass `('', word, '')`. Maybe: ```suggestion lead, middle, trail = self.trim_punctuation(word) ``` and ```python def trim_punctuation(self, word): """ Trim trailing and wrapping punctuation from `word`. Return the items of the new state. """ lead, middle, trail = '', word, '' ... return lead, middle, trail ```
`quote` isn't used anywhere.
Need spaces around `+` sign.
Need spaces around `-` sign.
Generally, I'd have put the static regex directly as the first argument of `re.search()`.
Should also include `block_size` and `parallelism`
Such an extensive docstring is not necessary, IMO.
Need spaces around `+` sign.
`quote` isn't used anywhere.
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Good point, updated :+1:
Need spaces around `-` sign.
Would there be any utility to sorted() on files/sub_folders? AFAICT, os.walk will use the os.listdir() results directly for filenames. os.listdir() is doc'ed as 'arbitrary order' (iirc, more or less up to fs). Wondering if sorting those results would make for more reproducible behavior when trying to replicate errors. It shouldn't matter at all if there are no bugs, but it could make tracking down odd failures easier.
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
nit: it's not a regex and there's no escaped symbols so there's really no need to make use of raw-strings
Maybe this should be a class docstring :thinking:
```suggestion return ''.join([ self.handle_word(word) for word in words ]) ``` I changed `handle_word` to return `word` in remaining cases.
> They can't be multiline, can they? Yep. According to [ECMA 262 5.1](http://www.ecma-international.org/ecma-262/5.1/), CR (U+000D), LF (U+000A), LS (U+2028) and PS (U+2029) are not allowed in RegExp literals
``` >>> re.match(r'/(?=[^*])[^/\n]*/[gimy]{0,4}', r'''/\/\/\//''') <_sre.SRE_Match object; span=(0, 3), match='/\\/'> ```
`quote` isn't used anywhere.
Need spaces around `+` sign.
Need spaces around `-` sign.
Passing `lead` and `trail` to the `trim_punctuation()` looks unnecessary. This method is called only here so we always pass `('', word, '')`. Maybe: ```suggestion lead, middle, trail = self.trim_punctuation(word) ``` and ```python def trim_punctuation(self, word): """ Trim trailing and wrapping punctuation from `word`. Return the items of the new state. """ lead, middle, trail = '', word, '' ... return lead, middle, trail ```
I'd go for underlining.
`quote` isn't used anywhere.
Maybe this should be a class docstring :thinking:
Need spaces around `+` sign.
```suggestion return ''.join([ self.handle_word(word) for word in words ]) ``` I changed `handle_word` to return `word` in remaining cases.
```suggestion pkg_name = line.partition(':')[-1].strip() ``` idem, no need to lower.
@thomwiggers `line.partition(':')[2]` yes, I have no problem with that.
If you're going to do partition, I'd rather use the fact that it'll always return a 3-tuple instead of using `-1` to index the third element. i.e.: ```python line.partition(':')[2] ```
Need spaces around `-` sign.
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
Passing `lead` and `trail` to the `trim_punctuation()` looks unnecessary. This method is called only here so we always pass `('', word, '')`. Maybe: ```suggestion lead, middle, trail = self.trim_punctuation(word) ``` and ```python def trim_punctuation(self, word): """ Trim trailing and wrapping punctuation from `word`. Return the items of the new state. """ lead, middle, trail = '', word, '' ... return lead, middle, trail ```
Such an extensive docstring is not necessary, IMO.
```suggestion return ''.join([ self.handle_word(word) for word in words ]) ``` I changed `handle_word` to return `word` in remaining cases.
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
`quote` isn't used anywhere.
Need spaces around `+` sign.
Need spaces around `-` sign.
Would there be any utility to sorted() on files/sub_folders? AFAICT, os.walk will use the os.listdir() results directly for filenames. os.listdir() is doc'ed as 'arbitrary order' (iirc, more or less up to fs). Wondering if sorting those results would make for more reproducible behavior when trying to replicate errors. It shouldn't matter at all if there are no bugs, but it could make tracking down odd failures easier.
Passing `lead` and `trail` to the `trim_punctuation()` looks unnecessary. This method is called only here so we always pass `('', word, '')`. Maybe: ```suggestion lead, middle, trail = self.trim_punctuation(word) ``` and ```python def trim_punctuation(self, word): """ Trim trailing and wrapping punctuation from `word`. Return the items of the new state. """ lead, middle, trail = '', word, '' ... return lead, middle, trail ```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
This needs to be a failure, the `OrderedDict` object is used. Maybe just do a straight import with the try / except ``` python try: from collections import OrderedDict except ImportError: from ordereddict import OrderedDict ```
we should not be adding a python dependency on ordereddict here, as python2.7 can also use ordereddict from collections: https://docs.python.org/2/library/collections.html#collections.OrderedDict This also means, that python2.7 users now need an additional python dependency installed.
Since we are already using `six`, we should use `six.moves` here instead. ``` from six.moves.urllib.parse import urlencode ```
py3.x-only code; can safely ditch the args to `super()`
Minor consistency nit- our stuff seems to be using `[action]_[backend]()`, where these aliased imports have that reversed. I'd vote for `[action]_[backend]()` over this- makes the code a little easier to read...
In Python 3.x `bytearray` accepts only bytes-like objects, and strings are not.
`xrange` is not defined in Python 3.x
This doesn't really matter since only a single instance of ForemanInventory is created in this script but config_paths should really be an instance attribute (created and given an initial value in ```__init__```) rather than a class attribute. Class attributes exist a single time per class. All instances of the class would share the same instance of that attribute. Instance attributes exist, one per instance and thus can change independently. Modifying config_paths later in the code is a tip-off that this should be an instance-attribute rather than a class-attribute. Also, you can probably move the setting from an environment variable to be with this code when you do that. It would seem to make sense to keep that all together.
On further thought, this actually might break something with the new stuff, since you're relying on pyyaml blindly `call`ing whatever is passed in, but the prototype logic that supports object instances only does that call if `isinstance(loader, Reader)` is true. We could probably tweak that somehow, like `callable()` instead, which might be a little more resilient/Pythonic anyway... So this is definitely fine for released code, and it's something I'll keep in mind for the new stuff.
please add `no_log=True`
no `u''` prefixes on strings please
I would change this to a `NotSupportedError`, e.g. ```python raise NotSupportedError( 'Bulk create a multi-table inherited model is not supported ' 'on this database backend.' ) ```
Iâ€™m not comfortable with how this is calculated. There ought to be a better way to handle this, e.g. by inspecting `sys.path` for common ancestors.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
I'd suggest to remove the :param: stuff in the strings you've touched as that's not really something we've adopted throughout the code base.
It doesnâ€™t feel right to me to just silently drop things. I think itâ€™s better to fall back to pulling the filesystem in this scenario.
chop blank line
Does this really need to be a global? Can it not be a property on the `VarsModule` instead>
```suggestion self._display.debug("recursive_group_vars - Traversing dir : %s with groups : %s" % (path, to_text(groups))) ```
Try using `.format()` or `%s` formatting instead: ```suggestion self._display.debug("recursive_group_vars - Matched file : %s" % to_text(found)) ```
```suggestion b_opath = os.path.realpath(to_bytes(os.path.join(path, group))) ```
With `groups` below changing to just be a list of strings, this should be changed ```suggestion found_files = loader.find_vars_files(path, group, allow_dir=False) ```
The `to_bytes` should be on the `self._basedir` instead: ```suggestion b_opath = os.path.realpath(os.path.join(to_bytes(self._basedir), b'r_group_vars')) ```
Use dict literals: ```suggestion return {} ```
Use a tuple as a the cache key instead of stringifying this. This will likely result in a performance problem later if not resolved now. ```suggestion key = tuple(groups) + (b_opath,) ```
Please convert this to just a list comprehension: ```suggestion groups = [e.name for e in entities if isinstance(e, Group)] ``` Additionally, just convert to a list of names here, since you only ever need that, and not the object later.
We should probably do more here than just reraise the exception with a different type. Add a message here so it gives context about the failure. The same with the next one too.
please wrap {}
This could be simplified to match the implementation in `_sqlite_datetime_extract()`: ```suggestion month_in_quarter = ceil(dt.month / 3) ```
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
Error is raised only on PostreSQL. On Oracle and MySQL we simply return the same date. Even if we want to raise an exception on SQLite, `NotImplementedError` is probably not the best choice. It's not sth that will or may be implemented in the feature. I'd use `ValueError` as we do in similar cases.
I'm not sure about raising these exceptions (here, in `_sqlite_datetime_trunc()` and in`_sqlite_datetime_trunc()`), user will get rather unhelpful `OperationalError`: ``` django.db.utils.OperationalError: user-defined function raised exception ``` and we don't do this on other backends.
Sure, perhaps `ValueError` is better. I think PostgreSQL does the nice thing here - silently returning the value unchanged is ugly. Given this is our own implementation, having a third way - returning `NULL` - isn't great. We should align to one of the other two behaviours, and raising an error seems best.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Does this really need to be a global? Can it not be a property on the `VarsModule` instead>
```suggestion self._display.debug("recursive_group_vars - Traversing dir : %s with groups : %s" % (path, to_text(groups))) ```
Try using `.format()` or `%s` formatting instead: ```suggestion self._display.debug("recursive_group_vars - Matched file : %s" % to_text(found)) ```
```suggestion b_opath = os.path.realpath(to_bytes(os.path.join(path, group))) ```
With `groups` below changing to just be a list of strings, this should be changed ```suggestion found_files = loader.find_vars_files(path, group, allow_dir=False) ```
The `to_bytes` should be on the `self._basedir` instead: ```suggestion b_opath = os.path.realpath(os.path.join(to_bytes(self._basedir), b'r_group_vars')) ```
Use dict literals: ```suggestion return {} ```
Use a tuple as a the cache key instead of stringifying this. This will likely result in a performance problem later if not resolved now. ```suggestion key = tuple(groups) + (b_opath,) ```
Please convert this to just a list comprehension: ```suggestion groups = [e.name for e in entities if isinstance(e, Group)] ``` Additionally, just convert to a list of names here, since you only ever need that, and not the object later.
We should probably do more here than just reraise the exception with a different type. Add a message here so it gives context about the failure. The same with the next one too.
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
immediatelly -> immediately
Do we need to define `async_generator` in `__aiter__`? ```python async def _async_generator(self): # Generators don't actually start running until the first time you call # next() on them, so we make the generator object in the async thread # and then repeatedly dispatch to it in a sync thread. sync_generator = self.__iter__() # So, you can't raise StopIteration up through sync_to_async as # too much of the Python async system was originally built on # generators. Instead, we capture it inside a custom sync function # and change it to an exception we *can* propagate. # We should probably move this logic inside asgiref at some point. def custom_next(gen): try: return next(gen) except StopIteration: raise StopAsyncIteration() while True: try: yield await sync_to_async(custom_next)(sync_generator) except StopAsyncIteration: return # Note: __aiter__ is a *synchronous* method that has to then return an # *asynchronous* iterator/generator. Thus, we nest an async generator # inside it. # Also note: This is a generic iterable converter for now, and is going to # suffer a performance penalty on large sets of items due to the cost of # crossing over the sync barrier each time. We should write custom ones # for each Iterable subclass, but we'll need some work in Compiler first. def __aiter__(self): return self._async_generator() ```
Not sure it makes a difference but before it looks like we got `form=None` in the context.
Instead of defining `success`, just return directly in the above code. I think it is simple enough.
Now that Django only supports Python 3.5+ this can be much more simply defined: ```python try: ... except FileNotFoundError: return False ``` You don't need to catch any other type of `IOError` as it is just re-raised. Also don't forget to remove the `errno` import that was added.
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
For the cases where I've wanted to use a nose test generator, I've ended up just making the test class inherit from object. You lose the TestCase methods (various self.assert*) but usually test generators methods have pretty simple assertions so not too much of a issue. To me it is simpler than injecting the support into a TestCase subclass. (an example is https://github.com/alikins/ansible/blob/6e8a3d0c25fdeaa349351c6636b062376c30b0a6/test/units/plugins/lookup/test_ini.py)
Again, error handling changed.
Single quotes please.
okay, but would be helpful to say _why_ we need to always return True.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
we want want -> we want
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
single line looks more readable here
Returns -> Return use period
remove "0" in {0}
do we really need that `is not None` check? `check_token` should return `False` for None tokens.
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
immediatelly -> immediately
Yeah, I think what you have is fine, since we have to iterate over the generator one way or another, and the set comparison it's really straightforward either. I guess the only downside of this algorithm is that, if the broker nefariously listed the same topic multiple times, it would give us a false positive. But I seriously doubt that can happen.
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
@romgar If you find the time that would be great!
It might be smarter to validate the token first and only modify the session + redirect if it's valid. Otherwise it makes it really easy to create a session just by GET'ing a url (possible DoS vector). It also means you can't pass `accounts/password_reset` as the token and take advantage of our `request.path.replace()` code. It probably means validating the token twice, which is slightly slower. Seems fine to me if an invalid token gets leaked.
@collinanderson That is a good point, at first I wanted to be safe against leaking anything (just to be on the safe side), but the DOS argument is more important.
Not sure it makes a difference but before it looks like we got `form=None` in the context.
Single quotes please.
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Returns -> Return use period
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Returns -> Return use period
remove "0" in {0}
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Returns -> Return use period
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Returns -> Return use period
we want want -> we want
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
single line looks more readable here
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Returns -> Return use period
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
```suggestion item, fields=fields, using=self.db, ```
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
Can you re-warp this block to 79 chars? (First line is too short.)
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
I tried to fix these in #12645 but clearly that hasn't carried over to docstrings ðŸ¤”
chop trailing newline (check code with flake8 to see a couple other minor formatting things)
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
It's better to actually say that there's no file in place or it's inaccessible.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Returns -> Return use period
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
we want want -> we want
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
single line looks more readable here
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
Returns -> Return use period
remove "0" in {0}
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Returns -> Return use period
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Returns -> Return use period
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Returns -> Return use period
remove "0" in {0}
Can you re-warp this block to 79 chars? (First line is too short.)
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Returns -> Return use period
okay, but would be helpful to say _why_ we need to always return True.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
we want want -> we want
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
single line looks more readable here
Returns -> Return use period
remove "0" in {0}
Yeah, I think what you have is fine, since we have to iterate over the generator one way or another, and the set comparison it's really straightforward either. I guess the only downside of this algorithm is that, if the broker nefariously listed the same topic multiple times, it would give us a false positive. But I seriously doubt that can happen.
immediatelly -> immediately
Do we need to define `async_generator` in `__aiter__`? ```python async def _async_generator(self): # Generators don't actually start running until the first time you call # next() on them, so we make the generator object in the async thread # and then repeatedly dispatch to it in a sync thread. sync_generator = self.__iter__() # So, you can't raise StopIteration up through sync_to_async as # too much of the Python async system was originally built on # generators. Instead, we capture it inside a custom sync function # and change it to an exception we *can* propagate. # We should probably move this logic inside asgiref at some point. def custom_next(gen): try: return next(gen) except StopIteration: raise StopAsyncIteration() while True: try: yield await sync_to_async(custom_next)(sync_generator) except StopAsyncIteration: return # Note: __aiter__ is a *synchronous* method that has to then return an # *asynchronous* iterator/generator. Thus, we nest an async generator # inside it. # Also note: This is a generic iterable converter for now, and is going to # suffer a performance penalty on large sets of items due to the cost of # crossing over the sync barrier each time. We should write custom ones # for each Iterable subclass, but we'll need some work in Compiler first. def __aiter__(self): return self._async_generator() ```
I'm still unhappy about this: if for some reason you need to add another wrapper, it could hide the FilteringGeneratorDelegate and JsonXContentGenerator wouldn't know anymore that the content is filtered. I'd rather like something that does not depend on "instanceof".
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
`yield from` is not allowed in async functions.
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
You can use `self.fail('Cyclic reference in Exception Reporter.get_traceback_frames()')`
Single quotes please.
Not sure it makes a difference but before it looks like we got `form=None` in the context.
`json_dumps_params` should be after `safe` (reordering keywords could be backwards incompatible if passing them by argument).
Don't use a mutable default: `{}`. Should default to `None` and then add : ``` if json_dumps_params is None: json_dumps_params={} ```
Does this need to be a separate method? Seems unnecessary to me.
chop trailing newline (check code with flake8 to see a couple other minor formatting things)
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
For the cases where I've wanted to use a nose test generator, I've ended up just making the test class inherit from object. You lose the TestCase methods (various self.assert*) but usually test generators methods have pretty simple assertions so not too much of a issue. To me it is simpler than injecting the support into a TestCase subclass. (an example is https://github.com/alikins/ansible/blob/6e8a3d0c25fdeaa349351c6636b062376c30b0a6/test/units/plugins/lookup/test_ini.py)
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
List comprehensions create unnecessary throw-away lists in memory. It's better to use generator expressions because they are lazy. Also, this is a great case for using `all()`/`any()` (these two exit iterating through generators early, when possible): ```suggestion return all(part != ".." for part in path.split(os.sep)) ```
I tried to fix these in #12645 but clearly that hasn't carried over to docstrings ðŸ¤”
chop trailing newline (check code with flake8 to see a couple other minor formatting things)
Don't use a mutable default: `{}`. Should default to `None` and then add : ``` if json_dumps_params is None: json_dumps_params={} ```
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
The method name with "sane" suggests quite a broad meaning. It's better to be more specific about what that method does. Also, does it need to be a method? I'd have a pure function instead.
chop extra space after period
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Returns -> Return use period
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Returns -> Return use period
remove "0" in {0}
we want want -> we want
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
single line looks more readable here
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Returns -> Return use period
we want want -> we want
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
single line looks more readable here
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Returns -> Return use period
`django/core/files/copy.py:28:37: E127 continuation line over-indented for visual indent` pep8 wants you to indent it like this: ``` fd = os.open(new_file_name, os.O_WRONLY | os.O_CREAT | getattr(os, 'O_BINARY', 0) | (not allow_overwrite and os.O_EXCL or 0)) ```
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
It doesn't make sense to turn a unicode string into a unicode string. It's just `str(str(''))`, which is probably not what you want. Maybe you wanted to turn bytes into text here? In such case check'd be against `binary_type` and the translation should use `to_text()` shim.
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
It's better to actually say that there's no file in place or it's inaccessible.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
`get_object` will get the full item (including the body) - instead I think we should use `head_object` here as it'll still give back the etag without getting the full item. http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.head_object To test this, you could make a 1-2GB object and then try this codepath. It should take a long time, and consume 1-2GB of memory.
Curious what the behavior would be if the name/namespace/apiversion/kind were provided at the top level and here and did not match.
this would be equivalent of `if '-' in md5_remote:` since `etag_multipart` is never used other than this conditional.
Same here, default parameter is **validate_certs**, the aliases should be removed. We also simplify this by *not* adding `required=False`, and always starting with the type. Since every parameter has a type, starting with this adds consistency.
So removed `required=False` and add `type='str' instead. Do this for the others as well.
Use https://github.com/ansible/ansible/blob/cd988f645aaf774c55a98a90e3ef42cc5b1a1563/lib/ansible/module_utils/urls.py#L1147 instead of requests.get
Make `verify` configurable via parameter.
https://github.com/ansible/ansible/blob/cd988f645aaf774c55a98a90e3ef42cc5b1a1563/lib/ansible/module_utils/urls.py#L1147 gives all options.
Please put this on a single line. Lines can be 159 characters wide.
Please put this on a single line. Lines can be 159 characters wide.
This should be `RemovedInDjango50Warning` I think.
I suggested this realiasing for tests only, I'd prefer a `if six.PY3 unquote_to_bytes() else unquote()` in the actual function. It helps with comprehension.
Please remove `weakref_backports.py` and its mention in `setup.cfg`.
+1 it's better to just patch `time.time`
@samdoran then, I'd say that there's something wrong with the setup and TZ should be adjusted somewhere.
I think we don't need to patch `datetime.datetime.fromtimestamp()` and `datetime.datetime.utcfromtimestamp()` methods as they just return a datetime object from patched `time.time()`.
please alphabetize with the rest of the django imports
Unindent by 1 space. Indention must be a multiple of 4 spaces.
This is only ever called once. Do we need the default? (Same with SQL version)
This is only used once. Can we move it back to the `color` module? (That way `termcolors` is still only ever used by `color`)
a OrderedSet, like @timgraham suggested
This is lifted from Python 3.10 so we should attribute appropriately and also ensure that we remove it when no longer needed: ```suggestion if PY310: from contextlib import aclosing else: from contextlib import AbstractAsyncContextManager # Backport of contextlib.aclosing() from Python 3.10. # Copyright (C) 2020 Python Software Foundation (see LICENSE.python). class aclosing(AbstractAsyncContextManager): """ Async context manager for safely finalizing an asynchronously cleaned-up resource such as an async generator, calling its ``aclose()`` method. """ def __init__(self, thing): self.thing = thing async def __aenter__(self): return self.thing async def __aexit__(self, *exc_info): await self.thing.aclose() ```
This line is related to the `save` method, and should sit beneath it, rather than the new `get_updated_model` method.
`always_text` is gone.
chop trailing newline (check code with flake8 to see a couple other minor formatting things)
Does this need to be a separate method? Seems unnecessary to me.
chop extra space after period
Can you use `assertContains()` here to prevent errors due to attribute order in the HTML tags.
Same here, can you use `assertContains()` here to prevent errors due to attribute order in the HTML tags.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
Yeah, I think what you have is fine, since we have to iterate over the generator one way or another, and the set comparison it's really straightforward either. I guess the only downside of this algorithm is that, if the broker nefariously listed the same topic multiple times, it would give us a false positive. But I seriously doubt that can happen.
Does this need to be a separate method? Seems unnecessary to me.
@romgar If you find the time that would be great!
It might be smarter to validate the token first and only modify the session + redirect if it's valid. Otherwise it makes it really easy to create a session just by GET'ing a url (possible DoS vector). It also means you can't pass `accounts/password_reset` as the token and take advantage of our `request.path.replace()` code. It probably means validating the token twice, which is slightly slower. Seems fine to me if an invalid token gets leaked.
@collinanderson That is a good point, at first I wanted to be safe against leaking anything (just to be on the safe side), but the DOS argument is more important.
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
Not sure it makes a difference but before it looks like we got `form=None` in the context.
immediatelly -> immediately
For the cases where I've wanted to use a nose test generator, I've ended up just making the test class inherit from object. You lose the TestCase methods (various self.assert*) but usually test generators methods have pretty simple assertions so not too much of a issue. To me it is simpler than injecting the support into a TestCase subclass. (an example is https://github.com/alikins/ansible/blob/6e8a3d0c25fdeaa349351c6636b062376c30b0a6/test/units/plugins/lookup/test_ini.py)
we want want -> we want
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
I realize this predates your PR, but why is 'unused' not used? It appears to be the target field provided by the SQLUpdateCompiler during as_sql(). This seems like a really good spot to allow a backend to hook in and CAST the expression to a type that is allowed by the target column.
okay, but would be helpful to say _why_ we need to always return True.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
we want want -> we want
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Returns -> Return use period
set the safe
`json_dumps_params` should be after `safe` (reordering keywords could be backwards incompatible if passing them by argument).
Don't use a mutable default: `{}`. Should default to `None` and then add : ``` if json_dumps_params is None: json_dumps_params={} ```
chop trailing newline (check code with flake8 to see a couple other minor formatting things)
I find the inconsistent capitalization very confusing. There's `Name` and `Facts` but also `all_parameters`. Here we have `environment` but also `Location` and `Organization`. Ideally the original API response would be normalized.
```suggestion groupby = {k: self.to_safe(to_text(v)) for k, v in params.items()} ```
Oh yes, 2.6 didn't have dict comprehensions. Do we still need to care about that? setup.py indicates only 2.7 should be supported.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Should also include `block_size` and `parallelism`
Should also update based on `block_size` and `parallelism`
`get_object` will get the full item (including the body) - instead I think we should use `head_object` here as it'll still give back the etag without getting the full item. http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.head_object To test this, you could make a 1-2GB object and then try this codepath. It should take a long time, and consume 1-2GB of memory.
Rather than raising Exception that isn't caught anywhere, fail_json should be called.
this would be equivalent of `if '-' in md5_remote:` since `etag_multipart` is never used other than this conditional.
Don't use `C(...)` here, as this will be shown unformatted to the end-user.
Same here (and further below).
Use https://github.com/ansible/ansible/blob/cd988f645aaf774c55a98a90e3ef42cc5b1a1563/lib/ansible/module_utils/urls.py#L1147 instead of requests.get
Please put this on a single line. Lines can be 159 characters wide.
Please put this on a single line. Lines can be 159 characters wide.
okay, but would be helpful to say _why_ we need to always return True.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
we want want -> we want
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
single line looks more readable here
Returns -> Return use period
remove "0" in {0}
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Returns -> Return use period
remove "0" in {0}
okay, but would be helpful to say _why_ we need to always return True.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
we want want -> we want
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
single line looks more readable here
Returns -> Return use period
remove "0" in {0}
Can you re-warp this block to 79 chars? (First line is too short.)
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
Returns -> Return use period
remove "0" in {0}
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
I would chop blank lines in this test.
Chop `Ensure that`.
(Similarly, mark any string that you are going to call decode on with as a b"string".)
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
You're checking two separate properties here. This should be in a separate test.
For python 2.6 compatibility, this and the following four strings have to look like `cmd = "ping {0} {1}".format(vrf, dest)`
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
I learned recently that you can use actual separate literals to improve readability: ```suggestion @pytest.mark.parametrize(['url', 'expected'], [ ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
```suggestion item, fields=fields, using=self.db, ```
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
okay, but would be helpful to say _why_ we need to always return True.
we want want -> we want
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
single line looks more readable here
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
`import botocore` will make this code a lot more consistent with most other boto3-based ansible modules
I suggested this realiasing for tests only, I'd prefer a `if six.PY3 unquote_to_bytes() else unquote()` in the actual function. It helps with comprehension.
Please remove `weakref_backports.py` and its mention in `setup.cfg`.
I don't have strong feelings about this. One possibility is to remove this, and if we get complaints about the removal during alpha or beta we can reintroduce it back. Another approach is to check how hard it is to write code that works both in 1.7 and 1.8 for those cases that use RelatedObject.
please alphabetize with the rest of the django imports
Unindent by 1 space. Indention must be a multiple of 4 spaces.
This is only ever called once. Do we need the default? (Same with SQL version)
```suggestion b_colldirs = list_collection_dirs(coll_filter=coll_filter) ```
This is only used once. Can we move it back to the `color` module? (That way `termcolors` is still only ever used by `color`)
```suggestion for b_path in b_colldirs: ```
Maybe even in advance with the current form.
Maybe: ```python self.assertEqual(len(self.selenium.find_elements( By.CSS_SELECTOR, '.dynamic-profile_set#profile_set-0 input[name=profile_set-0-first_name]', )), 1) ``` or ```python selector = '.dynamic-profile_set#profile_set-0 input[name=profile_set-0-first_name]' self.assertEqual(len(self.selenium.find_elements(By.CSS_SELECTOR, selector)), 1) ```
This should go to the 2nd commit :pick:
This should go to the 2nd commit :gem:
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
I think this is a typo - `changed` should be `changes`
no, this refers to the standard `changed=True` / `changed=False` and `msg="..."` results returned by ansible modules.
OK, I see what you're doing here :+1:, I think there is slightly better way. Just suggestions, your call. Just removing these is not perfect. Ideally, omitting say `associations` in module args would behave same as writing explicit `associations: {}`. But currently explicit `associations: {}` won't match what's left after you remove empty values from API, causing unnecessary request and changed=True. - Args side can be normalized for you if you make attributes & associations first-class options, or suboptions, defaulting to `{}`. - Note that only recently manageiq master started sending empty values; nulls were omitted before. Not sure that's relevant Ã¢Â€Â” do you get Nones or empty hashes here? Anyway :+1: to normalizing API results in one direction or another Ã¢Â€Â” don't *rely* on getting empty values. - A pattern that works & reads nicely is defining a normalizing function and applying it on both sides of comparison. `if normalize(current) == normalized(desired)`. - IMHO, splitting logic between `has_field` and here is harder to follow, and it could be simple enough to inline, perhaps something like this: ``` current_properties = dynamic_resource_definition.get('properties', {}) current_name = dynamic_resource_definition.get('name') if (current_name == name and current_properties.get('attributes', {}) == properties.get('attributes', {}) and current_properties.get('associations', {}) == properties.get('associations', {}): ```
I think this validation can be dropped too. it's only `name` which is required=True. (plus we wouldn't even reach this function without finding a definition by name)
It is also OK as-is if you like it; I just wanted to make it clear you don't *have* to obey the consistency if you see a better way. [I think I was in a bike-shedding mood when I did this review... Feel free to push back :-)]
Yeah, I had the same problem. I tried changing the exception type raised in `templar.template()` and catching that here, but still couldn't get it quite right. Seems like putting it in `-v` is an improvement over what we have now until we can come up with something better.
I think it would be more helpful to the user to show them both errors by default rather than hiding one in `-v`. Ideally we could have templating errors take precedence over loop errors and only display the templating error first, but that may not make sense in all situations. If we don't want to display both as was done originally, then I'm fine with the current use of `-v` rather than using debug since debug is information overload for users.
this might end up confusing users as to which error produced the failure, i suggest pushing the conditional error into a debug statement
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Oh, I just meant to suggest the `value` to `values` (plural) renaming, not more...
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
`else` can be removed, i.e.: ```python ... elif end is None: return self.UNBOUNDED_FOLLOWING raise ValueError('Illegal argument for end, must be either a positive integer, zero or None') ```
Upon further reflection, I think these three lines along with `check_mutually_exclusive` are erroneous for subspecs. We don't currently have a way to specify `required_if/one_of/together` or `mutually_exclusive` for subspecs, so in every case *but* the networking "aggregate" thing, this is severely broken (as you'd be making nonsensical checks in most cases)
I think these temporary variables and blank lines are unnecessary, maybe sth like that: ```python return self.window_frame_start(start), self.window_frame_end(end) ```
They are helpful when using (i)pdb.
Maybe it will be better to raise `NotSupportedError` instead of `NotImplementedError`. I would say `Window expressions are not supported on this database backend`.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
I'd omit a blank line here.
There is some code duplicated between this pull-request and other FortisOS pull-requests ([`fortios_ipv4_policy`](https://github.com/ansible/ansible/pull/21849/files/d49860f735c162acda87f5232f1de0e148453203#r103181410) and [`fortios_address`](#21542)): - block calling `connect` method - block calling `load_config` method These blocks must be moved together in [`module_utils/fortios.py`](https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/fortios.py).
Same as https://github.com/ansible/ansible/pull/21849#discussion_r103172035, `timeout` isn't `username`.
use the `missing_required_lib` function from `ansible.module_utils.basic`
Use `warnings.simplefilter('once')` in this case. There has been a lot of stuff moving around lately in the `Field` and `_meta` API andI just want to make sure the backward compatibility shim you added doesn't use deprecated stuff itself.
Should have a trailing dot. (Only the short_description must not have one)
I'd go for `try/except IOError` in order to make a point that variable initialized above is in fact a fallback.
There's a few things that I'd change about this function. But I think the toplevel concern is that it's doing too much. It doesn't need to take req. It should just decide whether we're using the pycrypto or cryptography backend, format and return that one dependency. The calling code can then substitute the value.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Ah, i'd expect a bool to always be True/False, if that is not the case (no time to test really right now :)), then indeed, this needs to stay (and this goes for my other bool remarks)
We should avoid lambdas and filters most of the time. Use list comprehensions and generator expressions instead. It's both faster and more pythonic to use those idioms.
Nitpick, you can use a list comprehension here instead of calling list on a generator expression.
Maybe rename this to substitute_crypto_req since it's replacing the one from requirements.txt with the one we autodetect rather than simply appending.
`enable_3D` is a bool, so it will always be set, i think this can be a little simplified as well ```suggestion video_spec.device.enable3DSupport = self.params['enabled_3D'] enabled_3d = self.params['enabled_3D'] if self.params['enable_3D'] != video_card_facts['enable_3D_support']: self.change_detected = True ```
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
```suggestion msg = ... with self.assertRaisesMessage(ValueError, msg): ```
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
I would use `choices` in `labels` and `values` to simplify implementation, e.g. ```python @property def labels(cls): return [label for _, label in cls.choices] @property def values(cls): return [value for value, _ in cls.choices] ```
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
I think we can use `in cls` instead of `in cls.__members__.values()` (here and in `names`).
These parens aren't necessary for unpacking the return values.
Upon further reflection, I think these three lines along with `check_mutually_exclusive` are erroneous for subspecs. We don't currently have a way to specify `required_if/one_of/together` or `mutually_exclusive` for subspecs, so in every case *but* the networking "aggregate" thing, this is severely broken (as you'd be making nonsensical checks in most cases)
I think these temporary variables and blank lines are unnecessary, maybe sth like that: ```python return self.window_frame_start(start), self.window_frame_end(end) ```
They are helpful when using (i)pdb.
Maybe it will be better to raise `NotSupportedError` instead of `NotImplementedError`. I would say `Window expressions are not supported on this database backend`.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
Try except blocks should be as small as possible. In this case, if is only needed around the calls to modify and delete. Perhaps put the try except around the if state conditional to limit the statements that it surrounds
Right, I think we could use model fields as a parallel -- we don't include all options a field is initialized with in the `__repr__()`. Here's the implementation: ``` def __repr__(self): """Display the module, class, and name of the field.""" path = '%s.%s' % (self.__class__.__module__, self.__class__.__name__) name = getattr(self, 'name', None) if name is not None: return '<%s: %s>' % (path, name) return '<%s>' % path ``` So I'm +1 to not implementing a custom `__repr__()` for each subclass unless someone can present a counter argument. \cc @akki I created https://github.com/django/django/pull/8643 to remove `BrinIndex.__repr__()`.
Do you think we're adding value with such complicated `__repr__` methods? I've thought maybe we should ditch worrying about including of Index's params in them and just let `Index.__repr__` show the basics. Does the repr should up anywhere important? I can't think of anywhere.
The only place I can vaguely remember `repr` being used is during the migrations. If you have the `AddIndex/RemoveIndex` operation in your migrations file, it shows this representation when the migrations are run. Since it is very common that a dev might want to create multiple gin indexes in the same table, it is necessary to have the `fields` of the index as well to distinguish the representation of these indexes. So, my decision would be based on how commonly devs have two gin indexes in the same model with the same fields but with different values of `fastupdate` or `gin_pending_list_limit`. If it is a very common case we might want to keep them in `repr`.
What do you think about ```python def __repr__(self): template = "<%(name)s: fields='%(fields)s'%(fastupdate)s%(gin_pending_list_limit)s>" template_values = { 'name': self.__class__.__name__, 'fields': ', '.join(self.fields), 'fastupdate': '', 'gin_pending_list_limit': '', } if self.fastupdate is not None: template_values['fastupdate'] = ', fastupdate=%s' % ('True' if self.fastupdate else 'False') if self.gin_pending_list_limit is not None: template_values['gin_pending_list_limit'] = ', gin_pending_list_limit=%d' % self.gin_pending_list_limit return template % template_values```
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
Why not also do this when `module.check_mode` is `True`? Then the check mode behavior is closer to the real thing.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Unfortunately annotation names can contain LOOKUP_SEP - that is the reason why the ugly refs_aggregate method was added. For example qs.annotate(Max('id')) will create an annotation named max__id, and that can't be referred if the code checks for len(field_list) == 1.
rather than custom caching with a dict, this might be clearer with a module-level function using `@lru_cache(maxsize=2)`, with the current value of `USE_TZ` as the only argument. It would save some lines and clarify it's a cache.
Is there any reason why this method accepts `level` and `md_device` as argument? IMHO, it would be natural to use `self.level` and `self.md_device` instead.
I would not as .format breaks in older versions and we are trying to still keep this kind of module working on older machines
I know this is a small one, but I can imagine % formatting will be decommissioned at some point, I would change this to .format() method if it's not too much of a hassle. To back-up this up, ansible is undergoing an effort to be Python3 compatible, as per Python docs the recommendation is to prefer string.format() method against string % formatting operator. [1][2] Also keep in mind that % might have undesired effects [3] [1] https://docs.python.org/2/library/stdtypes.html#str.format [2] https://www.python.org/dev/peps/pep-3101/ [3] http://stackoverflow.com/questions/5082452/python-string-formatting-vs-format
oh I see, it makes sense then.
`User has been created`
`User has been deleted`
you should set the default state to "list" at argument_spec, not here.
Please add check-mode support (and if possible also diff support).
I moved this check to the `DurationExpression`.
This is not covered by tests, also raising an exceptions in user-defined functions is not really helpful for users: ```python django.db.utils.OperationalError: user-defined function raised exception ``` I think we should return `None` instead.
True, sorry for an undoable request. Folks can always use `sqlite3.enable_callback_tracebacks(True)` to see this message :shrug: .
`else` can be removed, i.e.: ```python ... elif end is None: return self.UNBOUNDED_FOLLOWING raise ValueError('Illegal argument for end, must be either a positive integer, zero or None') ```
They are helpful when using (i)pdb.
I think these temporary variables and blank lines are unnecessary, maybe sth like that: ```python return self.window_frame_start(start), self.window_frame_end(end) ```
Maybe it will be better to raise `NotSupportedError` instead of `NotImplementedError`. I would say `Window expressions are not supported on this database backend`.
As long as they both equals it shouldn't make a difference. By that I mean you should have a test asserts `Index('foo') == Index(F('foo'))` An alternative could be to defer that to `create_sql`.
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
They are helpful when using (i)pdb.
Maybe it will be better to raise `NotSupportedError` instead of `NotImplementedError`. I would say `Window expressions are not supported on this database backend`.
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
... also we cannot use `User` in the `BaseBackend` so it will be hard to return something consistent.
I don't that we should do this. `BaseBackend` contains only basic methods.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
These parens aren't necessary for unpacking the return values.
This is not strictly related with functional indexes, I'm going to move it to a separate PR.
Could be nice to fix and test this typo separately.
I was expecting to raise a ValueError if these conditions aren't met, similar to what we do with "Index names cannot be longer". I don't think it's a good idea to modify the user provided value as it seems like that would only cause confusion.
I think it's fine.
Use single quotes
point -> points
I think this could be `@cached_property` so it doesn't have to be calculated on every access.
As long as they both equals it shouldn't make a difference. By that I mean you should have a test asserts `Index('foo') == Index(F('foo'))` An alternative could be to defer that to `create_sql`.
Ah! Of course, sorry I missed that.
Won't this result in a confusing SQL-level error if you pass in `None` for `expression` or `pos` by accident? I'm assuming `length` is the only one we actually expect to possibly be `None`. If that's true, I think it would be better to do something like: ``` expressions = [expression, pos] if length is not None: expressions.append(length) ``` Or, if you prefer: `expressions = (expression, pos, length) if length is not None else (expression, pos)`
`Backend` supports negative precision, `SQLite` does not: ```suggestion raise ValueError('SQLite does not support negative precision.') ```
This line doesn't need to change. Passing `function='Max'` as a kwarg will do the right thing, since the super implementation will do the mixing of `self.extra` and `**kwargs` for you.
put closing parenthesis on the next line
The problem here is that you can't just use `Value('')` for the default. If you're doing `GREATEST(date_field, other_date_field)` then coalescing a date type with a char type is going to produce an error. The type itself will probably have to accept a default. ``` sentinel = object() def __init__(self, *expressions, **kwargs): ifnull = kwargs.pop('ifnull', sentinel) if ifnull == sentinel: raise ValueError('ifnull is required') if ifnull is None: # user has asked NOT to use coalesce else: self.ifnull = self._parse_expression(ifnull) ``` And then you would use `Coalesce(expression, self.ifnull)` in the coalesce method, or completely skip calling the coalesce method if `ifnull is None`. This is just one idea, but probably the best one I have right now. I don't really like forcing a user to provide an `ifnull` though, because it feels like we're disadvantaging the user. Another idea would be to use a backend feature. Something like `greatest_least_uses_nulls`, and then the tests could switch on that feature flag to provide different test results. I'd probably like to get a rough consensus on which way to go here.
Could be nice to fix and test this typo separately.
Nothing, it is just my personal preferences and it was just a suggestion.
I think we can move this under `try ... except`, e.g. ```python try: token, rest = parser.get_mailbox(addr) if rest: # The entire address must be parsed. raise ValueError nm = token.display_name or '' localpart = token.local_part domain = token.domain or '' except (HeaderParseError, ValueError, IndexError): raise ValueError("Invalid address '{}'".format(addr)) ```
I'd omit a blank line here.
As long as they both equals it shouldn't make a difference. By that I mean you should have a test asserts `Index('foo') == Index(F('foo'))` An alternative could be to defer that to `create_sql`.
Won't this result in a confusing SQL-level error if you pass in `None` for `expression` or `pos` by accident? I'm assuming `length` is the only one we actually expect to possibly be `None`. If that's true, I think it would be better to do something like: ``` expressions = [expression, pos] if length is not None: expressions.append(length) ``` Or, if you prefer: `expressions = (expression, pos, length) if length is not None else (expression, pos)`
I think these temporary variables and blank lines are unnecessary, maybe sth like that: ```python return self.window_frame_start(start), self.window_frame_end(end) ```
They are helpful when using (i)pdb.
Maybe it will be better to raise `NotSupportedError` instead of `NotImplementedError`. I would say `Window expressions are not supported on this database backend`.
Style note, I finally convinced people a year or so ago that ```{}``` is superior to ```dict()```. So new code should use the literal syntax.
I think we should just let this exception propagate. We aren't adding any value by catching it here (and might be removing value if the traceback were useful). This isn't code in ansible itself so we don't have to create a nicer error message either.
Kind of a nitpick since this is setup.py... but API-wise I prefer returning a small dict with the dynamic values and letting the caller merge it/copy the static dict if necessary.
A message string would good to say that image is not preset or something similar.
If you use a catchall exception handler like this then you should also provide the traceback like this: ``` python import traceback from ansible.module_utils._text import to_native [...] module.fail_json(msg='Unanticipated error running atomic: %s' % to_native(e), exception=traceback.format_exc()) ```
Can you make this into a couple statements for easier readability? Something like: ``` invalid_entries = [x for x in statement['Principal']['AWS'] if not x.startswith('arn:aws:iam::)] if clean_invalid_entries and len(invalid_entries): for entry in invalid_entries: statement['Principal']['AWS'].remove(entry) ``` Or there's always: ``` original_size = len(statement['Principal']['AWS']) statement['Principal']['AWS'] = [x for x in statement['Principal']['AWS'] if x.startswith('arn:aws:iam::)] have_invalid_entries = (original_size == len(statement['Principal']['AWS'])) ```
Add here that the `key_alias` or `key_arn` are both ways to provide it.
Ah. lambda is prettier though :). maybe we can run pep8 with ignore lambda error.
I don't think boto_params will include access_key or secret_key if a profile is being used (which might be through passing `profile` parameter or with `AWS_PROFILE` set) Looking at https://github.com/jmenga/requests-aws-sign the way to do it is to use ``` session = session.Session() credentials = session.get_credentials() ``` but even there, we'd need to cope with the `profile` parameter.
I wouldn't exactly call a dictionary `list`.
Here, `self.count_upgrade` is an int, and `outdated` (as above) a `dict` resp. `list`.
You are right! Fixed it in https://github.com/mheap/ansible/pull/3.
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
Here, `outdated` is a `dict` (or `list` in mheap/ansible#3).
Can you name this a little more verbosely? I can't unsee "get best"
These parens aren't necessary for unpacking the return values.
Need a colon at the end here
```suggestion 'conflicts with specifying unique fields that can ' ```
```suggestion raise NotSupportedError( 'This database backend does not support updating ' 'conflicts with specifying unique fields that will ' 'trigger the upsert.' ) ```
```suggestion raise NotSupportedError( 'This database backend does not support ignoring conflicts.' ) ```
Spurious `else:` Breaking the tests.
As above, wrap the format in quotes. "Your URL pattern '{}' has .."
i think that `flv_data` should be used with `data` param and `Content-Type` dict used with `headers` param of `_download_webpage`.
and the function that normally used to encode postdata is `urlencode_postdata`.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
Please add a trailing comma: ```suggestion on_conflict=None, update_fields=None, unique_fields=None, ```
```suggestion performed an initial sign in (meaning C(~/.op/config) exists), then only the C(master_password) is required. ```
```suggestion 'conflicts with specifying unique fields that can ' ```
```suggestion raise NotSupportedError( 'This database backend does not support updating ' 'conflicts with specifying unique fields that will ' 'trigger the upsert.' ) ```
```suggestion raise NotSupportedError( 'This database backend does not support ignoring conflicts.' ) ```
Spurious `else:` Breaking the tests.
As above, wrap the format in quotes. "Your URL pattern '{}' has .."
i think that `flv_data` should be used with `data` param and `Content-Type` dict used with `headers` param of `_download_webpage`.
and the function that normally used to encode postdata is `urlencode_postdata`.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
Please add a trailing comma: ```suggestion on_conflict=None, update_fields=None, unique_fields=None, ```
```suggestion performed an initial sign in (meaning C(~/.op/config) exists), then only the C(master_password) is required. ```
As long as they both equals it shouldn't make a difference. By that I mean you should have a test asserts `Index('foo') == Index(F('foo'))` An alternative could be to defer that to `create_sql`.
I'd omit a blank line here.
put closing parenthesis on the next line
`del` is a builtin, not a function. These parens don't have to be here
the `f.close()` is implied as part of the with block. Also, I don't think this should be in the module since it looks like a debug statement.
This should really be: ``` - Set custom DNS search domains. (Use I(dns_search) with C('') if you don't wish to set the search domain.) ```
Read operation: ditto
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
These parens aren't necessary for unpacking the return values.
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
Maybe add an empty line above this one and give it a name, it seems like the `assert` task is just a parameter of the `openssl_certificate_info` one.
single line docstring is okay if it fits
I think this is a typo - `changed` should be `changes`
no, this refers to the standard `changed=True` / `changed=False` and `msg="..."` results returned by ansible modules.
may be nice to support description in addition to name.
Perhaps `required_if` can help.
OK, I see what you're doing here :+1:, I think there is slightly better way. Just suggestions, your call. Just removing these is not perfect. Ideally, omitting say `associations` in module args would behave same as writing explicit `associations: {}`. But currently explicit `associations: {}` won't match what's left after you remove empty values from API, causing unnecessary request and changed=True. - Args side can be normalized for you if you make attributes & associations first-class options, or suboptions, defaulting to `{}`. - Note that only recently manageiq master started sending empty values; nulls were omitted before. Not sure that's relevant Ã¢Â€Â” do you get Nones or empty hashes here? Anyway :+1: to normalizing API results in one direction or another Ã¢Â€Â” don't *rely* on getting empty values. - A pattern that works & reads nicely is defining a normalizing function and applying it on both sides of comparison. `if normalize(current) == normalized(desired)`. - IMHO, splitting logic between `has_field` and here is harder to follow, and it could be simple enough to inline, perhaps something like this: ``` current_properties = dynamic_resource_definition.get('properties', {}) current_name = dynamic_resource_definition.get('name') if (current_name == name and current_properties.get('attributes', {}) == properties.get('attributes', {}) and current_properties.get('associations', {}) == properties.get('associations', {}): ```
I think this validation can be dropped too. it's only `name` which is required=True. (plus we wouldn't even reach this function without finding a definition by name)
It is also OK as-is if you like it; I just wanted to make it clear you don't *have* to obey the consistency if you see a better way. [I think I was in a bike-shedding mood when I did this review... Feel free to push back :-)]
Upon further reflection, I think these three lines along with `check_mutually_exclusive` are erroneous for subspecs. We don't currently have a way to specify `required_if/one_of/together` or `mutually_exclusive` for subspecs, so in every case *but* the networking "aggregate" thing, this is severely broken (as you'd be making nonsensical checks in most cases)
They are helpful when using (i)pdb.
I think these temporary variables and blank lines are unnecessary, maybe sth like that: ```python return self.window_frame_start(start), self.window_frame_end(end) ```
Maybe it will be better to raise `NotSupportedError` instead of `NotImplementedError`. I would say `Window expressions are not supported on this database backend`.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
There is some code duplicated between this pull-request and other FortisOS pull-requests ([`fortios_ipv4_policy`](https://github.com/ansible/ansible/pull/21849/files/d49860f735c162acda87f5232f1de0e148453203#r103181410) and [`fortios_address`](#21542)): - block calling `connect` method - block calling `load_config` method These blocks must be moved together in [`module_utils/fortios.py`](https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/fortios.py).
Same as https://github.com/ansible/ansible/pull/21849#discussion_r103172035, `timeout` isn't `username`.
Try except blocks should be as small as possible. In this case, if is only needed around the calls to modify and delete. Perhaps put the try except around the if state conditional to limit the statements that it surrounds
`else` can be removed, i.e.: ```python ... elif end is None: return self.UNBOUNDED_FOLLOWING raise ValueError('Illegal argument for end, must be either a positive integer, zero or None') ```
Upon further reflection, I think these three lines along with `check_mutually_exclusive` are erroneous for subspecs. We don't currently have a way to specify `required_if/one_of/together` or `mutually_exclusive` for subspecs, so in every case *but* the networking "aggregate" thing, this is severely broken (as you'd be making nonsensical checks in most cases)
I think these temporary variables and blank lines are unnecessary, maybe sth like that: ```python return self.window_frame_start(start), self.window_frame_end(end) ```
They are helpful when using (i)pdb.
Maybe it will be better to raise `NotSupportedError` instead of `NotImplementedError`. I would say `Window expressions are not supported on this database backend`.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
I'd omit a blank line here.
There is some code duplicated between this pull-request and other FortisOS pull-requests ([`fortios_ipv4_policy`](https://github.com/ansible/ansible/pull/21849/files/d49860f735c162acda87f5232f1de0e148453203#r103181410) and [`fortios_address`](#21542)): - block calling `connect` method - block calling `load_config` method These blocks must be moved together in [`module_utils/fortios.py`](https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/fortios.py).
Same as https://github.com/ansible/ansible/pull/21849#discussion_r103172035, `timeout` isn't `username`.
use the `missing_required_lib` function from `ansible.module_utils.basic`
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Missing `=dict` on this and the next few lines
`del` is a builtin, not a function. These parens don't have to be here
This can instead be `continue` and let the `else` unnest.
If `self._module.params['name'] is None`, this will never match and the module fails.
Can you make this into a couple statements for easier readability? Something like: ``` invalid_entries = [x for x in statement['Principal']['AWS'] if not x.startswith('arn:aws:iam::)] if clean_invalid_entries and len(invalid_entries): for entry in invalid_entries: statement['Principal']['AWS'].remove(entry) ``` Or there's always: ``` original_size = len(statement['Principal']['AWS']) statement['Principal']['AWS'] = [x for x in statement['Principal']['AWS'] if x.startswith('arn:aws:iam::)] have_invalid_entries = (original_size == len(statement['Principal']['AWS'])) ```
put closing parenthesis on the next line
I think we have a problem here. The version that is supposed to be supplied as a parameter should only consist of major.minor version like `2.0` (so that all 2.x version go into the same repository) - this one is `2.0.0-beta1` though.
Yes, that's a better solution. BTW, why not put all functionality calling `cryptsetup` into `CryptHandler`, and giving `ConditionsHandler` a reference to `CryptHandler` for the calls it needs to do? Then there's a more clear separation into `CryptHandler`, which does all the `cryptsetup` calls, and `ConditionsHandler`, which contains most of the decision logic.
Instead of deriving from `CryptHandler`, why not pass `CryptHandler` as a constructor argument? Otherwise, I don't see why you need both a `CryptHandler` and a `Conditions` instance in the main module code (since you can use the `Conditions` instance for both).
I think these temporary variables and blank lines are unnecessary, maybe sth like that: ```python return self.window_frame_start(start), self.window_frame_end(end) ```
They are helpful when using (i)pdb.
Upon further reflection, I think these three lines along with `check_mutually_exclusive` are erroneous for subspecs. We don't currently have a way to specify `required_if/one_of/together` or `mutually_exclusive` for subspecs, so in every case *but* the networking "aggregate" thing, this is severely broken (as you'd be making nonsensical checks in most cases)
Maybe it will be better to raise `NotSupportedError` instead of `NotImplementedError`. I would say `Window expressions are not supported on this database backend`.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
There is some code duplicated between this pull-request and other FortisOS pull-requests ([`fortios_ipv4_policy`](https://github.com/ansible/ansible/pull/21849/files/d49860f735c162acda87f5232f1de0e148453203#r103181410) and [`fortios_address`](#21542)): - block calling `connect` method - block calling `load_config` method These blocks must be moved together in [`module_utils/fortios.py`](https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/fortios.py).
Same as https://github.com/ansible/ansible/pull/21849#discussion_r103172035, `timeout` isn't `username`.
Try except blocks should be as small as possible. In this case, if is only needed around the calls to modify and delete. Perhaps put the try except around the if state conditional to limit the statements that it surrounds
Upon further reflection, I think these three lines along with `check_mutually_exclusive` are erroneous for subspecs. We don't currently have a way to specify `required_if/one_of/together` or `mutually_exclusive` for subspecs, so in every case *but* the networking "aggregate" thing, this is severely broken (as you'd be making nonsensical checks in most cases)
I think these temporary variables and blank lines are unnecessary, maybe sth like that: ```python return self.window_frame_start(start), self.window_frame_end(end) ```
They are helpful when using (i)pdb.
Maybe it will be better to raise `NotSupportedError` instead of `NotImplementedError`. I would say `Window expressions are not supported on this database backend`.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
I'd omit a blank line here.
Same as https://github.com/ansible/ansible/pull/21849#discussion_r103172035, `timeout` isn't `username`.
Try except blocks should be as small as possible. In this case, if is only needed around the calls to modify and delete. Perhaps put the try except around the if state conditional to limit the statements that it surrounds
There is some code duplicated between this pull-request and other FortisOS pull-requests ([`fortios_ipv4_policy`](https://github.com/ansible/ansible/pull/21849/files/d49860f735c162acda87f5232f1de0e148453203#r103181410) and [`fortios_address`](#21542)): - block calling `connect` method - block calling `load_config` method These blocks must be moved together in [`module_utils/fortios.py`](https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/fortios.py).
@wrouesnel CI failure due to PEP 8 issue: ``` 2017-01-28 07:26:38 ERROR: PEP 8: lib/ansible/modules/cloud/amazon/ec2.py:930:1: W293 blank line contains whitespace (legacy) ```
if min_count <=
```suggestion module.fail_json(msg="The following packages are absent in the currently booted rpm-ostree commit: %s" % ' '.join(pkgs)) ```
Same here (and further below).
`Current host '%s' can not be %s...`
A message string would good to say that image is not preset or something similar.
If you use a catchall exception handler like this then you should also provide the traceback like this: ``` python import traceback from ansible.module_utils._text import to_native [...] module.fail_json(msg='Unanticipated error running atomic: %s' % to_native(e), exception=traceback.format_exc()) ```
`Current host '%s' can not be shut down...`
`Current host '%s' can not be rebooted...`
Above, you wrote that `ov_eligible` is returned when `success and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING).`. But here, you only return it if it has value `True`.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
What is this supposed to achieve? Modules are cached on import, they'll be picked up from that cache regardless of the pointers being listed in `sys.modules`.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
`del` is a builtin, not a function. These parens don't have to be here
Can you make this into a couple statements for easier readability? Something like: ``` invalid_entries = [x for x in statement['Principal']['AWS'] if not x.startswith('arn:aws:iam::)] if clean_invalid_entries and len(invalid_entries): for entry in invalid_entries: statement['Principal']['AWS'].remove(entry) ``` Or there's always: ``` original_size = len(statement['Principal']['AWS']) statement['Principal']['AWS'] = [x for x in statement['Principal']['AWS'] if x.startswith('arn:aws:iam::)] have_invalid_entries = (original_size == len(statement['Principal']['AWS'])) ```
Add here that the `key_alias` or `key_arn` are both ways to provide it.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
colors should all be configurable
You don't need this conditional, since Ansible enforces that these are the only choices.
Upon further reflection, I think these three lines along with `check_mutually_exclusive` are erroneous for subspecs. We don't currently have a way to specify `required_if/one_of/together` or `mutually_exclusive` for subspecs, so in every case *but* the networking "aggregate" thing, this is severely broken (as you'd be making nonsensical checks in most cases)
I think these temporary variables and blank lines are unnecessary, maybe sth like that: ```python return self.window_frame_start(start), self.window_frame_end(end) ```
They are helpful when using (i)pdb.
Maybe it will be better to raise `NotSupportedError` instead of `NotImplementedError`. I would say `Window expressions are not supported on this database backend`.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
Try except blocks should be as small as possible. In this case, if is only needed around the calls to modify and delete. Perhaps put the try except around the if state conditional to limit the statements that it surrounds
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
What is this supposed to achieve? Modules are cached on import, they'll be picked up from that cache regardless of the pointers being listed in `sys.modules`.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
`del` is a builtin, not a function. These parens don't have to be here
Can you make this into a couple statements for easier readability? Something like: ``` invalid_entries = [x for x in statement['Principal']['AWS'] if not x.startswith('arn:aws:iam::)] if clean_invalid_entries and len(invalid_entries): for entry in invalid_entries: statement['Principal']['AWS'].remove(entry) ``` Or there's always: ``` original_size = len(statement['Principal']['AWS']) statement['Principal']['AWS'] = [x for x in statement['Principal']['AWS'] if x.startswith('arn:aws:iam::)] have_invalid_entries = (original_size == len(statement['Principal']['AWS'])) ```
Add here that the `key_alias` or `key_arn` are both ways to provide it.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
colors should all be configurable
You don't need this conditional, since Ansible enforces that these are the only choices.
The current idiom might be required because some backends (perhaps third-party ones) choke on empty `params`. I'd keep it.
What do you think about ```python def __repr__(self): template = "<%(name)s: fields='%(fields)s'%(fastupdate)s%(gin_pending_list_limit)s>" template_values = { 'name': self.__class__.__name__, 'fields': ', '.join(self.fields), 'fastupdate': '', 'gin_pending_list_limit': '', } if self.fastupdate is not None: template_values['fastupdate'] = ', fastupdate=%s' % ('True' if self.fastupdate else 'False') if self.gin_pending_list_limit is not None: template_values['gin_pending_list_limit'] = ', gin_pending_list_limit=%d' % self.gin_pending_list_limit return template % template_values```
The only place I can vaguely remember `repr` being used is during the migrations. If you have the `AddIndex/RemoveIndex` operation in your migrations file, it shows this representation when the migrations are run. Since it is very common that a dev might want to create multiple gin indexes in the same table, it is necessary to have the `fields` of the index as well to distinguish the representation of these indexes. So, my decision would be based on how commonly devs have two gin indexes in the same model with the same fields but with different values of `fastupdate` or `gin_pending_list_limit`. If it is a very common case we might want to keep them in `repr`.
Right, I think we could use model fields as a parallel -- we don't include all options a field is initialized with in the `__repr__()`. Here's the implementation: ``` def __repr__(self): """Display the module, class, and name of the field.""" path = '%s.%s' % (self.__class__.__module__, self.__class__.__name__) name = getattr(self, 'name', None) if name is not None: return '<%s: %s>' % (path, name) return '<%s>' % path ``` So I'm +1 to not implementing a custom `__repr__()` for each subclass unless someone can present a counter argument. \cc @akki I created https://github.com/django/django/pull/8643 to remove `BrinIndex.__repr__()`.
Do you think we're adding value with such complicated `__repr__` methods? I've thought maybe we should ditch worrying about including of Index's params in them and just let `Index.__repr__` show the basics. Does the repr should up anywhere important? I can't think of anywhere.
True, sorry for an undoable request. Folks can always use `sqlite3.enable_callback_tracebacks(True)` to see this message :shrug: .
I moved this check to the `DurationExpression`.
This is not covered by tests, also raising an exceptions in user-defined functions is not really helpful for users: ```python django.db.utils.OperationalError: user-defined function raised exception ``` I think we should return `None` instead.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
```suggestion raise ValueError('Error while adding new LUKS keyslot to %s: %s' ``` While you're at it ;)
```suggestion url_json = self._parse_json(self._html_search_regex(r'''<div\b[^>]+\bdata-item\s*=\s*(["'])(?P<videourls>\{.*})\1''', webpage, 'videourls', group='videourls', default='{}'), video_id, fatal=False) or {} ```
Right, I think we could use model fields as a parallel -- we don't include all options a field is initialized with in the `__repr__()`. Here's the implementation: ``` def __repr__(self): """Display the module, class, and name of the field.""" path = '%s.%s' % (self.__class__.__module__, self.__class__.__name__) name = getattr(self, 'name', None) if name is not None: return '<%s: %s>' % (path, name) return '<%s>' % path ``` So I'm +1 to not implementing a custom `__repr__()` for each subclass unless someone can present a counter argument. \cc @akki I created https://github.com/django/django/pull/8643 to remove `BrinIndex.__repr__()`.
What do you think about ```python def __repr__(self): template = "<%(name)s: fields='%(fields)s'%(fastupdate)s%(gin_pending_list_limit)s>" template_values = { 'name': self.__class__.__name__, 'fields': ', '.join(self.fields), 'fastupdate': '', 'gin_pending_list_limit': '', } if self.fastupdate is not None: template_values['fastupdate'] = ', fastupdate=%s' % ('True' if self.fastupdate else 'False') if self.gin_pending_list_limit is not None: template_values['gin_pending_list_limit'] = ', gin_pending_list_limit=%d' % self.gin_pending_list_limit return template % template_values```
Do you think we're adding value with such complicated `__repr__` methods? I've thought maybe we should ditch worrying about including of Index's params in them and just let `Index.__repr__` show the basics. Does the repr should up anywhere important? I can't think of anywhere.
The only place I can vaguely remember `repr` being used is during the migrations. If you have the `AddIndex/RemoveIndex` operation in your migrations file, it shows this representation when the migrations are run. Since it is very common that a dev might want to create multiple gin indexes in the same table, it is necessary to have the `fields` of the index as well to distinguish the representation of these indexes. So, my decision would be based on how commonly devs have two gin indexes in the same model with the same fields but with different values of `fastupdate` or `gin_pending_list_limit`. If it is a very common case we might want to keep them in `repr`.
It seems like returning this string is the expected path through the function. So this should probably just be a return instead of raising an exception. Then the code that's calling it can decide to print it and reraise the exception that got us here if it chooses.
Okay... what about using `sys.stderr.write('%s\n' % ('\n'.join(errors)))` at the end of this function and then the calling function does sys.exit(1) afterwards? Almost the same result but it doesn't abuse the exception mechanism to make it happen :-) Probably should do something similar with the exception raised by the new connect method... maybe make the exception thrown there specific and then in this toplevel exception handler, print the exception message to stderr and sys.exit(1). The general rule is exceptions should be thrown and unhandled if there are programming errors. Error messages and sys.exit should be used when there are errors the user can correct without diving into the ansible code.
We should avoid lambdas and filters most of the time. Use list comprehensions and generator expressions instead. It's both faster and more pythonic to use those idioms.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Tests for this method seem missing. It seems like we need a better way to build these reprs that's not so complicated and repetitive for each index.
What do you think about ```python def __repr__(self): template = "<%(name)s: fields='%(fields)s'%(fastupdate)s%(gin_pending_list_limit)s>" template_values = { 'name': self.__class__.__name__, 'fields': ', '.join(self.fields), 'fastupdate': '', 'gin_pending_list_limit': '', } if self.fastupdate is not None: template_values['fastupdate'] = ', fastupdate=%s' % ('True' if self.fastupdate else 'False') if self.gin_pending_list_limit is not None: template_values['gin_pending_list_limit'] = ', gin_pending_list_limit=%d' % self.gin_pending_list_limit return template % template_values```
Do you think we're adding value with such complicated `__repr__` methods? I've thought maybe we should ditch worrying about including of Index's params in them and just let `Index.__repr__` show the basics. Does the repr should up anywhere important? I can't think of anywhere.
Right, I think we could use model fields as a parallel -- we don't include all options a field is initialized with in the `__repr__()`. Here's the implementation: ``` def __repr__(self): """Display the module, class, and name of the field.""" path = '%s.%s' % (self.__class__.__module__, self.__class__.__name__) name = getattr(self, 'name', None) if name is not None: return '<%s: %s>' % (path, name) return '<%s>' % path ``` So I'm +1 to not implementing a custom `__repr__()` for each subclass unless someone can present a counter argument. \cc @akki I created https://github.com/django/django/pull/8643 to remove `BrinIndex.__repr__()`.
The only place I can vaguely remember `repr` being used is during the migrations. If you have the `AddIndex/RemoveIndex` operation in your migrations file, it shows this representation when the migrations are run. Since it is very common that a dev might want to create multiple gin indexes in the same table, it is necessary to have the `fields` of the index as well to distinguish the representation of these indexes. So, my decision would be based on how commonly devs have two gin indexes in the same model with the same fields but with different values of `fastupdate` or `gin_pending_list_limit`. If it is a very common case we might want to keep them in `repr`.
It seems like returning this string is the expected path through the function. So this should probably just be a return instead of raising an exception. Then the code that's calling it can decide to print it and reraise the exception that got us here if it chooses.
Okay... what about using `sys.stderr.write('%s\n' % ('\n'.join(errors)))` at the end of this function and then the calling function does sys.exit(1) afterwards? Almost the same result but it doesn't abuse the exception mechanism to make it happen :-) Probably should do something similar with the exception raised by the new connect method... maybe make the exception thrown there specific and then in this toplevel exception handler, print the exception message to stderr and sys.exit(1). The general rule is exceptions should be thrown and unhandled if there are programming errors. Error messages and sys.exit should be used when there are errors the user can correct without diving into the ansible code.
We should avoid lambdas and filters most of the time. Use list comprehensions and generator expressions instead. It's both faster and more pythonic to use those idioms.
Style note, I finally convinced people a year or so ago that ```{}``` is superior to ```dict()```. So new code should use the literal syntax.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
`return '%s-%s-%s' % (y or 0, m or 0, d or 0)` can be moved here.
"is_required=False and an initial value that's a file, renders..."
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
Please ignore, my suggestion is invalid syntax.
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
This sentence is too long, maybe: ```python choice = self._choice_input( f"It is impossible to add the field '{field_name}' with " f"'auto_now_add=True' to {model_name} without providing a " f"default. This is because the database needs something to " f"populate existing rows.\n", [ ... ```
No need to parametrize with just one case.
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
Yes, please rebase the branch and remove the try/fail pattern as done in 6729b96d8a15048b2295c916c5b881a59d9417a0. If you're unfamiliar with the process you might find https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/working-with-git/#rebasing-branches helpful.
no blank line needed
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
Yes, please rebase the branch and remove the try/fail pattern as done in 6729b96d8a15048b2295c916c5b881a59d9417a0. If you're unfamiliar with the process you might find https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/working-with-git/#rebasing-branches helpful.
Same here, not following order `(value, expected)`
(And round-tripping of the messages is already tested in other tests)
Guess it's better to use `self.assertGreater(len(para), 0)` instead
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Docstring shouldn't have a trailing space. Docstring sentences should end with a period.
Same here, not following order `(value, expected)`
This one is a bit newer to CliBase, but also implemented verbatim in superclass
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
disconnect() is likewise handled in CliBase, and should be entirely unnecessary here.
self._connected is set by CliBase.connect(), shouldn't need to specify it here
like diff = load_config(self._module, config_xml, [])
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
You probably should just exit here with `changed=False`
All these methods can be clubbed into a single method that takes data and pattern string as arguments and returns the match else None
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
This seems random and can lead to flaky tests. So there is no way to measure latency while Trogdor task is running? For the rate limit test below we do wait for task to run.
"for working with retry limiting"
nit: doc how the output of `iperf` looks like. helps with understanding this code and regex.
nit: add a newline here too.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
For integers please: use `type='int'` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int'` remove the `isdigit` check from `check_params`
I would use a different var name, `file` is a builtin within Python. I also don't know anything about Kubernetes but should this path be hard coded.
```suggestion with open('/var/run/secrets/kubernetes.io/serviceaccount/token') as file: ```
Use `with open(` instead of `open(`
Use [`fetch_url`](https://github.com/ansible/ansible/blob/240d1a6afb43982f16acebef16778d17aab58160/lib/ansible/module_utils/urls.py#L1197) instead of `requests.get`
You have some unmerged lines here
This method isn't necessary.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
It would be awesome if buildah supported copying from a container.
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
Please chop blank line.
I think using a semantic name would help here, e.g. `lookup_kwarg_null`
should be `extra_context or {},` (include a trailing comma)
I'm not convinced about this change is a bit backward incompatible, e.g. ```python >>> from django.views.debug import ExceptionReporter >>> ExceptionReporter.html_template_path Traceback (most recent call last): File "<stdin>", line 1, in <module> AttributeError: type object 'ExceptionReporter' has no attribute 'html_template_path' ``` Can we use a `@classproperty` (with `@lru_cache(maxsize=1)`) to keep the current behavior? :thinking:
assertRaisesRegexp is deprecated on Python 3, you use `self.assertRaisesMessage` I think
I guess we could eliminate the `has_none` variable and simply check `if none_title` -- I don't think a user would define an expected title of an empty string.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Features deprecated in Django 4.2 will be removed in Django 5.1 ```suggestion warnings.warn(DEFAULT_FILE_STORAGE_DEPRECATED_MSG, RemovedInDjango51Warning) ```
Shouldn't this be `find_cluster_by_name(self.content, self.cluster, dc_obj)`? As you have the DC, it will speed up the search
Only for consistency. Not sure it's a good enough reason.
@charettes Ideas welcome. Can you take a look? I really don't like the idea of using `psycopg2` in expressions.
I would prefer to wrap value with `Value()` and compile `options` separately.
like diff = load_config(self._module, config_xml, [])
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
Yeah, we still can't use dict comprehensions until 2.6 is formally dropped, sorry.
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
You probably should just exit here with `changed=False`
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
This seems random and can lead to flaky tests. So there is no way to measure latency while Trogdor task is running? For the rate limit test below we do wait for task to run.
"for working with retry limiting"
nit: doc how the output of `iperf` looks like. helps with understanding this code and regex.
nit: add a newline here too.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
For integers please: use `type='int'` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int'` remove the `isdigit` check from `check_params`
This seems random and can lead to flaky tests. So there is no way to measure latency while Trogdor task is running? For the rate limit test below we do wait for task to run.
"for working with retry limiting"
nit: doc how the output of `iperf` looks like. helps with understanding this code and regex.
nit: add a newline here too.
This can be set, but not in the netflow/ipfix, config. It can be set on the dvSwitch itself using its configSpec: https://code.vmware.com/apis/358#/doc/vim.DistributedVirtualSwitch.ConfigSpec.html#switchIpAddress
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int'` remove the `isdigit` check from `check_params`
For integers please: use `type='int'` remove the `isdigit` check from `check_params`
For integers please: use `type='int'` remove the `isdigit` check from `check_params`
For integers please: use `type='int'` remove the `isdigit` check from `check_params`
@charettes Ideas welcome. Can you take a look? I really don't like the idea of using `psycopg2` in expressions.
I would prefer to wrap value with `Value()` and compile `options` separately.
like diff = load_config(self._module, config_xml, [])
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
Yeah, we still can't use dict comprehensions until 2.6 is formally dropped, sorry.
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
You probably should just exit here with `changed=False`
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
This can be simplified. Both Random and SystemRandom accept a seed value, in both cases the default value is None. So there is no need for a condition. The default value can be passed unconditionally
ditto ```suggestion ```
I don't see any need for this attribute.
I don't see a reason we can't use Python's `hash()` builtin, which is even faster and cached on strings I also don't think we need a class here - a single function to do the shuffling would do.
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
Can we display a similar message with the test results? It can be helpful when you have plenty of errors, .e.g. ``` ... FAILED (errors=206, skipped=1094, expected failures=4) Used shuffle seed: 556738431 (generated) ```
You have some unmerged lines here
I blame my `black` setting.
This does not need to be wrapped.
This does not need to be wrapped.
@charettes Ideas welcome. Can you take a look? I really don't like the idea of using `psycopg2` in expressions.
I would prefer to wrap value with `Value()` and compile `options` separately.
like diff = load_config(self._module, config_xml, [])
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
Yeah, we still can't use dict comprehensions until 2.6 is formally dropped, sorry.
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
You probably should just exit here with `changed=False`
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
This can be simplified. Both Random and SystemRandom accept a seed value, in both cases the default value is None. So there is no need for a condition. The default value can be passed unconditionally
ditto ```suggestion ```
I don't see any need for this attribute.
I don't see a reason we can't use Python's `hash()` builtin, which is even faster and cached on strings I also don't think we need a class here - a single function to do the shuffling would do.
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
Can we display a similar message with the test results? It can be helpful when you have plenty of errors, .e.g. ``` ... FAILED (errors=206, skipped=1094, expected failures=4) Used shuffle seed: 556738431 (generated) ```
You have some unmerged lines here
I blame my `black` setting.
This does not need to be wrapped.
This does not need to be wrapped.
Just a thought -- would an `expression.flatten()` method be useful? The check below would then be `any(expr for expr in expression.flatten() if isinstance(expr, F))`. Might need to look more closely at other places in the ORM to figure out if it'd be a useful construct.
Inner functions are slow, especially for pypy - best to extract this!
Any problem with: ``` @property def media(self): ```
Can you name this a little more verbosely? I can't unsee "get best"
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
This isn't needed, since we now only support 2.6+ anyway.
Just a thought -- would an `expression.flatten()` method be useful? The check below would then be `any(expr for expr in expression.flatten() if isinstance(expr, F))`. Might need to look more closely at other places in the ORM to figure out if it'd be a useful construct.
Inner functions are slow, especially for pypy - best to extract this!
Any problem with: ``` @property def media(self): ```
Can you name this a little more verbosely? I can't unsee "get best"
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
This isn't needed, since we now only support 2.6+ anyway.
This change breaks MTVServicesEmbeddedIE
It's an attribute so folks can try to change it dynamically. I would add `if self.default_bounds and ..`
We should omit `default_bounds` when the default value is used: ```suggestion if self.default_bounds and self.default_bounds != '[)': kwargs['default_bounds'] = self.default_bounds ```
`form_class` is defined in `RangeField.formfield()` so this is redundant.
For optional fields like `duration`, `feed['feed']['duration']` should be `feed.get('feed', {}).get('duration')`. All fields other than `id``,``title``and``formats`` are optional.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
Any problem with: ``` @property def media(self): ```
Inner functions are slow, especially for pypy - best to extract this!
Just a thought -- would an `expression.flatten()` method be useful? The check below would then be `any(expr for expr in expression.flatten() if isinstance(expr, F))`. Might need to look more closely at other places in the ORM to figure out if it'd be a useful construct.
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
Can you name this a little more verbosely? I can't unsee "get best"
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
This isn't needed, since we now only support 2.6+ anyway.
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
I guess `get_admin_readonly_field()` could take `response` instead of `response.context['adminform']`.
I don't think this assertion is necessary -- if it's None we'll get an error in the next assertion which should be just as easy to debug.
Actually `assertContains` checks the `status_code` too, so the other assertion is redundant.
It's ugly either way, but this way might make it slightly easier to debug if the test fails at some later point. ``` python plot_details_field = list(list(list(response.context['adminform'])[0])[3])[0] self.assertEqual(plot_details_field.field['name'], 'plotdetails') self.assertTrue(plot_details_field.is_readonly) self.assertEqual(plot_details_field.contents(), 'Brand New Plot') ``` I guess it we used the pattern more widely, it might be worth some helper functions to make it easy to extract fields without using magic numbers in the indexing.
No, it would be admin specific so it doesn't belong there. Just a private API mixin for the Django tests is that I was thinking. It might live in this file, for example.
We could also check `response.context`. Something like (needs to be cleaned up a bit): ``` >>> field_line = [field_line for field_line in [fieldset for fieldset in response.context['adminform']][0]][-1] >>> for f in field_line: print(f.contents()) Brand New Plot ``` This is easier to debug when it fails than `assertContains`.
Use another lookup instead of `epoch` e.g. `second`.
```suggestion Test that the returned value for timezone consists of only uppercase ```
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
it should also check if it can write there
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
This bare `except` will catch anything, including the exception that marks a failing test, I think :-) Better to use `assertRaises` and explicitly assert that the specific expected uncaught exception bubbles up.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Use another lookup instead of `epoch` e.g. `second`.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
also need to check whether the workspace is name or resource id
Could add the expected keys to the error since CamelCase may be unexpected.
Can `state` default to `present`? I usually expect this, I'm not sure if the flaw is with me or not :)
Not a blocker, but do we want a purge_listeners option? To do any modification to the ELB all the listeners must be specified each time.
it should also check if it can write there
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
This bare `except` will catch anything, including the exception that marks a failing test, I think :-) Better to use `assertRaises` and explicitly assert that the specific expected uncaught exception bubbles up.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Use another lookup instead of `epoch` e.g. `second`.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
also need to check whether the workspace is name or resource id
Could add the expected keys to the error since CamelCase may be unexpected.
Can `state` default to `present`? I usually expect this, I'm not sure if the flaw is with me or not :)
Not a blocker, but do we want a purge_listeners option? To do any modification to the ELB all the listeners must be specified each time.
it should also check if it can write there
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
This bare `except` will catch anything, including the exception that marks a failing test, I think :-) Better to use `assertRaises` and explicitly assert that the specific expected uncaught exception bubbles up.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Use another lookup instead of `epoch` e.g. `second`.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
also need to check whether the workspace is name or resource id
Could add the expected keys to the error since CamelCase may be unexpected.
Can `state` default to `present`? I usually expect this, I'm not sure if the flaw is with me or not :)
Not a blocker, but do we want a purge_listeners option? To do any modification to the ELB all the listeners must be specified each time.
This bare `except` will catch anything, including the exception that marks a failing test, I think :-) Better to use `assertRaises` and explicitly assert that the specific expected uncaught exception bubbles up.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
also need to check whether the workspace is name or resource id
The properties' key-value and default/required can be defined in the spec.
```python current_config['received'] = (current_config.get('received') == 'yes') ```
```python if match: current_config['format_welf_host'] = match.group(1) continue ``` no need to proceed to other checks when you find a match do it for all match check
Duplicate with `get` method? This logic can be handled as `if get_traffic_manager_profile`
Could add the expected keys to the error since CamelCase may be unexpected.
Can `state` default to `present`? I usually expect this, I'm not sure if the flaw is with me or not :)
Not a blocker, but do we want a purge_listeners option? To do any modification to the ELB all the listeners must be specified each time.
Minor but I'd move this control flow block after the `weights` one to match the args order.
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
> scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.), compute a sum You missed the parentheses.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Sorry, I know it was in my changes, but I forgot to remove that lineÃ¢Â€Â¦ Changing the data seems a bit ugly here, could you remove this line and changing the next one by: ``` prefix = ipaddress.ip_network(data["prefix"] + "/" + data["prefix_length"]) ```
I don't see a need for string interpolation in cases like this.
Minor but I'd move this control flow block after the `weights` one to match the args order.
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
```suggestion type: list suboptions: ```
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Minor but I'd move this control flow block after the `weights` one to match the args order.
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
> scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.), compute a sum You missed the parentheses.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Sorry, I know it was in my changes, but I forgot to remove that lineÃ¢Â€Â¦ Changing the data seems a bit ugly here, could you remove this line and changing the next one by: ``` prefix = ipaddress.ip_network(data["prefix"] + "/" + data["prefix_length"]) ```
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
TBH this is not what I suggested, I wanted to use `mocked_mode` to keep code DRY. I pushed edits.
I would override `clean()` as described in the ticket, e.g. ```python class PubForm(forms.ModelForm): mode = forms.CharField(max_length=255, required=False) mocked_mode = None def clean(self): self.cleaned_data['mode'] = self.mocked_mode return self.cleaned_data class Meta: model = PublicationDefaults fields = ('mode',) default_mode = 'di' pub_form = PubForm({}) pub_form.mocked_mode = 'de' pub = pub_form.save(commit=False) ... ```
chop blank line
I don't see any need for this attribute.
Spurious `else:` Breaking the tests.
I think that there is no need to check all empty values, so maybe: ``` # Default should be populated on an empty value. pub_form = PubForm({}) pub_form.mocked_mode = '' pub = mf2.save(commit=False) self.assertEqual(pub.mode, default_mode) ```
This line should use `connection.features.introspected_field_types["IntegerField"]`. (INTEGER is introspected as `BigIntegerField` on CockroachDB.) PR: https://github.com/django/django/pull/15750
Ahh, yes, sorry :facepalm: Good catch :dart:
```suggestion Test that the returned value for timezone consists of only uppercase ```
You need to wrap the second instantiation in its own assertRaises to actually test it.
You might append this to the `using` sql passed to `super()` to fix the test failure on Jenkins (due to usage of TABLESPACE).
Can you please run `pages_per_range` through `quote_value()` (internal function on the schema editor).
Can you re-warp this block to 79 chars? (First line is too short.)
This should probably be the default for postgresql's `schema_editor.sql_create_index`.
The proposed implementation isn't exactly what I had in mind, but I'll have to look a bit later to see if the idea I had in mind is any better. For `BrinIndex`, I guess those features weren't considered by the patch author. Probably no reason not to add them.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
It's not obvious to me where limit_choices_to comes into play.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
The usual pattern is to implement `get_source_expressions` and `set_source_expressions`. ```python def get_source_expressions(self): return [self.config] def set_source_expressions(self, expressions): self.config, = expressions
`extra.get()` returns `None` by default, so we can simplify this: ```python self.options = extra.get('options') ```
IMO we should check options against PostreSQL names.
We can reuse existing states.
@hannseman Thanks :+1: > I prefer it over the mixin approach. Yes me too :+1: . We can move `Value()` wrapping to the `__init__()` and simplify it a bit, e.g.: ```python class SearchConfig(Expression): def __init__(self, config): super().__init__(output_field=None) if not (config and hasattr(config, 'resolve_expression')): config = Value(config) self.config = config def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False): resolved = super().resolve_expression(query, allow_joins, reuse, summarize, for_save) resolved.config = self.config.resolve_expression(query, allow_joins, reuse, summarize, for_save) return resolved def as_sql(self, compiler, connection): sql, params = compiler.compile(self.config) return '%s::regconfig' % sql, params ``` Please move introducing a `SearchConfig` expression to the separate commit, or even PR.
I wonder if we could add a `SearchConfig` expression :thinking: (maybe a subclass of `Value`): ```python class SearchConfig(Expression): def __init__(self, config): super().__init__(output_field=None) ... def as_sql(self, compiler, connection): ... return '%::regconfig', params ``` This should simplify everything.
`resolve_expression()` implementation is the same as in `SearchQuery` and `SearchVector` so maybe we can add (in a separate commit) some mixin for this.
I moved this check to a separate helper.
I guess it would be easier to understand, if you'd name the separate boolean expression first before combining them, eg: ```python is_join = isinstance(table, Join) is_base_table = isinstance(table, BaseTable) clone.external_aliases[alias] = is_join and not is_base_table ``` Something along those lines.
The current form is fine, IMO. I don't think that ```python is_join = isinstance(table, Join) is_base_table = isinstance(table, BaseTable) clone.external_aliases[alias] = ( (is_join and table.join_field.related_model._meta.db_table != alias) or (is_base_table and table.table_name != table.table_alias) ) ``` is more readable.
`# Without form data` seem sufficient.
and please format it with indent, so it's more clear
add this condition also to lin 2063
Could you try to write these tests at a lower level, without the `as_ul()` output methods? They're difficult to debug now.
`# With form data. ...`
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
But it must be in this if.
use same indent style as previous item
might be better to have some kind of mapping here ``` convert_list = ['image_id', 'instance_type', 'instance_id', ...] camel_params = dict((k,v) for k, v in snake_dict_to_camel_dict(module.params).items() if v is not None and k is in convert_list) ``` and then special case any exceptions like IamInstanceProfile and InstanceMonitoring
Tests are missing for some validators, e.g. `DecimalValidator` and codes `invalid`, `max_digits`, `max_whole_digits`, and `max_decimal_places`.
In Python, it's common to include docstrings as per PEP 257: ```suggestion def fake_now(monkeypatch): """Patch `datetime.datetime.now()` to return a deterministic value.""" ```
FYI when you accept the suggested changes using GitHub UI, it preserves the authorship of the patch.
```suggestion short_description: Execute tasks inside a VM via VMware Tools ```
```suggestion - Use VMware tools to run tasks in, or put/fetch files to VM running in VMware infrastructure. ```
```suggestion - In case of Windows VMs, set C(ansible_shell_type) to C(powershell). ``` I am OK with current description as well.
```suggestion - requests (Python library) ```
```suggestion - Does not work with 'become'. ```
```suggestion - pyvmomi (Python library) ```
```suggestion - pyvmomi (python library) - requests ```
Detail why this may be useful
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
okay, but would be helpful to say _why_ we need to always return True.
These parens aren't necessary for unpacking the return values.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
This isn't needed, since we now only support 2.6+ anyway.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
You should emphasize that the module can and will not do any idempotence checking for this.
`# Without form data` seem sufficient.
and please format it with indent, so it's more clear
add this condition also to lin 2063
Could you try to write these tests at a lower level, without the `as_ul()` output methods? They're difficult to debug now.
`# With form data. ...`
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
But it must be in this if.
use same indent style as previous item
might be better to have some kind of mapping here ``` convert_list = ['image_id', 'instance_type', 'instance_id', ...] camel_params = dict((k,v) for k, v in snake_dict_to_camel_dict(module.params).items() if v is not None and k is in convert_list) ``` and then special case any exceptions like IamInstanceProfile and InstanceMonitoring
Tests are missing for some validators, e.g. `DecimalValidator` and codes `invalid`, `max_digits`, `max_whole_digits`, and `max_decimal_places`.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
okay, but would be helpful to say _why_ we need to always return True.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
This isn't needed, since we now only support 2.6+ anyway.
(In general, I don't think modules should have such options.)
These parens aren't necessary for unpacking the return values.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
I would remove all aliases if possible.
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
This isn't needed, since we now only support 2.6+ anyway.
(In general, I don't think modules should have such options.)
You should emphasize that the module can and will not do any idempotence checking for this.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
These parens aren't necessary for unpacking the return values.
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
This isn't needed, since we now only support 2.6+ anyway.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
You should emphasize that the module can and will not do any idempotence checking for this.
(In general, I don't think modules should have such options.)
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
okay, but would be helpful to say _why_ we need to always return True.
These parens aren't necessary for unpacking the return values.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
This isn't needed, since we now only support 2.6+ anyway.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
You should emphasize that the module can and will not do any idempotence checking for this.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
```not (foo is None)``` => ```foo is not None```
`import ansible.module_utils.parsing.convert_bool import BOOLEANS` and use that constant
This should just force strict and leave unchanged on typeerror ```diff --git a/lib/ansible/plugins/action/set_fact.py b/lib/ansible/plugins/action/set_fact.py index d7fe573c1a..934245d07c 100644 --- a/lib/ansible/plugins/action/set_fact.py +++ b/lib/ansible/plugins/action/set_fact.py @@ -51,8 +51,11 @@ class ActionModule(ActionBase): "letters, numbers and underscores." % k) return result - if not C.DEFAULT_JINJA2_NATIVE and isinstance(v, string_types) and v.lower() in ('true', 'false', 'yes', 'no'): - v = boolean(v, strict=False) + if not C.DEFAULT_JINJA2_NATIVE and isinstance(v, string_types): + try: + v = boolean(v, strict=True) + except TypeError: + pass # not valid bool string facts[k] = v result['changed'] = False```
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
That was the thing I was saying earlier- checking for/failing on undefined inputs upfront would probably be what most people would expect, but that'd be a breaking change. Even if we chose that breaking change, we'd probably need a way for filters that *want* to allow undefined inputs to do so (transparently to the caller of the filter). And yeah, we probably need an analogue for tests, since the same problems exist.
Another option might be to build it the way we wish it were, then start a deprecation warning cycle on this bridging behavior (eg, do something like this and/or input inspection with an explicit optin/optout in plugin metadata, decorator, or whatever, then issue a dep warning on the exception catch or undefined inputs for plugins that haven't explicitly opted into receiving them). :shrug:
I think we should have an entry-check to protect from reuse: ```suggestion def __enter__(self): assert not self.timed_out ```
I would just make this a plain function that calls fetch_url and then lowercases the keys instead of making it a decorator. I'm a little hesitant about overriding the name (I'd use a different name than fetch_url) but I can see you do that so you don't have to change the name everywhere. Since I'm not the module author, I won't block it over that but I think it's better style to use a new name.
This could possibly be split into additional simple wrappers based on `parameter.kind` if it were `POSITIONAL_ONLY` or `KEYWORD_ONLY`? Then you could have simple conditions: ```python if has_multiple_parameters: def wrapper(*args, **kwargs): if any( isinstance(arg, Promise) for arg in itertools.chain(args, kwargs.values()) ): return lazy_func(*args, **kwargs) return func(*args, **kwargs) elif parameter.kind == parameter.POSITIONAL_ONLY: def wrapper(*args, **kwargs): if isinstance(args[0], Promise): return lazy_func(*args, **kwargs) return func(*args, **kwargs) elif parameter.kind == parameter.KEYWORD_ONLY: def wrapper(*args, **kwargs): if isinstance(kwargs[first_parameter.name], Promise): return lazy_func(*args, **kwargs) return func(*args, **kwargs) else: # parameter.POSITIONAL_OR_KEYWORD def wrapper(*args, **kwargs): if (args and isinstance(args[0], Promise)) or ( first_parameter.name in kwargs and isinstance(kwargs[first_parameter.name], Promise) ): return lazy_func(*args, **kwargs) return func(*args, **kwargs) return wraps(func)(wrapper) ``` Although maybe that is overkill if we just simplify the last case: ```python def wrapper(*args, **kwargs): arg = args[0] if args else kwargs[first_parameter.name] if isinstance(arg, Promise): return lazy_func(*args, **kwargs) return func(*args, **kwargs) ``` (Note that we can assume it exists in `kwargs` if `args` was empty and drop the additional containment check.)
If we `from itertools import chain` we can save the overhead of attribute access here.
I don't believe this parameter should exist. I believe we should rely on the ability for libraries to use environment variables for `http_proxy` and `https_proxy`. Both `ansible.module_utils.urls` and `requests` can both utilize these environment vars. Setting these values via the `environment` keyword on a task is accepted. The module should not have a deviating method for applying proxies.
Just want to verify - will it be possible to apply these environment variables per host in the inventory file? The idea is that you may have multiple firewalls, some of which need proxy (or even different proxies) to be managed and others don't.
This should be modeled in the same way that all other modules within ansible currently work. Variables can be set per host and applied to tasks.
lines can be longer
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
Do we need to call `list(fields)` here? :thinking:
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
```suggestion item, fields=fields, using=self.db, ```
Few naming suggestions: `ordering_element` -> `expr` `ord_sql` -> `expr_sql` `ord_sql_params` -> `expr_params` `additional_sql_params` -> `ordering_params` `ord_clauses` -> `ordering_expr_sql`
We can ignore `name`, `path`, and `args`, e.g. `*_, kwargs = field.deconstruct()`
Chop blank line.
Please add trailing comma.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
Style: Can you move that `add_child()` above `add_parent()`.
Style: can you sort `__str__` and `__repr__` between the other private methods on top.
Use _ (underline) instead of webpage if the value is not used.
Are all of these necessary? I think youtube-dl defaults suffice.
I believe this relates to team accounts. Users are able to create an image for their account and not expose it to additional team members.
@poleha why do you say so. `MangerDescriptor.__get__` will run on each access to `Model.objects`.
I think some caching would make sense here.
The main issue here is that overriding `DATABASE_ROUTERS` doesn't allow passing initialization arguments to the specified classes hence why I suggested overriding `routers` directly. Another solution could be to make `Router` depend on a class attribute instead and override it in the loop: ``` python class Router(object): target = None def db_for_read(self, model, **hints): return self.target db_for_write = db_for_read @override_settings(DATABASE_ROUTERS=['admin_views.test_multidb.Router']) def test_foo(self): for db in connections: Router.target = db ... ```
Or use the `self.settings()` context manager if you can't decorate the class since you have a loop.
I feel like it should be `_delete_unique_sql`'s decision to do the bool casting. Maybe some backends will need access to the condition to appropriately delete it. Lets just pass `condition=condition` and let database backends do `if condition`.
I notice that there is a check here on `has_add_permission`. Should the `InlineAdminForm`s above this check for `has_change_permission`? Also I don't understand why we would only need to check for the add case and not the change case when adding view permissions...
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
Ok I didn't think this through, I assumed that one could add inlines with the add permission. Of course for the author. If you like I can also withdraw my PR to django's repo and do the PR to you branch, so you keep control on this (you did all the work, and you're obviously more experienced than me in this, I just thought you may have no more time to spend on this). Also, I'm willing to backport this patch to 1.9 (or 1.10, but can't wait for 1.11 to use it in my projects). I see you have a 1.9 backport branch too, we may want to share efforts on this. My backport branch isn't in production yet, but will probably be in the two or three weeks. It seems to work well for now.
Use another lookup instead of `epoch` e.g. `second`.
Please use a single quote.
@poleha why do you say so. `MangerDescriptor.__get__` will run on each access to `Model.objects`.
I think some caching would make sense here.
The main issue here is that overriding `DATABASE_ROUTERS` doesn't allow passing initialization arguments to the specified classes hence why I suggested overriding `routers` directly. Another solution could be to make `Router` depend on a class attribute instead and override it in the loop: ``` python class Router(object): target = None def db_for_read(self, model, **hints): return self.target db_for_write = db_for_read @override_settings(DATABASE_ROUTERS=['admin_views.test_multidb.Router']) def test_foo(self): for db in connections: Router.target = db ... ```
Or use the `self.settings()` context manager if you can't decorate the class since you have a loop.
I feel like it should be `_delete_unique_sql`'s decision to do the bool casting. Maybe some backends will need access to the condition to appropriately delete it. Lets just pass `condition=condition` and let database backends do `if condition`.
I notice that there is a check here on `has_add_permission`. Should the `InlineAdminForm`s above this check for `has_change_permission`? Also I don't understand why we would only need to check for the add case and not the change case when adding view permissions...
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
Ok I didn't think this through, I assumed that one could add inlines with the add permission. Of course for the author. If you like I can also withdraw my PR to django's repo and do the PR to you branch, so you keep control on this (you did all the work, and you're obviously more experienced than me in this, I just thought you may have no more time to spend on this). Also, I'm willing to backport this patch to 1.9 (or 1.10, but can't wait for 1.11 to use it in my projects). I see you have a 1.9 backport branch too, we may want to share efforts on this. My backport branch isn't in production yet, but will probably be in the two or three weeks. It seems to work well for now.
Use another lookup instead of `epoch` e.g. `second`.
Please use a single quote.
@poleha why do you say so. `MangerDescriptor.__get__` will run on each access to `Model.objects`.
I think some caching would make sense here.
The main issue here is that overriding `DATABASE_ROUTERS` doesn't allow passing initialization arguments to the specified classes hence why I suggested overriding `routers` directly. Another solution could be to make `Router` depend on a class attribute instead and override it in the loop: ``` python class Router(object): target = None def db_for_read(self, model, **hints): return self.target db_for_write = db_for_read @override_settings(DATABASE_ROUTERS=['admin_views.test_multidb.Router']) def test_foo(self): for db in connections: Router.target = db ... ```
Or use the `self.settings()` context manager if you can't decorate the class since you have a loop.
I feel like it should be `_delete_unique_sql`'s decision to do the bool casting. Maybe some backends will need access to the condition to appropriately delete it. Lets just pass `condition=condition` and let database backends do `if condition`.
I notice that there is a check here on `has_add_permission`. Should the `InlineAdminForm`s above this check for `has_change_permission`? Also I don't understand why we would only need to check for the add case and not the change case when adding view permissions...
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
Ok I didn't think this through, I assumed that one could add inlines with the add permission. Of course for the author. If you like I can also withdraw my PR to django's repo and do the PR to you branch, so you keep control on this (you did all the work, and you're obviously more experienced than me in this, I just thought you may have no more time to spend on this). Also, I'm willing to backport this patch to 1.9 (or 1.10, but can't wait for 1.11 to use it in my projects). I see you have a 1.9 backport branch too, we may want to share efforts on this. My backport branch isn't in production yet, but will probably be in the two or three weeks. It seems to work well for now.
Use another lookup instead of `epoch` e.g. `second`.
Please use a single quote.
```suggestion f'site={self.admin_site!r}>' ) ```
n.b. just noticed these tests could also use `assertIn` / `assertNotIn` rather than `find()`. But it seems the tests in this file mix the two, so no worries.
`test_jsonfield` looks good to me (since this is InspectDBTests, _introspection seems redundant).
It looks that `test_sqlmigrate_replaced_second_migration()` and `test_sqlmigrate_replaced_migration()` are redundant. Please remove one of them.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
> Shall I also remove `test_target_model_is_unsaved_save` and `test_target_model_is_unsaved_bulk_create`? Yes, they are redundant.
It looks like a duplicate of `test_save_unsaved_instance_on_generic_foreign_key` :thinking:
I would use `%s` formatting consistently.
For python 2.6 compatibility, this and the following four strings have to look like `cmd = "ping {0} {1}".format(vrf, dest)`
It would have been more appropriate if `run_checks()` took an `Apps` instance as a parameter instead of a list of `AppConfig`s.
single line as above
use a single line or use hanging indent (we avoid non-4 space indents)
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
single line as above
use a single line or use hanging indent (we avoid non-4 space indents)
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
```suggestion self, objs, fields, batch_size, on_conflict=None, update_fields=None, unique_fields=None, ```
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
you could also write this as: h_item = search_obj_in_list(w_item['member'], h, key='member') or {}
Ditto. ```suggestion # Check empty result with non-execution optimization. with self.assertNumQueries(0): ```
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
you could also write this as: h_item = search_obj_in_list(w_item['member'], h, key='member') or {}
Ditto. ```suggestion # Check empty result with non-execution optimization. with self.assertNumQueries(0): ```
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
Does the result provide any insight into if anything's changed? Looks like put and delete are currently both hard coded to return ``changed=True``
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
you could also write this as: h_item = search_obj_in_list(w_item['member'], h, key='member') or {}
Ditto. ```suggestion # Check empty result with non-execution optimization. with self.assertNumQueries(0): ```
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
I'd suggest this for the indentation: ```python return [ force_text( capi.get_field_name(capi.get_field_defn(self._layer._ldefn, i)), self.encoding, strings_only=True, ) for i in range(self.num_fields) ] ``` Any concern about having no tests for the `self.encoding, strings_only=True` bit? I'm not sure we would expect non-string values in the first place? Also, at least on my system, there are no failures with it removed here or in `layer_name` above.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
I don't see a need for string interpolation in cases like this.
Yes, that's more helpful for me.
I think `name.rsplit('-', 1)[-1]` is easier to read.
```suggestion type: list suboptions: ```
F() expressions aren't the only ones that can refer to other columns in the query. How about Q(other_field__isnull=True). Also, expressions are free to resolve columns of the query without using F-expressions. We need some other way to know if the expression refers to columns of the query. Maybe we could first resolve the expression, the check for Col references? That might be better. The check should be done in the expression, so that the expression tells Django if it is referring to any columns. Making the compiler guess this is the wrong way in my opinion.
I _think_ that `Ref` will also need to return True, since it is a named reference to an existing `Col`.
Inner functions are slow, especially for pypy - best to extract this!
Just a thought -- would an `expression.flatten()` method be useful? The check below would then be `any(expr for expr in expression.flatten() if isinstance(expr, F))`. Might need to look more closely at other places in the ORM to figure out if it'd be a useful construct.
Right, but the order in which you're doing things would at a glance result in the not-uncommon case of, eg `foo (cr)(lf)` not being properly treated, since the break `sub` wouldn't see that, but the char drop would remove the (cr), leaving a line ending that wouldn't format the right way.
This can be changed to look for only one space now that #57209 is merged. Thanks again for writing tests. Much appreciated.
When there is no `topicTitle`, `categories` will be `[None]` that does not make any sense.
Use _ (underline) instead of webpage if the value is not used.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
Use _download_webpage is urlh is not used. And note should be meaningful for typical users.
Are all of these necessary? I think youtube-dl defaults suffice.
urlencode_postdata should be better than urlencode + encode
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Use json.dumps instead
flake8 complains about missing spaces around `*`
I'm not sure whether it should be considered part of a separate clean up across all cases of this in the code base or whether we can handle this here for all of the adjacent lines, but it would be nice to fix the indentation: ```python self.management_form_html + ( '<tr><th>Choice:</th><td><input type="text" name="choices-0-choice" value="Calexico"></td></tr>' '<tr><th>Votes:</th><td><input type="number" name="choices-0-votes" value="100"></td></tr>' ) ```
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
Oh, I just meant to suggest the `value` to `values` (plural) renaming, not more...
Match the error message
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
I think `enumerate` would work here
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
I don't see a need for string interpolation in cases like this.
I'd suggest this for the indentation: ```python return [ force_text( capi.get_field_name(capi.get_field_defn(self._layer._ldefn, i)), self.encoding, strings_only=True, ) for i in range(self.num_fields) ] ``` Any concern about having no tests for the `self.encoding, strings_only=True` bit? I'm not sure we would expect non-string values in the first place? Also, at least on my system, there are no failures with it removed here or in `layer_name` above.
I think `name.rsplit('-', 1)[-1]` is easier to read.
```suggestion type: list suboptions: ```
F() expressions aren't the only ones that can refer to other columns in the query. How about Q(other_field__isnull=True). Also, expressions are free to resolve columns of the query without using F-expressions. We need some other way to know if the expression refers to columns of the query. Maybe we could first resolve the expression, the check for Col references? That might be better. The check should be done in the expression, so that the expression tells Django if it is referring to any columns. Making the compiler guess this is the wrong way in my opinion.
I _think_ that `Ref` will also need to return True, since it is a named reference to an existing `Col`.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
Inner functions are slow, especially for pypy - best to extract this!
Just a thought -- would an `expression.flatten()` method be useful? The check below would then be `any(expr for expr in expression.flatten() if isinstance(expr, F))`. Might need to look more closely at other places in the ORM to figure out if it'd be a useful construct.
I don't see a need for string interpolation in cases like this.
I'd suggest this for the indentation: ```python return [ force_text( capi.get_field_name(capi.get_field_defn(self._layer._ldefn, i)), self.encoding, strings_only=True, ) for i in range(self.num_fields) ] ``` Any concern about having no tests for the `self.encoding, strings_only=True` bit? I'm not sure we would expect non-string values in the first place? Also, at least on my system, there are no failures with it removed here or in `layer_name` above.
I think `name.rsplit('-', 1)[-1]` is easier to read.
```suggestion type: list suboptions: ```
F() expressions aren't the only ones that can refer to other columns in the query. How about Q(other_field__isnull=True). Also, expressions are free to resolve columns of the query without using F-expressions. We need some other way to know if the expression refers to columns of the query. Maybe we could first resolve the expression, the check for Col references? That might be better. The check should be done in the expression, so that the expression tells Django if it is referring to any columns. Making the compiler guess this is the wrong way in my opinion.
I _think_ that `Ref` will also need to return True, since it is a named reference to an existing `Col`.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
Inner functions are slow, especially for pypy - best to extract this!
Just a thought -- would an `expression.flatten()` method be useful? The check below would then be `any(expr for expr in expression.flatten() if isinstance(expr, F))`. Might need to look more closely at other places in the ORM to figure out if it'd be a useful construct.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
Scanning a list will not be faster than a membership test of a set (unless the list of words is very small).
Since the list is in order of most common use, the code detects incorrect passwords slightly faster if you preserve the order. :bikeshed:
Oh I missed that. Sorry!
This would be better as a set rather than a list.
Not sure how much a difference it makes, but it seems better to store this in Python rather than having to read from a text file. Worth it to make the file location customizable? If so, it might be nice to make "common passwords" a separate package so we don't have to include that list in Django. I guess users might not care for the additional setup tasks though.
having `main()` call `run_module()` feels a bit redundant, I see no reason to not put all of the main logic in `main` (and splitting actions into functions where it makes sense)
Chop the blank lines
class attribute `output_field = IntegerField()` * fairly sure that's acceptable Then drop the `__init__`
class attribute `output_field = IntegerField()` * fairly sure that's acceptable Then drop the `__init__`
It seems I was wrong - there are so many examples in the code that override init just to force the output field it's not worth worrying about here. Maybe in the future we can make it easier, but not necessary for this patch.
Defining `__init__()` to specify a default `output_field` is required until we re-arrange `runtests.py` to avoid importing settings dependant modules before calling `django.setup()`.
Nitpick, but shouldn't these be assertEqual()? This would be consistent with `if len(queryset) == 2` instead of `if len(queryset) is 2`.
here you need just a 'steps' not whole module as well.
also please rename entity_id to job_id, no need to have it too generic here
change 'type(job).__name__.lower()' to just 'job' no need to do it generic here
I would add here fetch_nested=True, because we always want to return steps.
This might fail on python 3 and json.loads expects string and getvalue() will return bytes .
Playlist metadata must not be fatal.
Playlist id and title should not be fatal.
This shouldn't be in `response.keys()`, it's in `response['ins_api'].keys()`. I would rather `if response['ins_api'].get('outputs'):`
Typo? ```suggestion versions_url = base_url + 'versions/' ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
will easily match outside the element.
- use `sample_mp3` as the primary source. - match only up to mp3 extension.
use `_hidden_inputs` method.
Please rewrite as ``` if __name__ == '__main__': main() ```
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
This seems problematic. What happens if you do this: ``` reused_f = F('pk') qs1 = Model1.objects.filter(pk=reused_f) qs2 = Model2.objects.filter(pk=reused_f) ``` By my reading the reused_f will be first prepared for qs1, getting Model1.pk as col. Then the same object is reused and prepared for qs2, this time the reused_f's col is overwritten with Model2.pk. The ExpressionNodes must be either stateless, or they must be copied on prepare(). Doing the latter will be easier. A basic copy method could do the following: ``` def copy(self): copy = Empty() copy.__class__ = self.__class__ copy.__dict__ = self.__dict__.copy() return copy ```
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
These don't appear to be "public" attributes, so we should stay consistent with indicating their privateness by prepending their names with an `_` character.
```suggestion Test that the returned value for timezone consists of only uppercase ```
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I would change this to a `NotSupportedError`, e.g. ```python raise NotSupportedError( 'Bulk create a multi-table inherited model is not supported ' 'on this database backend.' ) ```
single line as above
Please add a trailing comma: ```suggestion on_conflict=None, update_fields=None, unique_fields=None, ```
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
```suggestion self, objs, fields, batch_size, on_conflict=None, update_fields=None, unique_fields=None, ```
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Ditto for `[]` â†’ `None` and `ON_CONFLICTS_NONE` â†’ `None`.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
```suggestion item, fields=fields, using=self.db, ```
Chop the blank lines
class attribute `output_field = IntegerField()` * fairly sure that's acceptable Then drop the `__init__`
here you need just a 'steps' not whole module as well.
also please rename entity_id to job_id, no need to have it too generic here
class attribute `output_field = IntegerField()` * fairly sure that's acceptable Then drop the `__init__`
Nitpick, but shouldn't these be assertEqual()? This would be consistent with `if len(queryset) == 2` instead of `if len(queryset) is 2`.
State should have the choices 'present' or 'absent' to be like other Ansible modules.
pass here just description, as it's the only parameter you need.
I would add here fetch_nested=True, because we always want to return steps.
change 'type(job).__name__.lower()' to just 'job' no need to do it generic here
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
I realize this predates your PR, but why is 'unused' not used? It appears to be the target field provided by the SQLUpdateCompiler during as_sql(). This seems like a really good spot to allow a backend to hook in and CAST the expression to a type that is allowed by the target column.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
```suggestion Test that the returned value for timezone consists of only uppercase ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Chop the blank lines
class attribute `output_field = IntegerField()` * fairly sure that's acceptable Then drop the `__init__`
here you need just a 'steps' not whole module as well.
also please rename entity_id to job_id, no need to have it too generic here
class attribute `output_field = IntegerField()` * fairly sure that's acceptable Then drop the `__init__`
Nitpick, but shouldn't these be assertEqual()? This would be consistent with `if len(queryset) == 2` instead of `if len(queryset) is 2`.
State should have the choices 'present' or 'absent' to be like other Ansible modules.
pass here just description, as it's the only parameter you need.
I would add here fetch_nested=True, because we always want to return steps.
change 'type(job).__name__.lower()' to just 'job' no need to do it generic here
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
This seems problematic. What happens if you do this: ``` reused_f = F('pk') qs1 = Model1.objects.filter(pk=reused_f) qs2 = Model2.objects.filter(pk=reused_f) ``` By my reading the reused_f will be first prepared for qs1, getting Model1.pk as col. Then the same object is reused and prepared for qs2, this time the reused_f's col is overwritten with Model2.pk. The ExpressionNodes must be either stateless, or they must be copied on prepare(). Doing the latter will be easier. A basic copy method could do the following: ``` def copy(self): copy = Empty() copy.__class__ = self.__class__ copy.__dict__ = self.__dict__.copy() return copy ```
These don't appear to be "public" attributes, so we should stay consistent with indicating their privateness by prepending their names with an `_` character.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I would add here fetch_nested=True, because we always want to return steps.
change 'type(job).__name__.lower()' to just 'job' no need to do it generic here
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
here you need just a 'steps' not whole module as well.
also please rename entity_id to job_id, no need to have it too generic here
change 'type(job).__name__.lower()' to just 'job' no need to do it generic here
I would add here fetch_nested=True, because we always want to return steps.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
Nitpick, but shouldn't these be assertEqual()? This would be consistent with `if len(queryset) == 2` instead of `if len(queryset) is 2`.
pass here just description, as it's the only parameter you need.
also please rename entity_id to job_id, no need to have it too generic here
here you need just a 'steps' not whole module as well.
State should have the choices 'present' or 'absent' to be like other Ansible modules.
change 'type(job).__name__.lower()' to just 'job' no need to do it generic here
I would add here fetch_nested=True, because we always want to return steps.
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
also please rename entity_id to job_id, no need to have it too generic here
here you need just a 'steps' not whole module as well.
change 'type(job).__name__.lower()' to just 'job' no need to do it generic here
I would add here fetch_nested=True, because we always want to return steps.
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
No worries :)
```suggestion Test that the returned value for timezone consists of only uppercase ```
pageParams is missing from the equality check
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
This is minor but so we don't confuse future readers of this code, I think the watermark is suppose to be `6L` instead of `4L`. The high watermark should always be at batch boundaries.
You are right @hachikuji . For line 1597 to be true, I think the test needs to do another round of fetch. > // The high watermark advances to be larger than log.endOffsetForEpoch(3), to test the case 3 Line 1614 wants to fail because of an invalid offset and epoch based on the leader epoch cache. Not because it is greater than the high watermark. ``` assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId4.offset, invalidSnapshotId4.epoch)); ```
Seems like we should use `3L` instead. The leader would not have been able to advance the high watermark past the fetch offset of `3L`.
Ah, I think this ought to be `local_size == remote_size`, since this conditional causes the file to be skipped for this strategy. That explains the odd behavior I saw with `date_size`
Is this going to be used for non-testing code in the future? If it is only going to be for metrics purposes maybe we can allow it to be non thread-safe just to not blocking on other critical paths.
Thanks. This PR is assigned to the 2.12 milestone, which we'll be going through after creating the stable-2.11 branch (which will be done when 2.11 RC1 comes out).
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
WDYT about adding `or self.title` here? ``` python self.title = title or self.title or self.lookup_title ``` This would add support for the following: ``` python class MyStaffFilter(BooleanFieldListFilter): title = "By can haz admin" # no effect :( overwritten in __init__ @admin.register(User) class UserAdmin(UserAdminBase): list_filter = ( ("is_staff", MyStaffFilter), ) ```
No worries :)
I believe you would need to add a custom `save_form` method to a `ModelAdmin` and somehow incorporate the `change` flag in it -- perhaps modify the form's cleaned_data to assign the field to a model field before save.
This should use `self.subTest`, I also don't think that we need a `namedtuple` and `self.script_name_test_cases` variable, e.g. ```python tests = ( # SCRIPT_NAME ends with no slash, settings start with slashes. ('/somesubpath', '/static/', '/somesubpath/static/', '/media/', '/somesubpath/media/'), # SCRIPT_NAME ends with no slash, settings start with no slashes. ('/somesubpath', 'static/', '/somesubpath/static/', 'media/', '/somesubpath/media/'), ... ) for script_name, initial_static_url, final_static_url, initial_media_url, final_media_url in tests: with self.subTest(...): ... ```
```suggestion Test that the returned value for timezone consists of only uppercase ```
Ah, I think this ought to be `local_size == remote_size`, since this conditional causes the file to be skipped for this strategy. That explains the odd behavior I saw with `date_size`
or `_('Add %s')`
It looks like this is just a serial upload, how is this faster than the current S3 module? I definitely see the benefit of the glob & sync strategies, but it seems like this would be just as fast.
Plz also use `match` arg here
```suggestion ({b'str'.encode(DEFAULT_ENCODING): b'str'.encode(DEFAULT_ENCODING)}, {u'str': u'str'}), ```
You should probably expect unicode strings
(Similarly, mark any string that you are going to call decode on with as a b"string".)
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
Having a class for just one test method is unnecessary.
You're checking two separate properties here. This should be in a separate test.
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
When referring to `bulk_save()` in messages, include parenthesis (and periods).
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
Do we need to call `list(fields)` here? :thinking:
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
```suggestion item, fields=fields, using=self.db, ```
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
you can use `state` to avoid the 'or' to the user
single line as above
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
(And round-tripping of the messages is already tested in other tests)
This test is already passing without the changes in this PR so perhaps better to omit or put in a separate commit? non_existing -> nonexistent
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
```suggestion item, fields=fields, using=self.db, ```
```suggestion - There is a small delay (typically about 5 seconds, but can be as long as 30 seconds) before obtaining the random values when requesting a validation. ```
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
In Python3, `super()` is enough.
I think that this docstring should be simplified. There is no need to mention that this originated from the admin site.
There is no need to provide a value for `max_pages_num` - I think it should be calculated automatically: ```python (self.on_each_side + self.on_ends) * 2 ``` Otherwise developers can provide values that are incompatible with each other...
I think you meant `get_ellipsized_page_range` here. Use `number` instead of `page_num` which matches the argument name used by `Paginator.get_page()`.
@graingert `cls` is passed here.
I think it's the right class: ``` In [38]: class desc: ...: def __get__(self, instance, cls): ...: if instance is None: ...: return self ...: return instance.__dict__['_%s__mangled' % cls.__name__] In [39]: class A: ...: d = desc() ...: ...: def __init__(self): ...: self.__mangled = 42 ...: In [40]: A().d Out[40]: 42 ```
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
Might make sense to raise exception in this case: ``` class Test: @cached_property def a(self): pass b = a ```
Can you just do `name.startswith('__') and not name.endswith('__')`? Simpler is better
I'd say `on Python < 3.6`
As Collin said, slightly longer lines are okay rather than awkward wrappings like this.
You can skip the blank lines in these tests as three lines long isn't too difficult too read with spaces.
Please use assertRaisesMessage to verify this is the ValueError we expect.
I'd use a name like `assertBackendInSession`.
also remove this line. you already have it at top level on L21. this is what causes the `'AnsibleUnicode' object has no attribute 'pop'` errors. here under `options:` ansible-doc assumes all entries will have a sub-hash as value... (ansible-doc is unhelpful when it crashes, but I run `bin/ansible-doc manageiq_dynamic_resource_definition -vvv` to get traceback, and added print statements from there...)
nit: capital List at start of sentence
nit: capital List at start of sentence
Is this (and `providers:` below) part of `property_attributes:`? Dedent if you want them to be example of top-level `providers` and `services` args.
Perhaps `required_if` can help.
I think this is a typo - `changed` should be `changes`
I think you meant `get_ellipsized_page_range` here. Use `number` instead of `page_num` which matches the argument name used by `Paginator.get_page()`.
There is no need to provide a value for `max_pages_num` - I think it should be calculated automatically: ```python (self.on_each_side + self.on_ends) * 2 ``` Otherwise developers can provide values that are incompatible with each other...
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Might make sense to raise exception in this case: ``` class Test: @cached_property def a(self): pass b = a ```
I'd say `on Python < 3.6`
@graingert `cls` is passed here.
I think it's the right class: ``` In [38]: class desc: ...: def __get__(self, instance, cls): ...: if instance is None: ...: return self ...: return instance.__dict__['_%s__mangled' % cls.__name__] In [39]: class A: ...: d = desc() ...: ...: def __init__(self): ...: self.__mangled = 42 ...: In [40]: A().d Out[40]: 42 ```
@graingert Probably it doesn't worth it, but still doable :-) ``` In [85]: class desc: ...: def __init__(self, f): ...: self.name = f.__name__ ...: ...: def __get__(self, instance, cls): ...: if instance is None: ...: return self ...: ...: for cls in cls.__mro__: ...: name = '_%s%s' % (cls.__name__, self.name) ...: if cls.__dict__.get(name) is self: ...: return name ...: ...: class A: ...: @desc ...: def __x(self): ...: pass ...: ...: ...: class B(A): ...: pass ...: In [86]: a = A() ...: a._A__x ...: Out[86]: '_A__x' In [87]: b = B() ...: b._A__x ...: Out[87]: '_A__x' ```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
okay, but would be helpful to say _why_ we need to always return True.
I would remove all aliases if possible.
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
These parens aren't necessary for unpacking the return values.
This isn't needed, since we now only support 2.6+ anyway.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
okay, but would be helpful to say _why_ we need to always return True.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
These parens aren't necessary for unpacking the return values.
This isn't needed, since we now only support 2.6+ anyway.
Since Ansible subscribes to unicode sandwich, this should be the callers' (actually, where the data enters the system) responsibility.
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
okay, but would be helpful to say _why_ we need to always return True.
This isn't needed, since we now only support 2.6+ anyway.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
Since for Python, a non-empy string is True, you can just do: `if self.params['esxi_hostname']:`.
version added requires quotes, otherwise it will be processed as number
```suggestion I(minvalue), I(maxvalue), I(start), I(cache), I(cycle), I(rename_to), ```
Would it be better to raise `NotImplementedError()` so backends don't overlook implementing it? They can always return an empty list if it's not supported.
```suggestion - If C(false) (NO CYCLE) is specified, any calls to nextval after the sequence ```
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Why have both `schema` and `newschema`? I would assume that if I specify another value for `schema`, that the schema will be changed.
This should not result in a warning, but simply result in `changed == False`.
I would expect a module to issue warnings only for something which the user needs to be warned about. If the user wants something to be absent and it already is absent, that's not something I would expect a warning about. You also don't issue a warning if the user has `state == 'present'` and the sequence is already there.
Yep, simply ignoring all other options when `state == 'absent'` is the easiest solution (that's what most other modules do, too).
Use `==` to compare booleans. The `is` test should *only* be used when you really want to compare identities of objects! Finally, there's no need to compare a boolean to `True` or `False` explicitly; simply write `elif self._has_migs(local):`.
Don't you simply want an `else:` here? Or do you explicitly want to have the case that you call `self._has_migs(local)` twice in a row (without a sleep inbetween), and once it returns `False` and then `True`? Otherwise this `elif` makes no sense.
Ah, I didn't knew there was a template :) Well, in that case, keep it. It doesn't really hurt.
https://stackoverflow.com/a/2239753 ```suggestion if consecutive_good == self.module.params[ ```
Same function in `docker_swarm` module. It will be better to create a new module `docker_swarm_common` to assemble common code.
From the way the docker modules currently operate, it probably makes sense to add a `AnsibleDockerSwarmClient` to `module_utils/docker_swarm.py`, which extends `AnsibleDockerClient`. Then `docker_swarm`, `docker_swarm_facts`, `docker_node` and `docker_node_facts` could use `AnsibleDockerSwarmClient` instead of using `AnsibleDockerClient` directly.
If this docstring is for the `_cluster_stable()` method, it must be *inside* that method.
Same function in `docker_swarm` module. It will be better to create a new module `docker_swarm_common` to assemble common code.
`return migs != 0` is equivalent ot lines 380 to 382.
I think you can simply delete this and the next line. `namespace_tx` and `namespace_rx` are already `int`s at this point.
I think it's a better API; a custom manager could be using its name in a `__new__` method.
This seems problematic. What happens if you do this: ``` reused_f = F('pk') qs1 = Model1.objects.filter(pk=reused_f) qs2 = Model2.objects.filter(pk=reused_f) ``` By my reading the reused_f will be first prepared for qs1, getting Model1.pk as col. Then the same object is reused and prepared for qs2, this time the reused_f's col is overwritten with Model2.pk. The ExpressionNodes must be either stateless, or they must be copied on prepare(). Doing the latter will be easier. A basic copy method could do the following: ``` def copy(self): copy = Empty() copy.__class__ = self.__class__ copy.__dict__ = self.__dict__.copy() return copy ```
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
I realize this predates your PR, but why is 'unused' not used? It appears to be the target field provided by the SQLUpdateCompiler during as_sql(). This seems like a really good spot to allow a backend to hook in and CAST the expression to a type that is allowed by the target column.
IIRC get_cols() was renamed to get_group_by_cols() some time ago - no need to have both.
okay, but would be helpful to say _why_ we need to always return True.
No need to use quotes. We tend to not quote when it's not needed, as this helps people understand the YAML rules better.
```suggestion type: list suboptions: ```
Why not use keyed groups and let the users decide themselves whether they want to create such a group, instead of creating it by default? (There should be an example of how to do this if the user is suposed to do it by herself.)
Thanks for that note ewen. I learned something!
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
From the way the docker modules currently operate, it probably makes sense to add a `AnsibleDockerSwarmClient` to `module_utils/docker_swarm.py`, which extends `AnsibleDockerClient`. Then `docker_swarm`, `docker_swarm_facts`, `docker_node` and `docker_node_facts` could use `AnsibleDockerSwarmClient` instead of using `AnsibleDockerClient` directly.
That would be a Python module in lib/ansible/module_utils, not a Ansible module (in case this confuses anyone). I guess just naming it `docker_swarm` would be fine, too (everything in module_utils is already expected to be common to multiple modules).
Same function in `docker_swarm` module. It will be better to create a new module `docker_swarm_common` to assemble common code.
Ah, ok. As I said, I've never used docker-machine, so I assumed that it actually connects to the machine (using that shell) and exports the environment from there. If that's just the format, then yes, it really doesn't matter (as long as it is a format you can parse :) ). Both `bash` and `sh` are fine for me, use whatever you want then.
For automation, `yes` or `accept-new` are probably more secure choices than `no`. In the end, it's up to the user to decide what kind of security requirements they have and which default setting they want.
I'm not sure whether this is a good idea. Maybe you should allow the user to choose between `yes`, `no`, `ask` and `accept-new` (with the default being `accept-new` or `ask`).
`return migs != 0` is equivalent ot lines 380 to 382.
Use `==` to compare booleans. The `is` test should *only* be used when you really want to compare identities of objects! Finally, there's no need to compare a boolean to `True` or `False` explicitly; simply write `elif self._has_migs(local):`.
Don't you simply want an `else:` here? Or do you explicitly want to have the case that you call `self._has_migs(local)` twice in a row (without a sleep inbetween), and once it returns `False` and then `True`? Otherwise this `elif` makes no sense.
From the way the docker modules currently operate, it probably makes sense to add a `AnsibleDockerSwarmClient` to `module_utils/docker_swarm.py`, which extends `AnsibleDockerClient`. Then `docker_swarm`, `docker_swarm_facts`, `docker_node` and `docker_node_facts` could use `AnsibleDockerSwarmClient` instead of using `AnsibleDockerClient` directly.
Same function in `docker_swarm` module. It will be better to create a new module `docker_swarm_common` to assemble common code.
I think you can simply delete this and the next line. `namespace_tx` and `namespace_rx` are already `int`s at this point.
https://stackoverflow.com/a/2239753 ```suggestion if consecutive_good == self.module.params[ ```
Ah, I didn't knew there was a template :) Well, in that case, keep it. It doesn't really hurt.
Same function in `docker_swarm` module. It will be better to create a new module `docker_swarm_common` to assemble common code.
If this docstring is for the `_cluster_stable()` method, it must be *inside* that method.
Shouldn't this be `return isinstance(x, collections.Iterator)` instead? Actually we should replace all reference to this function by a `collection.Iterator` instance check.
Same question for dropping lambda here as well.
Rather than use a `lamdba` here, it would be better to use a generator expression for clarity ```suggestion lines = line.split('::::')[1].split(';') tmp = dict(x.split('=') for x in lines) ```
IMO we could simplify this with using `dict`, e.g. ```python def choices(self, changelist): choices = { None: _('All'), True: _('Yes'), False: _('No'), **{lookup: title for lookup, title in self.field.flatchoices}, } for lookup, title in choices.items(): lookup_val = str(int(lookup)) if lookup is not None else lookup yield { 'selected': self.lookup_val == lookup_val and not self.lookup_val2, 'query_string': changelist.get_query_string({self.lookup_kwarg: lookup_val}, [self.lookup_kwarg2]), 'display': title, } ... ```
I needed to add a `set_source_expressions` method which sets `self.subquery = source_expressions` here to get relabelling to work.
We should respect also description for `None` (test is required).
```suggestion return '-' + value if neg else int(value) ```
I had to change this to `template_params['subquery'], sql_params = self.subquery.query.get_compiler(connection=connection).as_sql()` in order to compile correctly when using a database other than the `DEFAULT_DB_ALIAS`.
As discussed on IRC: no.
```suggestion return '-' + value if neg else value ```
Shouldn't this be `return isinstance(x, collections.Iterator)` instead? Actually we should replace all reference to this function by a `collection.Iterator` instance check.
I needed to add a `set_source_expressions` method which sets `self.subquery = source_expressions` here to get relabelling to work.
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
As discussed on IRC: no.
I think we can use `in cls` instead of `in cls.__members__.values()` (here and in `names`).
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
I would use `choices` in `labels` and `values` to simplify implementation, e.g. ```python @property def labels(cls): return [label for _, label in cls.choices] @property def values(cls): return [value for value, _ in cls.choices] ```
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Use of the mixin isn't ideal here since there a lot of unrelated tests which aren't affected. If we have some other deprecation that affects these tests, we might miss updating them.
This version downgrade is not acceptable. This module can call update_stack(), which is was added in 1.8.0, so you'd be breaking others by allowing an older version. Please don't change required versions.
Oh wow, I *totally* misread this version as a LOWER version. I should not review before at least 2 cups of coffee! Apologies. Yes, upgrading the version for a new feature is fine. However, we should only require the newer version if a tag is specified. Users not specifying a tag shouldn't be required to upgrade shade.
``` msg = '...' with self.assertRaisesMessage(NotSupportedError, msg): ```
```suggestion msg = ... with self.assertRaisesMessage(ValueError, msg): ```
Couple of typos: remporary & copites
Why are these in a separate block when compared to `offsets-for-times-supported`, etc.
IMO we could simplify this with using `dict`, e.g. ```python def choices(self, changelist): choices = { None: _('All'), True: _('Yes'), False: _('No'), **{lookup: title for lookup, title in self.field.flatchoices}, } for lookup, title in choices.items(): lookup_val = str(int(lookup)) if lookup is not None else lookup yield { 'selected': self.lookup_val == lookup_val and not self.lookup_val2, 'query_string': changelist.get_query_string({self.lookup_kwarg: lookup_val}, [self.lookup_kwarg2]), 'display': title, } ... ```
We should respect also description for `None` (test is required).
The other formats.py seem to use `'\xa0'`
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
include trailing ,
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
include trailing ,
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Shouldn't this be `return isinstance(x, collections.Iterator)` instead? Actually we should replace all reference to this function by a `collection.Iterator` instance check.
I needed to add a `set_source_expressions` method which sets `self.subquery = source_expressions` here to get relabelling to work.
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
As discussed on IRC: no.
I think we can use `in cls` instead of `in cls.__members__.values()` (here and in `names`).
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
I would use `choices` in `labels` and `values` to simplify implementation, e.g. ```python @property def labels(cls): return [label for _, label in cls.choices] @property def values(cls): return [value for value, _ in cls.choices] ```
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
include trailing ,
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
There is no need to provide a value for `max_pages_num` - I think it should be calculated automatically: ```python (self.on_each_side + self.on_ends) * 2 ``` Otherwise developers can provide values that are incompatible with each other...
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
This seems problematic. What happens if you do this: ``` reused_f = F('pk') qs1 = Model1.objects.filter(pk=reused_f) qs2 = Model2.objects.filter(pk=reused_f) ``` By my reading the reused_f will be first prepared for qs1, getting Model1.pk as col. Then the same object is reused and prepared for qs2, this time the reused_f's col is overwritten with Model2.pk. The ExpressionNodes must be either stateless, or they must be copied on prepare(). Doing the latter will be easier. A basic copy method could do the following: ``` def copy(self): copy = Empty() copy.__class__ = self.__class__ copy.__dict__ = self.__dict__.copy() return copy ```
Yep, simply ignoring all other options when `state == 'absent'` is the easiest solution (that's what most other modules do, too).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
This should not result in a warning, but simply result in `changed == False`.
I would expect a module to issue warnings only for something which the user needs to be warned about. If the user wants something to be absent and it already is absent, that's not something I would expect a warning about. You also don't issue a warning if the user has `state == 'present'` and the sequence is already there.
Why not also do this when `module.check_mode` is `True`? Then the check mode behavior is closer to the real thing.
Historic moment! I don't see a reason why we shouldn't use them.
Is it too late to move the conversion of `auto` to an integer to a post-processing step (e.g. your `get_num_test_processes()` function)? I feel like the `parallel_type` function's job here should only be to check that the value equals `auto` if the value is a string, but not to apply the environment-dependent business logic to convert `auto` to a number. (I also see that `get_num_test_processes()` is already calling `multiprocessing.cpu_count()`, so there may be some duplication of logic with the way things are currently structured.)
version added requires quotes, otherwise it will be processed as number
this probably needs updating to ansible.legacy.X see constants.py for function that deals with the multiple possible names
we probably want to move this 'adhoc list' into constants.py anyways
This doesn't look right. This basically allows Python 3.0.1+, not 3.0.0+ as you probably intended. But even with that, it's best to just compare the major version and use "py2 or not py2" because py3+ also means py4+. OTOH we vendor `six` which contains proper constants already. So just import that: ```python from ansible.module_utils import six if six.PY2: ... else: ... ```
I think it's enough to check `stderr`, e.g. ```python with self.assertRaises(SystemExit), captured_stderr() as stderr: self.get_parser().parse_args(['--parallel', 'unaccepted']) msg = "argument --parallel: 'unaccepted' is not an integer or the string 'auto" self.assertIn(msg, stderr.getvalue()) ```
I think you can avoid most of the boiler plate here by using the pool as a context manager. ```suggestion with multiprocessing.Pool(1) as pool: is_open = pool.apply(connection_is_open) ```
We should also assert that the connection in the parent/current process remains usable. ```suggestion self.assertIs(connection.is_usable(), True) ```
We usually prefer `assertIs` here as it will also catch issues with an invalid return type ```suggestion self.assertIs(is_open, False) ```
Do we need to define `async_generator` in `__aiter__`? ```python async def _async_generator(self): # Generators don't actually start running until the first time you call # next() on them, so we make the generator object in the async thread # and then repeatedly dispatch to it in a sync thread. sync_generator = self.__iter__() # So, you can't raise StopIteration up through sync_to_async as # too much of the Python async system was originally built on # generators. Instead, we capture it inside a custom sync function # and change it to an exception we *can* propagate. # We should probably move this logic inside asgiref at some point. def custom_next(gen): try: return next(gen) except StopIteration: raise StopAsyncIteration() while True: try: yield await sync_to_async(custom_next)(sync_generator) except StopAsyncIteration: return # Note: __aiter__ is a *synchronous* method that has to then return an # *asynchronous* iterator/generator. Thus, we nest an async generator # inside it. # Also note: This is a generic iterable converter for now, and is going to # suffer a performance penalty on large sets of items due to the cost of # crossing over the sync barrier each time. We should write custom ones # for each Iterable subclass, but we'll need some work in Compiler first. def __aiter__(self): return self._async_generator() ```
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
I feel like you're just testing argparse here, and this test can be removed. We don't test parsing any other arguments, since we can assume argparse works as advertised.
Maybe: ```suggestion state.pop('_stdout_buffer', None) state.pop('_stderr_buffer', None) state.pop('_original_stdout', None) state.pop('_original_stderr', None) ```
Missing full stop.
Missing full stop.
+ `Only one of I(name), I(id) or I(content) can be set.`
Full stop, rather than comma at the end.
Should become parameter `password` (with a backward-compatible alias). (See #20160 and #25398)
I would still leave one example without the implicit `state: present`
There is no need to provide a value for `max_pages_num` - I think it should be calculated automatically: ```python (self.on_each_side + self.on_ends) * 2 ``` Otherwise developers can provide values that are incompatible with each other...
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
This seems problematic. What happens if you do this: ``` reused_f = F('pk') qs1 = Model1.objects.filter(pk=reused_f) qs2 = Model2.objects.filter(pk=reused_f) ``` By my reading the reused_f will be first prepared for qs1, getting Model1.pk as col. Then the same object is reused and prepared for qs2, this time the reused_f's col is overwritten with Model2.pk. The ExpressionNodes must be either stateless, or they must be copied on prepare(). Doing the latter will be easier. A basic copy method could do the following: ``` def copy(self): copy = Empty() copy.__class__ = self.__class__ copy.__dict__ = self.__dict__.copy() return copy ```
Yep, simply ignoring all other options when `state == 'absent'` is the easiest solution (that's what most other modules do, too).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
This should not result in a warning, but simply result in `changed == False`.
I would expect a module to issue warnings only for something which the user needs to be warned about. If the user wants something to be absent and it already is absent, that's not something I would expect a warning about. You also don't issue a warning if the user has `state == 'present'` and the sequence is already there.
Why not also do this when `module.check_mode` is `True`? Then the check mode behavior is closer to the real thing.
`json_dumps_params` should be after `safe` (reordering keywords could be backwards incompatible if passing them by argument).
Don't use a mutable default: `{}`. Should default to `None` and then add : ``` if json_dumps_params is None: json_dumps_params={} ```
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
Do we need to define `async_generator` in `__aiter__`? ```python async def _async_generator(self): # Generators don't actually start running until the first time you call # next() on them, so we make the generator object in the async thread # and then repeatedly dispatch to it in a sync thread. sync_generator = self.__iter__() # So, you can't raise StopIteration up through sync_to_async as # too much of the Python async system was originally built on # generators. Instead, we capture it inside a custom sync function # and change it to an exception we *can* propagate. # We should probably move this logic inside asgiref at some point. def custom_next(gen): try: return next(gen) except StopIteration: raise StopAsyncIteration() while True: try: yield await sync_to_async(custom_next)(sync_generator) except StopAsyncIteration: return # Note: __aiter__ is a *synchronous* method that has to then return an # *asynchronous* iterator/generator. Thus, we nest an async generator # inside it. # Also note: This is a generic iterable converter for now, and is going to # suffer a performance penalty on large sets of items due to the cost of # crossing over the sync barrier each time. We should write custom ones # for each Iterable subclass, but we'll need some work in Compiler first. def __aiter__(self): return self._async_generator() ```
Does this need to be a separate method? Seems unnecessary to me.
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
immediatelly -> immediately
This seems problematic. What happens if you do this: ``` reused_f = F('pk') qs1 = Model1.objects.filter(pk=reused_f) qs2 = Model2.objects.filter(pk=reused_f) ``` By my reading the reused_f will be first prepared for qs1, getting Model1.pk as col. Then the same object is reused and prepared for qs2, this time the reused_f's col is overwritten with Model2.pk. The ExpressionNodes must be either stateless, or they must be copied on prepare(). Doing the latter will be easier. A basic copy method could do the following: ``` def copy(self): copy = Empty() copy.__class__ = self.__class__ copy.__dict__ = self.__dict__.copy() return copy ```
Not sure it makes a difference but before it looks like we got `form=None` in the context.
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
A little confusing to set `local_temp_dir = "/tmp"` since it in theory should never be set to `/tmp` I think this line is actually not necessary since `local_temp_dir` can be set inside the try block, and be available to the surrounding scope
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
Maybe this copy block should be wrapped in `try/except/finally` so you can be sure to clean up temp files even if a copy phase fails. Then probably raise exception or rethrow if there was a problem copying Also, for the cleanup in the finally block, consider `shutil.rmtree(local_temp_dir, ignore_errors=True)`
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
```suggestion Test that the returned value for timezone consists of only uppercase ```
The task name is incorrect (copy-paste error). Please, consider introducing a proper task name: ```yaml - name: gather the time at the end of the operation ```
Please use a single quote.
Use another lookup instead of `epoch` e.g. `second`.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
I don't see any need for this attribute.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
Can we display a similar message with the test results? It can be helpful when you have plenty of errors, .e.g. ``` ... FAILED (errors=206, skipped=1094, expected failures=4) Used shuffle seed: 556738431 (generated) ```
Might want to join both branches now that they are mostly similar. ```python select_list = [] cols = self.query.default_cols or self.query.select for col in cols: select_list.append(select_idx) select.append((col, None)) select_idx += 1 klass_info = { 'model': self.query.model, 'select_fields': select_list, } ```
You have some unmerged lines here
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
``` for i, video_url in enumerate(video_urls): ```
I think [`_extract_m3u8_formats`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L847) does the work better. If the existing method does not fit the need, feel free to modify it.
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
Missing `=dict` on this and the next few lines
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
Please rewrite as ``` if __name__ == '__main__': main() ```
For integers please: use `type='int` remove the `isdigit` check from `check_params`
I think a brief sentence is worthy. Nothing is worse than trying something only to figure out the target API on the server isn't compatible. We should try to remember to ask this question of all new modules and substantial changes.
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
missing space between `,` and `and`
```python if step_size is not None: self.validators.append(validators.StepValueValidator(step_size)) ``` Use `django.core.validators.MinValueValidator` as a blueprint and create a `StepValueValidator`. Put the validation code from `to_python` in there.
Have a look at the `IntegerField` how it's done there. This has the advantage, that your `StepSizeValidator` can be reused. And it shall be reused, because `step` can also be applied to an `IntegerField`.
`items = value.split(self.delimiter) if value else []` is slightly faster.
What if the prepared value contains unicode? (As I said before, you should use a list comprehension instead of a generator, it'll be about 2 times faster here)
~~A list comprehension would be a little faster.~~ EDIT: Forgive me, I misread the code.
~~Maybe a list comprehension here too.~~
That's why I wrote "maybe" ;)
consider assertRaisesMessage to make the test a bit more specific.
Use hanging indentation (the same in the second test).
Add trailing comma.
Don't mix unrelated changed in single PR.
This regex does not make any sense.
Remove all pointless changes.
I don't think the default is string. The default is to use whatever YAML makes of it IMO (that's the only backward compatible way to add type-support when we did). This also makes it possible to have a parameter that accepts both e.g. a list or dict and let the module figure it out (like it used to be in the old days ;-)) There are modules that use YAML as input, so in this case it doesn't matter whether it is a string, dict or list. (e.g. the xml module) and there is no type-indication for those parameters.
I prefer explicit types for all the parameters (unless they are multi-type parameters)
Mea culpa. I told them to take the defaults out for brevity. I'll keep your preference in mind in the future.
@dagwieers @s-hertel The default **is** str. It changed (from what is now type=raw) a while ago, maybe 2.0 or 2.1. One of the major drivers for the change was that while yaml has types, k=v parsing does not. things like ports and timeouts were then getting to the modules as ints if they were specificed via yaml or strings if they were specified via k=v parsing. There are a few parameters in a few modules (for instance, the infamous file mode parameter) which use type=raw so that the module can accept different types of values for the parameter. We try not to use that in modules if we can get away with it but there are a few places where it's needed.
I prefer that the module would check the connection itself as well, without actually sending the message. If the API supports stub messages (or empty messages?), use that. Otherwise just test the authentication/connection some other way.
I think that should be changed=True, since check mode is used to determine if a change will occur if run without check mode.
type='str' is the default so I think you can remove those.
I think we should test output for different scenarios not only for basic one i.e. _"John Smith"_ , e.g. ```python @classmethod def setUpTestData(cls): cls.john = Author.objects.create(name='John Smith', alias='smithj') cls.elena = Author.objects.create(name='Ã‰lena Jordan', alias='elena') cls.python = Author.objects.create(name='ãƒ‘ã‚¤ã‚½ãƒ³') def test_basic(self): authors = Author.objects.annotate(backward=Reverse('name')).order_by('name') self.assertQuerysetEqual( authors, [ ('John Smith', 'htimS nhoJ'), ('Ã‰lena Jordan', 'nadroJ anelÃ‰'), ('ãƒ‘ã‚¤ã‚½ãƒ³', 'ãƒ³ã‚½ã‚¤ãƒ‘'), ], lambda a: (a.name, a.backward) ) ```
Put the } on the next line and add a trailing comma on this line. That's our convention to ease later adding more items to a dictionary, tuple, etc., if needed.
These assertions are redundant with tests where `qs1.intersection(qs2).exists()` is `False`.
I'd use rename_forwards/backwards for consistency with other methods like database_forwards.
It's not required here. It was used in f179113e6cbc8ba0a8d4e87e1d4410fb61d63e75 because we were handling a possible `IntegrityError`.
chop trailing ", "
I guess some tests might be needed for the router stuff.
use a single line or use hanging indent (we avoid non-4 space indents)
These assertions are redundant with tests where `qs1.difference(qs2).exists()` is `True`.
I guess these could be merged by doing a ```python self.assertEqual( qstr.count("LIMIT 1"), 3 if connection.features.supports_limiting_in_compound else 1 ) ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
put closing parenthesis on the next line
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
It is usually a good idea to prepend some text so that it is easier to find out where an error was raised (in case it's not clear from the error message).
Can it be simpler? E.g. without nic_device/device_type ```python for device in vm.config.hardware.device: if device.macAddress == mac: return device ```
Maybe wrapping mac comparison with try/except/pass will work on nonexistent attribute? Just to avoid additional iteration, though I still not fully understand how often are there cases when device with certain device type won't have macAddress attribute.
Please don't use lists for tracking differences, but `DifferenceTracker`. That produces a much better output.
What if `connected` attribute is set to `false` ? It's ignored!
I think we should test output for different scenarios not only for basic one i.e. _"John Smith"_ , e.g. ```python @classmethod def setUpTestData(cls): cls.john = Author.objects.create(name='John Smith', alias='smithj') cls.elena = Author.objects.create(name='Ã‰lena Jordan', alias='elena') cls.python = Author.objects.create(name='ãƒ‘ã‚¤ã‚½ãƒ³') def test_basic(self): authors = Author.objects.annotate(backward=Reverse('name')).order_by('name') self.assertQuerysetEqual( authors, [ ('John Smith', 'htimS nhoJ'), ('Ã‰lena Jordan', 'nadroJ anelÃ‰'), ('ãƒ‘ã‚¤ã‚½ãƒ³', 'ãƒ³ã‚½ã‚¤ãƒ‘'), ], lambda a: (a.name, a.backward) ) ```
Put the } on the next line and add a trailing comma on this line. That's our convention to ease later adding more items to a dictionary, tuple, etc., if needed.
These assertions are redundant with tests where `qs1.intersection(qs2).exists()` is `False`.
I'd use rename_forwards/backwards for consistency with other methods like database_forwards.
It's not required here. It was used in f179113e6cbc8ba0a8d4e87e1d4410fb61d63e75 because we were handling a possible `IntegrityError`.
chop trailing ", "
I guess some tests might be needed for the router stuff.
use a single line or use hanging indent (we avoid non-4 space indents)
These assertions are redundant with tests where `qs1.difference(qs2).exists()` is `True`.
I guess these could be merged by doing a ```python self.assertEqual( qstr.count("LIMIT 1"), 3 if connection.features.supports_limiting_in_compound else 1 ) ```
I think this might be important to have up front but I'm paranoid about filling up memory because I've been paged too many times for things like this. Some kind of numeric limit is good enough for me though. Like "only 10,000 `ByteSequenceLeafNode`s are allowed in the entire tree". Or something.
I see now. I think it is worth mentioning that this is controlled by the circuit breaker. Maybe right after the big about English not having that many variations.
It looks like you have proper ram usage stuff. Maybe it'd be simpler to refuse to expand the tree if it'd put the `bytesAllocated` above a certain size.
Well, basically that's the problem of these people not us. We don't care whether one can read regexp or not. Moreover most likely next time this code is read by someone is when extractor breaks due to layout change. Chances are this snippet is already irrelevant by that time.
Use `(?i)` in regex itself if you want case insensitivity.
That's simulated. No such URLs is seen in the wild so far and no one will ever intentionally upper case some part of it.
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
Here's something wrong. If you want to continue a line, you shouldn't start it with a dash.
Here's something wrong. If you want to continue a line, you shouldn't start it with a dash.
Here's something wrong. If you want to continue a line, you shouldn't start it with a dash.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Chop blank line (many blank lines in these tests are unnecessary).
no blank line needed
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
Do we need to call `list(fields)` here? :thinking:
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
This one could do with assigning to a variable: ```suggestion path = base_path / f self.assertTrue(path.exists()) with path.open() as fh: ```
```suggestion files = sorted(self.storage_dir.joinpath("dotted.path").iterdir()) self.assertFalse(self.storage_dir.joinpath("dotted_.path").exists()) ```
```suggestion files = sorted(self.storage_dir.joinpath("dotted.path").iterdir()) self.assertFalse(self.storage_dir.joinpath("dotted_.path").exists()) ```
We want to save file with the `symlink.txt` name so this assertion doesn't have much value IMO. I would check that the new file exists.
`try/finally` should be a bit smaller. If there's an exception before `form.save()` the photo won't exist so there's no need to cleanup. I'll make this change and merge it.
```suggestion self.assertTrue(self.temp_dir.joinpath(name).exists()) ```
Much better IMO, I haven't managed to fail it but let's see what @aaugustin thinks. I didn't find any other usages of `get_app_paths()`. Edit: I probably should have posted that globally since it's not directly connected to this line of code.
```suggestion self.assertIs(self.temp_dir.joinpath(f_name).exists(), True) ```
The use of `os.sep` doesn't make sense as this is normalized away by `pathlib`: ```suggestion template_path = custom_templates_dir / "project_template" ```
I learned recently that you can use actual separate literals to improve readability: ```suggestion @pytest.mark.parametrize(['url', 'expected'], [ ```
We want to save file with the `symlink.txt` name so this assertion doesn't have much value IMO. I would check that the new file exists.
> Shall I also remove `test_target_model_is_unsaved_save` and `test_target_model_is_unsaved_bulk_create`? Yes, they are redundant.
It looks like a duplicate of `test_save_unsaved_instance_on_generic_foreign_key` :thinking:
Much better IMO, I haven't managed to fail it but let's see what @aaugustin thinks. I didn't find any other usages of `get_app_paths()`. Edit: I probably should have posted that globally since it's not directly connected to this line of code.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
```suggestion self.assertIs(self.temp_dir.joinpath(f_name).exists(), True) ```
Prefer this indent style: ``` response = self.client.post(reverse('...'), { .... }) ```
`assertNotContains` is a bit fragile; instead I'd like if you could check `response.context` variables.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
This should set self._connected = True (not really needed right now but could have ramifications later) and also use display to print a message. For instance, this is what the docker connect plugin does: ``` def _connect(self, port=None): """ Connect to the container. Nothing to do """ super(Connection, self)._connect() if not self._connected: display.vvv(u"ESTABLISH DOCKER CONNECTION FOR USER: {0}".format( self.actual_user or '?'), host=self._play_context.remote_addr ) self._connected = True ```
This could be moved inside the conditional that follows to avoid checking when there are no vendored modules.
This looks to be incorrect, there should be if not os.path.exists() protection, or better yet, have _createdir() do (very pseudocodish) try: makedirs(); except: if already-exists: pass; else raise EDIT: Sorry, was reading the old version of _createdir()...
An example of using the `mocker` fixture. ```suggestion def mock_NamedTemporaryFile(mocker, **args): mock_ntf = mocker.MagicMock() mock_ntf.write = mocker.MagicMock() mock_ntf.close = mocker.MagicMock() ```
Sure, a separate PR sounds good.
you can move it to before `if` as just `docs = {}` line, this should read better.
When using pytest, create top-level functions without using a class.
Again, error handling changed.
Perhaps: ```suggestion def esxi_version_at_least(self, version): """ Check that the ESXi Host is at least a specific version number. Inputs: - version (tuple): a version tuple, for example (6, 7, 1) Returns: bool """ ``` Suggest moving into module_utils/vmware.py and providing a unit test.
```suggestion except OSError as err: ```
The usual pattern is ``` try: import IPython except ImportError: IPython = None ``` No need for an extra variable.
also, this should not be by default as it changes current operations and might surprise users that expect it to fail when the path is 'polluted'.
Sure. We can address this in a follow up PR.
This could be moved inside the conditional that follows to avoid checking when there are no vendored modules.
This looks to be incorrect, there should be if not os.path.exists() protection, or better yet, have _createdir() do (very pseudocodish) try: makedirs(); except: if already-exists: pass; else raise EDIT: Sorry, was reading the old version of _createdir()...
An example of using the `mocker` fixture. ```suggestion def mock_NamedTemporaryFile(mocker, **args): mock_ntf = mocker.MagicMock() mock_ntf.write = mocker.MagicMock() mock_ntf.close = mocker.MagicMock() ```
I'd sort the set when showing the error message, to guarantee consistent results for the same package list.
This is only ever called once. Do we need the default? (Same with SQL version)
Again, error handling changed.
This is only used once. Can we move it back to the `color` module? (That way `termcolors` is still only ever used by `color`)
It looks like a duplicate of `test_save_unsaved_instance_on_generic_foreign_key` :thinking:
> Shall I also remove `test_target_model_is_unsaved_save` and `test_target_model_is_unsaved_bulk_create`? Yes, they are redundant.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
Do we need to call `list(fields)` here? :thinking:
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
`assertNotContains` is a bit fragile; instead I'd like if you could check `response.context` variables.
We want to save file with the `symlink.txt` name so this assertion doesn't have much value IMO. I would check that the new file exists.
> Shall I also remove `test_target_model_is_unsaved_save` and `test_target_model_is_unsaved_bulk_create`? Yes, they are redundant.
It looks like a duplicate of `test_save_unsaved_instance_on_generic_foreign_key` :thinking:
```suggestion self.assertIs(self.temp_dir.joinpath(f_name).exists(), True) ```
please include the trailing comma on the last item in a dictionary so if more items are added we don't have to modify this line again
chop newline for consistency with other tests
chop first comma
Prefer this indent style: ``` response = self.client.post(reverse('...'), { .... }) ```
`assertNotContains` is a bit fragile; instead I'd like if you could check `response.context` variables.
you can use `a` here again.
Notice that, conceptually, the `set(a) | set(b)` operation is different than `a + b`. The former performs set union meaning that duplicates are removed, whereas the latter is a simple concatenation with the possibility for the same item to appear twice. If these are considered "set operations", then the else part should be replaced by something like: ``` c = a + filter(lambda x: x not in a, b) ```
Chop `Ensure that`.
I would chop blank lines in this test.
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Thanks - I've replaced all uses of `filter` with comprehensions.
This will fail on python3 for a similar reason as the conditional check noted above. filter returns a generator. The generator can't be indexed. Use a list comprehension instead: ``` python block = [b for b in blocks if b.base_ip == base_ip] ```
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
extract mandatory information before optional information.
`enumerate` on for range.
Same as above, I'd revert to the simpler version. The main idea behind `decode` for me is a consistent interface where it makes sense, we don't have to bend backwards just to use it.
Sorry, I had a typo in https://github.com/django/django/pull/12675#discussion_r418965162 -- what I ment to say was: keep `work factor` in safe summary (since it's for display and already translated) and `work_factor` in `decode`. This will also require less changes in the tests.
kind of, i will try to abstract it further later(the `source` format also shares a bit code with this part).
the process to extract the format and the thumbnail is similar, so these part needs to be abstracted to remove duplication.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
When there is no `topicTitle`, `categories` will be `[None]` that does not make any sense.
Don't capture groups you don't use. Unused captured group.
Useless with timestamp available.
add trailing comma
include trailing ,
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
Good catch, I will remove it before final squash.
I would use a list comprehension rather than `list(map())`
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
excellent handling of congestion control
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
could switch to single quotes for consistency
`always_text` is gone.
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
add trailing comma
add trailing comma
include trailing ,
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
I would use a list comprehension rather than `list(map())`
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
add trailing comma
include trailing ,
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
I would use a list comprehension rather than `list(map())`
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Any problem with: ``` @property def media(self): ```
I think `name.rsplit('-', 1)[-1]` is easier to read.
This will probably break for `zh-CN` and others because Django returns lower cased language names. I do not have a good idea on how to fix that though.
We could lowercase the vendored files, that would help at least for the `zh-*` variants.
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
include trailing ,
add trailing comma
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
I would use a list comprehension rather than `list(map())`
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Any problem with: ``` @property def media(self): ```
I think `name.rsplit('-', 1)[-1]` is easier to read.
We could lowercase the vendored files, that would help at least for the `zh-*` variants.
This will probably break for `zh-CN` and others because Django returns lower cased language names. I do not have a good idea on how to fix that though.
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
add trailing comma
We can start using f-string ```suggestion raise base.DeserializationError( f"Invalid model identifier: {model_identifier!r}" ) ```
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
`field_names` are used only when `self.ignorenonexistent is True`, so we can optimize this part, e.g.: ```python field_names = set() if self.ignorenonexistent: if Model not in field_names_cache: self.field_names_cache[Model] = {f.name for f in Model._meta.get_fields()} field_names = self.field_names_cache[Model] ```
add trailing comma
include trailing ,
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
I would use a list comprehension rather than `list(map())`
I needed to add a `set_source_expressions` method which sets `self.subquery = source_expressions` here to get relabelling to work.
Delete this blank line.
Yeah that's what I suspected too. Stupid SQL.
colors should all be configurable
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
Usage of `template=None, **extra_context` params seems untested. Not sure if it's important.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
IIRC get_cols() was renamed to get_group_by_cols() some time ago - no need to have both.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
I realize this predates your PR, but why is 'unused' not used? It appears to be the target field provided by the SQLUpdateCompiler during as_sql(). This seems like a really good spot to allow a backend to hook in and CAST the expression to a type that is allowed by the target column.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
I needed to add a `set_source_expressions` method which sets `self.subquery = source_expressions` here to get relabelling to work.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Delete this blank line.
You can likely just resolve the F() expression using resolve_expression() once you have removed the outerq prefix from the F obj.
Usage of `template=None, **extra_context` params seems untested. Not sure if it's important.
IIRC get_cols() was renamed to get_group_by_cols() some time ago - no need to have both.
I realize this predates your PR, but why is 'unused' not used? It appears to be the target field provided by the SQLUpdateCompiler during as_sql(). This seems like a really good spot to allow a backend to hook in and CAST the expression to a type that is allowed by the target column.
colors should all be configurable
Yeah that's what I suspected too. Stupid SQL.
Shouldn't this be `return isinstance(x, collections.Iterator)` instead? Actually we should replace all reference to this function by a `collection.Iterator` instance check.
In theory a locale dependant pattern could be provided and `locale.setlocale` be called but this is really unlikely.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
use x.items() here, no need for the iteritems import
use tags.items() here, no need for iteritems import
In theory a locale dependant pattern could be provided and `locale.setlocale` be called but this is really unlikely.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
Both [`exit_json`](https://github.com/ansible/ansible/blob/ebd08d2a01d96d41f8bc1ccf27bc1bbf91060a44/lib/ansible/module_utils/basic.py#L2254) and [`fail_json`](https://github.com/ansible/ansible/blob/ebd08d2a01d96d41f8bc1ccf27bc1bbf91060a44/lib/ansible/module_utils/basic.py#L2261) methods call `sys.exit`: the old code checked that `fail_json` has been called and would have failed if `exit_json` were called in `pip.main`.
It's already stripped: ```suggestion (PODMAN_OUTPUT, ''), ```
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
It probably makes sense to test that the exception reason also matches expectations
It's a standard practice to use separate args for different params. Also, autogenerated param ids aren't very readable when they are complex so in such cases it's better to assign them meaningful names (they are displayed in the pytest report): ```suggestion @pytest.mark.parametrize( ('returned_items_count', 'patched_dc_stdout'), ( (3, (DOCKER_OUTPUT_MULTIPLE, '')), (2, (PODMAN_OUTPUT, '')), (0, ('', '')), ), ids=('docker JSONL', 'podman JSON sequence', 'empty output'), ) def test_docker_images(docker_images, mocker, returned_items_count, patched_dc_stdout): mocker.patch( 'ansible_test._internal.docker_util.docker_command', return_value=patched_dc_stdout) ret = docker_images('', 'quay.io/ansible/centos7-test-container') assert len(ret) == returned_items_count ```
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
(Similarly, mark any string that you are going to call decode on with as a b"string".)
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
Match the error message
In this case, I think a ternary is more complicated to read than: ``` if srid == -1: srid = None ```
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
Is `plan` meant as the second argument? Looks like we're missing a test for this branch.
I don't like how this entire if block looks. Is `plan` allowed to be `None` considering the check/assignment on line 73 and 74? I'd prefer to call out that it can not be forwards and backwards by doing something like: ``` if all_forwards == all_backards: raise.. # this may not be correct if `plan` can be `None` elif all_forwards: migrate_forwards() else: migrate_backwards() ``` I feel this highlights the invariant, and avoids potential problems of misreading the bool combinations in the original code.
That code is marginally longer, but I think it's clearer. I'll leave it up to what you think is best though.
Migrations plans with both forwards and backwards migrations are not supported.
We've been using "Take / apply" verb-style in new docstrings.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
returning `b''` for BinaryField (as original code did) should be required under Python 3, IIRC.
I believe you would need to add a custom `save_form` method to a `ModelAdmin` and somehow incorporate the `change` flag in it -- perhaps modify the form's cleaned_data to assign the field to a model field before save.
This should be a feature flag as CockroachDB (which tries to emulate PostgreSQL in a lot of ways) has the same restriction.
I don't see a need for string interpolation in cases like this.
```suggestion type: list suboptions: ```
So I guess that this is where we need to set the fact in results. This is the only place where we know that we've had to run the facts module in order to retrieve the package manager.
@bcoca corrected me that _execute_module() takes care of delegation so this is fine. After discussion, though, we did decide that we should set the ansible_pkg_mgr fact when we return from this action plugin so that we don't end up having to discover it every time the actin plugin is run on the same host.
bcoca also mentioned that the facts end up being set on the host that's being processed in the task loop, not one that's being delegated_to. So we should only return the ansible_pkg_mgr fact if we are not delegating.
Still need to add the ansible_pkg-mgr fact to results if: * We selected one of yum, yum4, or dnf * And we are not delegating.
When delegating, I'd think that we'd need to run the facts module for the delegated host rather than for the inventory_hostname host....
```suggestion module.fail_json(msg="The following packages are absent in the currently booted rpm-ostree commit: %s" % ' '.join(pkgs)) ```
This doesn't appear to support the use parameter but probably should.
This should be a feature flag as CockroachDB (which tries to emulate PostgreSQL in a lot of ways) has the same restriction.
I don't see a need for string interpolation in cases like this.
```suggestion type: list suboptions: ```
So I guess that this is where we need to set the fact in results. This is the only place where we know that we've had to run the facts module in order to retrieve the package manager.
@bcoca corrected me that _execute_module() takes care of delegation so this is fine. After discussion, though, we did decide that we should set the ansible_pkg_mgr fact when we return from this action plugin so that we don't end up having to discover it every time the actin plugin is run on the same host.
bcoca also mentioned that the facts end up being set on the host that's being processed in the task loop, not one that's being delegated_to. So we should only return the ansible_pkg_mgr fact if we are not delegating.
Still need to add the ansible_pkg-mgr fact to results if: * We selected one of yum, yum4, or dnf * And we are not delegating.
When delegating, I'd think that we'd need to run the facts module for the delegated host rather than for the inventory_hostname host....
```suggestion module.fail_json(msg="The following packages are absent in the currently booted rpm-ostree commit: %s" % ' '.join(pkgs)) ```
This doesn't appear to support the use parameter but probably should.
Maybe you can use `subTest` here, e.g.: ```python for model, pk_pos in ( (Book, -1), # Unmanaged origin model. (Author, 0), # Unmanaged related model. ): with self.subTest(model=model, pk_pos=pk_pos): with mock.patch.object(model._meta, 'managed', False): _, _, grouping = queryset.query.get_compiler(using='default').pre_sql_setup() self.assertEqual(len(grouping), len(model._meta.fields) + 1) self.assertIn(Author._meta.pk.name, grouping[pk_pos][0]) for index, field in enumerate(model._meta.fields): self.assertIn(field.name, grouping[index + pk_pos + 1][0]) assert_queryset_results(queryset) ``` but I'm not convinced that it isn't less readable.
`test_jsonfield` looks good to me (since this is InspectDBTests, _introspection seems redundant).
consider assertRaisesMessage to make the test a bit more specific.
Use `assertIn` instead.
reordering (chop dash)
Use `assertIn` instead.
This is not used only by `get_or_create`.
`mentionned` => `mentioned`
redundant, remove ```suggestion ```
Use the context manager version for better readability: ``` msg = '....' with self.assertRaisesMessage(ImproperlyConfigured, msg): self.set_up_test_model(True) ```
chop "one of" add comma before "or"
put closing parenthesis on the next line
IMO it will me more readable to move `base` outside, also we resolve `F()` expressions twice (line 28 & 29), so maybe, e.g. ```python if isinstance(expression, str): expression = F(expression) if isinstance(expression, F): expression = base.resolve_expression(query=query, simple_col=True) else: expression = expression.resolve_expression(query=query) ``` Moreover I think we don't need to iterate twice over the expressions: ```python for expression, operator in self.expressions: ... expressions.append('%s WITH %s' % (sql % params, operator) return expressions ```
I don't see a need for string interpolation in cases like this.
I think we can use the same check like in `UniqueConstraint`: ``` if not isinstance(condition, (type(None), Q)): raise ValueError('ExclusionConstraint.condition must be a Q instance.') ```
Use `xpath_text` for all `track.find('XXX').text` occurrences. This function provides more information for debugging.
```suggestion type: list suboptions: ```
```suggestion module.fail_json(msg="The following packages are absent in the currently booted rpm-ostree commit: %s" % ' '.join(pkgs)) ```
Maybe `_parse_where()` -> `_get_condition_sql()`, I would return condition and add `WHERE` in `constraint_sql()`, e.g. ```python def _get_condition_sql(self, compiler, schema_editor, query): if self.condition is None: return None where = query.build_where(self.condition) sql, params = where.as_sql(compiler, schema_editor.connection) return sql % tuple(schema_editor.quote_value(p) for p in params) def constraint_sql(self, model, schema_editor): ... condition = self._get_condition_sql(compiler, schema_editor, query) return self.template % { ... 'where': ' WHERE (%s)' % condition if condition else '', } ```
As long as they both equals it shouldn't make a difference. By that I mean you should have a test asserts `Index('foo') == Index(F('foo'))` An alternative could be to defer that to `create_sql`.
This could fit on a single line: `# Subqueries must use a different set of aliases than the outer query.`
avoid "we" to simplify, e.g. "Copy the subquery because it'll be modified."
1 line is okay --- we prefer longer lines up to 119 characters when it helps readability.
Subtests can also be used here.
A trailing comma is preferred.
if datastore already exists
As stated below, this should become: ```python for store in host.datastore: _tmp = { 'name': store.summary.name, 'total': bytes_to_human(store.summary.capacity), 'free': bytes_to_human(store.summary.freeSpace), } facts['ansible_datastore'].append(_tmp) ```
I prefer that the module would check the connection itself as well, without actually sending the message. If the API supports stub messages (or empty messages?), use that. Otherwise just test the authentication/connection some other way.
Instead of managing the SSL context yourself, would it be easier to use `SmartConnectNoSSL` (has been added few years back to PyVmomi).
extra space after ,
Please do not remove this trailing comma, this is our preference with dicts and lists to minimize future diffs when lines are changed.
Is the `sorted()` actually useful here? You're adding the keys to a standard python dict, which is unsorted by nature.
Here you pass the variable `label`. You have never defined it, though (you defined `ch_label` -- maybe you meant that one?). That's the first of the sanity checks which fails. You might already get that output by running `flake8` on your module.
Worth wrapping all these long fail_json calls at the comma before exception - otherwise it goes off the side at PR (I know there are lots of lines that are already too long but let's not add more)
if datastore already exists
extra space after ,
Can you line up `src`, `dest` and `overwrite` please.
`module.params['vm_id_type'] == 'vm_name'` Ã¢Â†Â’ `module.params['vm_id_type'] == 'inventory_path'`
I think that should be changed=True, since check mode is used to determine if a change will occur if run without check mode.
As there is a required_if, this can be removed
1 line is okay --- we prefer longer lines up to 119 characters when it helps readability.
Subtests can also be used here.
if datastore already exists
We don't like these catch-all statements in Ansible. In this case I would expect the library to handle any exceptions caused by connect_to_api. `all_facts()` should be quite safe, if not the smaller pieces that may raise exceptions should be wrapped. `exit_json()` is not expected to raise exceptions. So I'd remove the whole try-block here altogether.
Worth wrapping all these long fail_json calls at the comma before exception - otherwise it goes off the side at PR (I know there are lots of lines that are already too long but let's not add more)
I prefer that the module would check the connection itself as well, without actually sending the message. If the API supports stub messages (or empty messages?), use that. Otherwise just test the authentication/connection some other way.
I think that should be changed=True, since check mode is used to determine if a change will occur if run without check mode.
Instead of managing the SSL context yourself, would it be easier to use `SmartConnectNoSSL` (has been added few years back to PyVmomi).
extra space after ,
As there is a required_if, this can be removed
single line as above
```suggestion self, objs, fields, batch_size, on_conflict=None, update_fields=None, unique_fields=None, ```
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
This is not the most explicit way to chain those lists. I recommend to use `itertools.chain.from_iterable(batch_querysets)`.
```suggestion item, fields=fields, using=self.db, ```
```suggestion self, objs, fields, batch_size, on_conflict=None, update_fields=None, unique_fields=None, ```
remove this assertion? (see 90af278203963e3e3f96e443971cd38a2dad34e4)
Ditto for `[]` â†’ `None` and `ON_CONFLICTS_NONE` â†’ `None`.
Maybe we can close the first group here and verify that the sensors/metrics are no longer registered? A similar check for the sink would be good.
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
`key: write_csv` should be `key: write_files`
Please add a trailing comma: ```suggestion on_conflict=None, update_fields=None, unique_fields=None, ```
Spelling error: directory -> directory. Also should mention that the directory will be created if it doesn't exist (but the path leading up to it won't be created).
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
As Collin said, slightly longer lines are okay rather than awkward wrappings like this.
Yes, I think Django would be obviously broken in such a configuration anyway.
I don't think there's a use case for `settings.AUTHENTICATION_BACKENDS` without any backends that have a `get_user()` method.
It seems appropriate to remove `and callable(backend.get_user)` then.
You can skip the blank lines in these tests as three lines long isn't too difficult too read with spaces.
```suggestion raise InvalidCacheBackendError('Could not find backend {backend!r}: {e}') from e ```
Please use assertRaisesMessage to verify this is the ValueError we expect.
I'd use a name like `assertBackendInSession`.
```suggestion params = self.settings[alias].copy() ```
Nitpick, you can use a list comprehension here instead of calling list on a generator expression.
I would chop blank lines in this test.
Chop `Ensure that`.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
You're checking two separate properties here. This should be in a separate test.
Docstring shouldn't have a trailing space. Docstring sentences should end with a period.
Same here, not following order `(value, expected)`
Guess it's better to use `self.assertGreater(len(para), 0)` instead
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Extractors should extract information of all possible formats, not the specified format only. Instead, `YoutubeDL` class handles format selection.
What's the error indicated by `raw_data['code'] != 'A000000'`? It's better to give an informational message.
I would chop blank lines in this test.
Chop `Ensure that`.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
You're checking two separate properties here. This should be in a separate test.
Docstring shouldn't have a trailing space. Docstring sentences should end with a period.
Same here, not following order `(value, expected)`
Guess it's better to use `self.assertGreater(len(para), 0)` instead
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Extractors should extract information of all possible formats, not the specified format only. Instead, `YoutubeDL` class handles format selection.
What's the error indicated by `raw_data['code'] != 'A000000'`? It's better to give an informational message.
missing space after the second comma (please check code with flake8)
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
Could be nice to fix and test this typo separately.
Can the new tests go in a separate test case? I think that would make things simpler.
Discussed in slack. We can use to_text(e).index('str') and to_text(e).index('tuple') to find where in the error message's string, the types occur. And then use that to decide which is the string and should be sorted after the other one. (The message changed between pyhton-3.5 and python-3.6 for those who are looking at this at home)
I think this is failing in shippable because the error message has changed at some point: ``` >>> 'str' > ('test', 'tsr') Traceback (most recent call last): File "<stdin>", line 1, in <module> TypeError: '>' not supported between instances of 'str' and 'tuple' ``` I recommend checking the types of a and b like this: ``` python if isinstance(a, string_types) and isinstance(b, tuple): return -1 elif isinstance(a, tuple) and isinstance(b, string_types): return 1 raise ```
Use hanging indent: ``` python RangesModel.objects.create( ints=None, dates=(cls.dates[0], cls.dates[3]), timestamps=(cls.timestamps[0], cls.timestamps[3])) ) ```
oh, multi-bucket comparisons are ready already? :)
I'd omit a blank line here.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
comma after tuple
`enumerate` on for range.
Could be nice to fix and test this typo separately.
Can the new tests go in a separate test case? I think that would make things simpler.
Discussed in slack. We can use to_text(e).index('str') and to_text(e).index('tuple') to find where in the error message's string, the types occur. And then use that to decide which is the string and should be sorted after the other one. (The message changed between pyhton-3.5 and python-3.6 for those who are looking at this at home)
I think this is failing in shippable because the error message has changed at some point: ``` >>> 'str' > ('test', 'tsr') Traceback (most recent call last): File "<stdin>", line 1, in <module> TypeError: '>' not supported between instances of 'str' and 'tuple' ``` I recommend checking the types of a and b like this: ``` python if isinstance(a, string_types) and isinstance(b, tuple): return -1 elif isinstance(a, tuple) and isinstance(b, string_types): return 1 raise ```
Use hanging indent: ``` python RangesModel.objects.create( ints=None, dates=(cls.dates[0], cls.dates[3]), timestamps=(cls.timestamps[0], cls.timestamps[3])) ) ```
I'd omit a blank line here.
Actually I think it might be possible to reuse most of `super()._create_index_sql` by using `expressions=[RawSQL(...)]` instead of `columns` to avoid heavy duplication between both methods.
You want to do it the other way around; no adjustments should be required to the base schema adaptor. In order to achieve that you'll want to have the PostGIS `_create_index_sql` method *not* pass `fields` but `expressions` when necessary ```python def _create_index_sql(self, model, *, fields=None, **kwargs): if fields is None or len(fields) != 1 or not hasattr(fields[0], 'geodetic'): return super()._create_index_sql(model, fields=fields, **kwargs) field = fields[0] template = None if field.geom_type == 'RASTER': # For raster fields, wrap index creation SQL statement with ST_ConvexHull. # Indexes on raster columns are based on the convex hull of the raster. template = self.rast_index_wrapper % '%(expressions)s' elif field.dim > 2 and not field.geography: # Use "nd" ops which are fast on multidimensional cases template = "%%(expressions)s %s" % self.geom_index_ops_nd expressions = None if template is not None: fields = None expressions = [Func(Col(field.column), template=template)] using = ' USING %s' % self.geom_index_type return super()._create_index_sql(model, fields=fields, expressions=expressions, using=using) ```
I don't think that a separate ticket is necessary, using `super()._create_index_sql()` will fix described issue.
Test failure is because this is missing `geography=True` on PostGIS.
We can ignore `name`, `path`, and `args`, e.g. `*_, kwargs = field.deconstruct()`
Chop blank line.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Might want to use simple quote here.
Actually I think it might be possible to reuse most of `super()._create_index_sql` by using `expressions=[RawSQL(...)]` instead of `columns` to avoid heavy duplication between both methods.
You want to do it the other way around; no adjustments should be required to the base schema adaptor. In order to achieve that you'll want to have the PostGIS `_create_index_sql` method *not* pass `fields` but `expressions` when necessary ```python def _create_index_sql(self, model, *, fields=None, **kwargs): if fields is None or len(fields) != 1 or not hasattr(fields[0], 'geodetic'): return super()._create_index_sql(model, fields=fields, **kwargs) field = fields[0] template = None if field.geom_type == 'RASTER': # For raster fields, wrap index creation SQL statement with ST_ConvexHull. # Indexes on raster columns are based on the convex hull of the raster. template = self.rast_index_wrapper % '%(expressions)s' elif field.dim > 2 and not field.geography: # Use "nd" ops which are fast on multidimensional cases template = "%%(expressions)s %s" % self.geom_index_ops_nd expressions = None if template is not None: fields = None expressions = [Func(Col(field.column), template=template)] using = ' USING %s' % self.geom_index_type return super()._create_index_sql(model, fields=fields, expressions=expressions, using=using) ```
I don't think that a separate ticket is necessary, using `super()._create_index_sql()` will fix described issue.
Test failure is because this is missing `geography=True` on PostGIS.
We can ignore `name`, `path`, and `args`, e.g. `*_, kwargs = field.deconstruct()`
Chop blank line.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Might want to use simple quote here.
You can drop the `.all()` here.
super nit: the string is slightly malformatted. The script displays: Enter reviewers in the format of "name1 <email1>", "name2 <email2>: Also, Its not clear if I should actually type the quotes when entering reviewers.
Please ignore, my suggestion is invalid syntax.
docstring with example input/output would be really helpful
no blank line needed
longer lines here are okay, we try to avoid non-multiple of 4 indents
I think a list comprehension would be more readable.
both are valid tests, i don't see why you need to eliminate the existing one
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
This is always a tough question, and I'm not sure what's the best solution :) In Ansible, things are usually lower-case. I guess you have to decide what you want in the end :)
How about lower-case? ```suggestion choices: [ 'dns', 'email', 'manual', 'webserver'] ```
This shouldn't be automatically adjusted, just a note saying refs style must be 64.
Add a task ```.yaml - debug: var: result.docker_host_facts ``` And similar tasks after the other examples.
You could add boto3/botocore as requirements.
Shouldn't this be `When I(containers) is C(yes)`? (Same for the others.)
```suggestion description: Can be used to request certificates from ECS, with C(provider=entrust). ```
Yes, so what I am saying is to keep it simple, and if no name was provided (with `state=absent`), delete the path. If a `(default)` name was provided, remove that key (as in unsetting it, just as the registry editor is not showing an unset `(default)' key). So you don't need the additional `delete_key` parameter, and it's actually as one would expect it to work. The `name` parameter was optional.
I would have preferred a more explicit removal of `name=(default)` for the removal of the default key. This is to me more confusing (because whether it is set to `yes` or `no` it will delete a key. So I would get rid of this option altogether.
This is not how it is used, the option used is `--check`.
I don't see a need for string interpolation in cases like this.
As you've noticed, we don't want to do this here because in this case the pkg_mgr is coming from a pre-existing fact.
we should also return if we both delegate executions and delegate_facts
```suggestion type: list suboptions: ```
Still need to add the ansible_pkg-mgr fact to results if: * We selected one of yum, yum4, or dnf * And we are not delegating.
So I guess that this is where we need to set the fact in results. This is the only place where we know that we've had to run the facts module in order to retrieve the package manager.
bcoca also mentioned that the facts end up being set on the host that's being processed in the task loop, not one that's being delegated_to. So we should only return the ansible_pkg_mgr fact if we are not delegating.
@bcoca corrected me that _execute_module() takes care of delegation so this is fine. After discussion, though, we did decide that we should set the ansible_pkg_mgr fact when we return from this action plugin so that we don't end up having to discover it every time the actin plugin is run on the same host.
When delegating, I'd think that we'd need to run the facts module for the delegated host rather than for the inventory_hostname host....
IMHO it's an antipattern when you catch everything wholesale. You mask future bugs this way.
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
I would say `Deploy key has been updated` instead of `should have been updated`
Please use 'msg' for returned messages, this is a standardized return value.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
So removed `required=False` and add `type='str' instead. Do this for the others as well.
Same here, default parameter is **validate_certs**, the aliases should be removed. We also simplify this by *not* adding `required=False`, and always starting with the type. Since every parameter has a type, starting with this adds consistency.
So add `type='str'` here too. And we tend to sort lists if the order is of no importance.
This function is too complex. Could you please factor and avoid: - more than 2 level of condition - inner loop. You can probably break down the function in several small functions.
I feel like it might be a good idea to eventually require that one or both of these be set explicitly, rather than relying on these hardcoded defaults. (I can't completely articulate why, though, so maybe it's not that good an idea.)
returning `b''` for BinaryField (as original code did) should be required under Python 3, IIRC.
with -> width
width, height, and offset
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
comma after tuple
I don't understand why we have methods with a double underscores prefix which are copies from `SessionBase`, e.g. `__hash()`, `__legacy_encode()`, `__legacy_decode()` :thinking:
I assume you meant to say `Unable to decrypt value using KMS`
This function is too complex. Could you please factor and avoid: - more than 2 level of condition - inner loop. You can probably break down the function in several small functions.
I feel like it might be a good idea to eventually require that one or both of these be set explicitly, rather than relying on these hardcoded defaults. (I can't completely articulate why, though, so maybe it's not that good an idea.)
returning `b''` for BinaryField (as original code did) should be required under Python 3, IIRC.
with -> width
width, height, and offset
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
comma after tuple
I don't understand why we have methods with a double underscores prefix which are copies from `SessionBase`, e.g. `__hash()`, `__legacy_encode()`, `__legacy_decode()` :thinking:
I assume you meant to say `Unable to decrypt value using KMS`
This function is too complex. Could you please factor and avoid: - more than 2 level of condition - inner loop. You can probably break down the function in several small functions.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
This can be a single line.
with -> width
width, height, and offset
```suggestion raise AnsibleError('Invalid setting identifier, "%s" is not a string, it is a %s' % (term, type(term))) ```
comma after tuple
```suggestion raise AnsibleError('Unable to use "%s" as a search parameter: %s' % (term, to_native(e))) ```
For the cases where I've wanted to use a nose test generator, I've ended up just making the test class inherit from object. You lose the TestCase methods (various self.assert*) but usually test generators methods have pretty simple assertions so not too much of a issue. To me it is simpler than injecting the support into a TestCase subclass. (an example is https://github.com/alikins/ansible/blob/6e8a3d0c25fdeaa349351c6636b062376c30b0a6/test/units/plugins/lookup/test_ini.py)
This function is too complex. Could you please factor and avoid: - more than 2 level of condition - inner loop. You can probably break down the function in several small functions.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
This can be a single line.
with -> width
width, height, and offset
```suggestion raise AnsibleError('Invalid setting identifier, "%s" is not a string, it is a %s' % (term, type(term))) ```
comma after tuple
```suggestion raise AnsibleError('Unable to use "%s" as a search parameter: %s' % (term, to_native(e))) ```
For the cases where I've wanted to use a nose test generator, I've ended up just making the test class inherit from object. You lose the TestCase methods (various self.assert*) but usually test generators methods have pretty simple assertions so not too much of a issue. To me it is simpler than injecting the support into a TestCase subclass. (an example is https://github.com/alikins/ansible/blob/6e8a3d0c25fdeaa349351c6636b062376c30b0a6/test/units/plugins/lookup/test_ini.py)
```suggestion item, fields=fields, using=self.db, ```
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
Similarly, ```if tc['skip'].get('i')```
Maybe the following will be more explicit: ```python first_author_books = result[0].books.all() self.assertEqual([self.book1], [first_author_books]) self.assertEqual([self.reader], [first_author_books[0].read_by.all()])) ```
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
```suggestion item, fields=fields, using=self.db, ```
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
Maybe the following will be more explicit: ```python first_author_books = result[0].books.all() self.assertEqual([self.book1], [first_author_books]) self.assertEqual([self.reader], [first_author_books[0].read_by.all()])) ```
Shouldn't this be `return isinstance(x, collections.Iterator)` instead? Actually we should replace all reference to this function by a `collection.Iterator` instance check.
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
The code from here until line 224 needs to be indented by one level. Otherwise the module will crash if `params` is not used.
I think you have to do the same for ldap_attr. Anyway, with the commit you added ldap_entry no longer crashes for me (my playbooks no longer use ldap_attr, I vendored the 2.10 ldap_attrs a longer time ago).
Ah. lambda is prettier though :). maybe we can run pep8 with ignore lambda error.
```suggestion msg = ... with self.assertRaisesMessage(ValueError, msg): ```
longer lines here are okay, we try to avoid non-multiple of 4 indents
I think a list comprehension would be more readable.
one more for the single line version
Shouldn't this be `return isinstance(x, collections.Iterator)` instead? Actually we should replace all reference to this function by a `collection.Iterator` instance check.
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
The code from here until line 224 needs to be indented by one level. Otherwise the module will crash if `params` is not used.
I think you have to do the same for ldap_attr. Anyway, with the commit you added ldap_entry no longer crashes for me (my playbooks no longer use ldap_attr, I vendored the 2.10 ldap_attrs a longer time ago).
Ah. lambda is prettier though :). maybe we can run pep8 with ignore lambda error.
```suggestion msg = ... with self.assertRaisesMessage(ValueError, msg): ```
longer lines here are okay, we try to avoid non-multiple of 4 indents
I think a list comprehension would be more readable.
one more for the single line version
`enumerate` on for range.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
This is no longer actual.
This is useless at the end.
Don't capture groups you don't use. Unused captured group.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
Useless with timestamp available.
No brackets needed.
If `video_detail.get('spl')` should be `None`, or something else that can't have a `compat_str` added, this will crash. The extraction would have failed, but it might be better to crash in `_extract_sdn_formats() ` instead. Try (eg) `'%sspl2,3,VOD' % (str_or_none(video_detail.get('spl')) or '', )`. Or make sure it does crash here with `['spl']` instead of `.get(...)`.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
This can be changed to look for only one space now that #57209 is merged. Thanks again for writing tests. Much appreciated.
```suggestion module.exit_json(changed=True,**camel_dict_to_snake_dict(res))) ```
Plz also use `match` arg here
I haven't actually used stepfunctions myself so I'm just going off the docs. AIUI, `ExecutionAlreadyExists` would only be returned on a running execution if the `execution_input` is different, not in a case where the input is the same. As an ansible user who is new to step functions, I feel like if I gave the same input and ran the playbook twice I would expect the 2nd time to be `changed=false` if the first execution is still running. But that doesn't mean that's how AWS users familiar with step functions will expect it.
`client.start_execution` is idempotent but `start_execution` always returns `changed=True` if `action=start`. We probably want to `client.list_executions` to see if the execution is already running before proceeding.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
I thought about this and it is wrong. We need to specify the existing official repo here. So, if we release 2.0.0 we retain the beta release in there. Just remove this line and the `download.elasticsearch.co` is used to sync and all works as expected.
253, can go, this one... ``` print (' export S3_BUCKET_SYNC_FROM="%s"' % (s3_bucket_sync_to)) ```
Please add a docstring explaining this.
Oh, I see `raise_last_exception` also looks like it moved. Maybe it makes sense to do some reordering to keep the diff a bit smaller. Maybe not.
Do we still need kind? For service I think it should always be v1
Maybe add an empty line above this one and give it a name, it seems like the `assert` task is just a parameter of the `openssl_certificate_info` one.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Since we're now targetting python-2.6+ we can just ```import json```
I thought about this and it is wrong. We need to specify the existing official repo here. So, if we release 2.0.0 we retain the beta release in there. Just remove this line and the `download.elasticsearch.co` is used to sync and all works as expected.
253, can go, this one... ``` print (' export S3_BUCKET_SYNC_FROM="%s"' % (s3_bucket_sync_to)) ```
I think we have a problem here. The version that is supposed to be supplied as a parameter should only consist of major.minor version like `2.0` (so that all 2.x version go into the same repository) - this one is `2.0.0-beta1` though.
extra space after `*` needs to be removed here too
extra space after `*` needs to be removed
nit: remove empty line
nit: needs a comma after the `{@link ...}`
I don't think we can do this. Also, I would only mention it, when we start to deprecate an API.
nit: remove empty line
In `transform()` we have a sentence: ``` The {@link Transformer} must return a {@link KeyValue} type in {@link Transformer#transform(Object, Object) transform()} ``` Might be good to add this here too and point out it might be a an `Iterable` plus example. Compare `flatMap()`: ``` * The provided {@link KeyValueMapper} must return an {@link Iterable} (e.g., any {@link java.util.Collection} type) * and the return value must not be {@code null}. ```
and -> a
and -> a
base -> based progress -> progressed
```suggestion - Facts representing the current state of the node. Matches the C(docker node inspect) output. ```
You should mention instead of this that the required API version is 1.24. ```suggestion - "Docker API >= 1.24" ```
```suggestion - Returns whether the node exists in docker swarm cluster. ``` I don't think it is necessary to mention `Module will fail if executed on a non-manager node` here, since you already mention `Must be executed on a host running as Swarm Manager.` in the module's description.
```suggestion - Short Name. ``` Or possibly just 'Host name for the VM'.
Maybe add an empty line above this one and give it a name, it seems like the `assert` task is just a parameter of the `openssl_certificate_info` one.
Since we're now targetting python-2.6+ we can just ```import json```
Do we still need kind? For service I think it should always be v1
```suggestion description: Returns a dictionary for every extension OID ```
Will the azure API always return a non-zero list or could this result in an `IndexError`? (If so, please wrap in `try`/`except`)
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
This can be changed to look for only one space now that #57209 is merged. Thanks again for writing tests. Much appreciated.
You can achieve this same result output with `module.exit_json(changed=True,**camel_dict_to_snake_dict(execution))` which can be imported from `ansible.module_utils.ec2`. That will automatically cover if AWS ever starts returning additional keys from this API and is generally easier to maintain.
```suggestion module.exit_json(changed=True,**camel_dict_to_snake_dict(res))) ```
Match the error message
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Plz also use `match` arg here
I haven't actually used stepfunctions myself so I'm just going off the docs. AIUI, `ExecutionAlreadyExists` would only be returned on a running execution if the `execution_input` is different, not in a case where the input is the same. As an ansible user who is new to step functions, I feel like if I gave the same input and ran the playbook twice I would expect the 2nd time to be `changed=false` if the first execution is still running. But that doesn't mean that's how AWS users familiar with step functions will expect it.
`client.start_execution` is idempotent but `start_execution` always returns `changed=True` if `action=start`. We probably want to `client.list_executions` to see if the execution is already running before proceeding.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
This can be changed to look for only one space now that #57209 is merged. Thanks again for writing tests. Much appreciated.
You can achieve this same result output with `module.exit_json(changed=True,**camel_dict_to_snake_dict(execution))` which can be imported from `ansible.module_utils.ec2`. That will automatically cover if AWS ever starts returning additional keys from this API and is generally easier to maintain.
```suggestion module.exit_json(changed=True,**camel_dict_to_snake_dict(res))) ```
Plz also use `match` arg here
`client.start_execution` is idempotent but `start_execution` always returns `changed=True` if `action=start`. We probably want to `client.list_executions` to see if the execution is already running before proceeding.
I haven't actually used stepfunctions myself so I'm just going off the docs. AIUI, `ExecutionAlreadyExists` would only be returned on a running execution if the `execution_input` is different, not in a case where the input is the same. As an ansible user who is new to step functions, I feel like if I gave the same input and ran the playbook twice I would expect the 2nd time to be `changed=false` if the first execution is still running. But that doesn't mean that's how AWS users familiar with step functions will expect it.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
I thought about this and it is wrong. We need to specify the existing official repo here. So, if we release 2.0.0 we retain the beta release in there. Just remove this line and the `download.elasticsearch.co` is used to sync and all works as expected.
Do we still need kind? For service I think it should always be v1
Please add a docstring explaining this.
Oh, I see `raise_last_exception` also looks like it moved. Maybe it makes sense to do some reordering to keep the diff a bit smaller. Maybe not.
Will the azure API always return a non-zero list or could this result in an `IndexError`? (If so, please wrap in `try`/`except`)
@Rahul-CSI Hi, Thanks for writing `gce_facts` module. I used this PR for my task. and found 1 error related to this line. I think you can get the disk name from `deviceName`. I had no issue with following code block. please check it out ```python if 'disks' in inst.extra: disk_names = [disk_info['deviceName'] for disk_info in sorted(inst.extra['disks'], key=lambda disk_info: disk_info['index'])] ``` Thank you.
Please separate this into separate lines for maximum readability. Something like: ``` if inst.image is not None: image = inst.image.split('/')[-1] else: image = None ```
Please separate this clause into separate lines for maximum readability (like above).
Please separate this clause into separate lines for maximum readability (like above).
Please separate this clause into separate lines for maximum readability (like above).
From what I can see here, these EXAMPLES are not valid YAML, and would not be operational if someone were to copy paste them. They should be laid out in pure/full YAML such as: ``` - name: Test Vlan - Create a vlan, name it cnos_vlan: host: "{{ inventory_hostname }}" username: "{{ hostvars[inventory_hostname]['username'] }}" ```
I would move it to `_init__()`: ```python def __init__(self, *, name, expressions, index_type=None, condition=None): ... self.index_type = index_type or `GIST` self.condition = condition super().__init__(name=name) ``` and skip the default in `deconstruct()` and `__str__()`: ```python def deconstruct(self): ... if self.index_type != 'GIST': kwargs['index_type'] = self.index_type ```
Also make the comparison case-insensitive, i.e. ```python if self.index_type.lower() != 'gist': ```
`SearchedCase` is no longer it's own type so this should be named `Case conditions must be lookups`. Alternatively, do this checking directly inside the `When` types, since users will be using these directly.
We can use `compiler = query.get_compiler(connection=schema_editor.connection)`
IMO it will me more readable to move `base` outside, also we resolve `F()` expressions twice (line 28 & 29), so maybe, e.g. ```python if isinstance(expression, str): expression = F(expression) if isinstance(expression, F): expression = base.resolve_expression(query=query, simple_col=True) else: expression = expression.resolve_expression(query=query) ``` Moreover I think we don't need to iterate twice over the expressions: ```python for expression, operator in self.expressions: ... expressions.append('%s WITH %s' % (sql % params, operator) return expressions ```
You aren't changing here, except for the style. Please revert.
Maybe `_parse_where()` -> `_get_condition_sql()`, I would return condition and add `WHERE` in `constraint_sql()`, e.g. ```python def _get_condition_sql(self, compiler, schema_editor, query): if self.condition is None: return None where = query.build_where(self.condition) sql, params = where.as_sql(compiler, schema_editor.connection) return sql % tuple(schema_editor.quote_value(p) for p in params) def constraint_sql(self, model, schema_editor): ... condition = self._get_condition_sql(compiler, schema_editor, query) return self.template % { ... 'where': ' WHERE (%s)' % condition if condition else '', } ```
Remove the blank line.
I'm not sure there is any benefit assigning this local variable here when it is only used once.
Why we don't use `super().deconstruct()`? ```python def deconstruct(self): path, args, kwargs = super().deconstruct() kwargs['expressions'] = self.expressions if self.condition is not None: kwargs['condition'] = self.condition if self.index_type != 'GIST': kwargs['index_type'] = self.index_type return path, args, kwargs ```
IMO it will me more readable to move `base` outside, also we resolve `F()` expressions twice (line 28 & 29), so maybe, e.g. ```python if isinstance(expression, str): expression = F(expression) if isinstance(expression, F): expression = base.resolve_expression(query=query, simple_col=True) else: expression = expression.resolve_expression(query=query) ``` Moreover I think we don't need to iterate twice over the expressions: ```python for expression, operator in self.expressions: ... expressions.append('%s WITH %s' % (sql % params, operator) return expressions ```
`SearchedCase` is no longer it's own type so this should be named `Case conditions must be lookups`. Alternatively, do this checking directly inside the `When` types, since users will be using these directly.
Also make the comparison case-insensitive, i.e. ```python if self.index_type.lower() != 'gist': ```
I would move it to `_init__()`: ```python def __init__(self, *, name, expressions, index_type=None, condition=None): ... self.index_type = index_type or `GIST` self.condition = condition super().__init__(name=name) ``` and skip the default in `deconstruct()` and `__str__()`: ```python def deconstruct(self): ... if self.index_type != 'GIST': kwargs['index_type'] = self.index_type ```
We can use `compiler = query.get_compiler(connection=schema_editor.connection)`
You aren't changing here, except for the style. Please revert.
Maybe `_parse_where()` -> `_get_condition_sql()`, I would return condition and add `WHERE` in `constraint_sql()`, e.g. ```python def _get_condition_sql(self, compiler, schema_editor, query): if self.condition is None: return None where = query.build_where(self.condition) sql, params = where.as_sql(compiler, schema_editor.connection) return sql % tuple(schema_editor.quote_value(p) for p in params) def constraint_sql(self, model, schema_editor): ... condition = self._get_condition_sql(compiler, schema_editor, query) return self.template % { ... 'where': ' WHERE (%s)' % condition if condition else '', } ```
It's fine to manually substituting parameters here.
Remove the blank line.
I'm not sure there is any benefit assigning this local variable here when it is only used once.
We can use `compiler = query.get_compiler(connection=schema_editor.connection)`
`SearchedCase` is no longer it's own type so this should be named `Case conditions must be lookups`. Alternatively, do this checking directly inside the `When` types, since users will be using these directly.
Also make the comparison case-insensitive, i.e. ```python if self.index_type.lower() != 'gist': ```
I would move it to `_init__()`: ```python def __init__(self, *, name, expressions, index_type=None, condition=None): ... self.index_type = index_type or `GIST` self.condition = condition super().__init__(name=name) ``` and skip the default in `deconstruct()` and `__str__()`: ```python def deconstruct(self): ... if self.index_type != 'GIST': kwargs['index_type'] = self.index_type ```
IMO it will me more readable to move `base` outside, also we resolve `F()` expressions twice (line 28 & 29), so maybe, e.g. ```python if isinstance(expression, str): expression = F(expression) if isinstance(expression, F): expression = base.resolve_expression(query=query, simple_col=True) else: expression = expression.resolve_expression(query=query) ``` Moreover I think we don't need to iterate twice over the expressions: ```python for expression, operator in self.expressions: ... expressions.append('%s WITH %s' % (sql % params, operator) return expressions ```
Maybe `_parse_where()` -> `_get_condition_sql()`, I would return condition and add `WHERE` in `constraint_sql()`, e.g. ```python def _get_condition_sql(self, compiler, schema_editor, query): if self.condition is None: return None where = query.build_where(self.condition) sql, params = where.as_sql(compiler, schema_editor.connection) return sql % tuple(schema_editor.quote_value(p) for p in params) def constraint_sql(self, model, schema_editor): ... condition = self._get_condition_sql(compiler, schema_editor, query) return self.template % { ... 'where': ' WHERE (%s)' % condition if condition else '', } ```
It will be `django.contrib.postgres.constraints.XXX` but I don't think that's an issue for core constraints we return `django.db.models.XXX`.
Why we don't use `super().deconstruct()`? ```python def deconstruct(self): path, args, kwargs = super().deconstruct() kwargs['expressions'] = self.expressions if self.condition is not None: kwargs['condition'] = self.condition if self.index_type != 'GIST': kwargs['index_type'] = self.index_type return path, args, kwargs ```
Also here: ```python if self.index_type.lower() != 'gist': ```
All tests work without it that's why I'm wondering if we need it.
We can use `compiler = query.get_compiler(connection=schema_editor.connection)`
`SearchedCase` is no longer it's own type so this should be named `Case conditions must be lookups`. Alternatively, do this checking directly inside the `When` types, since users will be using these directly.
Also make the comparison case-insensitive, i.e. ```python if self.index_type.lower() != 'gist': ```
I would move it to `_init__()`: ```python def __init__(self, *, name, expressions, index_type=None, condition=None): ... self.index_type = index_type or `GIST` self.condition = condition super().__init__(name=name) ``` and skip the default in `deconstruct()` and `__str__()`: ```python def deconstruct(self): ... if self.index_type != 'GIST': kwargs['index_type'] = self.index_type ```
IMO it will me more readable to move `base` outside, also we resolve `F()` expressions twice (line 28 & 29), so maybe, e.g. ```python if isinstance(expression, str): expression = F(expression) if isinstance(expression, F): expression = base.resolve_expression(query=query, simple_col=True) else: expression = expression.resolve_expression(query=query) ``` Moreover I think we don't need to iterate twice over the expressions: ```python for expression, operator in self.expressions: ... expressions.append('%s WITH %s' % (sql % params, operator) return expressions ```
Maybe `_parse_where()` -> `_get_condition_sql()`, I would return condition and add `WHERE` in `constraint_sql()`, e.g. ```python def _get_condition_sql(self, compiler, schema_editor, query): if self.condition is None: return None where = query.build_where(self.condition) sql, params = where.as_sql(compiler, schema_editor.connection) return sql % tuple(schema_editor.quote_value(p) for p in params) def constraint_sql(self, model, schema_editor): ... condition = self._get_condition_sql(compiler, schema_editor, query) return self.template % { ... 'where': ' WHERE (%s)' % condition if condition else '', } ```
It will be `django.contrib.postgres.constraints.XXX` but I don't think that's an issue for core constraints we return `django.db.models.XXX`.
Why we don't use `super().deconstruct()`? ```python def deconstruct(self): path, args, kwargs = super().deconstruct() kwargs['expressions'] = self.expressions if self.condition is not None: kwargs['condition'] = self.condition if self.index_type != 'GIST': kwargs['index_type'] = self.index_type return path, args, kwargs ```
Also here: ```python if self.index_type.lower() != 'gist': ```
All tests work without it that's why I'm wondering if we need it.
IMO it will me more readable to move `base` outside, also we resolve `F()` expressions twice (line 28 & 29), so maybe, e.g. ```python if isinstance(expression, str): expression = F(expression) if isinstance(expression, F): expression = base.resolve_expression(query=query, simple_col=True) else: expression = expression.resolve_expression(query=query) ``` Moreover I think we don't need to iterate twice over the expressions: ```python for expression, operator in self.expressions: ... expressions.append('%s WITH %s' % (sql % params, operator) return expressions ```
Maybe `_parse_where()` -> `_get_condition_sql()`, I would return condition and add `WHERE` in `constraint_sql()`, e.g. ```python def _get_condition_sql(self, compiler, schema_editor, query): if self.condition is None: return None where = query.build_where(self.condition) sql, params = where.as_sql(compiler, schema_editor.connection) return sql % tuple(schema_editor.quote_value(p) for p in params) def constraint_sql(self, model, schema_editor): ... condition = self._get_condition_sql(compiler, schema_editor, query) return self.template % { ... 'where': ' WHERE (%s)' % condition if condition else '', } ```
point -> points
Why we don't use `super().deconstruct()`? ```python def deconstruct(self): path, args, kwargs = super().deconstruct() kwargs['expressions'] = self.expressions if self.condition is not None: kwargs['condition'] = self.condition if self.index_type != 'GIST': kwargs['index_type'] = self.index_type return path, args, kwargs ```
It will be `django.contrib.postgres.constraints.XXX` but I don't think that's an issue for core constraints we return `django.db.models.XXX`.
Also here: ```python if self.index_type.lower() != 'gist': ```
Also make the comparison case-insensitive, i.e. ```python if self.index_type.lower() != 'gist': ```
I would move it to `_init__()`: ```python def __init__(self, *, name, expressions, index_type=None, condition=None): ... self.index_type = index_type or `GIST` self.condition = condition super().__init__(name=name) ``` and skip the default in `deconstruct()` and `__str__()`: ```python def deconstruct(self): ... if self.index_type != 'GIST': kwargs['index_type'] = self.index_type ```
We can use `compiler = query.get_compiler(connection=schema_editor.connection)`
`SearchedCase` is no longer it's own type so this should be named `Case conditions must be lookups`. Alternatively, do this checking directly inside the `When` types, since users will be using these directly.
point -> points
It will be `django.contrib.postgres.constraints.XXX` but I don't think that's an issue for core constraints we return `django.db.models.XXX`.
Why we don't use `super().deconstruct()`? ```python def deconstruct(self): path, args, kwargs = super().deconstruct() kwargs['expressions'] = self.expressions if self.condition is not None: kwargs['condition'] = self.condition if self.index_type != 'GIST': kwargs['index_type'] = self.index_type return path, args, kwargs ```
Also here: ```python if self.index_type.lower() != 'gist': ```
IMO it will me more readable to move `base` outside, also we resolve `F()` expressions twice (line 28 & 29), so maybe, e.g. ```python if isinstance(expression, str): expression = F(expression) if isinstance(expression, F): expression = base.resolve_expression(query=query, simple_col=True) else: expression = expression.resolve_expression(query=query) ``` Moreover I think we don't need to iterate twice over the expressions: ```python for expression, operator in self.expressions: ... expressions.append('%s WITH %s' % (sql % params, operator) return expressions ```
Also make the comparison case-insensitive, i.e. ```python if self.index_type.lower() != 'gist': ```
I would move it to `_init__()`: ```python def __init__(self, *, name, expressions, index_type=None, condition=None): ... self.index_type = index_type or `GIST` self.condition = condition super().__init__(name=name) ``` and skip the default in `deconstruct()` and `__str__()`: ```python def deconstruct(self): ... if self.index_type != 'GIST': kwargs['index_type'] = self.index_type ```
We can use `compiler = query.get_compiler(connection=schema_editor.connection)`
Maybe `_parse_where()` -> `_get_condition_sql()`, I would return condition and add `WHERE` in `constraint_sql()`, e.g. ```python def _get_condition_sql(self, compiler, schema_editor, query): if self.condition is None: return None where = query.build_where(self.condition) sql, params = where.as_sql(compiler, schema_editor.connection) return sql % tuple(schema_editor.quote_value(p) for p in params) def constraint_sql(self, model, schema_editor): ... condition = self._get_condition_sql(compiler, schema_editor, query) return self.template % { ... 'where': ' WHERE (%s)' % condition if condition else '', } ```
`SearchedCase` is no longer it's own type so this should be named `Case conditions must be lookups`. Alternatively, do this checking directly inside the `When` types, since users will be using these directly.
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
please fail if required stuff is null
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
++ thanks for changing this :)
`del` is a builtin, not a function. These parens don't have to be here
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
looks like there are two levels of indentation instead of one
Missing space after the `for`
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: cannot create point collection with empty set of points at __randomizedtesting.SeedInfo.seed([9959A23819CF838B:6FE2182D9705AE]:0) at org.elasticsearch.common.geo.builders.ShapeBuilder.<init>(ShapeBuilder.java:97) at org.elasticsearch.common.geo.builders.MultiPointBuilder.<init>(MultiPointBuilder.java:46) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseMultiPoint(GeoWKTShapeParserTests.java:82) ... ```
colors should all be configurable
test failure says this should be `(None, None)`
I believe this relates to team accounts. Users are able to create an image for their account and not expose it to additional team members.
Might be worth adding a `note:` to say that only the hash of the file is used for comparisons, not any filemode/permissions/etc.
Why do you wait to fail ? I think you must fail in "main"
Required=false are implicit could you remove them ? Default=None too in case of string
IMHO this check will always be True since the Cloudstack API will returns `providername` and not `provider`. Perhaps renaming `provider` to `providername` in args? ~~~ args['providername'] = args.pop('provider') ~~~
I don't like to fail at this point. There are way better options to handle this, we could implement a new force param to let a user "remove and add" an new image store by the users intention and if not force, just show a warning message to the users that we can not change the image store, but would recreate on force. We already have such a logic for in the cs_instance module for changing the offering on a running instance.
Seems `image_store` will be `None` on the first create pass. Right? In this case, I would suggest: ~~~ res = self.query_api('addImageStore', **args) image_store = res.get('imagestore') ~~~
The module should return `changed=True` even in check mode.
Just a small hint of "ansible magic": I would add an alisas e.g `aliases=['id']`. Ansible allows to use a single item for a list type: ~~~yaml - one_image_facts: ids: 124 ~~~ with an alias `id` it makes perfect sense: ~~~yaml - one_image_facts: id: 124 ~~~ also update the docs accordenly: ~~~diff ids: description: - A list of images ids whose facts you want to gather + aliases: [ id ] ~~~
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
The final command should probably return its stdout, stderr and rc back to the playbook.
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
I don't think this is working correctly, did you test it for all possible inputs ? - First of all the size_unit is only correct if it is a known unit. - And if that's not true, and it's not a digit, you have to escalate. - If it is a digit, you likely won't have a unit and then you could assume bytes.
The mixup to me is that people using incorrect units will silently be using bytes, whereas it should report this as an error.
This means that if you'd be using 16m, it will be 16 bytes too. You will have to do a lot more to avoid mixups here.
The final command should probably return its stdout, stderr and rc back to the playbook.
The final command should probably return its stdout, stderr and rc back to the playbook.
This doesn't seem right, size is an integer at this point.
test failure says this should be `(None, None)`
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
I don't think this is working correctly, did you test it for all possible inputs ? - First of all the size_unit is only correct if it is a known unit. - And if that's not true, and it's not a digit, you have to escalate. - If it is a digit, you likely won't have a unit and then you could assume bytes.
You could make the unit mandatory (which is what I would do), or you could allow no unit (which means in bytes) and in that case you will have to consider that case as well. (i.e.if unit is a digit)
There is a much easier way to do this, by using the index in the list. Something like: ```python unit = size[-1] units = list('b', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y') try: multiplier = 1024**units.index(unit) except ValueError:: e = get_exception() module.fail_json(msg="No valid size unit specified. %s" % e) ```
This doesn't seem right, size is an integer at this point.
This means that if you'd be using 16m, it will be 16 bytes too. You will have to do a lot more to avoid mixups here.
The mixup to me is that people using incorrect units will silently be using bytes, whereas it should report this as an error.
There is a much easier way to do this, by using the index in the list. Something like: ```python unit = size[-1] units = list('b', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y') try: multiplier = 1024**units.index(unit) except ValueError:: e = get_exception() module.fail_json(msg="No valid size unit specified. %s" % e) ```
You could make the unit mandatory (which is what I would do), or you could allow no unit (which means in bytes) and in that case you will have to consider that case as well. (i.e.if unit is a digit)
That's up to you. There is no requirement to have a 1:1 mapping between the command and the Ansible interface. And I personally prefer to always have units (e.g. the AIX LV interface assumes no unit to mean MB, not MiB so...)
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
I don't think this is working correctly, did you test it for all possible inputs ? - First of all the size_unit is only correct if it is a known unit. - And if that's not true, and it's not a digit, you have to escalate. - If it is a digit, you likely won't have a unit and then you could assume bytes.
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
This doesn't seem right, size is an integer at this point.
This means that if you'd be using 16m, it will be 16 bytes too. You will have to do a lot more to avoid mixups here.
The mixup to me is that people using incorrect units will silently be using bytes, whereas it should report this as an error.
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
I don't think this is working correctly, did you test it for all possible inputs ? - First of all the size_unit is only correct if it is a known unit. - And if that's not true, and it's not a digit, you have to escalate. - If it is a digit, you likely won't have a unit and then you could assume bytes.
This means that if you'd be using 16m, it will be 16 bytes too. You will have to do a lot more to avoid mixups here.
The mixup to me is that people using incorrect units will silently be using bytes, whereas it should report this as an error.
This doesn't seem right, size is an integer at this point.
The final command should probably return its stdout, stderr and rc back to the playbook.
The final command should probably return its stdout, stderr and rc back to the playbook.
The final command should probably return its stdout, stderr and rc back to the playbook.
These will need to be module parameters if they need to be configurable. Constants are not available to modules.
Modules do not have access to get or set configuration values. An action plugin should be able to check the configuration before invoking the module, but I don't think that is something we're doing with any other modules currently. You may want to bring this up in tomorrow's Network Working Group meeting on IRC to see what thoughts the network team has on this.
Unfortunately annotation names can contain LOOKUP_SEP - that is the reason why the ugly refs_aggregate method was added. For example qs.annotate(Max('id')) will create an annotation named max__id, and that can't be referred if the code checks for len(field_list) == 1.
I moved this test to the `tests/messages_tests/tests.py`.
It is thread safe as long as the integer_field and float_field are used in stateless manner. I think that is the case, but verifying that isn't easy.
I would put the arguments all on this line
no, if the variable is set but empty, you should empty out the options
so this assertion looks incorrect, i would expect and empty string as the ssh args
The following indentation should pass pep8 (with the `==` starting under the `r` in `assert`): ``` assert constants.get_config(cfgparser, 'defaults', 'unknown', 'ANSIBLE_TEST_VAR', '~/local', value_type='path', expand_relative_paths=False) \ == os.path.join(user['home'], 'local') ```
On further thought, this actually might break something with the new stuff, since you're relying on pyyaml blindly `call`ing whatever is passed in, but the prototype logic that supports object instances only does that call if `isinstance(loader, Reader)` is true. We could probably tweak that somehow, like `callable()` instead, which might be a little more resilient/Pythonic anyway... So this is definitely fine for released code, and it's something I'll keep in mind for the new stuff.
This error is raised when instantiating so we don't need to include a `route` in the message.
Can you re-warp this block to 79 chars? (First line is too short.)
i don't think we want roles in roles
Please remove `required=False` that's the default
chop extra space after period
Chop "Makes sure" prefix in favor of stating the expected behavior as described in https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Copy / paste? I would have expected the ordering to change for the `duth1` and `duth2` arguments.
non existing -> nonexistent
It would help readability to use a name like "nonexistent_app" rather than "duth..".
This should always be true if the receiver is connected with `sender=migrations.RenameModel` as it is right now.
Yes and no. Keeping the output explicit feels easier to read.
Since the same result is expected as above, wouldn't it be clearer to save the output in a variable? Something like: ``` expected = ( "[ ] author_app.0001_initial\n" "[ ] book_app.0001_initial\n" "[ ] author_app.0002_alter_id\n" "[ ] mutate_state_b.0001_initial\n" "[ ] mutate_state_b.0002_add_field\n" ) ```
Chop "Makes sure" prefix in favor of stating the expected behavior as described in https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/.
I made a few edits and squashed commits but before I push those updates I wanted to ask if this test is really needed. None of the changes seem related to verbosity so this test seems unnecessary to me.
Why's that? It's non-obvious at first glance.
Swap the apps in `call_command()` but leave them sorted in the error message.
Copy / paste? I would have expected the ordering to change for the `duth1` and `duth2` arguments.
non existing -> nonexistent
It would help readability to use a name like "nonexistent_app" rather than "duth..".
This doesn't look like it is tested.
Could you replace "create model" and "add field" with "CreateModel" and "AddField" respectively, please.
This doesn't look like it is tested.
This should always be true if the receiver is connected with `sender=migrations.RenameModel` as it is right now.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
`mentionned` => `mentioned`
move to finally
Copy / paste? I would have expected the ordering to change for the `duth1` and `duth2` arguments.
Swap the apps in `call_command()` but leave them sorted in the error message.
non existing -> nonexistent
Could you replace "create model" and "add field" with "CreateModel" and "AddField" respectively, please.
non existing -> nonexistent
It would help readability to use a name like "nonexistent_app" rather than "duth..".
Chop "Makes sure" prefix in favor of stating the expected behavior as described in https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/.
Yes and no. Keeping the output explicit feels easier to read.
Since the same result is expected as above, wouldn't it be clearer to save the output in a variable? Something like: ``` expected = ( "[ ] author_app.0001_initial\n" "[ ] book_app.0001_initial\n" "[ ] author_app.0002_alter_id\n" "[ ] mutate_state_b.0001_initial\n" "[ ] mutate_state_b.0002_add_field\n" ) ```
Why's that? It's non-obvious at first glance.
I made a few edits and squashed commits but before I push those updates I wanted to ask if this test is really needed. None of the changes seem related to verbosity so this test seems unnecessary to me.
Swap the apps in `call_command()` but leave them sorted in the error message.
Copy / paste? I would have expected the ordering to change for the `duth1` and `duth2` arguments.
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
It would help readability to use a name like "nonexistent_app" rather than "duth..".
non existing -> nonexistent
Chop "Makes sure" prefix in favor of stating the expected behavior as described in https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/.
Try except blocks should be as small as possible. In this case, if is only needed around the calls to modify and delete. Perhaps put the try except around the if state conditional to limit the statements that it surrounds
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
Why's that? It's non-obvious at first glance.
Copy / paste? I would have expected the ordering to change for the `duth1` and `duth2` arguments.
Swap the apps in `call_command()` but leave them sorted in the error message.
Could you replace "create model" and "add field" with "CreateModel" and "AddField" respectively, please.
non existing -> nonexistent
It would help readability to use a name like "nonexistent_app" rather than "duth..".
Chop "Makes sure" prefix in favor of stating the expected behavior as described in https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/.
Yes and no. Keeping the output explicit feels easier to read.
Since the same result is expected as above, wouldn't it be clearer to save the output in a variable? Something like: ``` expected = ( "[ ] author_app.0001_initial\n" "[ ] book_app.0001_initial\n" "[ ] author_app.0002_alter_id\n" "[ ] mutate_state_b.0001_initial\n" "[ ] mutate_state_b.0002_add_field\n" ) ```
Why's that? It's non-obvious at first glance.
Swap the apps in `call_command()` but leave them sorted in the error message.
Copy / paste? I would have expected the ordering to change for the `duth1` and `duth2` arguments.
I made a few edits and squashed commits but before I push those updates I wanted to ask if this test is really needed. None of the changes seem related to verbosity so this test seems unnecessary to me.
Please use a single quote.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Why's that? It's non-obvious at first glance.
Chop "Makes sure" prefix in favor of stating the expected behavior as described in https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
It would help readability to use a name like "nonexistent_app" rather than "duth..".
non existing -> nonexistent
Copy / paste? I would have expected the ordering to change for the `duth1` and `duth2` arguments.
Swap the apps in `call_command()` but leave them sorted in the error message.
```suggestion from django.utils.deprecation import RemovedInDjango50Warning ```
``` py self.assertTrue(r.closed) ```
up with Django imports
maybe add the set of wrappers to the message so taht it would be easier to debug
``` raise InvalidTemplateLibrary( "Unsupported arguments to ..." ```
, -> % (missing tests for this branch) same issue with InvalidTemplateLibrary raised below
I'm a bit concerned that this will be the case almost all the time except when we wrap. Can we avoid this if statement and just recreate an IndexSearcher all the time? (the else branch)
Fixed in https://github.com/ansible/ansible/pull/34186
These cause compilation errors for me...
It seems we can probably move deprecation warning handling to the actual test cases now. We can make it a follow-up item after merging the first version of this if you like.
```suggestion updates.extend(line for line in set_commands if line not in config) ```
This should really be: ``` - Set custom DNS search domains. (Use I(dns_search) with C('') if you don't wish to set the search domain.) ```
(Same for the related options.)
I would remove this `device` here, and similar the option name in the examples for `device_read_bps`, `device_read_iops` etc.
What if `connected` attribute is set to `false` ? It's ignored!
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
I would remove all aliases if possible.
We could add a check to send at least an warning if a `mac` is specified with `state=new` in place of `manual_mac`: ``` if network['state'].lower() == 'new' and 'mac' in network [Ã¢Â€Â¦]msg="MAC address '%s' define in `mac` attribute will not be used. If you want to define a manual MAC address, please use `manual_mac`." % (network['mac'])[Ã¢Â€Â¦] ```
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
Please remove deprecated parameters as this is new module.
Can it be simpler? E.g. without nic_device/device_type ```python for device in vm.config.hardware.device: if device.macAddress == mac: return device ```
Maybe wrapping mac comparison with try/except/pass will work on nonexistent attribute? Just to avoid additional iteration, though I still not fully understand how often are there cases when device with certain device type won't have macAddress attribute.
(Same for the related options.)
What if `connected` attribute is set to `false` ? It's ignored!
If the lsattr args aren't sensitive I would include the full cmd in the error message here.
We could add a check to send at least an warning if a `mac` is specified with `state=new` in place of `manual_mac`: ``` if network['state'].lower() == 'new' and 'mac' in network [Ã¢Â€Â¦]msg="MAC address '%s' define in `mac` attribute will not be used. If you want to define a manual MAC address, please use `manual_mac`." % (network['mac'])[Ã¢Â€Â¦] ```
This should really be: ``` - Set custom DNS search domains. (Use I(dns_search) with C('') if you don't wish to set the search domain.) ```
I would remove all aliases if possible.
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
As this method is called get_network, we could expect that it return a network device object (or None if not found). Ortherwise we could also rename the method into something like network_exists_by_name.
This will remove check for datacenter from line 215 ```suggestion dc_obj = self.find_datacenter_by_name(datacenter_name=self.params['datacenter']) if not dc_obj: self.module.fail_json(msg="Failed to find the datacenter %s" % self.params['datacenter']) objects = get_all_objs(content, vimtype, folder=dc_obj.networkFolder) ```
+1 for to_text
Maybe wrapping mac comparison with try/except/pass will work on nonexistent attribute? Just to avoid additional iteration, though I still not fully understand how often are there cases when device with certain device type won't have macAddress attribute.
Can it be simpler? E.g. without nic_device/device_type ```python for device in vm.config.hardware.device: if device.macAddress == mac: return device ```
What if `connected` attribute is set to `false` ? It's ignored!
Shouldn't the edit operation only be set if we want to make the actual change (inside the `if disk['size'] != disk_spec.device.capacityInKB` part)
We could add a check to send at least an warning if a `mac` is specified with `state=new` in place of `manual_mac`: ``` if network['state'].lower() == 'new' and 'mac' in network [Ã¢Â€Â¦]msg="MAC address '%s' define in `mac` attribute will not be used. If you want to define a manual MAC address, please use `manual_mac`." % (network['mac'])[Ã¢Â€Â¦] ```
from ansible.module_utils.vmware import get_parent_datacenter
All these methods can be clubbed into a single method that takes data and pattern string as arguments and returns the match else None
```suggestion updates.extend(line for line in set_commands if line not in config) ```
Maybe wrapping mac comparison with try/except/pass will work on nonexistent attribute? Just to avoid additional iteration, though I still not fully understand how often are there cases when device with certain device type won't have macAddress attribute.
Can it be simpler? E.g. without nic_device/device_type ```python for device in vm.config.hardware.device: if device.macAddress == mac: return device ```
(Same for the related options.)
What if `connected` attribute is set to `false` ? It's ignored!
We could add a check to send at least an warning if a `mac` is specified with `state=new` in place of `manual_mac`: ``` if network['state'].lower() == 'new' and 'mac' in network [Ã¢Â€Â¦]msg="MAC address '%s' define in `mac` attribute will not be used. If you want to define a manual MAC address, please use `manual_mac`." % (network['mac'])[Ã¢Â€Â¦] ```
This should really be: ``` - Set custom DNS search domains. (Use I(dns_search) with C('') if you don't wish to set the search domain.) ```
I would remove all aliases if possible.
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
Maybe wrapping mac comparison with try/except/pass will work on nonexistent attribute? Just to avoid additional iteration, though I still not fully understand how often are there cases when device with certain device type won't have macAddress attribute.
Can it be simpler? E.g. without nic_device/device_type ```python for device in vm.config.hardware.device: if device.macAddress == mac: return device ```
(Same for the related options.)
What if `connected` attribute is set to `false` ? It's ignored!
We could add a check to send at least an warning if a `mac` is specified with `state=new` in place of `manual_mac`: ``` if network['state'].lower() == 'new' and 'mac' in network [Ã¢Â€Â¦]msg="MAC address '%s' define in `mac` attribute will not be used. If you want to define a manual MAC address, please use `manual_mac`." % (network['mac'])[Ã¢Â€Â¦] ```
This should really be: ``` - Set custom DNS search domains. (Use I(dns_search) with C('') if you don't wish to set the search domain.) ```
I would remove all aliases if possible.
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
Please remove deprecated parameters as this is new module.
That does make sense. Thanks.
oh I see, it makes sense then.
I would not as .format breaks in older versions and we are trying to still keep this kind of module working on older machines
I know this is a small one, but I can imagine % formatting will be decommissioned at some point, I would change this to .format() method if it's not too much of a hassle. To back-up this up, ansible is undergoing an effort to be Python3 compatible, as per Python docs the recommendation is to prefer string.format() method against string % formatting operator. [1][2] Also keep in mind that % might have undesired effects [3] [1] https://docs.python.org/2/library/stdtypes.html#str.format [2] https://www.python.org/dev/peps/pep-3101/ [3] http://stackoverflow.com/questions/5082452/python-string-formatting-vs-format
Is there any reason why this method accepts `level` and `md_device` as argument? IMHO, it would be natural to use `self.level` and `self.md_device` instead.
It doesn't make sense to turn a unicode string into a unicode string. It's just `str(str(''))`, which is probably not what you want. Maybe you wanted to turn bytes into text here? In such case check'd be against `binary_type` and the translation should use `to_text()` shim.
I'd rephrase: 'Obtain data of `vm_name` if specified, ...'
Please add check-mode support (and if possible also diff support).
also would be super cool if we would move this to the top of the file. ``` 178 Python Imports 179 ============== 180 181 To make it clear what a module is importing, imports should not be sprinkled throughout the code. ```
Nowadays we list all the functionality we use from a specific library explicitly. So: ```python from ansible.module_utils.basic import AnsibleModule ```
As this method is called get_network, we could expect that it return a network device object (or None if not found). Ortherwise we could also rename the method into something like network_exists_by_name.
+1 for to_text
This will remove check for datacenter from line 215 ```suggestion dc_obj = self.find_datacenter_by_name(datacenter_name=self.params['datacenter']) if not dc_obj: self.module.fail_json(msg="Failed to find the datacenter %s" % self.params['datacenter']) objects = get_all_objs(content, vimtype, folder=dc_obj.networkFolder) ```
Maybe wrapping mac comparison with try/except/pass will work on nonexistent attribute? Just to avoid additional iteration, though I still not fully understand how often are there cases when device with certain device type won't have macAddress attribute.
Can it be simpler? E.g. without nic_device/device_type ```python for device in vm.config.hardware.device: if device.macAddress == mac: return device ```
from ansible.module_utils.vmware import get_parent_datacenter
What if `connected` attribute is set to `false` ? It's ignored!
We could add a check to send at least an warning if a `mac` is specified with `state=new` in place of `manual_mac`: ``` if network['state'].lower() == 'new' and 'mac' in network [Ã¢Â€Â¦]msg="MAC address '%s' define in `mac` attribute will not be used. If you want to define a manual MAC address, please use `manual_mac`." % (network['mac'])[Ã¢Â€Â¦] ```
mandatory msg is missing
Yes, 'msg' key and value should always present on *_exit() calls
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
Ah, i'd expect a bool to always be True/False, if that is not the case (no time to test really right now :)), then indeed, this needs to stay (and this goes for my other bool remarks)
As this method is called get_network, we could expect that it return a network device object (or None if not found). Ortherwise we could also rename the method into something like network_exists_by_name.
+1 for to_text
This will remove check for datacenter from line 215 ```suggestion dc_obj = self.find_datacenter_by_name(datacenter_name=self.params['datacenter']) if not dc_obj: self.module.fail_json(msg="Failed to find the datacenter %s" % self.params['datacenter']) objects = get_all_objs(content, vimtype, folder=dc_obj.networkFolder) ```
Maybe wrapping mac comparison with try/except/pass will work on nonexistent attribute? Just to avoid additional iteration, though I still not fully understand how often are there cases when device with certain device type won't have macAddress attribute.
Can it be simpler? E.g. without nic_device/device_type ```python for device in vm.config.hardware.device: if device.macAddress == mac: return device ```
from ansible.module_utils.vmware import get_parent_datacenter
What if `connected` attribute is set to `false` ? It's ignored!
All these methods can be clubbed into a single method that takes data and pattern string as arguments and returns the match else None
We could add a check to send at least an warning if a `mac` is specified with `state=new` in place of `manual_mac`: ``` if network['state'].lower() == 'new' and 'mac' in network [Ã¢Â€Â¦]msg="MAC address '%s' define in `mac` attribute will not be used. If you want to define a manual MAC address, please use `manual_mac`." % (network['mac'])[Ã¢Â€Â¦] ```
I would add a flag to `Index`, e.g. `is_functional` that could be used here together with `supports_expression_indexes` to skip such indexes, e.g. ```python if not index.is_functional or self.connection.features.supports_expression_indexes: output.append(index.create_sql(model, self)) ``` Also we should return `None` in `_create_index_sql()` and `_delete_index_sql` if `index.is_functional` and `self.connection.features.supports_expression_indexes`
We already compare columns in: ```python old_field.column != new_field.column or ``` so we can simply ignore `db_column` in kwargs, e.g. ```python def _field_should_be_altered(self, old_field, new_field): _, old_path, old_args, old_kwargs = old_field.deconstruct() _, new_path, new_args, new_kwargs = new_field.deconstruct() # Ignore db_column to not alter when changing only a db_column but it's # the same as a field name. old_kwargs.pop('db_column', None) new_kwargs.pop('db_column', None) # Don't alter when changing only a field name. return ( old_field.column != new_field.column or (old_path, old_args, old_kwargs) != (new_path, new_args, new_kwargs) ) ```
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
Does this need to be a separate method? Seems unnecessary to me.
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
There is no need to provide a value for `max_pages_num` - I think it should be calculated automatically: ```python (self.on_each_side + self.on_ends) * 2 ``` Otherwise developers can provide values that are incompatible with each other...
Single quotes please.
In Python3, `super()` is enough.
I think you meant `get_ellipsized_page_range` here. Use `number` instead of `page_num` which matches the argument name used by `Paginator.get_page()`.
Not sure it makes a difference but before it looks like we got `form=None` in the context.
Would `functools.partial` work here? ```python from functools import partial bound_method = partial(method.__get__(self, type(self))) ```
Is `expanduser()` needed? Seems like it's done automatically.
Once again, exec is not needed here.
Why use exec? You can just write contents of the string in the function.
Would there be any utility to sorted() on files/sub_folders? AFAICT, os.walk will use the os.listdir() results directly for filenames. os.listdir() is doc'ed as 'arbitrary order' (iirc, more or less up to fs). Wondering if sorting those results would make for more reproducible behavior when trying to replicate errors. It shouldn't matter at all if there are no bugs, but it could make tracking down odd failures easier.
single line looks more readable here
Returns -> Return use period
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
remove "0" in {0}
`yield from` is not allowed in async functions.
Yes, your version is simpler, Simon.
The variable name `shand` is non-descriptive
how about: ``` try: query_that_shouldnt_fail = ... except ..ProgrammingError: self.fail('Appropriate Error Message') ```
About Josh's suggestion, we've considered try/except/fail an antipattern because it hides the original exception and thus makes fixing a failure more difficult. There's no problem with a test erroring rather than failing.
This will not work for `OuterRef()` :disappointed: because we don't resolve it properly, so it generates: ```sql EXISTS ( SELECT (1) AS "a" FROM "expressions_company" U0 WHERE SUBSTRING(U0."name", 8, 1) = (SUBSTRING(U0."name", 3, 1)) LIMIT 1 ) ``` instead of ```sql EXISTS ( SELECT (1) AS "a" FROM "expressions_company" U0 WHERE SUBSTRING(U0."name", 8, 1) = (SUBSTRING("expressions_company"."name", 3, 1)) LIMIT 1 ) ``` see `test_slicing_of_outerref`.
`target` is only available for `Col`, so it crashes for expressions, see `test_slicing_of_f_expression_with_annotated_expression`.
The variable name `phand` is non-descriptive
`AnsibleConnectionFailure` is a generic error, it does not mean the resource is missing. It should be more like an err 500.
single line looks more readable here
It would be better if we could refactor the control flow so we don't have to repeat these lines which also occur after the last else statement in this file.
Is there the potential that the response could be 200, but the JSON not include these keys? Maybe they should be guarded with try/except to avoid exceptions.
Our code formatting standards allow for 160 character width, move this onto the same line to make it more readable.
A couple of things, we try to avoid 'catchalls' as they don't give good error messages. We try to capture the errors after a specific action "friendly' part that suggests what was being attempted and possibly what user can do to change error to success. Also, the 'exception' field is badly named for python, it is mostly for tracebacks and we normally append the exception string to the msg field after the 'friendly msg'
to_text() isnt imported
+1 and the same for all the other fail_json's as well.
this field is for tracebacks (i know, badly named for python) since you are already passing the exception message this is both redundant and not the purpose for the field. same issue in all subsequent fail_json
Confusing string concatenation. Look into `printf`-like string formatting or `.format()` I see other places where this should be changed.
use Errror instead of exception, most users know what an error is, only programmers understand what exception means in this context
I like the code organization here Ã°ÂŸÂ‘Â , but it is a little different than most modules. Some folks may prefer the deeply nested args style. I don't think you should change it, just a heads up that it might be a little unusual to some folks.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Does this check make sense now that we only handle it if it starts with a relative path? (ie if the prefix where there it would start with a slash anyways)
@jrwdunham I'm not an expert here, but from what I read the true assumption is that `SCRIPT_NAME` **never** ends with trailing slash. So in case you do not have subfolder, `SCRIPT_NAME` should be just empty/unset.
This can be single-lined: ```suggestion return (matched_tags or not tags) and not test_tags.intersection(exclude_tags) ```
`always_text` is gone.
If you use to_text(xxx, errors='surrogate_or_strict') it won't throw exceptions.
I would rather see ValueError instead of general exception
Yep, simply ignoring all other options when `state == 'absent'` is the easiest solution (that's what most other modules do, too).
I would expect a module to issue warnings only for something which the user needs to be warned about. If the user wants something to be absent and it already is absent, that's not something I would expect a warning about. You also don't issue a warning if the user has `state == 'present'` and the sequence is already there.
This should not result in a warning, but simply result in `changed == False`.
It would be useful to tell the user which `key` is invalid.
I don't think we need to get into this now. For now we can remove the test and just keep a test where we have some nodes with no shards assigned to them.
the module in general is just a thin api wrapper, not really that useful for users, I'm not against having an 'expert mode' but this is in addition to actual options usage.
The API currently allows multiple groups with the same name. Do we need some logic similar to the logic in `cloudscale_server` to detect this? Otherwise this could lead to surprising results... And tests for this case would be nice too.
I don't think the `or []` is needed here. If there are no server groups defined, the API already returns an empty list.
supporting snapmirror-label would affect this logic, are only one of the two could be present, or both of them.
```suggestion binary_path = module.get_bin_path('virsh', required=True) ```
This is not required if you use `required=True` in `get_bin_path` API.
Either use `result['changed'] = ` and remove the `changed =` OR use `changed =` and remove the `result['changed']` but do not use both. That applies to all `main()` function.
This should be on the top of the file.
couldn't -> can't
I'm not sure it really matters, but I'd put `self.ALLOW_BASE_THROTTLING` first.
Is there some other way other than doing a len on `self._workers`? With forking and your threading work this should be reliable, but is it possible that 3rd party process plugins in the future would not have a static length list? Maybe they pop and append, and this could catch at a point where it's not at the max.
should lock -> locks
so we can -> to
chop blank line
~~ use the shared open_url function, it takes care of many issues with python's ssl ~~
ignore it then, I stopped reading at import ssl, did not realize it is an encrypted tcp socket connection and assumed http/s
remove "0" in {0}
I'm not a huge fan of this, we should be able to build the result set from module.params and the response.
`always_text` is gone.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Can we deprecate passing `None` in `errors` in a separate PR? and do this in advance. There is already a lot of changes in this patch, that should make it more readable and easier to review. I can try to refactor this out if you don't have time to keep working on this.
Thanks :+1: , IMO a separate ticket is not necessary, we can `Refs #33348 -- ...`.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
~~ use the shared open_url function, it takes care of many issues with python's ssl ~~
ignore it then, I stopped reading at import ssl, did not realize it is an encrypted tcp socket connection and assumed http/s
single line looks more readable here
Returns -> Return use period
Same order, type first.
can be ignored
I think this should error somehow get printed or a warning given or something (maybe only with verbose?). (in my case it's happening if pywatchman is installed, but not watchman.): `pywatchman.WatchmanError: "watchman" executable not in PATH (%s), while executing [Errno 2] No such file or directory: 'watchman': 'watchman'`
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
This logic seems ignore the use case of removing all tags.
remove "0" in {0}
It looks like a duplicate of `test_save_unsaved_instance_on_generic_foreign_key` :thinking:
> Shall I also remove `test_target_model_is_unsaved_save` and `test_target_model_is_unsaved_bulk_create`? Yes, they are redundant.
I removed it.
Ahh true, sorry for the noise. No changes are required.
This can be single-lined: ```suggestion return (matched_tags or not tags) and not test_tags.intersection(exclude_tags) ```
This must be checked **before** any processing.
This must be checked **before** any processing.
I would chop `does_`: ```suggestion if test_match_tags(test, tags, exclude_tags) ```
For python 2.6 compatibility, this and the following four strings have to look like `cmd = "ping {0} {1}".format(vrf, dest)`
Since this isn't implemented, perhaps lets not mention it? I found it confusing
It looks like a duplicate of `test_save_unsaved_instance_on_generic_foreign_key` :thinking:
> Shall I also remove `test_target_model_is_unsaved_save` and `test_target_model_is_unsaved_bulk_create`? Yes, they are redundant.
I removed it.
Ahh true, sorry for the noise. No changes are required.
This can be single-lined: ```suggestion return (matched_tags or not tags) and not test_tags.intersection(exclude_tags) ```
This must be checked **before** any processing.
This must be checked **before** any processing.
I would chop `does_`: ```suggestion if test_match_tags(test, tags, exclude_tags) ```
For python 2.6 compatibility, this and the following four strings have to look like `cmd = "ping {0} {1}".format(vrf, dest)`
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
Drop the comma/space in `[FakeFieldFile(), ]`
```suggestion item, fields=fields, using=self.db, ```
Need to import reduce from ansible.module_utils.six.moves.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
Does the result provide any insight into if anything's changed? Looks like put and delete are currently both hard coded to return ``changed=True``
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
Drop the comma/space in `[FakeFieldFile(), ]`
```suggestion item, fields=fields, using=self.db, ```
Need to import reduce from ansible.module_utils.six.moves.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
Does the result provide any insight into if anything's changed? Looks like put and delete are currently both hard coded to return ``changed=True``
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
Drop the comma/space in `[FakeFieldFile(), ]`
```suggestion item, fields=fields, using=self.db, ```
Need to import reduce from ansible.module_utils.six.moves.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
Does the result provide any insight into if anything's changed? Looks like put and delete are currently both hard coded to return ``changed=True``
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
Drop the comma/space in `[FakeFieldFile(), ]`
```suggestion item, fields=fields, using=self.db, ```
Need to import reduce from ansible.module_utils.six.moves.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
Does the result provide any insight into if anything's changed? Looks like put and delete are currently both hard coded to return ``changed=True``
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Unless I missed something: - before: all `OSError` exceptions are converted to `CommandError`; in addition a specific message is added when the file already exists - after: only `FileExistsError` is converted to `CommandError`
IIRC raising CommandError prevents management commands from displaying a stack trace. This doesn't seem very important but I wanted to point out the change in behavior in case it was accidental.
I think we should use `relpath()` but only if it is below the working directory, e.g. ```diff diff --git a/django/core/management/templates.py b/django/core/management/templates.py index c7252a5ad2..db417443b2 100644 --- a/django/core/management/templates.py +++ b/django/core/management/templates.py @@ -65,6 +65,7 @@ class TemplateCommand(BaseCommand): self.validate_name(name) # if some directory is given, make sure it's nicely expanded + app_python_path = name if target is None: top_dir = path.join(os.getcwd(), name) try: @@ -77,6 +78,11 @@ class TemplateCommand(BaseCommand): if app_or_project == 'app': self.validate_name(os.path.basename(target), 'directory') top_dir = os.path.abspath(path.expanduser(target)) + # Use a relative path if it's below the current working + # directory, or an app name otherwise. + rel_path = os.path.relpath(top_dir) + if not rel_path.startswith('..'): + app_python_path = rel_path.replace('\\', '/').replace('/', '.') if not os.path.exists(top_dir): raise CommandError("Destination directory '%s' does not " "exist, please create it first." % top_dir) @@ -101,6 +107,7 @@ class TemplateCommand(BaseCommand): context = Context({ **options, + **({'app_python_path': app_python_path} if app_or_project == 'app' else {}), base_name: name, base_directory: top_dir, camel_case_name: camel_case_value, ```
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
Sure, a separate PR sounds good.
you can move it to before `if` as just `docs = {}` line, this should read better.
Please revert unrelated cosmetic changes to keep the diff clean.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
```suggestion help=( 'Shuffle the order of test cases to help check that tests are ' 'properly isolated.' ), ) ```
ditto ```suggestion ```
Not exactly, we only changed the `parallel` to `1` when it was not given. Please see an example call :point_up:
We shouldn't silently change passed parameters. IMO it better to raise an exception like we do now: ``` $ export DJANGO_SETTINGS_MODULE=test_oracle $ ./runtests.py queries --parallel=2 Testing against Django installed in '/django/django' with up to 2 processes Found 416 test(s). Creating test database for alias 'default'... Creating test user... Cloning test database for alias 'default'... Traceback (most recent call last): File "./runtests.py", line 659, in <module> failures = django_tests( File "./runtests.py", line 385, in django_tests failures = test_runner.run_tests(test_labels) File "/django/django/test/runner.py", line 881, in run_tests old_config = self.setup_databases( File "/django/django/test/runner.py", line 787, in setup_databases return _setup_databases( File "/django/django/test/utils.py", line 217, in setup_databases connection.creation.clone_test_db( File "/django/django/db/backends/base/creation.py", line 239, in clone_test_db self._clone_test_db(suffix, verbosity, keepdb) File "/django/django/db/backends/base/creation.py", line 255, in _clone_test_db raise NotImplementedError( NotImplementedError: The database backend doesn't support cloning databases. Disable the option to run tests in parallel processes. ```
Can we display a similar message with the test results? It can be helpful when you have plenty of errors, .e.g. ``` ... FAILED (errors=206, skipped=1094, expected failures=4) Used shuffle seed: 556738431 (generated) ```
Okay, I could take a crack at some of that follow-up clean-up afterwards, if you want.
I feel like you're just testing argparse here, and this test can be removed. We don't test parsing any other arguments, since we can assume argparse works as advertised.
I don't see any need for this attribute.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
Would there be any utility to sorted() on files/sub_folders? AFAICT, os.walk will use the os.listdir() results directly for filenames. os.listdir() is doc'ed as 'arbitrary order' (iirc, more or less up to fs). Wondering if sorting those results would make for more reproducible behavior when trying to replicate errors. It shouldn't matter at all if there are no bugs, but it could make tracking down odd failures easier.
2.6 or 2.7? Also you `requirements` listed here and the modules.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
In Python3, `super()` is enough.
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
```suggestion Test that the returned value for timezone consists of only uppercase ```
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
Would there be any utility to sorted() on files/sub_folders? AFAICT, os.walk will use the os.listdir() results directly for filenames. os.listdir() is doc'ed as 'arbitrary order' (iirc, more or less up to fs). Wondering if sorting those results would make for more reproducible behavior when trying to replicate errors. It shouldn't matter at all if there are no bugs, but it could make tracking down odd failures easier.
2.6 or 2.7? Also you `requirements` listed here and the modules.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
In Python3, `super()` is enough.
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Again, error handling changed.
Sorry, these parameters are already checked in `save()` so there is no need to change these assertions.
At first sight it could be simplified to: ``` [option not in [None, False] for option in mutually_exclusive_options].count(True) ``` But I have a feeling even more simplification might be possible. Note that we can't simply cast default to bool, which would make this even simpler, as some valid dates evaluate to false: https://mail.python.org/pipermail/python-ideas/2014-March/026446.html
fields -> options comma after auto_now_add
Why this restriction? If several files are saved in a quick sequence â€” for instance with a text editor's "save all" feature" â€” only the first change willl be taken into account.
One option would be to pass add an arg for the local fs path (or even an open file handler).
I talked to Jmainguy on IRC and he thinks that using mkdtemp() should be fine from a permissions standpoint. There are some other thoughts though: * Using the module tempdir, the temp dir should be cleaned up automatically by the controller. If we use a separate tempfile, it becomes the calling code's responsibility to clean up the temp file (and the temp file can be left if the module raises an exception before it cleans up). * the module doesn't always have a file (in pipelining mode). When that happens, we can't use __file__ to determine where to place a tempfile. bcoca proposed adding a mkdtemp/mktemp function that could handle this but that is probably worthy of a separate PR. * bcoca also noted that overall, it's better not to have modules do this. Having two separate tasks works just as well (however, we need to start shipping roles at some point so that composing multiple tasks into a single logical unit is as easy as creating a module). Perhaps a note in the docstring that modules shouldn't tack on downloading files as a secondary function would be satisfactory for now.
The code that gets replaced by this method (in crc32 for ex), the url is downloaded to a dir local to the module source. Here, a new temp dir is created. Afaict, those could have different permissions. If the mkdtemp() result is more open than the __file__/* path, a downloaded file may be readable with different permissions. Haven't tested to see if that is a problem so would like some clarification.
Please use parentheses rather than backslashes for line continuations.
Yes it was Ian, but my example used joined fields for an update which isn't (yet) allowed. How about something like: ``` Author.objects.update(alias=Greatest('name', 'goes_by') ``` Which will also test the handling of varchars in a Greatest.
Again, error handling changed.
Sorry, these parameters are already checked in `save()` so there is no need to change these assertions.
At first sight it could be simplified to: ``` [option not in [None, False] for option in mutually_exclusive_options].count(True) ``` But I have a feeling even more simplification might be possible. Note that we can't simply cast default to bool, which would make this even simpler, as some valid dates evaluate to false: https://mail.python.org/pipermail/python-ideas/2014-March/026446.html
fields -> options comma after auto_now_add
Why this restriction? If several files are saved in a quick sequence â€” for instance with a text editor's "save all" feature" â€” only the first change willl be taken into account.
One option would be to pass add an arg for the local fs path (or even an open file handler).
I talked to Jmainguy on IRC and he thinks that using mkdtemp() should be fine from a permissions standpoint. There are some other thoughts though: * Using the module tempdir, the temp dir should be cleaned up automatically by the controller. If we use a separate tempfile, it becomes the calling code's responsibility to clean up the temp file (and the temp file can be left if the module raises an exception before it cleans up). * the module doesn't always have a file (in pipelining mode). When that happens, we can't use __file__ to determine where to place a tempfile. bcoca proposed adding a mkdtemp/mktemp function that could handle this but that is probably worthy of a separate PR. * bcoca also noted that overall, it's better not to have modules do this. Having two separate tasks works just as well (however, we need to start shipping roles at some point so that composing multiple tasks into a single logical unit is as easy as creating a module). Perhaps a note in the docstring that modules shouldn't tack on downloading files as a secondary function would be satisfactory for now.
The code that gets replaced by this method (in crc32 for ex), the url is downloaded to a dir local to the module source. Here, a new temp dir is created. Afaict, those could have different permissions. If the mkdtemp() result is more open than the __file__/* path, a downloaded file may be readable with different permissions. Haven't tested to see if that is a problem so would like some clarification.
Please use parentheses rather than backslashes for line continuations.
Yes it was Ian, but my example used joined fields for an update which isn't (yet) allowed. How about something like: ``` Author.objects.update(alias=Greatest('name', 'goes_by') ``` Which will also test the handling of varchars in a Greatest.
I think we can increase the readability with constant for `1` and `2`. ```python DAILY_COUNTER=1 WEEKLY_COUNTER=2 ``` And then you can just do `key=WEEKLY_COUNTER,`
Was there a `stop_node` method here before? It's a good idea to have both `stop_node` and `clean_node` defined. Or, if you expect the process to actually be gone by the time we hit the "stop" portion of the service lifecycle (`stop_node` is the "graceful shutdown" option), it's helpful to do a check that the process is actually gone (i.e. check that the actual state matches our expectations of what the state should be) and at least log a warning if the process is still running
Perhaps: ```suggestion def esxi_version_at_least(self, version): """ Check that the ESXi Host is at least a specific version number. Inputs: - version (tuple): a version tuple, for example (6, 7, 1) Returns: bool """ ``` Suggest moving into module_utils/vmware.py and providing a unit test.
```suggestion except OSError as err: ```
Module argument specification ensures this cannot be true. The field is required.
this can be initialized to `result = {'changed': False}`
```suggestion ) ```
does not match docs
To me it looks the module does handle check mode (https://github.com/ansible/ansible/pull/20734/files#diff-672a20e0686da08f7554286ee4283346R423). So in my opinion `supports_check_mode` an be set to true
just noticed this - I think we want `clean_shutdown=False` here to ensure we really kill the process if the normal attempt to gracefully shut it down failed
Could you please use Python regex instead of external egrep command ? egrep command may not be installed on given system.
redundant parens `% (names)`
This one is not legal. In python, a dictionary key must be hashable. dictionaries themselves are not hashable. Anything containing a dictionary would also not be hashable. So the key isn't valid.
[run_command](https://github.com/gdelpierre/ansible/blob/a6742ea78fba6b8206eaef61c6410e985a9a61f7/lib/ansible/module_utils/basic.py#L2558) won't raise an error and `rc` will not be zero.
`try` and `except` statement seems useless: nothing will raise `KafkaError`.
There is a default value for `executable` module parameter: then `get_bin_path` will never be called.
I mean: I saw that `kafka-topics` has a `--disable-rack-aware` parameter, I don't known if the module should allow user to use it or not.
Users won't be able to use `--disable-rack-aware`.
`module.param['executable']` could be a non executable file. [`ansible.module_utils.basic.is_executable`](https://github.com/ansible/ansible/blob/68aeaa5/lib/ansible/module_utils/basic.py#L657) should be used in order to check that. Moving all these checks in specific method would make unit tests easier.
Error message could mention the erroneous value (`len(self.topic)`). DONE
`yield from` is not allowed in async functions.
Does this need to be a separate method? Seems unnecessary to me.
I think `get_internal_type` is better to use.
Do we need to define `async_generator` in `__aiter__`? ```python async def _async_generator(self): # Generators don't actually start running until the first time you call # next() on them, so we make the generator object in the async thread # and then repeatedly dispatch to it in a sync thread. sync_generator = self.__iter__() # So, you can't raise StopIteration up through sync_to_async as # too much of the Python async system was originally built on # generators. Instead, we capture it inside a custom sync function # and change it to an exception we *can* propagate. # We should probably move this logic inside asgiref at some point. def custom_next(gen): try: return next(gen) except StopIteration: raise StopAsyncIteration() while True: try: yield await sync_to_async(custom_next)(sync_generator) except StopAsyncIteration: return # Note: __aiter__ is a *synchronous* method that has to then return an # *asynchronous* iterator/generator. Thus, we nest an async generator # inside it. # Also note: This is a generic iterable converter for now, and is going to # suffer a performance penalty on large sets of items due to the cost of # crossing over the sync barrier each time. We should write custom ones # for each Iterable subclass, but we'll need some work in Compiler first. def __aiter__(self): return self._async_generator() ```
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
Another name might be `seek()`.
immediatelly -> immediately
we only need the lock for setting the seed and calling `random.nextInt`, right? for the rest of the function we can avoid holding the lock since we're just iterating over an immutable list that can't change and calling stuff that is threadsafe anyway
Not sure it makes a difference but before it looks like we got `form=None` in the context.
It would be great, if this is to be migrated to 2.x, that user-supplied functions are called in try-catch. Perhaps not here but in call().
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
Thanks for the patch @Giebisch, I think we should adjust the `expected_outputs` instead to be properly formatted in order to ensure formatting is actually used. https://github.com/django/django/blob/78657a201308a0b53661e2b267914407a9fd7fa1/tests/test_runner/test_debug_sql.py#L90-L111 https://github.com/django/django/blob/78657a201308a0b53661e2b267914407a9fd7fa1/tests/test_runner/test_debug_sql.py#L124-L133 The adjustments made make the tests pass with and without the proposed changes.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
Also probably put that in the examples, for if users want that behavior but maybe don't know about `failed_when`.
I'm thinking these parameters shouldn't be filled in, since we're popping `stack_description` so if a user wanted a hard-fail for nonexistent stacks, they could do: ``` - cloudformation_facts: stack_name: not-real failed_when: not cloudformation['not-real'] ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
`test_jsonfield` looks good to me (since this is InspectDBTests, _introspection seems redundant).
camel2snake should indeed handle NotificationARNs properly (#25105)
Just checked, ec2_asg_facts is the only other module that references 'ar_ns', and that has code that copes either way.
Ooh, this is a nasty bug - if this wasn't in this PR,I wouldn't have spotted this. If this code happens elsewhere, it'll break when 2.4 goes out (because a now valid key is overwritten by the content of a now missing key)
We need a trailing space to separate times from results, e.g. ``` test_order_index (schema.tests.SchemaTests) ... 0.000sok
How about putting this right above where it's first used rather than far about it? (`if csrf_token is None:`)
> Why? ðŸ¤” Do you mean that get() shouldn't be called in post()? We use this pattern in many places and I don't see anything wrong in it. Okay, maybe I'm not used to common flows in class-based views. > Is it not already applied when next_page is set? Yes it is applied in that case. Still, a user could: 1. Click "logout", see the "logged out" page 2. Open a new tab and login 3. Restart their browser 4. The browser resubmits the "logout" tab, and the user is logged out again. I guess this pattern was here before, and this could be a separate issue.
> Yes they do, but I think on restarting and being presented with confirm on top of a blank screen, most people would just click "yes, resubmit". Resubmit raises 403 in such case, so I don't think it's an issue: https://user-images.githubusercontent.com/2865885/160332701-2a502657-ebe6-4a37-97d7-fa625856e9c9.mp4
> The browser resubmits the "logout" tab, and the user is logged out again. The browser asks whether it should resubmit POST requests no? But you are right it is not 100% nice if `next_page` is not used.
Ah, CSRF changing on logout saves us, of course. Brilliant. Thank you for testing! Not the kindest user experience but at least nothing breaks, so good from me.
Yes they do, but I think on restarting and being presented with confirm on top of a blank screen, most people would just click "yes, resubmit".
This is funky and looks wrong, couldn't we just do the two lines here instead? ``` context = self.get_context_data(**kwargs) return self.render_to_response(context) ``` Also I don't know if it's been mentioned but rendering during a successful POST shouldn't really be done, Django uses the [â€œpost-redirect-getâ€ pattern](https://en.wikipedia.org/wiki/Post/Redirect/Get) everywhere in normal forms to avoid refreshes causing repeat actions. Couldn't we apply that here? Perhaps complicating things a lot though...
@adamchainz Does it work for you? :point_up:
I would use `if kwargs:` and swap the branches unless you see a reason not to. In that case "# Slower, kwargs-ready version." may not be needed.
We need a trailing space to separate times from results, e.g. ``` test_order_index (schema.tests.SchemaTests) ... 0.000sok
How about putting this right above where it's first used rather than far about it? (`if csrf_token is None:`)
Yes they do, but I think on restarting and being presented with confirm on top of a blank screen, most people would just click "yes, resubmit".
@adamchainz Does it work for you? :point_up:
> The browser resubmits the "logout" tab, and the user is logged out again. The browser asks whether it should resubmit POST requests no? But you are right it is not 100% nice if `next_page` is not used.
Ah, CSRF changing on logout saves us, of course. Brilliant. Thank you for testing! Not the kindest user experience but at least nothing breaks, so good from me.
> Why? ðŸ¤” Do you mean that get() shouldn't be called in post()? We use this pattern in many places and I don't see anything wrong in it. Okay, maybe I'm not used to common flows in class-based views. > Is it not already applied when next_page is set? Yes it is applied in that case. Still, a user could: 1. Click "logout", see the "logged out" page 2. Open a new tab and login 3. Restart their browser 4. The browser resubmits the "logout" tab, and the user is logged out again. I guess this pattern was here before, and this could be a separate issue.
This is funky and looks wrong, couldn't we just do the two lines here instead? ``` context = self.get_context_data(**kwargs) return self.render_to_response(context) ``` Also I don't know if it's been mentioned but rendering during a successful POST shouldn't really be done, Django uses the [â€œpost-redirect-getâ€ pattern](https://en.wikipedia.org/wiki/Post/Redirect/Get) everywhere in normal forms to avoid refreshes causing repeat actions. Couldn't we apply that here? Perhaps complicating things a lot though...
> Yes they do, but I think on restarting and being presented with confirm on top of a blank screen, most people would just click "yes, resubmit". Resubmit raises 403 in such case, so I don't think it's an issue: https://user-images.githubusercontent.com/2865885/160332701-2a502657-ebe6-4a37-97d7-fa625856e9c9.mp4
I would use `if kwargs:` and swap the branches unless you see a reason not to. In that case "# Slower, kwargs-ready version." may not be needed.
It's likely fine. ðŸ¤” Let me have a play in the debugger tomorrow.
This is probably fine... We loose the **this thread** check but...
This unfortunately doesn't guarantee that all transactions are always rolled back. _dirty is never set if you run read-only queries in the default autocommit mode, yet transaction is started by any query (the cursor.is_dirty() checks if transactions are managed before setting ._dirty). It seems making sure ._rollback is called after every request would be a god idea if the connection isn't going to be persisted. Even better approach is to make the _dirty flag behave somewhat sanely. But this is not this issue's problem.
```suggestion assertEqual(len(threads_and_connections), 4) assertEqual(len(set(threads_and_connections)), 1) ```
The docstring should explain why such proxy is needed.
change 'type(job).__name__.lower()' to just 'job' no need to do it generic here
I would add here fetch_nested=True, because we always want to return steps.
also please rename entity_id to job_id, no need to have it too generic here
here you need just a 'steps' not whole module as well.
pass here just description, as it's the only parameter you need.
Minor but I'd move this control flow block after the `weights` one to match the args order.
use US spelling (behavior)
I see, thanks!
Can we check `fk_field` instead to avoid unnecessary queries? e.g. ```python for field in self._meta.private_fields:) if field.is_relation and hasattr(field, 'fk_field') and field.is_cached(self): if getattr(self, field.fk_field, None) is None: raise ValueError( "%s() prohibited to prevent data loss due to unsaved " "related object '%s'." % (operation_name, field.name) ) ```
```suggestion "%s() prohibited to prevent data loss due to unsaved " "related object '%s'." % (operation_name, field.name) ```
Ahh true, sorry for the noise. No changes are required.
I removed it.
> Shall I also remove `test_target_model_is_unsaved_save` and `test_target_model_is_unsaved_bulk_create`? Yes, they are redundant.
It looks like a duplicate of `test_save_unsaved_instance_on_generic_foreign_key` :thinking:
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Minor but I'd move this control flow block after the `weights` one to match the args order.
use US spelling (behavior)
Can we check `fk_field` instead to avoid unnecessary queries? e.g. ```python for field in self._meta.private_fields:) if field.is_relation and hasattr(field, 'fk_field') and field.is_cached(self): if getattr(self, field.fk_field, None) is None: raise ValueError( "%s() prohibited to prevent data loss due to unsaved " "related object '%s'." % (operation_name, field.name) ) ```
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
> Shall I also remove `test_target_model_is_unsaved_save` and `test_target_model_is_unsaved_bulk_create`? Yes, they are redundant.
It looks like a duplicate of `test_save_unsaved_instance_on_generic_foreign_key` :thinking:
This should be: ``params = config_params + params``
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
For python 2.6 compatibility, this and the following four strings have to look like `cmd = "ping {0} {1}".format(vrf, dest)`
Minor but I'd move this control flow block after the `weights` one to match the args order.
use US spelling (behavior)
I see, thanks!
Can we check `fk_field` instead to avoid unnecessary queries? e.g. ```python for field in self._meta.private_fields:) if field.is_relation and hasattr(field, 'fk_field') and field.is_cached(self): if getattr(self, field.fk_field, None) is None: raise ValueError( "%s() prohibited to prevent data loss due to unsaved " "related object '%s'." % (operation_name, field.name) ) ```
```suggestion "%s() prohibited to prevent data loss due to unsaved " "related object '%s'." % (operation_name, field.name) ```
Ahh true, sorry for the noise. No changes are required.
I removed it.
> Shall I also remove `test_target_model_is_unsaved_save` and `test_target_model_is_unsaved_bulk_create`? Yes, they are redundant.
It looks like a duplicate of `test_save_unsaved_instance_on_generic_foreign_key` :thinking:
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Minor but I'd move this control flow block after the `weights` one to match the args order.
use US spelling (behavior)
Can we check `fk_field` instead to avoid unnecessary queries? e.g. ```python for field in self._meta.private_fields:) if field.is_relation and hasattr(field, 'fk_field') and field.is_cached(self): if getattr(self, field.fk_field, None) is None: raise ValueError( "%s() prohibited to prevent data loss due to unsaved " "related object '%s'." % (operation_name, field.name) ) ```
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
> Shall I also remove `test_target_model_is_unsaved_save` and `test_target_model_is_unsaved_bulk_create`? Yes, they are redundant.
It looks like a duplicate of `test_save_unsaved_instance_on_generic_foreign_key` :thinking:
This should be: ``params = config_params + params``
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
For python 2.6 compatibility, this and the following four strings have to look like `cmd = "ping {0} {1}".format(vrf, dest)`
It should give 'Modification de Title et Historique.'. I guess a gettext call is missing inside the `LogEntry.get_change_message`.
They should always be the same but you might want to use `model._meta.object_name` instead.
Argh, I just realized the PluginLoader attribute stuffing (update_object) has been happening on `class_only` cases, which is broken for "same name, different behavior" cases (since subsequent loads will overwrite the class vars on the existing type object, not instance vars). It works here, since `all` is `yield`ing the plugin class immediately after calling `update_object` on it, but the whole pattern is busted for `class_only`, esp for things that might consult the class var during or after `__init__`. We've needed to rethink that for awhile, and this kinda sets it in a little more concrete. :(
@charettes Thanks :+1: I removed unnecessary connector, see #15511. As far as I'm aware we now prefer non-kwargs constructions for internal usage, see 9662193aea2ee982bc8e553c62499aca5e606755 and #14699,
Minor but this could have likely be simplified by using `reduce` to avoid the private `_connector` usage ```python condition = reduce( (Q(app_label=app_label, model__in=models) for app_label, models in needed_models) , operator.or_) ``` In all cases `Q(("app_label", app_label), ("model__in", models), _connector=Q.AND)` can be simplified to `Q(app_label=app_label, model__in=models)` since `_connector` defaults to `Q.AND`.
```suggestion # try to get collection world name first ```
Are we doing singleton `__new__`/`__call__` magic internally on some of the callback plugin types? Seems like this would never be `False` otherwise...
`and Historique` does not look right. It should give the untranslated `History` here (we are outside of the override).
It would be nice to be consistent about the ordering in `assertEqual` using it's `(variable, 'expected value')` but here and a couple other places it's opposite.
This implies that constructing some callbacks doesn't result in an instance, but also doesn't raise an Exception? That doesn't sound right...
No such meta field.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
No such meta field.
No such meta field.
No such meta field.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
Don't capture groups you don't use. Unused captured group.
Useless with timestamp available.
`_` is idiomatic way to denote unused variables.
Referring `url` from `url` looks like nonsense. Provide rationale.
I would suggest to use the ungettext_lazy('%(number)d minute', ''%(number)d minutes', 'number') syntax, if possible.
This must be checked **before** any processing.
This must be checked **before** any processing.
This can be simplified to `[None] * 5` or `(None, ) * 5`.
This regex should be split into multiple lines for bettercode perception.
1 is ok.
I would prefer indent similar to the former code.
We should indicate that this is only a guess - the value may be smaller or larger than the actual size.
this is not a 1.0 callback, its using 2.0 API
I guess it's ok to always return stable float here.
This must be checked **before** any processing.
Duration calculation is incorrect.
This must be checked **before** any processing.
This must be checked **before** any processing.
1 is ok.
This can be simplified to `[None] * 5` or `(None, ) * 5`.
I guess it's ok to always return stable float here.
this is not a 1.0 callback, its using 2.0 API
We should indicate that this is only a guess - the value may be smaller or larger than the actual size.
Here you ignore the fractional parts (milliseconds, microseconds). You need to add `1000 * timedelta(**time_params).microseconds`. (Also, you should store `timedelta(**time_params)` in a variable, instead of `time_in_seconds`, and work with that one.) I.e. something like: ``` .py time = timedelta(**time_params) time_in_nanoseconds = (time.seconds * 1000000 + time.microseconds) * 1000 ```
I would suggest to use the ungettext_lazy('%(number)d minute', ''%(number)d minutes', 'number') syntax, if possible.
This must be checked **before** any processing.
This must be checked **before** any processing.
This can be simplified to `[None] * 5` or `(None, ) * 5`.
This regex should be split into multiple lines for bettercode perception.
1 is ok.
I would prefer indent similar to the former code.
We should indicate that this is only a guess - the value may be smaller or larger than the actual size.
this is not a 1.0 callback, its using 2.0 API
I guess it's ok to always return stable float here.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
Oh, I just meant to suggest the `value` to `values` (plural) renaming, not more...
add trailing comma
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
get_fields() (add parenthesis) to distinguish it from other alternatives which are properties
include trailing ,
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
I would remove all aliases if possible.
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
Please, at most one alias, even better none. Having a long list of aliases is really bad UX IMO.
The corresponding option for restart is called `force_restart`. Calling this one `force_recreate` would be much better.
Space missing before `(megabytes)`.
You should emphasize that the module can and will not do any idempotence checking for this.
This should also be a `dict`.
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
`always_text` is gone.
(In general, I don't think modules should have such options.)
You should emphasize that the module can and will not do any idempotence checking for this.
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
What kind of values are valid here? I would expect this to be `float` based on the `0.0` in the description.
A general remark: you should always use complete sentences. So this should end with a period.
(In general, I don't think modules should have such options.)
You should emphasize that the module can and will not do any idempotence checking for this.
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
I would remove all aliases if possible.
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
What kind of values are valid here? I would expect this to be `float` based on the `0.0` in the description.
A general remark: you should always use complete sentences. So this should end with a period.
(In general, I don't think modules should have such options.)
You should emphasize that the module can and will not do any idempotence checking for this.
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
I would remove all aliases if possible.
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
add trailing comma
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
include trailing ,
get_fields() (add parenthesis) to distinguish it from other alternatives which are properties
add trailing comma
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
add trailing comma
include trailing ,
I would remove all aliases if possible.
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
```suggestion description: Can be used to request certificates from ECS, with C(provider=entrust). ```
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
```suggestion description: Whether the domain is eligible for submission of "EV" certificates. Will never be C(true) if I(ov_eligible) is C(false) ```
```suggestion sample: true ```
```suggestion description: Whether the domain is eligible for submission of "OV" certificates. Will never be C(false) if I(ov_eligible) is C(true) ```
```suggestion returned: success and I(ev_eligible) is C(true) and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING). ```
```suggestion returned: success and I(ov_eligible) is C(true) and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING). ```
```suggestion sample: true ```
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
add trailing comma
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
get_fields() (add parenthesis) to distinguish it from other alternatives which are properties
include trailing ,
add trailing comma
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
add trailing comma
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
get_fields() (add parenthesis) to distinguish it from other alternatives which are properties
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
include trailing ,
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
Use another lookup instead of `epoch` e.g. `second`.
```suggestion Test that the returned value for timezone consists of only uppercase ```
I would add here fetch_nested=True, because we always want to return steps.
here you need just a 'steps' not whole module as well.
also please rename entity_id to job_id, no need to have it too generic here
change 'type(job).__name__.lower()' to just 'job' no need to do it generic here
Is this going to be used for non-testing code in the future? If it is only going to be for metrics purposes maybe we can allow it to be non thread-safe just to not blocking on other critical paths.
Ah, I think this ought to be `local_size == remote_size`, since this conditional causes the file to be skipped for this strategy. That explains the odd behavior I saw with `date_size`
It looks like this is just a serial upload, how is this faster than the current S3 module? I definitely see the benefit of the glob & sync strategies, but it seems like this would be just as fast.
no restructured text (:class:) in docstrings please
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Any problem with: ``` @property def media(self): ```
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
add trailing comma
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
include trailing ,
get_fields() (add parenthesis) to distinguish it from other alternatives which are properties
add trailing comma
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
flake8 complains about missing spaces around `*`
```suggestion item, fields=fields, using=self.db, ```
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
add trailing comma
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
include trailing ,
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
Oh, I just meant to suggest the `value` to `values` (plural) renaming, not more...
no restructured text (:class:) in docstrings please
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
add trailing comma
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
add trailing comma
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
get_fields() (add parenthesis) to distinguish it from other alternatives which are properties
include trailing ,
add trailing comma
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
add trailing comma
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
get_fields() (add parenthesis) to distinguish it from other alternatives which are properties
include trailing ,
add trailing comma
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
This shouldn't be a "class in a class"; it should be in a utility section at the bottom of the file.
add trailing comma
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
A minor matter of style, but "GO below this line" isn't needed. All you need is the section header. I'd also suggest making the header more prominent - precede and follow the header text line with a line of `####` that goes the full width (or to col 76, anyway - 1 indent back from max line length) of the code. ``` ########################################################################### # Deprecated methods ########################################################################### ```
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
get_fields() (add parenthesis) to distinguish it from other alternatives which are properties
include trailing ,
add trailing comma
I would use a list comprehension rather than `list(map())`
A general remark: you should always use complete sentences. So this should end with a period.
(Same for the related options.)
This should really be: ``` - Set custom DNS search domains. (Use I(dns_search) with C('') if you don't wish to set the search domain.) ```
What kind of values are valid here? I would expect this to be `float` based on the `0.0` in the description.
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
Space missing before `(megabytes)`.
(In general, I don't think modules should have such options.)
You should emphasize that the module can and will not do any idempotence checking for this.
I would remove all aliases if possible.
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
Minor but I'd move this control flow block after the `weights` one to match the args order.
I believe selinux uses native strings (byte strings in python2 and text strings in python3) rather than always using byte strings. So that's why we weren't using to_bytes here earlier. We may need to move the to_native call earlier, though. I'm not sure if it was all selinux functions or only some of them which had bugs if the wrong type of string was passed to them.
This logic seems a little convoluted. Consider: ``` python conns = connetions.values() if settings.DATABASE_ROUTERS else [connections[DEFAULT_DB_ALIAS]] for conn in conns: if (connection.settings_dict['ENGINE'] != 'django.db.backends.dummy' and any(router.allow_migrate(connection.alias, label) for label in labels)): ```
Right (actually there's a bug in that code, the `ImproperlyConfigured` can never be raised).
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Minor but I'd move this control flow block after the `weights` one to match the args order.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
Something like: ``` patch --- test/units/module_utils/facts/test_collectors.py +++ test/units/module_utils/facts/test_collectors.py @@ -258,12 +258,8 @@ class TestPkgMgrFactsAptFedora(BaseFactsTest): "ansible_pkg_mgr": "apt" } - import ansible.module_utils.facts.system.pkg_mgr - ansible.module_utils.facts.system.pkg_mgr.os = Mock() - ansible.module_utils.facts.system.pkg_mgr.os.path = Mock() - ansible.module_utils.facts.system.pkg_mgr.os.path.exists = Mock(side_effect=_sanitize_os_path_apt_get) - - def test_collect(self): + @patch('ansible.module_utils.facts.system.pkg_mgr.os.path.exists', side_effect=_sanitize_os_path_apt_get) + def test_collect(self, mock_os_path_exists): module = self._mock_module() fact_collector = self.collector_class() facts_dict = fact_collector.collect(module=module, collected_facts=self.collected_facts) ```
nothing outside the fail_json/exit_json should write to stdout/stderr
This could cause problems if certain characters sneak into the repr() of the exception. (For instance, "{") Probably need to avoid writing to stdout and stderr.
Minor but I'd move this control flow block after the `weights` one to match the args order.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Something like: ``` patch --- test/units/module_utils/facts/test_collectors.py +++ test/units/module_utils/facts/test_collectors.py @@ -258,12 +258,8 @@ class TestPkgMgrFactsAptFedora(BaseFactsTest): "ansible_pkg_mgr": "apt" } - import ansible.module_utils.facts.system.pkg_mgr - ansible.module_utils.facts.system.pkg_mgr.os = Mock() - ansible.module_utils.facts.system.pkg_mgr.os.path = Mock() - ansible.module_utils.facts.system.pkg_mgr.os.path.exists = Mock(side_effect=_sanitize_os_path_apt_get) - - def test_collect(self): + @patch('ansible.module_utils.facts.system.pkg_mgr.os.path.exists', side_effect=_sanitize_os_path_apt_get) + def test_collect(self, mock_os_path_exists): module = self._mock_module() fact_collector = self.collector_class() facts_dict = fact_collector.collect(module=module, collected_facts=self.collected_facts) ```
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
This could cause problems if certain characters sneak into the repr() of the exception. (For instance, "{") Probably need to avoid writing to stdout and stderr.
nothing outside the fail_json/exit_json should write to stdout/stderr
Is it possible ? For me it would mean that it exist a package without version.
It doesn't work if pkg_provides contain a version: ``` max@mde-oxalide % pacman -Qqei perl | grep Provide Provides : perl-archive-tar=2.30 perl-attribute-handlers=1.01 perl-autodie=2.29 perl-autoloader=5.74 perl-autouse=1.11 perl-b-debug=1.26 perl-base=2.27 perl-bignum=0.49 perl-carp=1.50 perl-compress-raw-bzip2=2.074 perl-compress-raw-zlib=2.076 perl-config-perl-v=0.29 perl-constant=1.33 perl-cpan-meta-requirements=2.140 perl-cpan-meta-yaml=0.018 perl-cpan-meta=2.150010 perl-cpan=2.20 perl-data-dumper=2.170 perl-db_file=1.840 perl-devel-ppport=3.40 perl-devel-selfstubber=1.06 perl-digest-md5=2.55 perl-digest-sha=6.01 perl-digest=1.17_01 perl-dumpvalue=1.18 perl-encode=2.97 perl-encoding-warnings=0.13 perl-env=1.04 perl-experimental=0.019 perl-exporter=5.73 perl-extutils-cbuilder=0.280230 perl-extutils-constant=0.25 perl-extutils-install=2.14 perl-extutils-makemaker=7.34 perl-extutils-manifest=1.70 perl-extutils-parsexs=3.39 perl-file-fetch=0.56 perl-file-path=2.15 perl-file-temp=0.2304 perl-filter-simple=0.95 perl-filter-util-call=1.58 perl-getopt-long=2.5 perl-http-tiny=0.070 perl-i18n-collate=1.02 perl-i18n-langtags=0.43 perl-if=0.0608 perl-io-compress=2.074 perl-io-socket-ip=0.39 perl-io-zlib=1.10 perl-io=1.39 perl-ipc-cmd=1.00 perl-ipc-sysv=2.07 perl-json-pp=2.97001 perl-lib=0.64 perl-libnet=3.11 perl-locale-codes=3.56 perl-locale-maketext-simple=0.21_01 perl-locale-maketext=1.29 perl-math-bigint-fastcalc=0.5006 perl-math-bigint=1.999811 perl-math-bigrat=0.2613 perl-math-complex=1.5901 perl-memoize=1.03_01 perl-mime-base64=3.15 perl-module-corelist=5.20180622 perl-module-load-conditional=0.68 perl-module-load=0.32 perl-module-loaded=0.08 perl-module-metadata=1.000033 perl-net-ping=2.62 perl-params-check=0.38 perl-parent=0.236 perl-pathtools=3.74 perl-perl-ostype=1.010 perl-perlfaq=5.021011 perl-perlio-via-quotedprint=0.08 perl-pod-checker=1.73 perl-pod-escapes=1.07 perl-pod-parser=1.63 perl-pod-perldoc=3.2801 perl-pod-simple=3.35 perl-pod-usage=1.69 perl-podlators=5.006 perl-safe=2.40 perl-scalar-list-utils=1.50 perl-search-dict=1.07 perl-selfloader=1.25 perl-socket=2.027 perl-storable=3.08 perl-sys-syslog=0.35 perl-term-ansicolor=4.06 perl-term-cap=1.17 perl-term-complete=1.403 perl-term-readline=1.17 perl-test-harness=3.42 perl-test-simple=1.302133 perl-test=1.31 perl-text-abbrev=1.02 perl-text-balanced=2.03 perl-text-parsewords=3.30 perl-text-tabs=2013.0523 perl-thread-queue=3.12 perl-thread-semaphore=2.13 perl-threads-shared=1.58 perl-threads=2.22 perl-tie-file=1.02 perl-tie-refhash=1.39 perl-time-hires=1.9759 perl-time-local=1.25 perl-time-piece=1.3204 perl-unicode-collate=1.25 perl-unicode-normalize=1.26 perl-version=0.9923 perl-xsloader=0.30 ```
```suggestion pkg_version = line.partition(':')[-1].strip() ```
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Outer parentheses are not idiomatic in python.
> return it directly ``` python return self.url_result('http://imgur.com/%s' % album_id) ```
You don't need list here. Just return it directly.
`.format()` is not supported for some Python versions
It's added in Python 2.6, so it's OK to use it as youtube-dl supports Python 2.6+ only.
Use the `query` parameter of `_download_webpage` instead of `sanitized_Request`.
i think that `flv_data` should be used with `data` param and `Content-Type` dict used with `headers` param of `_download_webpage`.
and the function that normally used to encode postdata is `urlencode_postdata`.
`_` is idiomatic way to denote unused variables.
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
Please remove this blank line as requested by Paolo.
I'd rather raise an exception to account for the fact that assertions are ignored with `python -O` unless there's some reason to prefer the assert.
hmmm.. now I think I'm wrong, you want to only have the first partial func.. `func.func` shouldn't anyway be another partial or else that would be problematic I think, as views have to return something else.. Sorry for disturbance ;)
happy new year ! yeah effectively finally this is required for python < 3.5 as you explain (I just tested it in an interactive session). I'm happy to open a PR with this specific change so.. should not be too long to come..
Please open a ticket to track the bug (all non-trivial changes should have a ticket).
chop "one of" add comma before "or"
It's nice to include a message for the exception (we had a ticket about adding messages to all of them)
You can also drop the parentheses :wink:
put closing parenthesis on the next line
Yeah that's what I suspected too. Stupid SQL.
Minor but I'd move this control flow block after the `weights` one to match the args order.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
If it is not required, you don't have to add `'required: False`.
If it is not required, you don't have to add `'required: False`.
I would simply use units_si and units_iec here. ```python 'choices': units_si + units_iec + ['', 'compact', 'cyl', 'chs'], ``` In fact, make these global variables anyhow.
Minor but I'd move this control flow block after the `weights` one to match the args order.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
If it is not required, you don't have to add `'required: False`.
If it is not required, you don't have to add `'required: False`.
I would simply use units_si and units_iec here. ```python 'choices': units_si + units_iec + ['', 'compact', 'cyl', 'chs'], ``` In fact, make these global variables anyhow.
Make sure you use `format_lazy()` to prevent issues with translated strings: ```suggestion self.verbose_name_plural = format_lazy('{}s', self.verbose_name) ``` This is also consistent with the following: https://github.com/django/django/blob/c70cd2a926ffab47f6613e83e0c8828eb6c2c064/django/db/models/options.py#L188-L191
I know this is preexisting issue, but the condition should be checking the boolean value, not just the existence
I'd vote for making `returning` a `property` instead of a stealth field option at least for now because this is not something we've done in the past. ```python @property def returning(self): return hasattr(self.default, 'as_sql') AutoField.returning = True ``` That would make `DateTimeField(default=Now)` work and avoid the ambiguity of `default=Now, returning=False`. We'd still have to deal with backends that don't support returning fields.
parentheses are also fine with me.
personally in favor of longer lines vs. backslashes
Maybe @felixxm or @carltongibson can guide, but I believe it'd be good practice to use a `warnings.warn` in `__init__`, although a deprecation timeline has not been determined for `django.contrib.postgres.field.JSONField`.
chop blank line
WDYT about adding `or self.title` here? ``` python self.title = title or self.title or self.lookup_title ``` This would add support for the following: ``` python class MyStaffFilter(BooleanFieldListFilter): title = "By can haz admin" # no effect :( overwritten in __init__ @admin.register(User) class UserAdmin(UserAdminBase): list_filter = ( ("is_staff", MyStaffFilter), ) ```
I see before: `to_fields=[], from_fields=[self.object_id_field_name]` after: `to_fields=[object_id_field], from_fields=[]` I could very well be missing something...
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
This should probably be the default for postgresql's `schema_editor.sql_create_index`.
Can you please run `pages_per_range` through `quote_value()` (internal function on the schema editor).
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
The proposed implementation isn't exactly what I had in mind, but I'll have to look a bit later to see if the idea I had in mind is any better. For `BrinIndex`, I guess those features weren't considered by the patch author. Probably no reason not to add them.
You have some unmerged lines here
consider assertRaisesMessage to make the test a bit more specific.
(And round-tripping of the messages is already tested in other tests)
I would suggest to set `type='int' for vlan.
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
You need to wrap the second instantiation in its own assertRaises to actually test it.
I don't see a need for string interpolation in cases like this.
Running the entire test suite I could reproduce the error, but not if only running `django.contrib.gis`. Not sure exactly the cause but this code seems suspicious as we leave a different model admin registered for other tests.
please use a variable for this string so that if it changes, we don't have to update it below as well
Also, I don't think it's a complete solution as annotations on the LHS still don't work properly, see failing test: ```python def test_exclude_nullable_fields_annotation(self): from django.db.models.functions import Abs number = Number.objects.create(num=1, other_num=1) Number.objects.create(num=2, other_num=2, another_num=2) self.assertSequenceEqual( Number.objects.annotate(x=Abs('another_num')).exclude(other_num=F('x')), [number], ) self.assertSequenceEqual( Number.objects.annotate(x=Abs('another_num')).exclude(num=F('x')), [number], ) ```
> my changs affect when LHS is annotation field and RHS is not field. I know, but this is also an issue with nullable annotation so we should fix it in this PR. If it is a different branch in `build_filter()` then we have another reason to add some hook.
We can try to refactor [existing code](https://github.com/django/django/blob/a9cf954e6174450057ea1065aa2ccbbd12f59b65/django/db/models/sql/query.py#L1351-L1375) and add some internal hook instead of having similar (identical?) logic in two places.
I meant for the entire string here to be a constant; otherwise looks good to me.
Shouldn't this be `When I(containers) is C(yes)`? (Same for the others.)
I think this description is off :)
Add a task ```.yaml - debug: var: result.docker_host_facts ``` And similar tasks after the other examples.
Minor but I'd move this control flow block after the `weights` one to match the args order.
There is no point to use `remove_start` since line is always a string.
this is a non atomic operation and can lead to file corruption, write to a temp file and use shared 'atomic_move' to put the file into place
this forcibly creates/removes the files w/o checking if they exist or not, doing so would allow you to offer a 'changed' s state. Also it is encouraged that modules are safe to re-run w/o affecting systems when not needed.
[`get_param`](https://github.com/spotify/pyfg/blob/master/pyFG/forticonfig.py#L249) should not raise an exception. I guess you could check if `f.running_config[path]` contains `module.params['name']ÃŒÂ€`.
Don't use except without an exception type. What could be the exceptions here ? It would be better to check if `get_param` returns `None`.
You may want to consider using set operations here.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I prefer `if not group_members`.
Parentheses around `e.message` are useless.
Good catch, I will remove it before final squash.
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Please don't make lines longer! There was nothing really wrong with this line before
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
Ah, yes, I miss read the code. Nothing to change here.
I'd expect to see a test for this. Please audit test coverage carefully and make sure all lines are covered.
ipt_load_stderr doesnt appear to be defined anywhere.
This 'default_chains' doesn't seem to be used anywhere.
I'd recommend a more explicit import, ie ``` python from ansible.module_utils.basic import AnsibleModule' ``` And using regular imports for 're', 'shlex', 'os', 'json', 'tempfile'
you can use `state` to avoid the 'or' to the user
Nitpick, but shouldn't these be assertEqual()? This would be consistent with `if len(queryset) == 2` instead of `if len(queryset) is 2`.
Chop the blank lines
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
change 'type(job).__name__.lower()' to just 'job' no need to do it generic here
I would add here fetch_nested=True, because we always want to return steps.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
here you need just a 'steps' not whole module as well.
also please rename entity_id to job_id, no need to have it too generic here
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Not something that you have to change but I think this could be simplified to ```python for obj_func in self.obj_func_mapping.values(): obj_func() ```
I think this might explode if `obj_type` is not one of `functions`, `indexes` or `tables`, maybe it would be safer to do something like? ```suggestion obj_func = self.obj_func_mapping.get(obj_type.strip()) if obj_func is not None: obj_func() else: module.warn("Unknown filter option '{}'".filter(obj_type.strip()) ```
This isn't needed, since we now only support 2.6+ anyway.
it seems this and other parameters are missing from docs
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
TextInput -> Input? I suppose a `test_input.py` file would be better. I wasn't sure about the `test_no_trailing_newline_in_attrs` test -- it's meant to test a template rather than Python code -- probably I could have clarified that. `strict=True` isn't needed since the newline isn't being tested.
I think we don't need it. but lets @felixxm decide about it. Thanks for the patch :+1:
You should probably expect unicode strings
(Similarly, mark any string that you are going to call decode on with as a b"string".)
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
```suggestion ({b'str'.encode(DEFAULT_ENCODING): b'str'.encode(DEFAULT_ENCODING)}, {u'str': u'str'}), ```
You're checking two separate properties here. This should be in a separate test.
Having a class for just one test method is unnecessary.
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Plz also use `match` arg here
This whole block could've been done with a nested list comprehension without any need for temporary variable introduction: ```python names = [ name for name_list in names for name in name_list.split(',') ] ``` Alternative FP approach would look like: ```python from itertools import partial, reduce ... names = reduce( lambda l, e: l + e, map( partial(str.split, sep=','), names ), [], ) ``` (but Pythonistas don't like such style in general)
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
You should probably expect unicode strings
```suggestion ({b'str'.encode(DEFAULT_ENCODING): b'str'.encode(DEFAULT_ENCODING)}, {u'str': u'str'}), ```
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
(Similarly, mark any string that you are going to call decode on with as a b"string".)
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
You're checking two separate properties here. This should be in a separate test.
Having a class for just one test method is unnecessary.
No need to parametrize with just one case.
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
It probably makes sense to test that the exception reason also matches expectations
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
Same as above: unnecessary fixture test.
```suggestion assert ansible_json_encoder.default(test_input) == {'__ansible_vault': expected} ```
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
If this test won't be implemented it should be removed.
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
This is lifted from Python 3.10 so we should attribute appropriately and also ensure that we remove it when no longer needed: ```suggestion if PY310: from contextlib import aclosing else: from contextlib import AbstractAsyncContextManager # Backport of contextlib.aclosing() from Python 3.10. # Copyright (C) 2020 Python Software Foundation (see LICENSE.python). class aclosing(AbstractAsyncContextManager): """ Async context manager for safely finalizing an asynchronously cleaned-up resource such as an async generator, calling its ``aclose()`` method. """ def __init__(self, thing): self.thing = thing async def __aenter__(self): return self.thing async def __aexit__(self, *exc_info): await self.thing.aclose() ```
Does this need to be a separate method? Seems unnecessary to me.
Need spaces around `+` sign.
`quote` isn't used anywhere.
These don't appear to be "public" attributes, so we should stay consistent with indicating their privateness by prepending their names with an `_` character.
This seems problematic. What happens if you do this: ``` reused_f = F('pk') qs1 = Model1.objects.filter(pk=reused_f) qs2 = Model2.objects.filter(pk=reused_f) ``` By my reading the reused_f will be first prepared for qs1, getting Model1.pk as col. Then the same object is reused and prepared for qs2, this time the reused_f's col is overwritten with Model2.pk. The ExpressionNodes must be either stateless, or they must be copied on prepare(). Doing the latter will be easier. A basic copy method could do the following: ``` def copy(self): copy = Empty() copy.__class__ = self.__class__ copy.__dict__ = self.__dict__.copy() return copy ```
Can you just do `name.startswith('__') and not name.endswith('__')`? Simpler is better
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Can you please test with Java 11 or newer? Looks like you tested with Java 8 which uses the slower crc32c method.
Moving `close` outside of locked scope LGTM
I think the expectation is that these 2 would be atomic (i.e. would be bad if one thread executed 615, then another thread executed 615 again and got the same sequence number, before the first thread got a chance to execute 616). Also I think the expectation is that batches that are ordered one after another in the queue would get the sequence numbers in the same order (i.e. that batch that is later in the queue would get higher sequence number). Previously these expectations were protected by the queue lock so "poll", "get sequence", "update sequence" would execute as atomic block, with this change the operations could interleave.
No `else` needed since we used `return` for both other cases. For the exception, I think we can just throw `ClassCastException` since `IllegalStateException` doesn't fit very well for this case. I would also make the message a bit more generic to avoid it going stale when we add more `Records` subtypes. For example: ```java "The record type is " + partition.records().getClass().getSimpleName() + ", which is not a subtype of " + Records.class.getSimpleName() + ". This method is only safe to call if the `FetchResponse` was deserialized from bytes."
One more thing, let's call this `recordsOrFail` to make it clear that the operation is not necessarily safe.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
For future reference, Kafka uses relatively long lines: up to 100 is considered fine. I can fix the instances in this PR during the merge, but good to take into account in future contributions.
Yes, they were.
`a['adress']` could be used instead of `(len(a['address']) > 0)`.
since the plugin is called `aws_ssm`, I'd change this to `ansible_aws_ssm_retries`
Please follow best practice for this fail message ```python if not HAS_GITLAB_PACKAGE: module.fail_json(msg=missing_required_lib("python-gitlab"), exception=GITLAB_IMP_ERR) ```
Should be interesting to return data to the user, like variables `added`, `updated`, `removed
Please add check-mode support (and if possible also diff support).
This shouldn't be needed, just do `to_bytes(cmd, errors='surrogate_or_strict')` without the PY3 conditional. Doing `""` on Python results in a byte string so `to_bytes()` will just be a no-op for that version. Also one style we try to follow in Ansible it to prefix any byte strings to `b_`. That was we can easily infer the type of string being used based on the variable name. If you have an explicit native string (bytes on 2, unicode on 3) then we prefix with `n_`. Otherwise a general text string has no prefix.
also would be super cool if we would move this to the top of the file. ``` 178 Python Imports 179 ============== 180 181 To make it clear what a module is importing, imports should not be sprinkled throughout the code. ```
Nowadays we list all the functionality we use from a specific library explicitly. So: ```python from ansible.module_utils.basic import AnsibleModule ```
Do you really need to write it character by character. Seems to be quite inefficient when you could write in chunks. Also use `to_bytes(c, errors='surrogate_or_strict')` instead of `.encode()`
Shouldn't we have a decorator for that? (kinda off-topic, I know)
+1 (in separate patch)
This is hard to parse visually. I suggest: ``` return '{} @> {}'.format(lhs, rhs), params ``` or even: ``` sql = '{} @> {}'.format(lhs, rhs) params = lhs_params + rhs_params return sql, params ``` The same pattern occurs several times in the file.
This is the way lookups (and SQL in general throughout the ORM) is written currently. We could pick some other way (and the latter one is clearly more readable), but it is best to keep this file consistent with the rest of the code base.
You are using different ids in url and here. Don't do that.
Must not be fatal.
This call on connection object can raise ConnectionError exception which needs to handled here else it will result in stack trace in output. I have raised a https://github.com/ansible/ansible/pull/43353 to fix it for other supported platforms
Future improvement: Along the same line of thought as above, should we rename request to something like santricity_request? Just trying to improve the readability.
Hm... How about then introducing a prefix for such settings at least for this module? So, if user wants to provide binary data to be stored in property, they'd have to say, e.g. 'B64:blah' (where blah would be base64-encoded data). Caveat being that if users wants to store string which starts with B64:, they'd have to do something along the lines of B64:QjY0Og== (QjY0Og== being base64-encoded B64:, I think you get the picture :) Now, truth be told, not sure if this would be more acceptable for Ansible as project, but I'd see it as more consistent. Maybe second opinions on this could be useful too :)
"Importing from django.core.urlresolvers is deprecated in favor of django.urls." (don't need to mention the version since RemovedInDjango20Warning appears with the message)
The following properties indicate if
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
++ thanks for changing this :)
please fail if required stuff is null
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: cannot create point collection with empty set of points at __randomizedtesting.SeedInfo.seed([9959A23819CF838B:6FE2182D9705AE]:0) at org.elasticsearch.common.geo.builders.ShapeBuilder.<init>(ShapeBuilder.java:97) at org.elasticsearch.common.geo.builders.MultiPointBuilder.<init>(MultiPointBuilder.java:46) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseMultiPoint(GeoWKTShapeParserTests.java:82) ... ```
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
looks like there are two levels of indentation instead of one
nit: formatting, add some whitespaces
```suggestion masked = var_list[key].get('masked', False) ```
Tests shouldn't rely on internal APIs to trigger a bug.
I would just make this a plain function that calls fetch_url and then lowercases the keys instead of making it a decorator. I'm a little hesitant about overriding the name (I'd use a different name than fetch_url) but I can see you do that so you don't have to change the name everywhere. Since I'm not the module author, I won't block it over that but I think it's better style to use a new name.
I was testing this in my threading fact gathering branch, the .get method won't help with 'blocked' processes and will inconsistently return or just hang forever (I'm still trying to get a reliable reproducer). I found this by 'simulating' bad NFS mounts by throttling the traffic to the nfs server the VMs are using.
In py3 this is mitigated by the subproccess methods having a timeout, but this is not available on py2
its not a question of slow, but 'blocking' i.e accessing a bad NFS mount, that puts the process in 'B' status for the kernel and does not allow interruption.
see my loop over waiting on threads here #49398 to get a py2/py3 compatible way
Seems close enough, so I'll take that test as good as the 'unkillable' status is the main issue once the problem surfaces. I still have not been able to reproduce the problem i saw reliably, it just has happens a few time across the many times I've tested the gathering threaded code. It seems to happen less with Py3 versions, but since I'm not sure about how to trigger it, that is just anecdotal data. I was thinking of downgrading to nfsv3 since that was a lot more prone to this kind of issue ... but its probably not worth it.
I don't see much value in this docstring.
lines can be longer
```suggestion masked = var_list[key].get('masked', False) ```
Tests shouldn't rely on internal APIs to trigger a bug.
I would just make this a plain function that calls fetch_url and then lowercases the keys instead of making it a decorator. I'm a little hesitant about overriding the name (I'd use a different name than fetch_url) but I can see you do that so you don't have to change the name everywhere. Since I'm not the module author, I won't block it over that but I think it's better style to use a new name.
I was testing this in my threading fact gathering branch, the .get method won't help with 'blocked' processes and will inconsistently return or just hang forever (I'm still trying to get a reliable reproducer). I found this by 'simulating' bad NFS mounts by throttling the traffic to the nfs server the VMs are using.
In py3 this is mitigated by the subproccess methods having a timeout, but this is not available on py2
its not a question of slow, but 'blocking' i.e accessing a bad NFS mount, that puts the process in 'B' status for the kernel and does not allow interruption.
see my loop over waiting on threads here #49398 to get a py2/py3 compatible way
Seems close enough, so I'll take that test as good as the 'unkillable' status is the main issue once the problem surfaces. I still have not been able to reproduce the problem i saw reliably, it just has happens a few time across the many times I've tested the gathering threaded code. It seems to happen less with Py3 versions, but since I'm not sure about how to trigger it, that is just anecdotal data. I was thinking of downgrading to nfsv3 since that was a lot more prone to this kind of issue ... but its probably not worth it.
I don't see much value in this docstring.
lines can be longer
```suggestion masked = var_list[key].get('masked', False) ```
Tests shouldn't rely on internal APIs to trigger a bug.
I would just make this a plain function that calls fetch_url and then lowercases the keys instead of making it a decorator. I'm a little hesitant about overriding the name (I'd use a different name than fetch_url) but I can see you do that so you don't have to change the name everywhere. Since I'm not the module author, I won't block it over that but I think it's better style to use a new name.
I was testing this in my threading fact gathering branch, the .get method won't help with 'blocked' processes and will inconsistently return or just hang forever (I'm still trying to get a reliable reproducer). I found this by 'simulating' bad NFS mounts by throttling the traffic to the nfs server the VMs are using.
In py3 this is mitigated by the subproccess methods having a timeout, but this is not available on py2
its not a question of slow, but 'blocking' i.e accessing a bad NFS mount, that puts the process in 'B' status for the kernel and does not allow interruption.
see my loop over waiting on threads here #49398 to get a py2/py3 compatible way
Seems close enough, so I'll take that test as good as the 'unkillable' status is the main issue once the problem surfaces. I still have not been able to reproduce the problem i saw reliably, it just has happens a few time across the many times I've tested the gathering threaded code. It seems to happen less with Py3 versions, but since I'm not sure about how to trigger it, that is just anecdotal data. I was thinking of downgrading to nfsv3 since that was a lot more prone to this kind of issue ... but its probably not worth it.
I don't see much value in this docstring.
lines can be longer
I'd use `ref.assert_called_once_with()` here.
From looking at the code the call could differ from Python 2 to 3 and is really an implementation detail which is not worth testing after all. My initial reflexion was more about the fact `assert_not_called()` was used below instead of `self.assertFalse(ref.called)` but now I realize there's no `assert_called()` method. LGTM
Can you elaborate on this except branch. Trying to figure out when this happens.
WDYT about adding `or self.title` here? ``` python self.title = title or self.title or self.lookup_title ``` This would add support for the following: ``` python class MyStaffFilter(BooleanFieldListFilter): title = "By can haz admin" # no effect :( overwritten in __init__ @admin.register(User) class UserAdmin(UserAdminBase): list_filter = ( ("is_staff", MyStaffFilter), ) ```
Are we doing singleton `__new__`/`__call__` magic internally on some of the callback plugin types? Seems like this would never be `False` otherwise...
Argh, I just realized the PluginLoader attribute stuffing (update_object) has been happening on `class_only` cases, which is broken for "same name, different behavior" cases (since subsequent loads will overwrite the class vars on the existing type object, not instance vars). It works here, since `all` is `yield`ing the plugin class immediately after calling `update_object` on it, but the whole pattern is busted for `class_only`, esp for things that might consult the class var during or after `__init__`. We've needed to rethink that for awhile, and this kinda sets it in a little more concrete. :(
It looks like both `__init__` method can be removed as they simply delegate to `AlterFooTogether.__init__`.
```suggestion # try to get collection world name first ```
Unlike an "app", I wouldn't say that a model is "installed". Two solutions: - check if the app is installed and adjust the error message accordingly: "app X isn't installed" or "app X doesn't provide model Y" - just rephrase do "which doesn't exist"
It would have been more appropriate if `run_checks()` took an `Apps` instance as a parameter instead of a list of `AppConfig`s.
no space between "e. g."
I guess we could simply use `f.content_type = Image.MIME.get(image.format)`
test failure says this should be `(None, None)`
```suggestion b_colldirs = list_collection_dirs(coll_filter=coll_filter) ```
```suggestion for b_path in b_colldirs: ```
A small oversight I noticed in an old Python 3.7.0 virtualenv: https://github.com/django/django/pull/13393
I've changes to `django.utils.inspect.func_supports_parameter()`.
`id`, `name` and `zone` (which should be 'zonename': 'zone') are returned by default. Can be omitted in modules,
The module should return `changed=True` even in check mode.
I don't like to fail at this point. There are way better options to handle this, we could implement a new force param to let a user "remove and add" an new image store by the users intention and if not force, just show a warning message to the users that we can not change the image store, but would recreate on force. We already have such a logic for in the cs_instance module for changing the offering on a running instance.
I like the fact that you move the potential conversion from text to bytes lower in the call stack, to where the API actually needs a byte string.
For things with this many arguments, I'd really like to see keywords used to avoid any order mixups in the future.
I have a concern on pagination here, `list_objects` is going to default to 100 or 500 items, which is fine most of the time (it was like this prior) but would make a nice enhancement.
Yeah, sorry, you're right, that's fairly obvious now you point that out. Again, we can use standard set operators for this ``` if set(users) - set(current_group_members_list): ```
`**{'Bucket': bucket}` is equivalent to `Bucket=bucket`. Please use the latter :)
indentation here isn't right, should only be four spaces. flake8 doesn't seem to catch that I strongly recommend looking at the results of flake8 --ignore=E402 --max-line-length=160 aws_api_gateway.py to make the code more like all the other code.
Should this have some exception handling? (I suggest here rather than paginated_list as paginated_list might not be able to handle exceptions if it does the retry)
By non-trivial I just mean values that aren't None or empty strings. I'm not sure how much user control we expect over those settings but I might not have read the parameters carefully enough. The following untested somewhat pseudocode illustrates the simpler approach: ``` @AWSRetry(**backoff_params) def list_keys_with_backoff(connection, bucket): pg = connection.get_paginator('list_objects_v2') return [obj['Key'] for obj in pg.paginate(Bucket=bucket).build_full_result()['Objects']] def list_keys(connection, bucket): try: return list_keys_with_backoff(connection, bucket) except botocore.exceptions.ClientError as e: etc... ```
This function seems to be much more complicated than it needs to be. Does anything call this function with non-trivial values for prefix, marker or max_keys? (I'm guessing previously the function called itself to get the next page). I would argue for using paginator with build_full_result in list_keys_with_backoff and then the calling functions (`delete_keys` etc.) can just use that directly rather than having to manage the page combination themselves.
You can do `return pg.paginate(Bucket=bucket).build_full_result()` Not a blocker though
I like the fact that you move the potential conversion from text to bytes lower in the call stack, to where the API actually needs a byte string.
For things with this many arguments, I'd really like to see keywords used to avoid any order mixups in the future.
I have a concern on pagination here, `list_objects` is going to default to 100 or 500 items, which is fine most of the time (it was like this prior) but would make a nice enhancement.
Yeah, sorry, you're right, that's fairly obvious now you point that out. Again, we can use standard set operators for this ``` if set(users) - set(current_group_members_list): ```
`**{'Bucket': bucket}` is equivalent to `Bucket=bucket`. Please use the latter :)
indentation here isn't right, should only be four spaces. flake8 doesn't seem to catch that I strongly recommend looking at the results of flake8 --ignore=E402 --max-line-length=160 aws_api_gateway.py to make the code more like all the other code.
Should this have some exception handling? (I suggest here rather than paginated_list as paginated_list might not be able to handle exceptions if it does the retry)
By non-trivial I just mean values that aren't None or empty strings. I'm not sure how much user control we expect over those settings but I might not have read the parameters carefully enough. The following untested somewhat pseudocode illustrates the simpler approach: ``` @AWSRetry(**backoff_params) def list_keys_with_backoff(connection, bucket): pg = connection.get_paginator('list_objects_v2') return [obj['Key'] for obj in pg.paginate(Bucket=bucket).build_full_result()['Objects']] def list_keys(connection, bucket): try: return list_keys_with_backoff(connection, bucket) except botocore.exceptions.ClientError as e: etc... ```
This function seems to be much more complicated than it needs to be. Does anything call this function with non-trivial values for prefix, marker or max_keys? (I'm guessing previously the function called itself to get the next page). I would argue for using paginator with build_full_result in list_keys_with_backoff and then the calling functions (`delete_keys` etc.) can just use that directly rather than having to manage the page combination themselves.
You can do `return pg.paginate(Bucket=bucket).build_full_result()` Not a blocker though
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
single line looks more readable here
Not sure it makes a difference but before it looks like we got `form=None` in the context.
can be ignored
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
hm maybe that's what I've searched for
can be ignored
This logic seems ignore the use case of removing all tags.
I'm not a huge fan of this, we should be able to build the result set from module.params and the response.
Same order, type first.
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
single line looks more readable here
Not sure it makes a difference but before it looks like we got `form=None` in the context.
can be ignored
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
hm maybe that's what I've searched for
can be ignored
This logic seems ignore the use case of removing all tags.
I'm not a huge fan of this, we should be able to build the result set from module.params and the response.
Same order, type first.
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
single line looks more readable here
Not sure it makes a difference but before it looks like we got `form=None` in the context.
can be ignored
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
hm maybe that's what I've searched for
can be ignored
This logic seems ignore the use case of removing all tags.
I'm not a huge fan of this, we should be able to build the result set from module.params and the response.
Same order, type first.
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
single line looks more readable here
Not sure it makes a difference but before it looks like we got `form=None` in the context.
can be ignored
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
hm maybe that's what I've searched for
can be ignored
This logic seems ignore the use case of removing all tags.
I'm not a huge fan of this, we should be able to build the result set from module.params and the response.
Same order, type first.
Does this need to be a separate method? Seems unnecessary to me.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Not sure it makes a difference but before it looks like we got `form=None` in the context.
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Yeah we need to handle all exceptions that would be otherwise handled in `TaskExecutor` (`_execute()`, `run()`) and `Worker.run()`.
I'm not a huge fan of this, we should be able to build the result set from module.params and the response.
single line looks more readable here
Does this need to be a separate method? Seems unnecessary to me.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Not sure it makes a difference but before it looks like we got `form=None` in the context.
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Yeah we need to handle all exceptions that would be otherwise handled in `TaskExecutor` (`_execute()`, `run()`) and `Worker.run()`.
I'm not a huge fan of this, we should be able to build the result set from module.params and the response.
single line looks more readable here
This module supports check mode, but you're not checking whether you're in check mode before adding tags.
can be ignored
when using dict you can just do `dict(msg=to_text(body), message_count=....`.
hm maybe that's what I've searched for
can be ignored
boto3_conn now handles region problems, no need to do it in the module
Same order, type first.
I'm not a huge fan of this, we should be able to build the result set from module.params and the response.
This logic seems ignore the use case of removing all tags.
Not required with AnsibleAWSModule
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
single line looks more readable here
Not sure it makes a difference but before it looks like we got `form=None` in the context.
can be ignored
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
hm maybe that's what I've searched for
can be ignored
This logic seems ignore the use case of removing all tags.
I'm not a huge fan of this, we should be able to build the result set from module.params and the response.
Same order, type first.
I like the fact that you move the potential conversion from text to bytes lower in the call stack, to where the API actually needs a byte string.
For things with this many arguments, I'd really like to see keywords used to avoid any order mixups in the future.
I have a concern on pagination here, `list_objects` is going to default to 100 or 500 items, which is fine most of the time (it was like this prior) but would make a nice enhancement.
Yeah, sorry, you're right, that's fairly obvious now you point that out. Again, we can use standard set operators for this ``` if set(users) - set(current_group_members_list): ```
`**{'Bucket': bucket}` is equivalent to `Bucket=bucket`. Please use the latter :)
indentation here isn't right, should only be four spaces. flake8 doesn't seem to catch that I strongly recommend looking at the results of flake8 --ignore=E402 --max-line-length=160 aws_api_gateway.py to make the code more like all the other code.
Should this have some exception handling? (I suggest here rather than paginated_list as paginated_list might not be able to handle exceptions if it does the retry)
By non-trivial I just mean values that aren't None or empty strings. I'm not sure how much user control we expect over those settings but I might not have read the parameters carefully enough. The following untested somewhat pseudocode illustrates the simpler approach: ``` @AWSRetry(**backoff_params) def list_keys_with_backoff(connection, bucket): pg = connection.get_paginator('list_objects_v2') return [obj['Key'] for obj in pg.paginate(Bucket=bucket).build_full_result()['Objects']] def list_keys(connection, bucket): try: return list_keys_with_backoff(connection, bucket) except botocore.exceptions.ClientError as e: etc... ```
This function seems to be much more complicated than it needs to be. Does anything call this function with non-trivial values for prefix, marker or max_keys? (I'm guessing previously the function called itself to get the next page). I would argue for using paginator with build_full_result in list_keys_with_backoff and then the calling functions (`delete_keys` etc.) can just use that directly rather than having to manage the page combination themselves.
You can do `return pg.paginate(Bucket=bucket).build_full_result()` Not a blocker though
Isn't this going to be really slow to create a `sync_to_asyc` instance and go through the sync-to-sync barrier for each item of the collection? Shouldn't we refactor iterables to separate their data-fetching (I/O) and data-massaging phases instead and only have `__aiter__` do `sync_to_async` for the fetching phase? By looking at `ModelIterable.__iter__` that seems doable.
single line looks more readable here
Not sure it makes a difference but before it looks like we got `form=None` in the context.
can be ignored
Yeah, only on the first load if possible, otherwise maybe pass in verbosity from the runserver command and only show it if a higher verbosity is passed in. I think it would also be neat if instead of saying "Watching for file changes with StatReloader" it could figure out the common error and just say something like "Watchman client not available, watching for file changes with StatReloader". That way it's more clear what's going on, but not spamming the console either.
hm maybe that's what I've searched for
can be ignored
This logic seems ignore the use case of removing all tags.
I'm not a huge fan of this, we should be able to build the result set from module.params and the response.
Same order, type first.
I like the fact that you move the potential conversion from text to bytes lower in the call stack, to where the API actually needs a byte string.
For things with this many arguments, I'd really like to see keywords used to avoid any order mixups in the future.
I have a concern on pagination here, `list_objects` is going to default to 100 or 500 items, which is fine most of the time (it was like this prior) but would make a nice enhancement.
Yeah, sorry, you're right, that's fairly obvious now you point that out. Again, we can use standard set operators for this ``` if set(users) - set(current_group_members_list): ```
`**{'Bucket': bucket}` is equivalent to `Bucket=bucket`. Please use the latter :)
indentation here isn't right, should only be four spaces. flake8 doesn't seem to catch that I strongly recommend looking at the results of flake8 --ignore=E402 --max-line-length=160 aws_api_gateway.py to make the code more like all the other code.
Should this have some exception handling? (I suggest here rather than paginated_list as paginated_list might not be able to handle exceptions if it does the retry)
By non-trivial I just mean values that aren't None or empty strings. I'm not sure how much user control we expect over those settings but I might not have read the parameters carefully enough. The following untested somewhat pseudocode illustrates the simpler approach: ``` @AWSRetry(**backoff_params) def list_keys_with_backoff(connection, bucket): pg = connection.get_paginator('list_objects_v2') return [obj['Key'] for obj in pg.paginate(Bucket=bucket).build_full_result()['Objects']] def list_keys(connection, bucket): try: return list_keys_with_backoff(connection, bucket) except botocore.exceptions.ClientError as e: etc... ```
This function seems to be much more complicated than it needs to be. Does anything call this function with non-trivial values for prefix, marker or max_keys? (I'm guessing previously the function called itself to get the next page). I would argue for using paginator with build_full_result in list_keys_with_backoff and then the calling functions (`delete_keys` etc.) can just use that directly rather than having to manage the page combination themselves.
You can do `return pg.paginate(Bucket=bucket).build_full_result()` Not a blocker though
Does this need to be a separate method? Seems unnecessary to me.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Not sure it makes a difference but before it looks like we got `form=None` in the context.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
Yeah we need to handle all exceptions that would be otherwise handled in `TaskExecutor` (`_execute()`, `run()`) and `Worker.run()`.
I'm not a huge fan of this, we should be able to build the result set from module.params and the response.
single line looks more readable here
```suggestion msg = ... with self.assertRaisesMessage(ValueError, msg): ```
As long as they both equals it shouldn't make a difference. By that I mean you should have a test asserts `Index('foo') == Index(F('foo'))` An alternative could be to defer that to `create_sql`.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
We should omit `default_bounds` when the default value is used: ```suggestion if self.default_bounds and self.default_bounds != '[)': kwargs['default_bounds'] = self.default_bounds ```
It's an attribute so folks can try to change it dynamically. I would add `if self.default_bounds and ..`
it seems this and other parameters are missing from docs
the `f.close()` is implied as part of the with block. Also, I don't think this should be in the module since it looks like a debug statement.
These parens aren't necessary for unpacking the return values.
"for BRIN indexes" doesn't seem consistent with usual error messages.
I think it's fine.
As long as they both equals it shouldn't make a difference. By that I mean you should have a test asserts `Index('foo') == Index(F('foo'))` An alternative could be to defer that to `create_sql`.
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
Ah! Of course, sorry I missed that.
It's an attribute so folks can try to change it dynamically. I would add `if self.default_bounds and ..`
We should omit `default_bounds` when the default value is used: ```suggestion if self.default_bounds and self.default_bounds != '[)': kwargs['default_bounds'] = self.default_bounds ```
Won't this result in a confusing SQL-level error if you pass in `None` for `expression` or `pos` by accident? I'm assuming `length` is the only one we actually expect to possibly be `None`. If that's true, I think it would be better to do something like: ``` expressions = [expression, pos] if length is not None: expressions.append(length) ``` Or, if you prefer: `expressions = (expression, pos, length) if length is not None else (expression, pos)`
Please ignore, my suggestion is invalid syntax.
comma after tuple
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
Oh, I just meant to suggest the `value` to `values` (plural) renaming, not more...
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
``` return files.getlist(name) if self.multiple else files.get(name) ```
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
```python self.assertHTMLEqual( field.widget.render('name', []), ( '<ul>' '<li><label><input type="checkbox" name="name" value="%d" ' 'data-slug="entertainment">Entertainment</label></li>' '<li><label><input type="checkbox" name="name" value="%d" ' 'data-slug="test">A test</label></li>' '<li><label><input type="checkbox" name="name" value="%d" ' 'data-slug="third-test">Third</label></li>' '</ul>' ) % (self.c1.pk, self.c2.pk, self.c3.pk), ) ```
I'm not sure what the best solution is, but at the moment I would lean towards not supporting `.repo` files and asking people to specify the repo URL directly. To properly support `.repo` files we would have to download them, parse them and compare them to the configured repos, which is a lot of effort. Are you aware of a usecase that actually requires `.repo` files? I no longer use suse in my day job and all repo usage that I can remember also worked fine with pointing to directories directly.
comma after tuple
chop blank line
width, height, and offset
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
should probably use `if stdout is not None:`
flake8 complains about missing spaces around `*`
Oh, I just meant to suggest the `value` to `values` (plural) renaming, not more...
I think that this can be replaced with the following as above: ```python @unittest.skipIf(geos_version_tuple() < (3, 7), 'GEOS >= 3.7.0 is required') ``` I think we only need to worry about testing the GEOS behaviour if it is actually supported; even if this is only testing the exception-raising behaviour.
I feel like it should be `_delete_unique_sql`'s decision to do the bool casting. Maybe some backends will need access to the condition to appropriately delete it. Lets just pass `condition=condition` and let database backends do `if condition`.
like diff = load_config(self._module, config_xml, [])
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
use a single loop? ~~~python for server in _retrieve_servers(api_key): server = Vultr.normalize_result(server, SCHEMA) .... ~~~ ~~~
`Check the configuration files` seems vague, I propose: `Check inventory file and vultr configuration files`.
When `hostname_preference` is equal to `name`, there is no need to define `ansible_host`.
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
@charettes Ideas welcome. Can you take a look? I really don't like the idea of using `psycopg2` in expressions.
I would prefer to wrap value with `Value()` and compile `options` separately.
Yeah, we still can't use dict comprehensions until 2.6 is formally dropped, sorry.
Another new addition is a bunch of *_config methods raising NotImplementedError if not overriden in the subclass. Which is to say, `replace_config()` is also unnecessary here
What I meant was that you could stick the following in the top of `_destruct_iterable_mapping_values()` to make it more DRY: ```python if isinstance(data, Mapping): yield from data.items() return ``` I don't see why you think we'd need an extra for-loop...
There are only two uses of `_destruct_iterable_mapping_values()` and we use this same pattern exactly. I think that you could push the `isinstance(..., Mapping)` check into that function.
As a separate commit, I think that it is also probably worth moving that global function to be a `@staticmethod` on the `CaseInsensitiveMapping` class. It is closely associated with this class and would avoid the need to import it separately in `django.http.response`.
No need to create a `dict` if you're simply iterating over values.
Might want to avoid `type` shadowing.
Might want to avoid `id` shadowing.
this class has a bunch of details of WSGI environ so I think it more accurately belongs in `django.http.request`
well, not die with unexpected exception .. tempted to say there is no real reason the type should be incorrect for any keys. So ending in an error should be fine, just not an unhandled one.
a simpler fix i was already working on: `+ display.display(json.dumps(text, cls=AnsibleJSONEncoder, sort_keys=True, indent=4))`
^ that seems to be an expression not really a data type issue (sorting keys, this is another known json issue), in any case, there is also an existing `jsonify` in module_utils.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
This isn't needed, since we now only support 2.6+ anyway.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
These parens aren't necessary for unpacking the return values.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
Any problem with: ``` @property def media(self): ```
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
no restructured text (:class:) in docstrings please
Is there a typo? I'm not sure what "hub" means in this sentence.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
This isn't needed, since we now only support 2.6+ anyway.
These parens aren't necessary for unpacking the return values.
I think that this can be replaced with the following as above: ```python @unittest.skipIf(geos_version_tuple() < (3, 7), 'GEOS >= 3.7.0 is required') ``` I think we only need to worry about testing the GEOS behaviour if it is actually supported; even if this is only testing the exception-raising behaviour.
chop blank line
should probably use `if stdout is not None:`
I'm not sure what the best solution is, but at the moment I would lean towards not supporting `.repo` files and asking people to specify the repo URL directly. To properly support `.repo` files we would have to download them, parse them and compare them to the configured repos, which is a lot of effort. Are you aware of a usecase that actually requires `.repo` files? I no longer use suse in my day job and all repo usage that I can remember also worked fine with pointing to directories directly.
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
Please check test coverage carefully. I didn't spot a test for this change.
Use another lookup instead of `epoch` e.g. `second`.
I feel like it should be `_delete_unique_sql`'s decision to do the bool casting. Maybe some backends will need access to the condition to appropriately delete it. Lets just pass `condition=condition` and let database backends do `if condition`.
```suggestion Test that the returned value for timezone consists of only uppercase ```
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
I think that this can be replaced with the following as above: ```python @unittest.skipIf(geos_version_tuple() < (3, 7), 'GEOS >= 3.7.0 is required') ``` I think we only need to worry about testing the GEOS behaviour if it is actually supported; even if this is only testing the exception-raising behaviour.
chop blank line
should probably use `if stdout is not None:`
I'm not sure what the best solution is, but at the moment I would lean towards not supporting `.repo` files and asking people to specify the repo URL directly. To properly support `.repo` files we would have to download them, parse them and compare them to the configured repos, which is a lot of effort. Are you aware of a usecase that actually requires `.repo` files? I no longer use suse in my day job and all repo usage that I can remember also worked fine with pointing to directories directly.
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
Please check test coverage carefully. I didn't spot a test for this change.
Use another lookup instead of `epoch` e.g. `second`.
I feel like it should be `_delete_unique_sql`'s decision to do the bool casting. Maybe some backends will need access to the condition to appropriately delete it. Lets just pass `condition=condition` and let database backends do `if condition`.
```suggestion Test that the returned value for timezone consists of only uppercase ```
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
Oops, I misread the diff and see that you only modified the existing archives. Still an explanation of exactly what going on would be nice as it's not obvious to me.
There's no need to define the extra `settings_dir` variable as `pathlib` gives us more flexibility: ```suggestion settings_file_path = self.test_dir / filename / "__init__.py" settings_file_path.parent.mkdir() ```
Does the raised exception have the same `errno` on both platforms? If that's the case you could do: ``` python with self.assertRaises(IOError) as ctx: # .... self.assertEqual(ctx.exception.errno, errno.WHATEVER) ```
Seems okay to me. I guess the alternative would to vary the message based on OS. Not sure that complexity is required though.
This test is problematic on Windows: ``` ====================================================================== FAIL: test_notafile_error (template_tests.test_loaders.FileSystemLoaderTests) ---------------------------------------------------------------------- Traceback (most recent call last): File "c:\Users\Tim\code\django\tests\template_tests\test_loaders.py", line 266 , in test_notafile_error self.engine.get_template('first') AssertionError: "Is\ a\ directory" does not match "[Errno 13] Permission denied: u'c:\\Users\\Tim\\code\\django\\tests\\template_tests\\templates\\first'" ```
Won't `symlink_path` and `original_path` be removed automatically as part of the cleanup? `tempfile.TemporaryDirectory` says "On completion of the context or destruction of the temporary directory object the newly created temporary directory and all its contents are removed from the filesystem."
```python msg = 'Script does-not-exist does not exist.' with self.assertRaisesMessage(RuntimeError, msg): ```
I think `finally` isn't needed also.
I know, was just wondering if it's intended that it works that way.
last loaded wins, but iirc, we reverse search on handlers list
```not (foo is None)``` => ```foo is not None```
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
I think all the calls to `render()` can be removed (it worked for me in this test at least)
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
```not (foo is None)``` => ```foo is not None```
I think all the calls to `render()` can be removed (it worked for me in this test at least)
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
Gotcha, okay I think this is acceptable.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
```not (foo is None)``` => ```foo is not None```
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Need to import reduce from ansible.module_utils.six.moves.
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
What's the purpose of http://example.com/v.flv here? It always gives a 404 error and I think it's unrelated to iQiyi
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
This sentence is too long, maybe: ```python choice = self._choice_input( f"It is impossible to add the field '{field_name}' with " f"'auto_now_add=True' to {model_name} without providing a " f"default. This is because the database needs something to " f"populate existing rows.\n", [ ... ```
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
```not (foo is None)``` => ```foo is not None```
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
```suggestion item, fields=fields, using=self.db, ```
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
@pilou-, cyberark-bizdev I had a chance to talk to bcoca today and he said the best way to do this is to pass a list of dicts to the lookup. Then either of these playbook constructs can be valid: ``` with_cyberaskpassword: - {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - {'appid': 'Application2', 'query': 'safe=Safe2;Folder=root;Object=User2', 'output': 'password,passprops.username,passprops.address'} with_cyberarkpassword: {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - debug: msg='{{ lookup('cyberark_password', {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'}) }}' ``` Implementing that will take a little restructuring. I'll go into that in the run method below.
Okay, so @cyberark-bizdev take out my first example. The last two are what we're shooting for. (The middle one is the same as bcoca's example, just in compact yaml instead of "readable" yaml ;-)
not a list of dicts, just pass a dict to the with: ```yaml with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ```
Use the Python 2 and low 3 Time Machine: `'url too short: %s' % (video_pre_parts, )` or: `'url too short: %(video_pre_parts)s' % {'video_pre_parts': video_pre_parts, }` or: `'url too short: {video_pre_parts}'.format(video_pre_parts=video_pre_parts)` or: `'url too short: {0}'.format(video_pre_parts)` No doubt there are other ways (eg `....format(**locals())`
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
It probably makes sense to test that the exception reason also matches expectations
Same here, not following order `(value, expected)`
Docstring shouldn't have a trailing space. Docstring sentences should end with a period.
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
consider assertRaisesMessage to make the test a bit more specific.
Guess it's better to use `self.assertGreater(len(para), 0)` instead
(And round-tripping of the messages is already tested in other tests)
Same here, not following order `(value, expected)`
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
(And round-tripping of the messages is already tested in other tests)
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
Use the Python 2 and low 3 Time Machine: `'url too short: %s' % (video_pre_parts, )` or: `'url too short: %(video_pre_parts)s' % {'video_pre_parts': video_pre_parts, }` or: `'url too short: {video_pre_parts}'.format(video_pre_parts=video_pre_parts)` or: `'url too short: {0}'.format(video_pre_parts)` No doubt there are other ways (eg `....format(**locals())`
Generally, I'd have put the static regex directly as the first argument of `re.search()`.
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
I would chop blank lines in this test.
Chop `Ensure that`.
Similarly, ```if tc['skip'].get('i')```
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
This is not fixed.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
I would chop blank lines in this test.
Chop `Ensure that`.
Similarly, ```if tc['skip'].get('i')```
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
This is not fixed.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
wo -> without
I would chop blank lines in this test.
Chop `Ensure that`.
Similarly, ```if tc['skip'].get('i')```
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Lack of data is denoted by `None` not 0.
Breaks. Read coding conventions.
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
I would chop blank lines in this test.
Chop `Ensure that`.
Breaks. Read coding conventions.
Lack of data is denoted by `None` not 0.
Similarly, ```if tc['skip'].get('i')```
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
I think `if opt_val:` is sufficient.
`print('Aborting: --%s must be a top-level module.' % opt_name.replace('_', '-'))`
@hramezani I think you removed `setattr(options, opt_name, os.path.normpath(opt_val))` by mistake. My proposition was to remove only `else`, i.e. ```python if '.' in opt_val: print('Aborting: --%s must be a top-level module.' % opt_name.replace('_', '-')) sys.exit(1) setattr(options, opt_name, os.path.normpath(opt_val)) ```
IMO `else`is unnecessary i.e. ```python if '.' in opt_val: ... setattr(options, opt_name, os.path.normpath(opt_val)) ```
Do note that this only looks at the primary credential cache. This means that if you use a collection credential cache such as DIR, KEYRING or KCM (default in most distributions these days), you might not see a ticket that exists in a non-primary credential cache. Maybe consider using `klist -A`, or try a `kswitch -p <principal>`? The latter is easier to parse, but might break stuff if people are still using the old krb5 APIs (rather than GSSAPI) and were using the primary credential that you just made secondary.
```suggestion Test that the returned value for timezone consists of only uppercase ```
the AnsibleModule class has a method to find executables. Use that method: kinit_cmd = module.get_bin_path("kinit", required=True)
Again, this only works on the primary credential cache. If the ticket is in another, this might not work.
Do note that this does not take `self.principal` into account, neither is that being checked. So you might return with `changed=False` if there's a tgt for a totally different principal.
I think what you've got it good, thanks
Capital letter at the beginning.
Dot at the end.
Capital letter at the beginning and dot at the end.
You don't have to quote the values in the list.
Dot at the end.
stray single quote
No need to quote most of these valuev in the examples unless they are needed. We try to avoid doing this for the examples.
Why have both `schema` and `newschema`? I would assume that if I specify another value for `schema`, that the schema will be changed.
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
This overload does not take `Materialized` parameter
Too long line.
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Breaks. Read coding conventions.
Lack of data is denoted by `None` not 0.
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
The Shippable CI failure is due to: ``` 2016-12-19 16:09:41 Run command: python2.4 -m compileall -fq ./lib/ansible/modules/infrastructure/stacki/stacki_host.py 2016-12-19 16:09:41 Compiling ./lib/ansible/modules/infrastructure/stacki/stacki_host.py ... 2016-12-19 16:09:41 File "./lib/ansible/modules/infrastructure/stacki/stacki_host.py", line 174 2016-12-19 16:09:41 rc = stack_r.status_code if stack_r.status_code != 200 else stack_r.status_code 2016-12-19 16:09:41 ^ 2016-12-19 16:09:41 SyntaxError: invalid syntax ``` This may also apply to line 210
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
ACk. Just FYI OS discovery can be done after CLICONF is implemented.
Too long line.
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Breaks. Read coding conventions.
Lack of data is denoted by `None` not 0.
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
The Shippable CI failure is due to: ``` 2016-12-19 16:09:41 Run command: python2.4 -m compileall -fq ./lib/ansible/modules/infrastructure/stacki/stacki_host.py 2016-12-19 16:09:41 Compiling ./lib/ansible/modules/infrastructure/stacki/stacki_host.py ... 2016-12-19 16:09:41 File "./lib/ansible/modules/infrastructure/stacki/stacki_host.py", line 174 2016-12-19 16:09:41 rc = stack_r.status_code if stack_r.status_code != 200 else stack_r.status_code 2016-12-19 16:09:41 ^ 2016-12-19 16:09:41 SyntaxError: invalid syntax ``` This may also apply to line 210
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
ACk. Just FYI OS discovery can be done after CLICONF is implemented.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
Breaks. Read coding conventions.
Lack of data is denoted by `None` not 0.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
ACk. Just FYI OS discovery can be done after CLICONF is implemented.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
Breaks. Read coding conventions.
Lack of data is denoted by `None` not 0.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
ACk. Just FYI OS discovery can be done after CLICONF is implemented.
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Can we calculate that beforehand? Also `os.environ.get` could make this much more readable.
Use a single line. Our [style guide](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style) allows lines up to 119 characters if it helps readability.
I don't think printing an informational message is that useful. Just `pass` here instead
f is already at 0, the `truncate()` is uselesss.
This variable is never assigned ```suggestion api.get_collection_versions('namespace', 'collection') ```
Why don't you just encode suffix like `u'-\u00c5\u00d1\u015a\u00cc\u03b2\u0141\u00c8'.encode('utf-8')` and for the no_temp_unicode override just do `suffix = b'-ansible'`? No need for this check which is arguably more confusing.
You can pass `verbosity=0` instead to completely silence the command instead of creating an unused `StringIO` container.
chop "should" (just state the behavior)
Again, error handling changed.
I learned recently that you can use actual separate literals to improve readability: ```suggestion @pytest.mark.parametrize(['url', 'expected'], [ ```
we should also return if we both delegate executions and delegate_facts
this got named use_backend
For the cases where I've wanted to use a nose test generator, I've ended up just making the test class inherit from object. You lose the TestCase methods (various self.assert*) but usually test generators methods have pretty simple assertions so not too much of a issue. To me it is simpler than injecting the support into a TestCase subclass. (an example is https://github.com/alikins/ansible/blob/6e8a3d0c25fdeaa349351c6636b062376c30b0a6/test/units/plugins/lookup/test_ini.py)
If you don't modify the task args then you don't need to copy() them. However, I think we want to add a ```use``` parameter for the action plugin and we will want to delete that parameter before we pass the args on to the module. So this section would look like: ``` python new_module_args = self._task.args.copy() del new_module_args['use'] [...] result.update(self._execute_module(module_name=module, module_args=new_module_args, task_vars=task_vars, wrap_async=self._task.async_val)) ```
Add that the user can specify the backend to use via the ```use``` parameter.
Still need to add something like ```You can manually specify use_backend to tell the module whether to use the yum (yum-3) or dnf (yum-4) backend.```
If you use a catchall exception handler like this then you should also provide the traceback like this: ``` python import traceback from ansible.module_utils._text import to_native [...] module.fail_json(msg='Unanticipated error running atomic: %s' % to_native(e), exception=traceback.format_exc()) ```
A message string would good to say that image is not preset or something similar.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Add that the user can specify the backend to use via the ```use``` parameter.
Still need to add something like ```You can manually specify use_backend to tell the module whether to use the yum (yum-3) or dnf (yum-4) backend.```
How about: ```suggestion from ansible.module_utils.parsing.convert_bool import boolean as to_bool try: verify = to_bool(option) except TypeError: # it wasn't a boolean value verify = option # Set to a CA bundle: finally: if verify is False: # is only set to bool if try block succeeds requests.packages.urllib3.disable_warnings() self._display.warning( u"SSL verification of %s disabled" % self.foreman_url, ) return verify ```
Explicit is better than what's happening implicitly: ```suggestion host=to_text(host), err=to_text(err), url=to_text(self.foreman_url))) ```
So add `type='str'` here too. And we tend to sort lists if the order is of no importance.
Used by the base class to be able to handle all ansible data https://github.com/ansible/ansible/blob/devel/lib/ansible/plugins/callback/__init__.py#L123
i recommend using the ansible json encoder as it deals with 'special' types and will avoid a lot of serialization errors you'll get with the stock json.
no need to do this check, the plugin never gets called if disabled
use `missing_required_lib` from `ansible.module_utils.basic`
use the `missing_required_lib` function from `ansible.module_utils.basic`
```suggestion raise InvalidCacheBackendError('Could not find backend {backend!r}: {e}') from e ```
this got named use_backend
This should set self._connected = True (not really needed right now but could have ramifications later) and also use display to print a message. For instance, this is what the docker connect plugin does: ``` def _connect(self, port=None): """ Connect to the container. Nothing to do """ super(Connection, self)._connect() if not self._connected: display.vvv(u"ESTABLISH DOCKER CONNECTION FOR USER: {0}".format( self.actual_user or '?'), host=self._play_context.remote_addr ) self._connected = True ```
I presume who ever put it there had a different opinion ;) I would recommend to leave it be.
we should also return if we both delegate executions and delegate_facts
As you've noticed, we don't want to do this here because in this case the pkg_mgr is coming from a pre-existing fact.
can we rename this to make clear that this has nothing to do with git, but is only about s3? `--upload-to-s3` seems a bit long
don't do this by default, set from cli `self._options` does not exist or is None
I think we have a problem here. The version that is supposed to be supplied as a parameter should only consist of major.minor version like `2.0` (so that all 2.x version go into the same repository) - this one is `2.0.0-beta1` though.
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
```suggestion - When using in a chroot environment you always need to specify the name of the unit with the extension. For example, C(crond.service). ```
`always_text` is gone.
Not strictly necessary as the default for parameter is that they're not required.
This seems problematic. What happens if you do this: ``` reused_f = F('pk') qs1 = Model1.objects.filter(pk=reused_f) qs2 = Model2.objects.filter(pk=reused_f) ``` By my reading the reused_f will be first prepared for qs1, getting Model1.pk as col. Then the same object is reused and prepared for qs2, this time the reused_f's col is overwritten with Model2.pk. The ExpressionNodes must be either stateless, or they must be copied on prepare(). Doing the latter will be easier. A basic copy method could do the following: ``` def copy(self): copy = Empty() copy.__class__ = self.__class__ copy.__dict__ = self.__dict__.copy() return copy ```
Omit these lines please.
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
What kind of values are valid here? I would expect this to be `float` based on the `0.0` in the description.
Can you name this a little more verbosely? I can't unsee "get best"
Same here, seems a ValueError would be cleaner.
Note that format strings changed in python-2.7 So things like "{} {}".format() needs to change to "{0} {1}".format() for python-2.6 compatibility.
When there is only one capture group just use unnamed one.
When hitting an error you must exit with module.fail_json
When hitting an error you must exit with module.fail_json
When hitting an error you must exit with module.fail_json
Not catching non-200 responses.
```python freq = current_config.get('Log rotation frequency') # daily (Once per day at midnight) if freq is not None: ```
I think a brief sentence is worthy. Nothing is worse than trying something only to figure out the target API on the server isn't compatible. We should try to remember to ask this question of all new modules and substantial changes.
Can you name this a little more verbosely? I can't unsee "get best"
here too: pysopenssl --> pyopenssl
If you don't modify the task args then you don't need to copy() them. However, I think we want to add a ```use``` parameter for the action plugin and we will want to delete that parameter before we pass the args on to the module. So this section would look like: ``` python new_module_args = self._task.args.copy() del new_module_args['use'] [...] result.update(self._execute_module(module_name=module, module_args=new_module_args, task_vars=task_vars, wrap_async=self._task.async_val)) ```
this got named use_backend
Still need to add something like ```You can manually specify use_backend to tell the module whether to use the yum (yum-3) or dnf (yum-4) backend.```
Add that the user can specify the backend to use via the ```use``` parameter.
So removed `required=False` and add `type='str' instead. Do this for the others as well.
Leave the trailing comma. This is explicitly allowed by python in order to make line-modifications (like moving the order of lines, or adding lines) without having to update unrelated lines.
So add `type='str'` here too. And we tend to sort lists if the order is of no importance.
here too: pysopenssl --> pyopenssl
use `missing_required_lib` from `ansible.module_utils.basic`
use the `missing_required_lib` function from `ansible.module_utils.basic`
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Similarly, ```if tc['skip'].get('i')```
I don't think this works? block_id defaults to the boolean False in the parameters to this function. So you probably need to check: ``` python if block_id is not False: ``` rather than checking against a string.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
This will fail on python3 for a similar reason as the conditional check noted above. filter returns a generator. The generator can't be indexed. Use a list comprehension instead: ``` python block = [b for b in blocks if b.base_ip == base_ip] ```
Thanks - I've replaced all uses of `filter` with comprehensions.
the message here looks like a failure case rather than an exit case... Perhaps it should use module.fail_json or the message should be changed? Stylistically, exit_json() cases should retun back to the main function in the module and let the exit_json() occur there. It's more flexible to changing needs inside of the module than calling exit_json() here.
In theory a locale dependant pattern could be provided and `locale.setlocale` be called but this is really unlikely.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Similarly, ```if tc['skip'].get('i')```
I don't think this works? block_id defaults to the boolean False in the parameters to this function. So you probably need to check: ``` python if block_id is not False: ``` rather than checking against a string.
Thanks - I've replaced all uses of `filter` with comprehensions.
This will fail on python3 for a similar reason as the conditional check noted above. filter returns a generator. The generator can't be indexed. Use a list comprehension instead: ``` python block = [b for b in blocks if b.base_ip == base_ip] ```
the message here looks like a failure case rather than an exit case... Perhaps it should use module.fail_json or the message should be changed? Stylistically, exit_json() cases should retun back to the main function in the module and let the exit_json() occur there. It's more flexible to changing needs inside of the module than calling exit_json() here.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
The argument will already be a character string, no need to decode it.
This must be checked **before** any processing.
And this one.
Omit 0/1. There was a past commit that removed all usage of that since it just adds verbosity.
I think we can drop the empty line here.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Also, it seems like our convention for docs versions is quite bad. If we didn't want to use `.`, we could use something else like `_`.
Hmm, doesn't seem like this is correct. >version ['1', '0', '0-SNAPSHOT'] major_minor ['1', '0'] Extracting ['tar', 'xf', '/Users/ijuma/src/kafka/core/build/distributions/kafka_2.11-1.0.0-SNAPSHOT-site-docs.tgz', '--strip-components', '1'] Traceback (most recent call last): File "./release.py", line 235, in <module> command_stage_docs() File "./release.py", line 227, in command_stage_docs cmd('Extracting ', 'tar xf %s --strip-components 1' % docs_tar, cwd=os.path.join(kafka_site_repo_path, docs_version(version))) File "./release.py", line 108, in cmd output = subprocess.check_output(cmd, *args, stderr=subprocess.STDOUT, **kwargs) File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py", line 566, in check_output process = Popen(stdout=PIPE, *popenargs, **kwargs) File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py", line 710, in __init__ errread, errwrite) File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py", line 1335, in _execute_child raise child_exception OSError: [Errno 2] No such file or directory: '/Users/ijuma/src/kafka/../kafka-site/10'
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
remove "0" in {0}
It would have been more appropriate if `run_checks()` took an `Apps` instance as a parameter instead of a list of `AppConfig`s.
`del` is a builtin, not a function. These parens don't have to be here
add trailing comma
include trailing ,
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
I would use a list comprehension rather than `list(map())`
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
Same order, type first.
Wording of this error is awkward. I'd suggest: "(old api) is an unofficial API that has been deprecated. Usage of (old API) may be able to be replaced with a call to (new API)"
Instead, you can use `return data.get('name')` and it will return None if name is undefined.
`get_random_string()` will never allow you to connect to the same memory database instance. You can probably use `self.connection.alias` for that so each database alias has it's own unique memory database. This also allows it to work with `threading.local`.
This hook is unnecessary, IMO. I would move the logic to `_select_on_conflict()`.
Please add a trailing comma: ```suggestion on_conflict=None, update_fields=None, unique_fields=None, ```
You could create a new field instead of requiring an extra column with an arbitrary default on existing models. e.g. `field = self.model._meta.get_field('value').__class__(validators=[MinValueValidator(1), MaxValueValidator(3)])` And remove `ranged_field` from all the models.
`del` is a builtin, not a function. These parens don't have to be here
No need for the list comprehension here, you can drop the `[`/`]`.
consider assertRaisesMessage to make the test a bit more specific.
It would have been more appropriate if `run_checks()` took an `Apps` instance as a parameter instead of a list of `AppConfig`s.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
It probably makes sense to test that the exception reason also matches expectations
No need to parametrize with just one case.
```suggestion Test that the returned value for timezone consists of only uppercase ```
(Similarly, mark any string that you are going to call decode on with as a b"string".)
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
You should probably expect unicode strings
You're checking two separate properties here. This should be in a separate test.
Docstring shouldn't have a trailing space. Docstring sentences should end with a period.
Yes please remove unnecessary blank lines.
Breaks extraction completely if `params` does not match number of values to unpack.
Similarly, ```if tc['skip'].get('i')```
Breaks if not `int`.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
```suggestion 'Add another Inner4 stacked', ```
With this simplification, the pythonic way of generating a list is to use list comprehension. You can replace the whole function body with: ```python return [parse_to_obj(r) for r in parse_to_logical_rows(vlan_out)] ```
Lack of data is denoted by `None` not 0.
Breaks. Read coding conventions.
ACk. Just FYI OS discovery can be done after CLICONF is implemented.
Maybe you can extract the test code from the Py3ExceptionReporterTests and skip importing the "actual test code" if six.PY3 is False? ``` python class Py3ExceptionReporterTests(TestCase): rf = RequestFactory() @unittest.skipIf(not six.PY3, "Python 3 only test") def test_reporting_of_nested_exceptions(self): from other_module import do_actual_testing do_actual_testing(self.rf) ```
This can be directly imported from six: ``` python from ansible.module_utils.six.moves import configparser ```
no space between "e. g."
test failure says this should be `(None, None)`
It doesn't look like you're using `to_native` anymore. I wouldn't normally nitpick for leftover imports, but we'd like to discourage use of to_native without a pretty good reason, and removing the import would make adding it back just a little bit more work.
maybe also here `"foo"` -> `{@code foo}`
Could you please use dict literals? ```suggestion fake_loader = DictDataLoader({}) ```
```suggestion variables = {} ```
Can we make these multi-line, such as... ``` 'SELECT "postgres_tests_hotelreservation"."id", "postgres_tests_hotelreservation"."room_id", ' '"postgres_tests_hotelreservation"."datespan", ...' '...' ``` ...and so on.
We should actually remove the reference to `KTable` as we don't currently support it
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
Maybe you can extract the test code from the Py3ExceptionReporterTests and skip importing the "actual test code" if six.PY3 is False? ``` python class Py3ExceptionReporterTests(TestCase): rf = RequestFactory() @unittest.skipIf(not six.PY3, "Python 3 only test") def test_reporting_of_nested_exceptions(self): from other_module import do_actual_testing do_actual_testing(self.rf) ```
This can be directly imported from six: ``` python from ansible.module_utils.six.moves import configparser ```
no space between "e. g."
test failure says this should be `(None, None)`
It doesn't look like you're using `to_native` anymore. I wouldn't normally nitpick for leftover imports, but we'd like to discourage use of to_native without a pretty good reason, and removing the import would make adding it back just a little bit more work.
maybe also here `"foo"` -> `{@code foo}`
Could you please use dict literals? ```suggestion fake_loader = DictDataLoader({}) ```
```suggestion variables = {} ```
Can we make these multi-line, such as... ``` 'SELECT "postgres_tests_hotelreservation"."id", "postgres_tests_hotelreservation"."room_id", ' '"postgres_tests_hotelreservation"."datespan", ...' '...' ``` ...and so on.
We should actually remove the reference to `KTable` as we don't currently support it
normalize_interface import is unnecessary here
As per naming convention, the name is `get_lldp_global_facts`
This condition can be removed
like diff = load_config(self._module, config_xml, [])
Name as per convention can be `lag_itnerfaces_facts`
The name as per conventions can be `existing_lag_interfaces_facts` and `get_lag_interfaces_facts`
Same as above and applicable at other places as well
That's a really interesting piece of information I did not know.
It seems we can probably move deprecation warning handling to the actual test cases now. We can make it a follow-up item after merging the first version of this if you like.
```suggestion if self.client.module.params['disk_usage']: ```
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
These parens aren't necessary for unpacking the return values.
This isn't needed, since we now only support 2.6+ anyway.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
okay, but would be helpful to say _why_ we need to always return True.
`del` is a builtin, not a function. These parens don't have to be here
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
These parens aren't necessary for unpacking the return values.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
Here, `self.count_upgrade` is an int, and `outdated` (as above) a `dict` resp. `list`.
```suggestion for app in sorted(set(apps)): ``` This way, you won't have trouble if apps show up more than once in the list (in which case `is_installed` and `is_outdated` can return wrong information), and the order is still deterministic also for older Python versions.
Here, `outdated` is a `dict` (or `list` in mheap/ansible#3).
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
I would remove all aliases if possible.
Can you name this a little more verbosely? I can't unsee "get best"
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
This isn't needed, since we now only support 2.6+ anyway.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
These parens aren't necessary for unpacking the return values.
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
okay, but would be helpful to say _why_ we need to always return True.
This isn't needed, since we now only support 2.6+ anyway.
These parens aren't necessary for unpacking the return values.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
(Same for the related options.)
A general remark: you should always use complete sentences. So this should end with a period.
This should really be: ``` - Set custom DNS search domains. (Use I(dns_search) with C('') if you don't wish to set the search domain.) ```
I would remove all aliases if possible.
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
Can you name this a little more verbosely? I can't unsee "get best"
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
(In general, I don't think modules should have such options.)
You should emphasize that the module can and will not do any idempotence checking for this.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
okay, but would be helpful to say _why_ we need to always return True.
This isn't needed, since we now only support 2.6+ anyway.
These parens aren't necessary for unpacking the return values.
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
normalize_interface import is unnecessary here
As per naming convention, the name is `get_lldp_global_facts`
This condition can be removed
like diff = load_config(self._module, config_xml, [])
Name as per convention can be `lag_itnerfaces_facts`
The name as per conventions can be `existing_lag_interfaces_facts` and `get_lag_interfaces_facts`
Same as above and applicable at other places as well
That's a really interesting piece of information I did not know.
It seems we can probably move deprecation warning handling to the actual test cases now. We can make it a follow-up item after merging the first version of this if you like.
An example of using the `mocker` fixture. ```suggestion def mock_NamedTemporaryFile(mocker, **args): mock_ntf = mocker.MagicMock() mock_ntf.write = mocker.MagicMock() mock_ntf.close = mocker.MagicMock() ```
Please use shorter name (less than 30 chars), e.g. `deferrable_pink_constraint` (the same in all new tests).
To catch this as a `IntegrityError` on Oracle we need to add `ORA-00001` to the wrapper: ```diff --git a/django/db/backends/oracle/base.py b/django/db/backends/oracle/base.py index 4d2f5b51f1..3908aebda1 100644 --- a/django/db/backends/oracle/base.py +++ b/django/db/backends/oracle/base.py @@ -73,7 +73,7 @@ def wrap_oracle_errors(): # _C00102056) violated - parent key not found' # Convert that case to Django's IntegrityError exception. x = e.args[0] - if hasattr(x, 'code') and hasattr(x, 'message') and x.code == 2091 and 'ORA-02291' in x.message: + if hasattr(x, 'code') and hasattr(x, 'message') and x.code == 2091 and ('ORA-02291' in x.message or 'ORA-00001' in x.message): raise IntegrityError(*tuple(e.args)) raise ```
Good catch, I will remove it before final squash.
Please use shorter name (less than 30 chars), e.g. `deferrable_pink_constraint`.
Did you try the ORM rather than raw SQL? I'm not sure if there's a reason the test above doesn't use it.
Please don't make lines longer! There was nothing really wrong with this line before
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Seems nicer to infer private_zone if vpc_id is set. But not a blocker
Remove the two extra double-quotes here.
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
We shouldn't change the context to keep this backward compatible: ```suggestion 'action_list': page_obj, ``` Updated.
Please wrap docstrings at 79 characters. Ticket references are only needed for obscure issues that benefit from the additional context of the ticket. Not sure that's the case here.
Good catch :+1: Thanks
If `formfield.queryset` is already filtered both the outer query and the subquery will have this filter applied which is unnecessary ```suggestion Exists(formfield.queryset.model._base_manager.filter(complex_filter)), ```
It looks like a duplicate of `test_save_unsaved_instance_on_generic_foreign_key` :thinking:
I'd move `'defaults': 'testing'` to the next line and include a trailing comma per our usual style.
`result` is empty and no capability is returned. Is this expected? You might want to call `get_device_info` here to get the device capabilities.
Ok, cool. :thumbsup:
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
consider assertRaisesMessage to make the test a bit more specific.
To catch this as a `IntegrityError` on Oracle we need to add `ORA-00001` to the wrapper: ```diff --git a/django/db/backends/oracle/base.py b/django/db/backends/oracle/base.py index 4d2f5b51f1..3908aebda1 100644 --- a/django/db/backends/oracle/base.py +++ b/django/db/backends/oracle/base.py @@ -73,7 +73,7 @@ def wrap_oracle_errors(): # _C00102056) violated - parent key not found' # Convert that case to Django's IntegrityError exception. x = e.args[0] - if hasattr(x, 'code') and hasattr(x, 'message') and x.code == 2091 and 'ORA-02291' in x.message: + if hasattr(x, 'code') and hasattr(x, 'message') and x.code == 2091 and ('ORA-02291' in x.message or 'ORA-00001' in x.message): raise IntegrityError(*tuple(e.args)) raise ```
Please use shorter name (less than 30 chars), e.g. `deferrable_pink_constraint`.
Did you try the ORM rather than raw SQL? I'm not sure if there's a reason the test above doesn't use it.
Good catch, I will remove it before final squash.
`UniqueConstraint` not `Index`.
Bravo on tackling one of the gnarlier test setups ;-> :+1:
This message is different on PostgreSQL 13+, it should be enough to check that a constraint name is in the message.
> Is there any specific reason why we would prefer using the operation in this case? Yes, because we have it. Using a RAW SQL is the last option, we're developing the ORM in order not to use them.
redundant, remove ```suggestion ```
Could use `bulk_create` ```suggestion Product.objects.bulk_create([ Product(name='p1'), Product(name='p1', color='red'), ]) ```
Minor but I'd move this control flow block after the `weights` one to match the args order.
> scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.), compute a sum You missed the parentheses.
Also missing parentheses: > during vm execution (e.g. due to a vm label update),
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
> during vm execution (e.g. due to an update),
Minor but I'd move this control flow block after the `weights` one to match the args order.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
If it is not required, you don't have to add `'required: False`.
If it is not required, you don't have to add `'required: False`.
I would simply use units_si and units_iec here. ```python 'choices': units_si + units_iec + ['', 'compact', 'cyl', 'chs'], ``` In fact, make these global variables anyhow.
```suggestion database='default', verbosity=0, skip_empty=True, ```
I don't like how this entire if block looks. Is `plan` allowed to be `None` considering the check/assignment on line 73 and 74? I'd prefer to call out that it can not be forwards and backwards by doing something like: ``` if all_forwards == all_backards: raise.. # this may not be correct if `plan` can be `None` elif all_forwards: migrate_forwards() else: migrate_backwards() ``` I feel this highlights the invariant, and avoids potential problems of misreading the bool combinations in the original code.
That code is marginally longer, but I think it's clearer. I'll leave it up to what you think is best though.
Migrations plans with both forwards and backwards migrations are not supported.
```suggestion # just get value from attribute itself as normal ```
You can join this line with the previous.
We've been using "Take / apply" verb-style in new docstrings.
```suggestion value = super().__get__(obj, obj_type=obj_type) ```
An example of using the `mocker` fixture. ```suggestion def mock_NamedTemporaryFile(mocker, **args): mock_ntf = mocker.MagicMock() mock_ntf.write = mocker.MagicMock() mock_ntf.close = mocker.MagicMock() ```
If we're just testing broker compatibility I don't think we even need this part of the test.
```suggestion database='default', verbosity=0, skip_empty=True, ```
I don't like how this entire if block looks. Is `plan` allowed to be `None` considering the check/assignment on line 73 and 74? I'd prefer to call out that it can not be forwards and backwards by doing something like: ``` if all_forwards == all_backards: raise.. # this may not be correct if `plan` can be `None` elif all_forwards: migrate_forwards() else: migrate_backwards() ``` I feel this highlights the invariant, and avoids potential problems of misreading the bool combinations in the original code.
That code is marginally longer, but I think it's clearer. I'll leave it up to what you think is best though.
Migrations plans with both forwards and backwards migrations are not supported.
```suggestion # just get value from attribute itself as normal ```
You can join this line with the previous.
We've been using "Take / apply" verb-style in new docstrings.
```suggestion value = super().__get__(obj, obj_type=obj_type) ```
An example of using the `mocker` fixture. ```suggestion def mock_NamedTemporaryFile(mocker, **args): mock_ntf = mocker.MagicMock() mock_ntf.write = mocker.MagicMock() mock_ntf.close = mocker.MagicMock() ```
If we're just testing broker compatibility I don't think we even need this part of the test.
Minor but I'd move this control flow block after the `weights` one to match the args order.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
If it is not required, you don't have to add `'required: False`.
If it is not required, you don't have to add `'required: False`.
I would simply use units_si and units_iec here. ```python 'choices': units_si + units_iec + ['', 'compact', 'cyl', 'chs'], ``` In fact, make these global variables anyhow.
Minor but I'd move this control flow block after the `weights` one to match the args order.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Please add `type='str'`
If it is not required, you don't have to add `'required: False`.
If it is not required, you don't have to add `'required: False`.
If it is not required, you don't have to add `'required: False`.
If it is not required, you don't have to add `'required: False`.
If it is not required, you don't have to add `'required: False`.
If it is not required, you don't have to add `'required: False`.
I would simply use units_si and units_iec here. ```python 'choices': units_si + units_iec + ['', 'compact', 'cyl', 'chs'], ``` In fact, make these global variables anyhow.
Minor but I'd move this control flow block after the `weights` one to match the args order.
```suggestion type: list suboptions: ```
@bcoca corrected me that _execute_module() takes care of delegation so this is fine. After discussion, though, we did decide that we should set the ansible_pkg_mgr fact when we return from this action plugin so that we don't end up having to discover it every time the actin plugin is run on the same host.
When delegating, I'd think that we'd need to run the facts module for the delegated host rather than for the inventory_hostname host....
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
```suggestion database='default', verbosity=0, skip_empty=True, ```
I don't like how this entire if block looks. Is `plan` allowed to be `None` considering the check/assignment on line 73 and 74? I'd prefer to call out that it can not be forwards and backwards by doing something like: ``` if all_forwards == all_backards: raise.. # this may not be correct if `plan` can be `None` elif all_forwards: migrate_forwards() else: migrate_backwards() ``` I feel this highlights the invariant, and avoids potential problems of misreading the bool combinations in the original code.
That code is marginally longer, but I think it's clearer. I'll leave it up to what you think is best though.
Migrations plans with both forwards and backwards migrations are not supported.
```suggestion # just get value from attribute itself as normal ```
You can join this line with the previous.
We've been using "Take / apply" verb-style in new docstrings.
```suggestion value = super().__get__(obj, obj_type=obj_type) ```
An example of using the `mocker` fixture. ```suggestion def mock_NamedTemporaryFile(mocker, **args): mock_ntf = mocker.MagicMock() mock_ntf.write = mocker.MagicMock() mock_ntf.close = mocker.MagicMock() ```
If we're just testing broker compatibility I don't think we even need this part of the test.
Minor but I'd move this control flow block after the `weights` one to match the args order.
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
lines can be longer
Generally, I'd have put the static regex directly as the first argument of `re.search()`.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
Minor but I'd move this control flow block after the `weights` one to match the args order.
```suggestion type: list suboptions: ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
@bcoca corrected me that _execute_module() takes care of delegation so this is fine. After discussion, though, we did decide that we should set the ansible_pkg_mgr fact when we return from this action plugin so that we don't end up having to discover it every time the actin plugin is run on the same host.
When delegating, I'd think that we'd need to run the facts module for the delegated host rather than for the inventory_hostname host....
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
Minor but I'd move this control flow block after the `weights` one to match the args order.
```suggestion type: list suboptions: ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
@bcoca corrected me that _execute_module() takes care of delegation so this is fine. After discussion, though, we did decide that we should set the ansible_pkg_mgr fact when we return from this action plugin so that we don't end up having to discover it every time the actin plugin is run on the same host.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I'd probably leave `get_aws_connection_info` as is for backwards compatibility. The new function would be the one you would use.
```suggestion on_conflict=on_conflict, ```
Getting rid of the unnecessary constant for the `None` case, `ON_CONFLICTS_NONE`, will simplify the diff and make review easier: ```suggestion if connection.features.can_return_rows_from_bulk_insert and not on_conflict: ```
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
```suggestion self, objs, fields, batch_size, on_conflict=None, update_fields=None, unique_fields=None, ```
Please add a trailing comma: ```suggestion on_conflict=None, update_fields=None, unique_fields=None, ```
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
Minor but I'd move this control flow block after the `weights` one to match the args order.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Please don't make lines longer! There was nothing really wrong with this line before
this can be initialized to `result = {'changed': False}`
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
```suggestion description: Whether the domain is eligible for submission of "EV" certificates. Will never be C(true) if I(ov_eligible) is C(false) ```
```suggestion sample: true ```
```suggestion sample: true ```
```suggestion description: Whether the domain is eligible for submission of "OV" certificates. Will never be C(false) if I(ov_eligible) is C(true) ```
Minor but I'd move this control flow block after the `weights` one to match the args order.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
```suggestion type: list suboptions: ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
Minor but I'd move this control flow block after the `weights` one to match the args order.
Same here, don't we usually use byte strings for file operations.
@smithdc1 it does thanks!
There is no point to use `remove_start` since line is always a string.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Do you need that step? `clients = client_list` should be enough
IMHO this statement is hard to read. Woul rewrite to simple if else and for loop.
This is error prone, `len` of the actual string should be used instead.
>besides the test is there to make sure that breakage in this part of code will be detected That's a doubtful argument considering broken core tests at the beginning of this PR. The length of this string is const until one decides to refactor here something. Using 10 is a variation of code duplication since the length is already implicitly defined in the string literal itself. Also using 10 indicates no relation to the string literal so that one unfamiliar with code who decides to refactor it may forgot to change the number and may be unaware of the tests at all.
Yes please, I didn't audit for all instances.
Minor but I'd move this control flow block after the `weights` one to match the args order.
>besides the test is there to make sure that breakage in this part of code will be detected That's a doubtful argument considering broken core tests at the beginning of this PR. The length of this string is const until one decides to refactor here something. Using 10 is a variation of code duplication since the length is already implicitly defined in the string literal itself. Also using 10 indicates no relation to the string literal so that one unfamiliar with code who decides to refactor it may forgot to change the number and may be unaware of the tests at all.
This is error prone, `len` of the actual string should be used instead.
There is no point to use `remove_start` since line is always a string.
[`get_param`](https://github.com/spotify/pyfg/blob/master/pyFG/forticonfig.py#L249) should not raise an exception. I guess you could check if `f.running_config[path]` contains `module.params['name']ÃŒÂ€`.
Don't use except without an exception type. What could be the exceptions here ? It would be better to check if `get_param` returns `None`.
You may want to consider using set operations here.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I prefer `if not group_members`.
Parentheses around `e.message` are useless.
`enumerate` on for range.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
`expected_status` to `_download_json` instead.
Don't capture groups you don't use. Unused captured group.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
No brackets needed.
Should not break if no `type`.
eg find type by URL ext
Useless with timestamp available.
You need either `to_bytes(text)` or `text.encode('utf-8')` here as well.
Use dict literals: ```suggestion return {} ```
Please convert this to just a list comprehension: ```suggestion groups = [e.name for e in entities if isinstance(e, Group)] ``` Additionally, just convert to a list of names here, since you only ever need that, and not the object later.
The `to_bytes` should be on the `self._basedir` instead: ```suggestion b_opath = os.path.realpath(os.path.join(to_bytes(self._basedir), b'r_group_vars')) ```
Use a tuple as a the cache key instead of stringifying this. This will likely result in a performance problem later if not resolved now. ```suggestion key = tuple(groups) + (b_opath,) ```
Again, error handling changed.
Try using `.format()` or `%s` formatting instead: ```suggestion self._display.debug("recursive_group_vars - Matched file : %s" % to_text(found)) ```
With `groups` below changing to just be a list of strings, this should be changed ```suggestion found_files = loader.find_vars_files(path, group, allow_dir=False) ```
this is an unsafe way to write the file, other processes might get incorrect/incomplete data and if it fails it leaves a corrupt file. write to a temp file and use `atomic_move` instead
this is an unsafe way to update the file, it can lead to data corruption and other processes reading incorrect/incomplete data. write to a tmp file and use atomic_move from basic.py
Use a tuple as a the cache key instead of stringifying this. This will likely result in a performance problem later if not resolved now. ```suggestion key = tuple(groups) + (b_opath,) ```
this is a non atomic operation and can lead to file corruption, write to a temp file and use shared 'atomic_move' to put the file into place
Again, error handling changed.
It might be safer to perform the `sys.stdout` switcharoo *inside* the `try: finally`: ```python try: old_stdout = sys.stdout sys.stdout = captured_stdout = StringIO() ... finally: sys.stdout = old_stdout ```
this forcibly creates/removes the files w/o checking if they exist or not, doing so would allow you to offer a 'changed' s state. Also it is encouraged that modules are safe to re-run w/o affecting systems when not needed.
this looks wrong to me, so the file is changed if it already exists? also you don't compare the contents
``` return files.getlist(name) if self.multiple else files.get(name) ```
Just a note here. We decided that by convention we will use the same naming as maven. `groupId` has now changed to `org.elasticsearch.distribution.[packaging]` so I think we should also reflect that change here and use `org/elasticsearch/distribution/[packaging]` where `packaging` is: - rpm - deb - zip - tar
this is an unsafe way to write the file, other processes might get incorrect/incomplete data and if it fails it leaves a corrupt file. write to a temp file and use `atomic_move` instead
this is an unsafe way to update the file, it can lead to data corruption and other processes reading incorrect/incomplete data. write to a tmp file and use atomic_move from basic.py
Use a tuple as a the cache key instead of stringifying this. This will likely result in a performance problem later if not resolved now. ```suggestion key = tuple(groups) + (b_opath,) ```
this is a non atomic operation and can lead to file corruption, write to a temp file and use shared 'atomic_move' to put the file into place
Again, error handling changed.
It might be safer to perform the `sys.stdout` switcharoo *inside* the `try: finally`: ```python try: old_stdout = sys.stdout sys.stdout = captured_stdout = StringIO() ... finally: sys.stdout = old_stdout ```
this forcibly creates/removes the files w/o checking if they exist or not, doing so would allow you to offer a 'changed' s state. Also it is encouraged that modules are safe to re-run w/o affecting systems when not needed.
this looks wrong to me, so the file is changed if it already exists? also you don't compare the contents
``` return files.getlist(name) if self.multiple else files.get(name) ```
Just a note here. We decided that by convention we will use the same naming as maven. `groupId` has now changed to `org.elasticsearch.distribution.[packaging]` so I think we should also reflect that change here and use `org/elasticsearch/distribution/[packaging]` where `packaging` is: - rpm - deb - zip - tar
this is an unsafe way to write the file, other processes might get incorrect/incomplete data and if it fails it leaves a corrupt file. write to a temp file and use `atomic_move` instead
this is an unsafe way to update the file, it can lead to data corruption and other processes reading incorrect/incomplete data. write to a tmp file and use atomic_move from basic.py
You're checking two separate properties here. This should be in a separate test.
Match the error message
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Plz also use `match` arg here
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
plz don't use EOL escaping, wrap with braces instead.
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
Match the error message
You're checking two separate properties here. This should be in a separate test.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
plz don't use EOL escaping, wrap with braces instead.
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
Match the error message
You're checking two separate properties here. This should be in a separate test.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
plz don't use EOL escaping, wrap with braces instead.
You're checking two separate properties here. This should be in a separate test.
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
Match the error message
plz don't use EOL escaping, wrap with braces instead.
Plz also use `match` arg here
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
Match the error message
plz don't use EOL escaping, wrap with braces instead.
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
Match the error message
plz don't use EOL escaping, wrap with braces instead.
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
Match the error message
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
The various `CALLBACK_` prefixed class attributes are missing. Take a look at the other callbacks for an example. In particular, this is needed: ``` python CALLBACK_NEEDS_WHITELIST = True ```
nit: doc how the output of `iperf` looks like. helps with understanding this code and regex.
Use `(?i)` in regex itself if you want case insensitivity.
No, this should not be global at least due to presence of potentially case sensitive parts and every regexp should not be touched either. Only those seen to be case insensitive in the wild should do.
That's simulated. No such URLs is seen in the wild so far and no one will ever intentionally upper case some part of it.
this is not a 1.0 callback, its using 2.0 API
this is a catchall, we normally frown on those. also note that for all of your `% e` they should be `% to_native(e)` to ensure proper character conversions
nit: add a newline here too.
Just an empty line, could be removed for cleaner code
In `check_mode: yes` the returned query will always be `SELECT 1 FROM 'tablename'`. But this is not what I expected. Normally I like to see what query will be executed for that change. Please change this behavior to always return the query which will be executed even if it is not in `check_mode: yes`
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
Match the error message
this is a catchall, we normally frown on those. also note that for all of your `% e` they should be `% to_native(e)` to ensure proper character conversions
```suggestion - There is a small delay (typically about 5 seconds, but can be as long as 30 seconds) before obtaining the random values when requesting a validation. ```
Sorry, I know it was in my changes, but I forgot to remove that lineÃ¢Â€Â¦ Changing the data seems a bit ugly here, could you remove this line and changing the next one by: ``` prefix = ipaddress.ip_network(data["prefix"] + "/" + data["prefix_length"]) ```
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
I think all the `serial_` prefixes are repetitive. I think this module should be renamed `cpm_serial_port` then you can just remove all the `serial_` prefixes. This would also remove confusion on whether this module is configuring serial or network ports.
```suggestion serial_port=dict(type='int', required=True), ```
I don't think this is a good idea. This prevents the valid value `""`. ```suggestion else: ```
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
Match the error message
plz don't use EOL escaping, wrap with braces instead.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
Match the error message
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
Thanks for that note ewen. I learned something!
is the `any` required here - `if metrics_to_disable:` should suffice, I'd think
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
don't do this by default, set from cli `self._options` does not exist or is None
This should return {} instead of None so camel_dict_to_snake_dict works.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Thanks for that note ewen. I learned something!
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
Since we expect the name to start with 'http://' or 'https://' I would so this: ```python if name.startswith('http://') or name.startswith('https://'): ... ```
Please use [standard exception handling guidelines](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/amazon/GUIDELINES.md#boto3-2)
its new in 2.4 so it doesn't apply to older versions
this whole section is not needed, just use `self._plugin_options[<option name>]`
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
is the `any` required here - `if metrics_to_disable:` should suffice, I'd think
don't do this by default, set from cli `self._options` does not exist or is None
This should return {} instead of None so camel_dict_to_snake_dict works.
Thanks for that note ewen. I learned something!
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
Please use [standard exception handling guidelines](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/amazon/GUIDELINES.md#boto3-2)
Since we expect the name to start with 'http://' or 'https://' I would so this: ```python if name.startswith('http://') or name.startswith('https://'): ... ```
its new in 2.4 so it doesn't apply to older versions
this whole section is not needed, just use `self._plugin_options[<option name>]`
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
is the `any` required here - `if metrics_to_disable:` should suffice, I'd think
don't do this by default, set from cli `self._options` does not exist or is None
This should return {} instead of None so camel_dict_to_snake_dict works.
`for data in json.loads(...):`
`Klass` â†’ `model`
maybe: `# If no browsers were specified, skip this class (it'll still be discovered).`
could create a module variable so we don't have a second "magic string" in `runtests.py`: `SELENIUM_WEBDRIVER_PATH = 'selenium.webdriver.%s.webdriver.WebDriver'`
Username shouldn't need `no_log`
Please remove `no_log=False` as it's the default
Please change the default to true (assuming this is what other Forman modules do)
quit() .... to avoid a dead.... (chop "we" stuff)
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
Need to import reduce from ansible.module_utils.six.moves.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
This should return {} instead of None so camel_dict_to_snake_dict works.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
`for data in json.loads(...):`
`Klass` â†’ `model`
maybe: `# If no browsers were specified, skip this class (it'll still be discovered).`
could create a module variable so we don't have a second "magic string" in `runtests.py`: `SELENIUM_WEBDRIVER_PATH = 'selenium.webdriver.%s.webdriver.WebDriver'`
Username shouldn't need `no_log`
Please remove `no_log=False` as it's the default
Please change the default to true (assuming this is what other Forman modules do)
quit() .... to avoid a dead.... (chop "we" stuff)
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Thanks for that note ewen. I learned something!
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
don't do this by default, set from cli `self._options` does not exist or is None
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
This should return {} instead of None so camel_dict_to_snake_dict works.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Chop "Makes sure" prefix in favor of stating the expected behavior as described in https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/.
Since the same result is expected as above, wouldn't it be clearer to save the output in a variable? Something like: ``` expected = ( "[ ] author_app.0001_initial\n" "[ ] book_app.0001_initial\n" "[ ] author_app.0002_alter_id\n" "[ ] mutate_state_b.0001_initial\n" "[ ] mutate_state_b.0002_add_field\n" ) ```
Yes and no. Keeping the output explicit feels easier to read.
I made a few edits and squashed commits but before I push those updates I wanted to ask if this test is really needed. None of the changes seem related to verbosity so this test seems unnecessary to me.
You can use `assertRaisesMessage`.
Why's that? It's non-obvious at first glance.
Swap the apps in `call_command()` but leave them sorted in the error message.
Copy / paste? I would have expected the ordering to change for the `duth1` and `duth2` arguments.
non existing -> nonexistent
It would help readability to use a name like "nonexistent_app" rather than "duth..".
Chop "Makes sure" prefix in favor of stating the expected behavior as described in https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/.
Since the same result is expected as above, wouldn't it be clearer to save the output in a variable? Something like: ``` expected = ( "[ ] author_app.0001_initial\n" "[ ] book_app.0001_initial\n" "[ ] author_app.0002_alter_id\n" "[ ] mutate_state_b.0001_initial\n" "[ ] mutate_state_b.0002_add_field\n" ) ```
Yes and no. Keeping the output explicit feels easier to read.
I made a few edits and squashed commits but before I push those updates I wanted to ask if this test is really needed. None of the changes seem related to verbosity so this test seems unnecessary to me.
Why's that? It's non-obvious at first glance.
You can use `assertRaisesMessage`.
Copy / paste? I would have expected the ordering to change for the `duth1` and `duth2` arguments.
Swap the apps in `call_command()` but leave them sorted in the error message.
non existing -> nonexistent
It would help readability to use a name like "nonexistent_app" rather than "duth..".
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
Thanks for that note ewen. I learned something!
Here, you might want to return `out` if `re.findall` returns `[]`
You can use `mutually_exclusive` for `command` and `src`. You can check [here](https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/basic.py#L668) and [here](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/amazon/cloudformation.py#L410)
this is not a 1.0 callback, its using 2.0 API
This function call is not required.
Module argument specification ensures this cannot be true. The field is required.
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
Well, then they will never really know and think they always have to quote. I don't think it's better to teach them to always quote, and certainly not using double quotes. Besides, you're not being consistent, you're not quoting the task name value and delegate_to.
Most of these examples use quoted strings where it is not required. We tend to only quote strings that requires quoting (i.e. integers as string, colon-space combo, date formats, etc...) If we expect people to pick up how YAML works it helps to only quote when it is required.
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
Thanks for that note ewen. I learned something!
Here, you might want to return `out` if `re.findall` returns `[]`
You can use `mutually_exclusive` for `command` and `src`. You can check [here](https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/basic.py#L668) and [here](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/amazon/cloudformation.py#L410)
this is not a 1.0 callback, its using 2.0 API
This function call is not required.
Module argument specification ensures this cannot be true. The field is required.
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
Well, then they will never really know and think they always have to quote. I don't think it's better to teach them to always quote, and certainly not using double quotes. Besides, you're not being consistent, you're not quoting the task name value and delegate_to.
Most of these examples use quoted strings where it is not required. We tend to only quote strings that requires quoting (i.e. integers as string, colon-space combo, date formats, etc...) If we expect people to pick up how YAML works it helps to only quote when it is required.
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
Thanks for that note ewen. I learned something!
Here, you might want to return `out` if `re.findall` returns `[]`
You can use `mutually_exclusive` for `command` and `src`. You can check [here](https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/basic.py#L668) and [here](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/amazon/cloudformation.py#L410)
this is not a 1.0 callback, its using 2.0 API
This function call is not required.
Module argument specification ensures this cannot be true. The field is required.
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
Well, then they will never really know and think they always have to quote. I don't think it's better to teach them to always quote, and certainly not using double quotes. Besides, you're not being consistent, you're not quoting the task name value and delegate_to.
Most of these examples use quoted strings where it is not required. We tend to only quote strings that requires quoting (i.e. integers as string, colon-space combo, date formats, etc...) If we expect people to pick up how YAML works it helps to only quote when it is required.
That code is marginally longer, but I think it's clearer. I'll leave it up to what you think is best though.
I don't like how this entire if block looks. Is `plan` allowed to be `None` considering the check/assignment on line 73 and 74? I'd prefer to call out that it can not be forwards and backwards by doing something like: ``` if all_forwards == all_backards: raise.. # this may not be correct if `plan` can be `None` elif all_forwards: migrate_forwards() else: migrate_backwards() ``` I feel this highlights the invariant, and avoids potential problems of misreading the bool combinations in the original code.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
We've been using "Take / apply" verb-style in new docstrings.
Use another lookup instead of `epoch` e.g. `second`.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Please use a single quote.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
nit: doc how the output of `iperf` looks like. helps with understanding this code and regex.
nit: add a newline here too.
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
Thanks for that note ewen. I learned something!
Here, you might want to return `out` if `re.findall` returns `[]`
You can use `mutually_exclusive` for `command` and `src`. You can check [here](https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/basic.py#L668) and [here](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/amazon/cloudformation.py#L410)
this is not a 1.0 callback, its using 2.0 API
This function call is not required.
Module argument specification ensures this cannot be true. The field is required.
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
Well, then they will never really know and think they always have to quote. I don't think it's better to teach them to always quote, and certainly not using double quotes. Besides, you're not being consistent, you're not quoting the task name value and delegate_to.
Most of these examples use quoted strings where it is not required. We tend to only quote strings that requires quoting (i.e. integers as string, colon-space combo, date formats, etc...) If we expect people to pick up how YAML works it helps to only quote when it is required.
That code is marginally longer, but I think it's clearer. I'll leave it up to what you think is best though.
I don't like how this entire if block looks. Is `plan` allowed to be `None` considering the check/assignment on line 73 and 74? I'd prefer to call out that it can not be forwards and backwards by doing something like: ``` if all_forwards == all_backards: raise.. # this may not be correct if `plan` can be `None` elif all_forwards: migrate_forwards() else: migrate_backwards() ``` I feel this highlights the invariant, and avoids potential problems of misreading the bool combinations in the original code.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
We've been using "Take / apply" verb-style in new docstrings.
Use another lookup instead of `epoch` e.g. `second`.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Please use a single quote.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
nit: doc how the output of `iperf` looks like. helps with understanding this code and regex.
nit: add a newline here too.
That code is marginally longer, but I think it's clearer. I'll leave it up to what you think is best though.
I don't like how this entire if block looks. Is `plan` allowed to be `None` considering the check/assignment on line 73 and 74? I'd prefer to call out that it can not be forwards and backwards by doing something like: ``` if all_forwards == all_backards: raise.. # this may not be correct if `plan` can be `None` elif all_forwards: migrate_forwards() else: migrate_backwards() ``` I feel this highlights the invariant, and avoids potential problems of misreading the bool combinations in the original code.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
We've been using "Take / apply" verb-style in new docstrings.
Use another lookup instead of `epoch` e.g. `second`.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Please use a single quote.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
nit: doc how the output of `iperf` looks like. helps with understanding this code and regex.
nit: add a newline here too.
I feel like this changes too much, for what could be achieved with less: ```diff diff --git a/lib/ansible/plugins/callback/json.py b/lib/ansible/plugins/callback/json.py index 3961a78aab..fd344a0cf0 100644 --- a/lib/ansible/plugins/callback/json.py +++ b/lib/ansible/plugins/callback/json.py @@ -101,14 +101,16 @@ class CallbackModule(CallbackBase): summary[h] = s custom_stats = {} + global_custom_stats = {} if self.get_option('show_custom_stats') and stats.custom: custom_stats.update(dict((self._convert_host_to_name(k), v) for k, v in stats.custom.items())) - custom_stats.pop('_run', None) + global_custom_stats.update(custom_stats.pop('_run', {})) output = { 'plays': self.results, 'stats': summary, 'custom_stats': custom_stats, + 'global_custom_stats': global_custom_stats, } self._display.display(json.dumps(output, indent=4, sort_keys=True)) ```
Please do not remove this trailing comma, this is our preference with dicts and lists to minimize future diffs when lines are changed.
Upon further reflection, I think these three lines along with `check_mutually_exclusive` are erroneous for subspecs. We don't currently have a way to specify `required_if/one_of/together` or `mutually_exclusive` for subspecs, so in every case *but* the networking "aggregate" thing, this is severely broken (as you'd be making nonsensical checks in most cases)
may be blind, but think this will never get fired if v is empty. you create an empty list, which will be skipped in the for loop.
`self._options_context` stores the context of parent options in case of nested suboptions. As the `elif` block does not have a recursive call to `_handle_options` context handling is not required here. However, context information can be added as part of error message in case this argument is part of nested sub-options (similar to other error messages in sub-option handling).
use basic.py's `missing_required_lib` function as it gives details on host and python to avoid user confusion
You want to swap these conditionals, so that check mode can actually report anything. `load_config()` should be the only thing gated by `if not module.check_mode`, that way the result is correct
Is there a reason to sort this? Since we're just putting it into a set (to uniquify the list I assume) it doesn't seem necessary to sort. Since we're just iterating over sd_instances, a frozenset is more appropriate than a set.
this class has a bunch of details of WSGI environ so I think it more accurately belongs in `django.http.request`
Please don't use such generic env vars. ```suggestion fallback=(env_fallback, ['ANSIBLE_HWC_IDENTITY_ENDPOINT']), ```
I feel like this changes too much, for what could be achieved with less: ```diff diff --git a/lib/ansible/plugins/callback/json.py b/lib/ansible/plugins/callback/json.py index 3961a78aab..fd344a0cf0 100644 --- a/lib/ansible/plugins/callback/json.py +++ b/lib/ansible/plugins/callback/json.py @@ -101,14 +101,16 @@ class CallbackModule(CallbackBase): summary[h] = s custom_stats = {} + global_custom_stats = {} if self.get_option('show_custom_stats') and stats.custom: custom_stats.update(dict((self._convert_host_to_name(k), v) for k, v in stats.custom.items())) - custom_stats.pop('_run', None) + global_custom_stats.update(custom_stats.pop('_run', {})) output = { 'plays': self.results, 'stats': summary, 'custom_stats': custom_stats, + 'global_custom_stats': global_custom_stats, } self._display.display(json.dumps(output, indent=4, sort_keys=True)) ```
Please do not remove this trailing comma, this is our preference with dicts and lists to minimize future diffs when lines are changed.
Upon further reflection, I think these three lines along with `check_mutually_exclusive` are erroneous for subspecs. We don't currently have a way to specify `required_if/one_of/together` or `mutually_exclusive` for subspecs, so in every case *but* the networking "aggregate" thing, this is severely broken (as you'd be making nonsensical checks in most cases)
may be blind, but think this will never get fired if v is empty. you create an empty list, which will be skipped in the for loop.
`self._options_context` stores the context of parent options in case of nested suboptions. As the `elif` block does not have a recursive call to `_handle_options` context handling is not required here. However, context information can be added as part of error message in case this argument is part of nested sub-options (similar to other error messages in sub-option handling).
use basic.py's `missing_required_lib` function as it gives details on host and python to avoid user confusion
You want to swap these conditionals, so that check mode can actually report anything. `load_config()` should be the only thing gated by `if not module.check_mode`, that way the result is correct
Is there a reason to sort this? Since we're just putting it into a set (to uniquify the list I assume) it doesn't seem necessary to sort. Since we're just iterating over sd_instances, a frozenset is more appropriate than a set.
this class has a bunch of details of WSGI environ so I think it more accurately belongs in `django.http.request`
Please don't use such generic env vars. ```suggestion fallback=(env_fallback, ['ANSIBLE_HWC_IDENTITY_ENDPOINT']), ```
I feel like this changes too much, for what could be achieved with less: ```diff diff --git a/lib/ansible/plugins/callback/json.py b/lib/ansible/plugins/callback/json.py index 3961a78aab..fd344a0cf0 100644 --- a/lib/ansible/plugins/callback/json.py +++ b/lib/ansible/plugins/callback/json.py @@ -101,14 +101,16 @@ class CallbackModule(CallbackBase): summary[h] = s custom_stats = {} + global_custom_stats = {} if self.get_option('show_custom_stats') and stats.custom: custom_stats.update(dict((self._convert_host_to_name(k), v) for k, v in stats.custom.items())) - custom_stats.pop('_run', None) + global_custom_stats.update(custom_stats.pop('_run', {})) output = { 'plays': self.results, 'stats': summary, 'custom_stats': custom_stats, + 'global_custom_stats': global_custom_stats, } self._display.display(json.dumps(output, indent=4, sort_keys=True)) ```
Please do not remove this trailing comma, this is our preference with dicts and lists to minimize future diffs when lines are changed.
Upon further reflection, I think these three lines along with `check_mutually_exclusive` are erroneous for subspecs. We don't currently have a way to specify `required_if/one_of/together` or `mutually_exclusive` for subspecs, so in every case *but* the networking "aggregate" thing, this is severely broken (as you'd be making nonsensical checks in most cases)
may be blind, but think this will never get fired if v is empty. you create an empty list, which will be skipped in the for loop.
`self._options_context` stores the context of parent options in case of nested suboptions. As the `elif` block does not have a recursive call to `_handle_options` context handling is not required here. However, context information can be added as part of error message in case this argument is part of nested sub-options (similar to other error messages in sub-option handling).
use basic.py's `missing_required_lib` function as it gives details on host and python to avoid user confusion
You want to swap these conditionals, so that check mode can actually report anything. `load_config()` should be the only thing gated by `if not module.check_mode`, that way the result is correct
Is there a reason to sort this? Since we're just putting it into a set (to uniquify the list I assume) it doesn't seem necessary to sort. Since we're just iterating over sd_instances, a frozenset is more appropriate than a set.
this class has a bunch of details of WSGI environ so I think it more accurately belongs in `django.http.request`
Please don't use such generic env vars. ```suggestion fallback=(env_fallback, ['ANSIBLE_HWC_IDENTITY_ENDPOINT']), ```
I made a few edits and squashed commits but before I push those updates I wanted to ask if this test is really needed. None of the changes seem related to verbosity so this test seems unnecessary to me.
Since the same result is expected as above, wouldn't it be clearer to save the output in a variable? Something like: ``` expected = ( "[ ] author_app.0001_initial\n" "[ ] book_app.0001_initial\n" "[ ] author_app.0002_alter_id\n" "[ ] mutate_state_b.0001_initial\n" "[ ] mutate_state_b.0002_add_field\n" ) ```
Yes and no. Keeping the output explicit feels easier to read.
Chop "Makes sure" prefix in favor of stating the expected behavior as described in https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
Why's that? It's non-obvious at first glance.
Copy / paste? I would have expected the ordering to change for the `duth1` and `duth2` arguments.
Swap the apps in `call_command()` but leave them sorted in the error message.
non existing -> nonexistent
It would help readability to use a name like "nonexistent_app" rather than "duth..".
The input values for tags can be integers but they need to be converted to a string before using e.g. `compare_aws_tags` because they're always strings when returned by boto. I definitely think we should be able to do this in a better way (which is likely just forcing conversion to string, rather than declaring non-string things invalid)
@aioue Thanks for the patch for this. What do you think about doing away with the valid tags check altogether? Some other opinions on this would probably be good too. I haven't noticed this attempted verification in other modules and I'm not sure what purpose it has. Tags should be able to be integers as well.
Might want to avoid `id` shadowing.
Nice, I like this pattern :+1:
Might want to avoid `type` shadowing.
Please use more descriptive variable names for at least `_h` and `_k`, and probably `k` and `v` also. I think it's fine to use some single letter variables in list/dict comprehensions or simple loops, but once there is some additional logic I think readability is improved by using longer names. Also, it should be fine to reassign the lowercase version of `k` (or whatever it ends up being) to itself since the original value isn't used anywhere else and the type hasn't changed.
`self._options_context` stores the context of parent options in case of nested suboptions. As the `elif` block does not have a recursive call to `_handle_options` context handling is not required here. However, context information can be added as part of error message in case this argument is part of nested sub-options (similar to other error messages in sub-option handling).
Nitpick: Append a \ to the end so it doesn't generate an empty first line.
You are completely right. What about this? Too ugly?: ``` js_catalog_template = \ r"""{% autoescape off %} ... ```
this class has a bunch of details of WSGI environ so I think it more accurately belongs in `django.http.request`
It's probably better to use MiB instead of Mb or MB. (Here and few more times below.)
We tend to quote names and values in messages for readability. ```suggestion meraki.fail_json(msg="Parameters 'org_name' or 'org_id' parameters are required") ```
This regexp should be put into a constant, especially since it is reused multiple times. Also, maybe it makes sense to precompile it (and potentially others) with `re.compile()`.
Please create dicts immediately with the values you already have.
This is wrong, already explained.
Instead of calling `get_capabilities` twice in the module, maybe make it an attribute for `FactsBase` class in the `__init__` method and reuse? `self._capabilities = get_capabilities(self._module)`
Again, quote names and values: ```suggestion meraki.fail_json(msg="Parameters 'net_name' and 'net_id' are mutually exclusive") ```
All these methods can be clubbed into a single method that takes data and pattern string as arguments and returns the match else None
This is wrong. It is too easy. Check-mode in this case does not bring anything to the table. The user might have made mistakes and it has not been tested as it would be for a real run.
Sort alphabetically, add defaults before choices. ```suggestion state=dict(type='str', default='present', choices=['absent', 'present', 'query']), ```
don't need a trailing comma for lists with a single element (only needed for tuples)
For test doc strings, rather than "Test X" I try to describe the desired behavior: `A ValueError is raised when the incorrect object type is passed to a query lookup."
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
I would separate each `with` statement with a line break. right now it looks like a huge block of stuff.
flake8 complains about missing spaces around `*`
put this on the class so you don't have to repeat it in each method
no blank line needed
`del` is a builtin, not a function. These parens don't have to be here
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
As noted in https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style, we're not so strict about it in code.
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
(Similarly, mark any string that you are going to call decode on with as a b"string".)
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
`del` is a builtin, not a function. These parens don't have to be here
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
, keeping a reference to the cyptes object so that the vsimem file...
prefer hanging indent style with 1 arg per line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
I prefer putting the closing ) on the next line
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
I think `enumerate` would work here
It's not clear to me why it's valuable to change the semantics here so that this validation doesn't happen first if `fail_silently=False`. I don't think the message must be changed to add "like".
I would put the arguments all on this line
rather than custom caching with a dict, this might be clearer with a module-level function using `@lru_cache(maxsize=2)`, with the current value of `USE_TZ` as the only argument. It would save some lines and clarify it's a cache.
I think we can increase the readability with constant for `1` and `2`. ```python DAILY_COUNTER=1 WEEKLY_COUNTER=2 ``` And then you can just do `key=WEEKLY_COUNTER,`
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
You have some unmerged lines here
Unfortunately annotation names can contain LOOKUP_SEP - that is the reason why the ugly refs_aggregate method was added. For example qs.annotate(Max('id')) will create an annotation named max__id, and that can't be referred if the code checks for len(field_list) == 1.
Can this be added in agrspec as choices as well? Probably by having this list as a global variable.
this should be aggregate now, change it here , argspec and docstrings
type='str' is a default value not required to mention in separately.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
, keeping a reference to the cyptes object so that the vsimem file...
prefer hanging indent style with 1 arg per line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
I prefer putting the closing ) on the next line
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
I think `enumerate` would work here
no blank line
`field_names_cache` should be moved an initialized in the `__init__()`, currently we clean it for each `obj`.
returning `None` isn't perfectly equivalent to `continue`. I think it might be cleaner to move some or all of `_handle_object` back into `__iter__`. This allows us to use `continue` again.
it's a separate item, but I wonder if we could patch override_settings to handle DATABASE_ROUTERS like is done below
`field_names` are used only when `self.ignorenonexistent is True`, so we can optimize this part, e.g.: ```python field_names = set() if self.ignorenonexistent: if Model not in field_names_cache: self.field_names_cache[Model] = {f.name for f in Model._meta.get_fields()} field_names = self.field_names_cache[Model] ```
Please do not assume a model only inherits from `django.db.models.Model` I inherit from standard python classes as well. Add check for `issubclass(model, django.db.models.Model)`.
In `_handle_m2m_field()` and `_handle_foreign_key_field()` we can avoid of temporary variables (`value`) and return directly, e.g. ```python def _handle_foreign_key_field(self, field, field_value): return base.deserialize_fk_value(field, field_value, self.using, self.handle_forward_references) ```
`else` is unnecessary, I think we can leave: ```python if self.ignorenonexistent: continue raise ```
We can start using f-string ```suggestion raise base.DeserializationError( f"Invalid model identifier: {model_identifier!r}" ) ```
`assertEquals()` is deprecated, use `assertEqual()` instead.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
`enumerate` on for range.
Breaks. Read coding conventions.
Lack of data is denoted by `None` not 0.
Don't capture groups you don't use. Unused captured group.
This is useless at the end.
This is no longer actual.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
No brackets needed.
Useless with timestamp available.
Must not break extraction if missing.
Should contain `quality` key.
`enumerate` on for range.
Don't capture groups you don't use. Unused captured group.
Useless with timestamp available.
This is useless at the end.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
No brackets needed.
This is no longer actual.
49-95 code duplication.
Should contain `quality` key.
Must not break extraction if missing.
`enumerate` on for range.
Don't capture groups you don't use. Unused captured group.
Useless with timestamp available.
This is useless at the end.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
No brackets needed.
This is no longer actual.
49-95 code duplication.
Should contain `quality` key.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
`enumerate` on for range.
Don't capture groups you don't use. Unused captured group.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
No brackets needed.
This is useless at the end.
This is no longer actual.
Useless with timestamp available.
Lack of data is denoted by `None` not 0.
What kind of values are valid here? I would expect this to be `float` based on the `0.0` in the description.
A general remark: you should always use complete sentences. So this should end with a period.
2.6 or 2.7? Also you `requirements` listed here and the modules.
Please use formatting like `C(<device-on-host>[:<device-on-container>][:<permissions>])`, and `(e.g. device C(/dev/sdc:/dev/xvdc:rwm))` in the line below.
I would remove this `device` here, and similar the option name in the examples for `device_read_bps`, `device_read_iops` etc.
(Same for the related options.)
That does make sense. Thanks.
(In general, I don't think modules should have such options.)
You should emphasize that the module can and will not do any idempotence checking for this.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
Should contain `quality` key.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
`enumerate` on for range.
Don't capture groups you don't use. Unused captured group.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
No brackets needed.
This is useless at the end.
This is no longer actual.
Useless with timestamp available.
Lack of data is denoted by `None` not 0.
I thought `formset = ChoiceFormSet(self.data, auto_id=False, prefix='choices')` could go in `setUp()` also.
I'm not sure whether it should be considered part of a separate clean up across all cases of this in the code base or whether we can handle this here for all of the adjacent lines, but it would be nice to fix the indentation: ```python self.management_form_html + ( '<tr><th>Choice:</th><td><input type="text" name="choices-0-choice" value="Calexico"></td></tr>' '<tr><th>Votes:</th><td><input type="number" name="choices-0-votes" value="100"></td></tr>' ) ```
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
put closing parenthesis on the next line
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
... also we cannot use `User` in the `BaseBackend` so it will be hard to return something consistent.
I don't that we should do this. `BaseBackend` contains only basic methods.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
I thought `formset = ChoiceFormSet(self.data, auto_id=False, prefix='choices')` could go in `setUp()` also.
I'm not sure whether it should be considered part of a separate clean up across all cases of this in the code base or whether we can handle this here for all of the adjacent lines, but it would be nice to fix the indentation: ```python self.management_form_html + ( '<tr><th>Choice:</th><td><input type="text" name="choices-0-choice" value="Calexico"></td></tr>' '<tr><th>Votes:</th><td><input type="number" name="choices-0-votes" value="100"></td></tr>' ) ```
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
put closing parenthesis on the next line
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
... also we cannot use `User` in the `BaseBackend` so it will be hard to return something consistent.
I don't that we should do this. `BaseBackend` contains only basic methods.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
I'd include the min length in the error message
I would code it like this: ``` DEFAULT_USER_ATTRIBUTES = ('username', 'first_name', 'last_name', 'email') def __init__(self, user_attributes=DEFAULT_USER_ATTRIBUTES, max_similarity=0.7): self.user_attributes = user_attributes ```
If `user_attributes` is set to `()` or `[]`, the default set of attributes will be used. That may be surprising. Can you make a strict check for `user_attributes is None`? I understand that the validator doesn't do anything then and can simply be removed from the settings in that case, but I can imagine situations where someone would control `user_attributes` (e.g. through an UI) and not `AUTH_PASSWORD_VALIDATORS`.
`unordered_list` handles nesting which you don't seem to need here. A pedestrian implementation with `format_html` would be more readable: ``` help_items = [format_html('<li>{}</li>', help_text) for help_text in help_texts] return format_html('<ul>{}</ul>', ''.join(help_items)) ``` Furthermore, this implementation marks the result as safe, which is useful here. (Truth be told, I'm reluctant to use template tags or filters in Python code, for ideological reasons.)
its so that, for example, a ...
no dash in "email"
has a -> is of a
Oh I missed that. Sorry!
This would be better as a set rather than a list.
Not sure how much a difference it makes, but it seems better to store this in Python rather than having to read from a text file. Worth it to make the file location customizable? If so, it might be nice to make "common passwords" a separate package so we don't have to include that list in Django. I guess users might not care for the additional setup tasks though.
I'd include the min length in the error message
`unordered_list` handles nesting which you don't seem to need here. A pedestrian implementation with `format_html` would be more readable: ``` help_items = [format_html('<li>{}</li>', help_text) for help_text in help_texts] return format_html('<ul>{}</ul>', ''.join(help_items)) ``` Furthermore, this implementation marks the result as safe, which is useful here. (Truth be told, I'm reluctant to use template tags or filters in Python code, for ideological reasons.)
its so that, for example, a ...
no dash in "email"
has a -> is of a
If `user_attributes` is set to `()` or `[]`, the default set of attributes will be used. That may be surprising. Can you make a strict check for `user_attributes is None`? I understand that the validator doesn't do anything then and can simply be removed from the settings in that case, but I can imagine situations where someone would control `user_attributes` (e.g. through an UI) and not `AUTH_PASSWORD_VALIDATORS`.
I would code it like this: ``` DEFAULT_USER_ATTRIBUTES = ('username', 'first_name', 'last_name', 'email') def __init__(self, user_attributes=DEFAULT_USER_ATTRIBUTES, max_similarity=0.7): self.user_attributes = user_attributes ```
Not sure how much a difference it makes, but it seems better to store this in Python rather than having to read from a text file. Worth it to make the file location customizable? If so, it might be nice to make "common passwords" a separate package so we don't have to include that list in Django. I guess users might not care for the additional setup tasks though.
Since the list is in order of most common use, the code detects incorrect passwords slightly faster if you preserve the order. :bikeshed:
Scanning a list will not be faster than a membership test of a set (unless the list of words is very small).
If something on the managed machine messes up, this could be in some other encoding. Also, there's always the chance that the way the client machine handles non-utf-8 bytes will lead to those bytes coming through unescaped. Basically, the connection plugin is a boundary between Ansible and another system. So we need to be a little more paranoid in sanitizing our data in this area.
You might want to specify a different error handler here. The default handler is surrogate_or_replace. When non-utf8 byte sequences are encountered, it either uses surrogateescape if available or replace. This can munge output on python2. Since we're sending this to json.loads (rather than displaying it to the user) it might be better to use surrogate_or_strict as the error handler here.
I remember having to add the to_bytes()... it could be that an old version of winrm returned text here. I do not remember for sure if that was fixed in the lowest winrm version that we support. Two other notes: 1) This needs to return bytes on python3 as well as python2 2) to_bytes() is a no-op (overhead of the function call and a couple conditionals but no encoding is done) if the string is already a byte string.
Connection plugins return bytes for stdout and stderr (the callers are responsible for transforming to text or not). So this needs to remain with to_bytes().
instead of wrapping subprocess_with_retry with `to_text`, wrap this `std_out`
It is blocking, but that's not the problem - `parser.error` is guaranteed to terminate the program, so this line is indeed superfluous
We should also test the nonexistent time with `is_dst=True` and `is_dst=False`
**Never ever** use `eval` on data you don't control.
The convention in other connection plugins seems to be to raise an `AnsibleError` instead of `IOError`.
I'm not about the URL. **Do not touch** the global `std_headers`.
You might append this to the `using` sql passed to `super()` to fix the test failure on Jenkins (due to usage of TABLESPACE).
There is no need to provide a value for `max_pages_num` - I think it should be calculated automatically: ```python (self.on_each_side + self.on_ends) * 2 ``` Otherwise developers can provide values that are incompatible with each other...
I'm not sure if `isinstance(pages_per_range, int)` is required. I think Python/Django doesn't do strict type checking like that in general.
"for BRIN indexes" doesn't seem consistent with usual error messages.
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
Can you please run `pages_per_range` through `quote_value()` (internal function on the schema editor).
Use single quotes
This should probably be the default for postgresql's `schema_editor.sql_create_index`.
The proposed implementation isn't exactly what I had in mind, but I'll have to look a bit later to see if the idea I had in mind is any better. For `BrinIndex`, I guess those features weren't considered by the patch author. Probably no reason not to add them.
You need to wrap the second instantiation in its own assertRaises to actually test it.
You might append this to the `using` sql passed to `super()` to fix the test failure on Jenkins (due to usage of TABLESPACE).
There is no need to provide a value for `max_pages_num` - I think it should be calculated automatically: ```python (self.on_each_side + self.on_ends) * 2 ``` Otherwise developers can provide values that are incompatible with each other...
I'm not sure if `isinstance(pages_per_range, int)` is required. I think Python/Django doesn't do strict type checking like that in general.
"for BRIN indexes" doesn't seem consistent with usual error messages.
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
Can you please run `pages_per_range` through `quote_value()` (internal function on the schema editor).
Use single quotes
This should probably be the default for postgresql's `schema_editor.sql_create_index`.
The proposed implementation isn't exactly what I had in mind, but I'll have to look a bit later to see if the idea I had in mind is any better. For `BrinIndex`, I guess those features weren't considered by the patch author. Probably no reason not to add them.
You need to wrap the second instantiation in its own assertRaises to actually test it.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
~~~diff - state=dict(type='str', required=True, choices=['absent', 'present']), + state=dict(type='str', default='present', choices=['absent', 'present']), ~~~
```not (foo is None)``` => ```foo is not None```
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
Since we expect the name to start with 'http://' or 'https://' I would so this: ```python if name.startswith('http://') or name.startswith('https://'): ... ```
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
This should return {} instead of None so camel_dict_to_snake_dict works.
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
~~~diff - state=dict(type='str', required=True, choices=['absent', 'present']), + state=dict(type='str', default='present', choices=['absent', 'present']), ~~~
```not (foo is None)``` => ```foo is not None```
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
Since we expect the name to start with 'http://' or 'https://' I would so this: ```python if name.startswith('http://') or name.startswith('https://'): ... ```
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
This should return {} instead of None so camel_dict_to_snake_dict works.
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
This can be single lined: ```python '%s\n%s' % (combined_list[prev], combined_list[k]), ```
Please use a single quote.
one more for the single line version
```suggestion b_opath = os.path.realpath(to_bytes(os.path.join(path, group))) ```
It's better to actually say that there's no file in place or it's inaccessible.
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
What do you need this function for? Ansible already handles aliases, there's no need to do this manually.
```suggestion b_opath = os.path.realpath(to_bytes(os.path.join(path, group))) ```
Use a tuple as a the cache key instead of stringifying this. This will likely result in a performance problem later if not resolved now. ```suggestion key = tuple(groups) + (b_opath,) ```
The `to_bytes` should be on the `self._basedir` instead: ```suggestion b_opath = os.path.realpath(os.path.join(to_bytes(self._basedir), b'r_group_vars')) ```
It's better to actually say that there's no file in place or it's inaccessible.
What do you need this function for? Ansible already handles aliases, there's no need to do this manually.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
I'm not sure it really matters, but I'd put `self.ALLOW_BASE_THROTTLING` first.
Is it too late to move the conversion of `auto` to an integer to a post-processing step (e.g. your `get_num_test_processes()` function)? I feel like the `parallel_type` function's job here should only be to check that the value equals `auto` if the value is a string, but not to apply the environment-dependent business logic to convert `auto` to a number. (I also see that `get_num_test_processes()` is already calling `multiprocessing.cpu_count()`, so there may be some duplication of logic with the way things are currently structured.)
version added requires quotes, otherwise it will be processed as number
Is there some other way other than doing a len on `self._workers`? With forking and your threading work this should be reliable, but is it possible that 3rd party process plugins in the future would not have a static length list? Maybe they pop and append, and this could catch at a point where it's not at the max.
I think you can avoid most of the boiler plate here by using the pool as a context manager. ```suggestion with multiprocessing.Pool(1) as pool: is_open = pool.apply(connection_is_open) ```
Having an import in a test function is not the best idea since it mutates the whole test runtime process. You'd probably need to do the whole test in a subprocess to properly isolate it.
OTOH, this does nothing since the module is already cached by the time this gets executed. So it's a no-op.
We usually prefer `assertIs` here as it will also catch issues with an invalid return type ```suggestion self.assertIs(is_open, False) ```
We should also assert that the connection in the parent/current process remains usable. ```suggestion self.assertIs(connection.is_usable(), True) ```
this is the kind of assertion that could become flaky given incremental population of `connectors`.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
The CamelCase exception .response attribute is particular to boto3's ClientError. IOError doesn't have .response so you can remove the `**camel_dict_to_snake_dict(e.response)` bit of this.
It would be good to wrap this in a try/except botocore.exceptions.ClientError as e
Running the legacy ec2_asg integration tests that use this module, I think this line should be `if block_device_mapping:` because it doesn't look like it can be None or an empty list. The error: ``` An exception occurred during task execution. To see the full traceback, use -vvv. The error was: Missing required parameter in BlockDeviceMappings[0]: "DeviceName" fatal: [localhost]: FAILED! => {"changed": false, "failed": true, "module_stderr": "Traceback (most recent call last):\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 436, in <module>\n main()\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 430, in main\n create_launch_config(connection, module)\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 326, in create_launch_config\n connection.create_launch_configuration(**launch_config)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 253, in _api_call\n return self._make_api_call(operation_name, kwargs)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 517, in _make_api_call\n api_params, operation_model, context=request_context)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 572, in _convert_to_request_dict\n api_params, operation_model)\n File \"/Library/Python/2.7/site-packages/botocore/validate.py\", line 270, in serialize_to_request\n raise ParamValidationError(report=report.generate_report())\nbotocore.exceptions.ParamValidationError: Parameter validation failed:\nMissing required parameter in BlockDeviceMappings[0]: \"DeviceName\"\n", "module_stdout": "", "msg": "MODULE FAILURE", "rc": 0} ```
[] is False, so I don't think you need the `and len(launch_configs) > 0`.
You may want to import to_text() from ansible.module_utils._text and use that instead of str() to be sure of python3 compatibility. (But having trouble testing that since boto3 isn't returning the created time for me...)
If we get into this else block the lack of an exception is going to throw a traceback. I'm not really sure about this if/else (could it take a few moments to successfully create the launch config?). But you can just fail with the module.fail_json(msg="helpful message") here since there isn't a traceback or an exception to have a .response.
It looks like just setting `instance_monitoring` to `true` now won't really do anything, since basic monitoring is on by default and the value of `advanced_instance_monitoring` would default to `False`. This seems confusing.
This option isn't required but is throwing an error for me if not provided (because if not provided it attempts to iterate over a Nonetype). Can you add default=[]? fixes the problem for me.
Use `aliases` if you want to make these two synonymous.
Unnecessary line break
I think a more elegant solution would be to add one more list element and `str.join()` will do the rest: ```suggestion b_outs.append(b'') self.editor.write_data(b'\n'.join(b_outs), context.CLIARGS['output_file'] or '-') ```
Okay, I just tested this and it looks like fd.close() does not cause an error. It's useless to have it there but not strictly necessary to remove it.
This can be single-lined: ```suggestion return (matched_tags or not tags) and not test_tags.intersection(exclude_tags) ```
single line as above
I would chop `does_`: ```suggestion if test_match_tags(test, tags, exclude_tags) ```
Use single quotes for strings, unless there's a nested single quote. ([Python style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style))
nit: this doesn't need to be a field, you can just use a local variable
With this simplification, the pythonic way of generating a list is to use list comprehension. You can replace the whole function body with: ```python return [parse_to_obj(r) for r in parse_to_logical_rows(vlan_out)] ```
probably inside the `if` instead of before since it's kind of confusing to see this for a test that isn't going to actually read the DLQ
Maybe the following will be more explicit: ```python first_author_books = result[0].books.all() self.assertEqual([self.book1], [first_author_books]) self.assertEqual([self.reader], [first_author_books[0].read_by.all()])) ```
`yield from` is not allowed in async functions.
This will consume the `streaming_content` generator on Python 2. Use `django.utils.six.moves.map` instead.
Let's be consistent, if the previous test we have `newsecret`: ```suggestion SECRET_KEY_FALLBACKS=['oldsecret'], ```
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
immediatelly -> immediately
```suggestion SECRET_KEY_FALLBACKS=['oldsecret'], ```
```suggestion self.assertIs(p2.check_token(user, tk), True) ```
Single quotes please.
You can use `self.fail('Cyclic reference in Exception Reporter.get_traceback_frames()')`
I think this could be increased to 10 just in case the system is really slow. The important thing is that it doesn't loop forever.
`yield from` is not allowed in async functions.
This will consume the `streaming_content` generator on Python 2. Use `django.utils.six.moves.map` instead.
Let's be consistent, if the previous test we have `newsecret`: ```suggestion SECRET_KEY_FALLBACKS=['oldsecret'], ```
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
immediatelly -> immediately
```suggestion SECRET_KEY_FALLBACKS=['oldsecret'], ```
```suggestion self.assertIs(p2.check_token(user, tk), True) ```
Single quotes please.
You can use `self.fail('Cyclic reference in Exception Reporter.get_traceback_frames()')`
I think this could be increased to 10 just in case the system is really slow. The important thing is that it doesn't loop forever.
`yield from` is not allowed in async functions.
This will consume the `streaming_content` generator on Python 2. Use `django.utils.six.moves.map` instead.
Let's be consistent, if the previous test we have `newsecret`: ```suggestion SECRET_KEY_FALLBACKS=['oldsecret'], ```
This test is too specific, IMO. It's should be enough to check that `SECRET_KEY` is lazy validated.
immediatelly -> immediately
```suggestion SECRET_KEY_FALLBACKS=['oldsecret'], ```
```suggestion self.assertIs(p2.check_token(user, tk), True) ```
Single quotes please.
You can use `self.fail('Cyclic reference in Exception Reporter.get_traceback_frames()')`
I think this could be increased to 10 just in case the system is really slow. The important thing is that it doesn't loop forever.
I don't see a need for string interpolation in cases like this.
I'd suggest this for the indentation: ```python return [ force_text( capi.get_field_name(capi.get_field_defn(self._layer._ldefn, i)), self.encoding, strings_only=True, ) for i in range(self.num_fields) ] ``` Any concern about having no tests for the `self.encoding, strings_only=True` bit? I'm not sure we would expect non-string values in the first place? Also, at least on my system, there are no failures with it removed here or in `layer_name` above.
Running the entire test suite I could reproduce the error, but not if only running `django.contrib.gis`. Not sure exactly the cause but this code seems suspicious as we leave a different model admin registered for other tests.
```suggestion module.fail_json(msg="The following packages are absent in the currently booted rpm-ostree commit: %s" % ' '.join(pkgs)) ```
```suggestion type: list suboptions: ```
When delegating, I'd think that we'd need to run the facts module for the delegated host rather than for the inventory_hostname host....
Still need to add the ansible_pkg-mgr fact to results if: * We selected one of yum, yum4, or dnf * And we are not delegating.
So I guess that this is where we need to set the fact in results. This is the only place where we know that we've had to run the facts module in order to retrieve the package manager.
I meant for the entire string here to be a constant; otherwise looks good to me.
This doesn't appear to support the use parameter but probably should.
Similarly, ```if tc['skip'].get('i')```
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Line is too long.
Too long line.
This line is too long. Max line length allowed in Ansible is 120 characters.
Too long line.
Line is too long.
Too long line.
Line is too long.
use python bool for the default `default=False`
Line is too long.
Default type is `str` so you don't have to set it explicitly. Just use `size=dict(),` instead. The same bellow.
What's the purpose of http://example.com/v.flv here? It always gives a 404 error and I think it's unrelated to iQiyi
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
Similarly, ```if tc['skip'].get('i')```
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
docstring with example input/output would be really helpful
Please ignore, my suggestion is invalid syntax.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
docstring with example input/output would be really helpful
Please ignore, my suggestion is invalid syntax.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Here's how I'd fix this (since pylint complains about the change that autopep8 makes): ``` diff diff --git a/lib/ansible/modules/storage/netapp/netapp_e_storagepool.py b/lib/ansible/modules/storage/netapp/netapp_e_storagepool.py index cafcbdbbc5..c55e8132a4 100644 --- a/lib/ansible/modules/storage/netapp/netapp_e_storagepool.py +++ b/lib/ansible/modules/storage/netapp/netapp_e_storagepool.py @@ -120,14 +120,13 @@ def select(predicate, iterable): if predicate(x): yield x +def _identity(obj): + return obj class GroupBy(object): # python 2, 3 generic grouping. def __init__(self, iterable, key=None): - if key is None: - def key(x): - return x - self.keyfunc = key + self.keyfunc = key if key else _identity self.it = iter(iterable) self.tgtkey = self.currkey = self.currvalue = object() ```
docstring with example input/output would be really helpful
Notice that, conceptually, the `set(a) | set(b)` operation is different than `a + b`. The former performs set union meaning that duplicates are removed, whereas the latter is a simple concatenation with the possibility for the same item to appear twice. If these are considered "set operations", then the else part should be replaced by something like: ``` c = a + filter(lambda x: x not in a, b) ```
Please ignore, my suggestion is invalid syntax.
```suggestion return '-' + value if neg else int(value) ```
Same question for dropping lambda here as well.
Simplify, harmonise with yt-dlp pt6: ```suggestion formats = self._extract_m3u8_formats( 'https://b-%s.%s/hls/%d/%d.m3u8' % (server, host, model_id, model_id), video_id, ext='mp4', m3u8_id='hls', fatal=False, live=True) self._sort_formats(formats) ```
The pylint error is occurring because this `MockSelector` class shadows the one on line 59.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
docstring with example input/output would be really helpful
Please ignore, my suggestion is invalid syntax.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
docstring with example input/output would be really helpful
Notice that, conceptually, the `set(a) | set(b)` operation is different than `a + b`. The former performs set union meaning that duplicates are removed, whereas the latter is a simple concatenation with the possibility for the same item to appear twice. If these are considered "set operations", then the else part should be replaced by something like: ``` c = a + filter(lambda x: x not in a, b) ```
m3u8 can't be extracted now since `formats` will be overwritten by the code below.
Please ignore, my suggestion is invalid syntax.
Same question for dropping lambda here as well.
Makes sense, thanks for the investigation!
You can pass `key_map` directly as `dict.__iter__` yields keys.
The pylint error is occurring because this `MockSelector` class shadows the one on line 59.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
docstring with example input/output would be really helpful
Notice that, conceptually, the `set(a) | set(b)` operation is different than `a + b`. The former performs set union meaning that duplicates are removed, whereas the latter is a simple concatenation with the possibility for the same item to appear twice. If these are considered "set operations", then the else part should be replaced by something like: ``` c = a + filter(lambda x: x not in a, b) ```
m3u8 can't be extracted now since `formats` will be overwritten by the code below.
Please ignore, my suggestion is invalid syntax.
Same question for dropping lambda here as well.
Makes sense, thanks for the investigation!
You can pass `key_map` directly as `dict.__iter__` yields keys.
The pylint error is occurring because this `MockSelector` class shadows the one on line 59.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
Please ignore, my suggestion is invalid syntax.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
It seems these could fit on a single file. We could define `args = (str(self.name), bases, body)` to avoid repeating them.
Repeating would be quite rare situation, only for classes with multiple metaclasses, so defining new value used only once in 98% of time would be slower in the end.
Using `_` for unused variable is a common idiom.
I would suggest not using `_` as a variable name as that may conflict with gettext imports.
maybe should be get('properties', {}).get('ipConfigurations') so it will be shorter
Ah, I see. I was curious whether that was a common style in Django, etc. Thanks for the reply @timgraham > On Feb 24, 2015, at 7:49 PM, Tim Graham notifications@github.com wrote: > > In django/forms/forms.py: > > > @@ -580,12 +586,16 @@ def value(self): > > if not self.form.is_bound: > > data = self.form.initial.get(self.name, self.field.initial) > > if callable(data): > > - data = data() > > - # If this is an auto-generated default date, nix the > > - # microseconds for standardized handling. See #22502. > > - if (isinstance(data, (datetime.datetime, datetime.time)) and > > - not getattr(self.field.widget, 'supports_microseconds', True)): > > - data = data.replace(microsecond=0) > > - if self._initial_value is not UNSET: > > I don't really mind either way but discussed briefly in IRC and concluded: "Code is clearer when objects of a given type always have the same set of attributes - the presence of a sentinel value is a clearer marker of intent (as opposed to bug) than the absence of an attribute entirely." -Carl Meyer > > â€” > Reply to this email directly or view it on GitHub.
`hasattr` is a more idiomatic approach to this (also saves a couple lines of code), unless there is a specific reason for using an object like this.
Is there a reason to use `len()` instead of checking the objects? `len()` may take more effort to debug in the event of a failure.
Such cases should be handled, too. I guess a possible approach is creating a table with common video and audio codecs. If given codecs are not on the table, fallback to `video,audio` order.
Some more: avc2, avc3, avc4. These would be enough.
This is missing other likely candidates: `y` and `yes` I'd also argue it should be simplified to be case-insensitive so that `TRUE` == `true` (which requires you actually assert the incoming `val` is actuall stringy enough to have `.lower()`, rather than simply doing an equality match against any of the values)
I typically use something like the following: ```python return str(val).lower() in {'1', 't', 'y', 'on', 'true', 'yes'} ``` This works whether you pass a boolean, integer or string, although we are only expecting a string from the environment anyway. If we wanted to be strict, we should reject invalid values: ```python if str(val).lower() in {'1', 't', 'y', 'on', 'true', 'yes'}: return True if str(val).lower() in {'0', 'f', 'n', 'off', 'false', 'no'}: return False raise ValueError('Non-boolean string provided.') ```
2.6 or 2.7? Also you `requirements` listed here and the modules.
I would also point out that the `range` filter is a better alternative
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
FYI, in case you switch to working on whole config, `get_config.config` gives you the whole tree. (`get_config(:foo).config` is generally equivalent to `get_config.config[:foo]`.) Also, VMDB::Config is deprecated. May be better to use `::Settings.to_hash`.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
Can you name this a little more verbosely? I can't unsee "get best"
If you're just raising, you can skip the try/except since it's handled in the caller.
This is missing other likely candidates: `y` and `yes` I'd also argue it should be simplified to be case-insensitive so that `TRUE` == `true` (which requires you actually assert the incoming `val` is actuall stringy enough to have `.lower()`, rather than simply doing an equality match against any of the values)
I typically use something like the following: ```python return str(val).lower() in {'1', 't', 'y', 'on', 'true', 'yes'} ``` This works whether you pass a boolean, integer or string, although we are only expecting a string from the environment anyway. If we wanted to be strict, we should reject invalid values: ```python if str(val).lower() in {'1', 't', 'y', 'on', 'true', 'yes'}: return True if str(val).lower() in {'0', 'f', 'n', 'off', 'false', 'no'}: return False raise ValueError('Non-boolean string provided.') ```
2.6 or 2.7? Also you `requirements` listed here and the modules.
I would also point out that the `range` filter is a better alternative
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
FYI, in case you switch to working on whole config, `get_config.config` gives you the whole tree. (`get_config(:foo).config` is generally equivalent to `get_config.config[:foo]`.) Also, VMDB::Config is deprecated. May be better to use `::Settings.to_hash`.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
Can you name this a little more verbosely? I can't unsee "get best"
If you're just raising, you can skip the try/except since it's handled in the caller.
`_select_on_conflict` name is misleading because this method mainly checks options. Maybe `_check_bulk_create_options(...)` :thinking:
I would use an early return (in both cases): ```suggestion return OnConflict.IGNORE ```
Was already highlighted [here](https://github.com/django/django/pull/13065#discussion_r684521409) but was missed.
Was already highlighted [here](https://github.com/django/django/pull/13065#discussion_r684521409) but was missed.
... and here https://github.com/django/django/pull/13065#discussion_r668443543 :wink:
```suggestion 'ignore_conflicts and update_conflicts are mutually exclusive' ```
We try to avoid accessing the database connections when not necessary, so I'd move `db_features`: ```suggestion if ignore_conflicts and update_conflicts: raise ValueError( 'ignore_conflicts and update_conflicts are mutually exclusive.' ) db_features = connections[self.db].features ```
```suggestion 'ignore_conflicts and update_conflicts are mutually exclusive.' ```
```suggestion raise NotSupportedError( 'This database backend does not support ignoring conflicts.' ) ```
```suggestion raise NotSupportedError( 'This database backend does not support updating ' 'conflicts with specifying unique fields that will ' 'trigger the upsert.' ) ```
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
```suggestion type: list suboptions: ```
Running the legacy ec2_asg integration tests that use this module, I think this line should be `if block_device_mapping:` because it doesn't look like it can be None or an empty list. The error: ``` An exception occurred during task execution. To see the full traceback, use -vvv. The error was: Missing required parameter in BlockDeviceMappings[0]: "DeviceName" fatal: [localhost]: FAILED! => {"changed": false, "failed": true, "module_stderr": "Traceback (most recent call last):\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 436, in <module>\n main()\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 430, in main\n create_launch_config(connection, module)\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 326, in create_launch_config\n connection.create_launch_configuration(**launch_config)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 253, in _api_call\n return self._make_api_call(operation_name, kwargs)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 517, in _make_api_call\n api_params, operation_model, context=request_context)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 572, in _convert_to_request_dict\n api_params, operation_model)\n File \"/Library/Python/2.7/site-packages/botocore/validate.py\", line 270, in serialize_to_request\n raise ParamValidationError(report=report.generate_report())\nbotocore.exceptions.ParamValidationError: Parameter validation failed:\nMissing required parameter in BlockDeviceMappings[0]: \"DeviceName\"\n", "module_stdout": "", "msg": "MODULE FAILURE", "rc": 0} ```
```suggestion assert ansible_json_encoder.default(test_input) == {'__ansible_vault': expected} ```
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
Optional data should not break extraction if missing. Read coding conventions.
Need a colon at the end here
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
`{}` won't work in python 2.6.
```suggestion cmd = "%s --query --quiet --explicit --info" % pacman_path ```
```suggestion ``` I don't think you should do this, case is meaningful: ``` max@mde-oxalide % pacman -Q libreoffice-en-US libreoffice-fresh 6.1.3-1 max@mde-oxalide % pacman -Q libreoffice-en-us error: package 'libreoffice-en-us' was not found ```
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
Is it possible ? For me it would mean that it exist a package without version.
It doesn't work if pkg_provides contain a version: ``` max@mde-oxalide % pacman -Qqei perl | grep Provide Provides : perl-archive-tar=2.30 perl-attribute-handlers=1.01 perl-autodie=2.29 perl-autoloader=5.74 perl-autouse=1.11 perl-b-debug=1.26 perl-base=2.27 perl-bignum=0.49 perl-carp=1.50 perl-compress-raw-bzip2=2.074 perl-compress-raw-zlib=2.076 perl-config-perl-v=0.29 perl-constant=1.33 perl-cpan-meta-requirements=2.140 perl-cpan-meta-yaml=0.018 perl-cpan-meta=2.150010 perl-cpan=2.20 perl-data-dumper=2.170 perl-db_file=1.840 perl-devel-ppport=3.40 perl-devel-selfstubber=1.06 perl-digest-md5=2.55 perl-digest-sha=6.01 perl-digest=1.17_01 perl-dumpvalue=1.18 perl-encode=2.97 perl-encoding-warnings=0.13 perl-env=1.04 perl-experimental=0.019 perl-exporter=5.73 perl-extutils-cbuilder=0.280230 perl-extutils-constant=0.25 perl-extutils-install=2.14 perl-extutils-makemaker=7.34 perl-extutils-manifest=1.70 perl-extutils-parsexs=3.39 perl-file-fetch=0.56 perl-file-path=2.15 perl-file-temp=0.2304 perl-filter-simple=0.95 perl-filter-util-call=1.58 perl-getopt-long=2.5 perl-http-tiny=0.070 perl-i18n-collate=1.02 perl-i18n-langtags=0.43 perl-if=0.0608 perl-io-compress=2.074 perl-io-socket-ip=0.39 perl-io-zlib=1.10 perl-io=1.39 perl-ipc-cmd=1.00 perl-ipc-sysv=2.07 perl-json-pp=2.97001 perl-lib=0.64 perl-libnet=3.11 perl-locale-codes=3.56 perl-locale-maketext-simple=0.21_01 perl-locale-maketext=1.29 perl-math-bigint-fastcalc=0.5006 perl-math-bigint=1.999811 perl-math-bigrat=0.2613 perl-math-complex=1.5901 perl-memoize=1.03_01 perl-mime-base64=3.15 perl-module-corelist=5.20180622 perl-module-load-conditional=0.68 perl-module-load=0.32 perl-module-loaded=0.08 perl-module-metadata=1.000033 perl-net-ping=2.62 perl-params-check=0.38 perl-parent=0.236 perl-pathtools=3.74 perl-perl-ostype=1.010 perl-perlfaq=5.021011 perl-perlio-via-quotedprint=0.08 perl-pod-checker=1.73 perl-pod-escapes=1.07 perl-pod-parser=1.63 perl-pod-perldoc=3.2801 perl-pod-simple=3.35 perl-pod-usage=1.69 perl-podlators=5.006 perl-safe=2.40 perl-scalar-list-utils=1.50 perl-search-dict=1.07 perl-selfloader=1.25 perl-socket=2.027 perl-storable=3.08 perl-sys-syslog=0.35 perl-term-ansicolor=4.06 perl-term-cap=1.17 perl-term-complete=1.403 perl-term-readline=1.17 perl-test-harness=3.42 perl-test-simple=1.302133 perl-test=1.31 perl-text-abbrev=1.02 perl-text-balanced=2.03 perl-text-parsewords=3.30 perl-text-tabs=2013.0523 perl-thread-queue=3.12 perl-thread-semaphore=2.13 perl-threads-shared=1.58 perl-threads=2.22 perl-tie-file=1.02 perl-tie-refhash=1.39 perl-time-hires=1.9759 perl-time-local=1.25 perl-time-piece=1.3204 perl-unicode-collate=1.25 perl-unicode-normalize=1.26 perl-version=0.9923 perl-xsloader=0.30 ```
``` # (realname, version) return tuple(output.split()) ``` no need for named variables
```suggestion pkg_version = line.partition(':')[-1].strip() ```
```suggestion pkg_name = line.partition(':')[-1].strip() ``` idem, no need to lower.
If you're going to do partition, I'd rather use the fact that it'll always return a 3-tuple instead of using `-1` to index the third element. i.e.: ```python line.partition(':')[2] ```
@thomwiggers `line.partition(':')[2]` yes, I have no problem with that.
```suggestion cmd = "%s --query --quiet --explicit --info" % pacman_path ```
```suggestion ``` I don't think you should do this, case is meaningful: ``` max@mde-oxalide % pacman -Q libreoffice-en-US libreoffice-fresh 6.1.3-1 max@mde-oxalide % pacman -Q libreoffice-en-us error: package 'libreoffice-en-us' was not found ```
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
Is it possible ? For me it would mean that it exist a package without version.
It doesn't work if pkg_provides contain a version: ``` max@mde-oxalide % pacman -Qqei perl | grep Provide Provides : perl-archive-tar=2.30 perl-attribute-handlers=1.01 perl-autodie=2.29 perl-autoloader=5.74 perl-autouse=1.11 perl-b-debug=1.26 perl-base=2.27 perl-bignum=0.49 perl-carp=1.50 perl-compress-raw-bzip2=2.074 perl-compress-raw-zlib=2.076 perl-config-perl-v=0.29 perl-constant=1.33 perl-cpan-meta-requirements=2.140 perl-cpan-meta-yaml=0.018 perl-cpan-meta=2.150010 perl-cpan=2.20 perl-data-dumper=2.170 perl-db_file=1.840 perl-devel-ppport=3.40 perl-devel-selfstubber=1.06 perl-digest-md5=2.55 perl-digest-sha=6.01 perl-digest=1.17_01 perl-dumpvalue=1.18 perl-encode=2.97 perl-encoding-warnings=0.13 perl-env=1.04 perl-experimental=0.019 perl-exporter=5.73 perl-extutils-cbuilder=0.280230 perl-extutils-constant=0.25 perl-extutils-install=2.14 perl-extutils-makemaker=7.34 perl-extutils-manifest=1.70 perl-extutils-parsexs=3.39 perl-file-fetch=0.56 perl-file-path=2.15 perl-file-temp=0.2304 perl-filter-simple=0.95 perl-filter-util-call=1.58 perl-getopt-long=2.5 perl-http-tiny=0.070 perl-i18n-collate=1.02 perl-i18n-langtags=0.43 perl-if=0.0608 perl-io-compress=2.074 perl-io-socket-ip=0.39 perl-io-zlib=1.10 perl-io=1.39 perl-ipc-cmd=1.00 perl-ipc-sysv=2.07 perl-json-pp=2.97001 perl-lib=0.64 perl-libnet=3.11 perl-locale-codes=3.56 perl-locale-maketext-simple=0.21_01 perl-locale-maketext=1.29 perl-math-bigint-fastcalc=0.5006 perl-math-bigint=1.999811 perl-math-bigrat=0.2613 perl-math-complex=1.5901 perl-memoize=1.03_01 perl-mime-base64=3.15 perl-module-corelist=5.20180622 perl-module-load-conditional=0.68 perl-module-load=0.32 perl-module-loaded=0.08 perl-module-metadata=1.000033 perl-net-ping=2.62 perl-params-check=0.38 perl-parent=0.236 perl-pathtools=3.74 perl-perl-ostype=1.010 perl-perlfaq=5.021011 perl-perlio-via-quotedprint=0.08 perl-pod-checker=1.73 perl-pod-escapes=1.07 perl-pod-parser=1.63 perl-pod-perldoc=3.2801 perl-pod-simple=3.35 perl-pod-usage=1.69 perl-podlators=5.006 perl-safe=2.40 perl-scalar-list-utils=1.50 perl-search-dict=1.07 perl-selfloader=1.25 perl-socket=2.027 perl-storable=3.08 perl-sys-syslog=0.35 perl-term-ansicolor=4.06 perl-term-cap=1.17 perl-term-complete=1.403 perl-term-readline=1.17 perl-test-harness=3.42 perl-test-simple=1.302133 perl-test=1.31 perl-text-abbrev=1.02 perl-text-balanced=2.03 perl-text-parsewords=3.30 perl-text-tabs=2013.0523 perl-thread-queue=3.12 perl-thread-semaphore=2.13 perl-threads-shared=1.58 perl-threads=2.22 perl-tie-file=1.02 perl-tie-refhash=1.39 perl-time-hires=1.9759 perl-time-local=1.25 perl-time-piece=1.3204 perl-unicode-collate=1.25 perl-unicode-normalize=1.26 perl-version=0.9923 perl-xsloader=0.30 ```
``` # (realname, version) return tuple(output.split()) ``` no need for named variables
```suggestion pkg_version = line.partition(':')[-1].strip() ```
```suggestion pkg_name = line.partition(':')[-1].strip() ``` idem, no need to lower.
If you're going to do partition, I'd rather use the fact that it'll always return a 3-tuple instead of using `-1` to index the third element. i.e.: ```python line.partition(':')[2] ```
@thomwiggers `line.partition(':')[2]` yes, I have no problem with that.
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
It probably makes sense to test that the exception reason also matches expectations
You should probably expect unicode strings
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
`>` appears to be a typo
unicode -> str (Python 3, first)
You're checking two separate properties here. This should be in a separate test.
I know, was just wondering if it's intended that it works that way.
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
(Similarly, mark any string that you are going to call decode on with as a b"string".)
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
nit: this doesn't need to be a field, you can just use a local variable
`deferred_sql` [will automatically be added to `collected_sql` on `__exit__` so iterating over it outside of the context should make it less awkward](https://github.com/django/django/blob/c1c361677d9400c8e2cdaddda0c16086bb358492/django/db/backends/base/schema.py#L112-L137). ```suggestion with connection.schema_editor(collect_sql=True) as editor: editor.create_model(CacheTable) for statement in editor.collected_sql: self.stdout.write(statement) ```
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Okay, so @cyberark-bizdev take out my first example. The last two are what we're shooting for. (The middle one is the same as bcoca's example, just in compact yaml instead of "readable" yaml ;-)
not a list of dicts, just pass a dict to the with: ```yaml with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ```
@pilou-, cyberark-bizdev I had a chance to talk to bcoca today and he said the best way to do this is to pass a list of dicts to the lookup. Then either of these playbook constructs can be valid: ``` with_cyberaskpassword: - {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - {'appid': 'Application2', 'query': 'safe=Safe2;Folder=root;Object=User2', 'output': 'password,passprops.username,passprops.address'} with_cyberarkpassword: {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - debug: msg='{{ lookup('cyberark_password', {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'}) }}' ``` Implementing that will take a little restructuring. I'll go into that in the run method below.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
FWIW using `monkeypatch` is preferable because it's pytest-native unless I need something like mocks or spies.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
No such meta field.
If we're just testing broker compatibility I don't think we even need this part of the test.
Here you pass the variable `label`. You have never defined it, though (you defined `ch_label` -- maybe you meant that one?). That's the first of the sanity checks which fails. You might already get that output by running `flake8` on your module.
Why uppercase? I think the standard is to use lowercase. It's bad practice to pass args through that use * and ** to unpack. Better to be explicit about what arguments are being passed in.
Change this argument from bool to choices with the following choices `[never, always, if-supported]` In the `never` case, do not attempt to lock the conf data store In the `always` case, try to lock the conf data store and error if the lock operation failes In the `if-supported` case, try to lock the conf data store and continue unlocked if the lock operation fails The default value should be `never`
```suggestion current_channels = get_current_channels(module, command) if state == 'present': ``` Since it is common to both `absent` and `present`
I would prefer this to be a configurable default. End users have a strong tendency to relocate packages without asking vendors first.
Ok, cool. :thumbsup:
Typically the keywords for state we use are `started`, `stopped`, `present`, and `absent` since these are states rather than commands.
no blank line needed
Any problem with: ``` @property def media(self): ```
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
`result` is empty and no capability is returned. Is this expected? You might want to call `get_device_info` here to get the device capabilities.
I would use `%s` formatting consistently.
I'd omit the `shortcut_url` variable and put this directly in the `get()`.
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
no blank line needed
Any problem with: ``` @property def media(self): ```
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
`result` is empty and no capability is returned. Is this expected? You might want to call `get_device_info` here to get the device capabilities.
I would use `%s` formatting consistently.
I'd omit the `shortcut_url` variable and put this directly in the `get()`.
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
`>` appears to be a typo
If we're just testing broker compatibility I don't think we even need this part of the test.
unicode -> str (Python 3, first)
It would help readability to use a name like "nonexistent_app" rather than "duth..".
non existing -> nonexistent
Copy / paste? I would have expected the ordering to change for the `duth1` and `duth2` arguments.
Swap the apps in `call_command()` but leave them sorted in the error message.
I'd omit the blank line after the docstring as you've done in most places.
preferred format: "#12554 - Make sure ..."
wrap docstrings at 80 chars
Move into `_download_json`.
Here you pass the variable `label`. You have never defined it, though (you defined `ch_label` -- maybe you meant that one?). That's the first of the sanity checks which fails. You might already get that output by running `flake8` on your module.
No such meta field.
No such meta field.
`acodec == 'none'`.
Basically @gundalow's point is that you don't need to repeat the work that Ansible is already doing enforcing required parameters
But this code is never going to be hit if the argument_spec is set to required=True, unless someone set `url: ""`, but it's difficult to test for every bad input format - someone could equally pass `url: "not_a_protocol://thisisnonsense"`
`field_preference` must be `list` or `tuple`. There is no need to touch this usually since default sorting works fine.
Python (capital letter when referring go the software)
Typically the keywords for state we use are `started`, `stopped`, `present`, and `absent` since these are states rather than commands.
no blank line needed
Any problem with: ``` @property def media(self): ```
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
`result` is empty and no capability is returned. Is this expected? You might want to call `get_device_info` here to get the device capabilities.
I would use `%s` formatting consistently.
I'd omit the `shortcut_url` variable and put this directly in the `get()`.
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
```suggestion cmd = "%s --query --quiet --explicit --info" % pacman_path ```
```suggestion ``` I don't think you should do this, case is meaningful: ``` max@mde-oxalide % pacman -Q libreoffice-en-US libreoffice-fresh 6.1.3-1 max@mde-oxalide % pacman -Q libreoffice-en-us error: package 'libreoffice-en-us' was not found ```
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
Is it possible ? For me it would mean that it exist a package without version.
It doesn't work if pkg_provides contain a version: ``` max@mde-oxalide % pacman -Qqei perl | grep Provide Provides : perl-archive-tar=2.30 perl-attribute-handlers=1.01 perl-autodie=2.29 perl-autoloader=5.74 perl-autouse=1.11 perl-b-debug=1.26 perl-base=2.27 perl-bignum=0.49 perl-carp=1.50 perl-compress-raw-bzip2=2.074 perl-compress-raw-zlib=2.076 perl-config-perl-v=0.29 perl-constant=1.33 perl-cpan-meta-requirements=2.140 perl-cpan-meta-yaml=0.018 perl-cpan-meta=2.150010 perl-cpan=2.20 perl-data-dumper=2.170 perl-db_file=1.840 perl-devel-ppport=3.40 perl-devel-selfstubber=1.06 perl-digest-md5=2.55 perl-digest-sha=6.01 perl-digest=1.17_01 perl-dumpvalue=1.18 perl-encode=2.97 perl-encoding-warnings=0.13 perl-env=1.04 perl-experimental=0.019 perl-exporter=5.73 perl-extutils-cbuilder=0.280230 perl-extutils-constant=0.25 perl-extutils-install=2.14 perl-extutils-makemaker=7.34 perl-extutils-manifest=1.70 perl-extutils-parsexs=3.39 perl-file-fetch=0.56 perl-file-path=2.15 perl-file-temp=0.2304 perl-filter-simple=0.95 perl-filter-util-call=1.58 perl-getopt-long=2.5 perl-http-tiny=0.070 perl-i18n-collate=1.02 perl-i18n-langtags=0.43 perl-if=0.0608 perl-io-compress=2.074 perl-io-socket-ip=0.39 perl-io-zlib=1.10 perl-io=1.39 perl-ipc-cmd=1.00 perl-ipc-sysv=2.07 perl-json-pp=2.97001 perl-lib=0.64 perl-libnet=3.11 perl-locale-codes=3.56 perl-locale-maketext-simple=0.21_01 perl-locale-maketext=1.29 perl-math-bigint-fastcalc=0.5006 perl-math-bigint=1.999811 perl-math-bigrat=0.2613 perl-math-complex=1.5901 perl-memoize=1.03_01 perl-mime-base64=3.15 perl-module-corelist=5.20180622 perl-module-load-conditional=0.68 perl-module-load=0.32 perl-module-loaded=0.08 perl-module-metadata=1.000033 perl-net-ping=2.62 perl-params-check=0.38 perl-parent=0.236 perl-pathtools=3.74 perl-perl-ostype=1.010 perl-perlfaq=5.021011 perl-perlio-via-quotedprint=0.08 perl-pod-checker=1.73 perl-pod-escapes=1.07 perl-pod-parser=1.63 perl-pod-perldoc=3.2801 perl-pod-simple=3.35 perl-pod-usage=1.69 perl-podlators=5.006 perl-safe=2.40 perl-scalar-list-utils=1.50 perl-search-dict=1.07 perl-selfloader=1.25 perl-socket=2.027 perl-storable=3.08 perl-sys-syslog=0.35 perl-term-ansicolor=4.06 perl-term-cap=1.17 perl-term-complete=1.403 perl-term-readline=1.17 perl-test-harness=3.42 perl-test-simple=1.302133 perl-test=1.31 perl-text-abbrev=1.02 perl-text-balanced=2.03 perl-text-parsewords=3.30 perl-text-tabs=2013.0523 perl-thread-queue=3.12 perl-thread-semaphore=2.13 perl-threads-shared=1.58 perl-threads=2.22 perl-tie-file=1.02 perl-tie-refhash=1.39 perl-time-hires=1.9759 perl-time-local=1.25 perl-time-piece=1.3204 perl-unicode-collate=1.25 perl-unicode-normalize=1.26 perl-version=0.9923 perl-xsloader=0.30 ```
``` # (realname, version) return tuple(output.split()) ``` no need for named variables
```suggestion pkg_version = line.partition(':')[-1].strip() ```
```suggestion pkg_name = line.partition(':')[-1].strip() ``` idem, no need to lower.
If you're going to do partition, I'd rather use the fact that it'll always return a 3-tuple instead of using `-1` to index the third element. i.e.: ```python line.partition(':')[2] ```
@thomwiggers `line.partition(':')[2]` yes, I have no problem with that.
```suggestion cmd = "%s --query --quiet --explicit --info" % pacman_path ```
```suggestion ``` I don't think you should do this, case is meaningful: ``` max@mde-oxalide % pacman -Q libreoffice-en-US libreoffice-fresh 6.1.3-1 max@mde-oxalide % pacman -Q libreoffice-en-us error: package 'libreoffice-en-us' was not found ```
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
Is it possible ? For me it would mean that it exist a package without version.
It doesn't work if pkg_provides contain a version: ``` max@mde-oxalide % pacman -Qqei perl | grep Provide Provides : perl-archive-tar=2.30 perl-attribute-handlers=1.01 perl-autodie=2.29 perl-autoloader=5.74 perl-autouse=1.11 perl-b-debug=1.26 perl-base=2.27 perl-bignum=0.49 perl-carp=1.50 perl-compress-raw-bzip2=2.074 perl-compress-raw-zlib=2.076 perl-config-perl-v=0.29 perl-constant=1.33 perl-cpan-meta-requirements=2.140 perl-cpan-meta-yaml=0.018 perl-cpan-meta=2.150010 perl-cpan=2.20 perl-data-dumper=2.170 perl-db_file=1.840 perl-devel-ppport=3.40 perl-devel-selfstubber=1.06 perl-digest-md5=2.55 perl-digest-sha=6.01 perl-digest=1.17_01 perl-dumpvalue=1.18 perl-encode=2.97 perl-encoding-warnings=0.13 perl-env=1.04 perl-experimental=0.019 perl-exporter=5.73 perl-extutils-cbuilder=0.280230 perl-extutils-constant=0.25 perl-extutils-install=2.14 perl-extutils-makemaker=7.34 perl-extutils-manifest=1.70 perl-extutils-parsexs=3.39 perl-file-fetch=0.56 perl-file-path=2.15 perl-file-temp=0.2304 perl-filter-simple=0.95 perl-filter-util-call=1.58 perl-getopt-long=2.5 perl-http-tiny=0.070 perl-i18n-collate=1.02 perl-i18n-langtags=0.43 perl-if=0.0608 perl-io-compress=2.074 perl-io-socket-ip=0.39 perl-io-zlib=1.10 perl-io=1.39 perl-ipc-cmd=1.00 perl-ipc-sysv=2.07 perl-json-pp=2.97001 perl-lib=0.64 perl-libnet=3.11 perl-locale-codes=3.56 perl-locale-maketext-simple=0.21_01 perl-locale-maketext=1.29 perl-math-bigint-fastcalc=0.5006 perl-math-bigint=1.999811 perl-math-bigrat=0.2613 perl-math-complex=1.5901 perl-memoize=1.03_01 perl-mime-base64=3.15 perl-module-corelist=5.20180622 perl-module-load-conditional=0.68 perl-module-load=0.32 perl-module-loaded=0.08 perl-module-metadata=1.000033 perl-net-ping=2.62 perl-params-check=0.38 perl-parent=0.236 perl-pathtools=3.74 perl-perl-ostype=1.010 perl-perlfaq=5.021011 perl-perlio-via-quotedprint=0.08 perl-pod-checker=1.73 perl-pod-escapes=1.07 perl-pod-parser=1.63 perl-pod-perldoc=3.2801 perl-pod-simple=3.35 perl-pod-usage=1.69 perl-podlators=5.006 perl-safe=2.40 perl-scalar-list-utils=1.50 perl-search-dict=1.07 perl-selfloader=1.25 perl-socket=2.027 perl-storable=3.08 perl-sys-syslog=0.35 perl-term-ansicolor=4.06 perl-term-cap=1.17 perl-term-complete=1.403 perl-term-readline=1.17 perl-test-harness=3.42 perl-test-simple=1.302133 perl-test=1.31 perl-text-abbrev=1.02 perl-text-balanced=2.03 perl-text-parsewords=3.30 perl-text-tabs=2013.0523 perl-thread-queue=3.12 perl-thread-semaphore=2.13 perl-threads-shared=1.58 perl-threads=2.22 perl-tie-file=1.02 perl-tie-refhash=1.39 perl-time-hires=1.9759 perl-time-local=1.25 perl-time-piece=1.3204 perl-unicode-collate=1.25 perl-unicode-normalize=1.26 perl-version=0.9923 perl-xsloader=0.30 ```
``` # (realname, version) return tuple(output.split()) ``` no need for named variables
```suggestion pkg_version = line.partition(':')[-1].strip() ```
```suggestion pkg_name = line.partition(':')[-1].strip() ``` idem, no need to lower.
If you're going to do partition, I'd rather use the fact that it'll always return a 3-tuple instead of using `-1` to index the third element. i.e.: ```python line.partition(':')[2] ```
@thomwiggers `line.partition(':')[2]` yes, I have no problem with that.
```suggestion cmd = "%s --query --quiet --explicit --info" % pacman_path ```
```suggestion ``` I don't think you should do this, case is meaningful: ``` max@mde-oxalide % pacman -Q libreoffice-en-US libreoffice-fresh 6.1.3-1 max@mde-oxalide % pacman -Q libreoffice-en-us error: package 'libreoffice-en-us' was not found ```
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
Is it possible ? For me it would mean that it exist a package without version.
It doesn't work if pkg_provides contain a version: ``` max@mde-oxalide % pacman -Qqei perl | grep Provide Provides : perl-archive-tar=2.30 perl-attribute-handlers=1.01 perl-autodie=2.29 perl-autoloader=5.74 perl-autouse=1.11 perl-b-debug=1.26 perl-base=2.27 perl-bignum=0.49 perl-carp=1.50 perl-compress-raw-bzip2=2.074 perl-compress-raw-zlib=2.076 perl-config-perl-v=0.29 perl-constant=1.33 perl-cpan-meta-requirements=2.140 perl-cpan-meta-yaml=0.018 perl-cpan-meta=2.150010 perl-cpan=2.20 perl-data-dumper=2.170 perl-db_file=1.840 perl-devel-ppport=3.40 perl-devel-selfstubber=1.06 perl-digest-md5=2.55 perl-digest-sha=6.01 perl-digest=1.17_01 perl-dumpvalue=1.18 perl-encode=2.97 perl-encoding-warnings=0.13 perl-env=1.04 perl-experimental=0.019 perl-exporter=5.73 perl-extutils-cbuilder=0.280230 perl-extutils-constant=0.25 perl-extutils-install=2.14 perl-extutils-makemaker=7.34 perl-extutils-manifest=1.70 perl-extutils-parsexs=3.39 perl-file-fetch=0.56 perl-file-path=2.15 perl-file-temp=0.2304 perl-filter-simple=0.95 perl-filter-util-call=1.58 perl-getopt-long=2.5 perl-http-tiny=0.070 perl-i18n-collate=1.02 perl-i18n-langtags=0.43 perl-if=0.0608 perl-io-compress=2.074 perl-io-socket-ip=0.39 perl-io-zlib=1.10 perl-io=1.39 perl-ipc-cmd=1.00 perl-ipc-sysv=2.07 perl-json-pp=2.97001 perl-lib=0.64 perl-libnet=3.11 perl-locale-codes=3.56 perl-locale-maketext-simple=0.21_01 perl-locale-maketext=1.29 perl-math-bigint-fastcalc=0.5006 perl-math-bigint=1.999811 perl-math-bigrat=0.2613 perl-math-complex=1.5901 perl-memoize=1.03_01 perl-mime-base64=3.15 perl-module-corelist=5.20180622 perl-module-load-conditional=0.68 perl-module-load=0.32 perl-module-loaded=0.08 perl-module-metadata=1.000033 perl-net-ping=2.62 perl-params-check=0.38 perl-parent=0.236 perl-pathtools=3.74 perl-perl-ostype=1.010 perl-perlfaq=5.021011 perl-perlio-via-quotedprint=0.08 perl-pod-checker=1.73 perl-pod-escapes=1.07 perl-pod-parser=1.63 perl-pod-perldoc=3.2801 perl-pod-simple=3.35 perl-pod-usage=1.69 perl-podlators=5.006 perl-safe=2.40 perl-scalar-list-utils=1.50 perl-search-dict=1.07 perl-selfloader=1.25 perl-socket=2.027 perl-storable=3.08 perl-sys-syslog=0.35 perl-term-ansicolor=4.06 perl-term-cap=1.17 perl-term-complete=1.403 perl-term-readline=1.17 perl-test-harness=3.42 perl-test-simple=1.302133 perl-test=1.31 perl-text-abbrev=1.02 perl-text-balanced=2.03 perl-text-parsewords=3.30 perl-text-tabs=2013.0523 perl-thread-queue=3.12 perl-thread-semaphore=2.13 perl-threads-shared=1.58 perl-threads=2.22 perl-tie-file=1.02 perl-tie-refhash=1.39 perl-time-hires=1.9759 perl-time-local=1.25 perl-time-piece=1.3204 perl-unicode-collate=1.25 perl-unicode-normalize=1.26 perl-version=0.9923 perl-xsloader=0.30 ```
``` # (realname, version) return tuple(output.split()) ``` no need for named variables
```suggestion pkg_version = line.partition(':')[-1].strip() ```
```suggestion pkg_name = line.partition(':')[-1].strip() ``` idem, no need to lower.
If you're going to do partition, I'd rather use the fact that it'll always return a 3-tuple instead of using `-1` to index the third element. i.e.: ```python line.partition(':')[2] ```
@thomwiggers `line.partition(':')[2]` yes, I have no problem with that.
It looks like a duplicate of `test_save_unsaved_instance_on_generic_foreign_key` :thinking:
> Shall I also remove `test_target_model_is_unsaved_save` and `test_target_model_is_unsaved_bulk_create`? Yes, they are redundant.
@timgraham It might be more appropriate in another commit then. I believe I wanted to make sure nothing was logged if a m2m backed inline was submitted without changes.
consider assertRaisesMessage to make the test a bit more specific.
WDYT about adding `or self.title` here? ``` python self.title = title or self.title or self.lookup_title ``` This would add support for the following: ``` python class MyStaffFilter(BooleanFieldListFilter): title = "By can haz admin" # no effect :( overwritten in __init__ @admin.register(User) class UserAdmin(UserAdminBase): list_filter = ( ("is_staff", MyStaffFilter), ) ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
I removed it.
Ahh true, sorry for the noise. No changes are required.
Match the error message
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
This would be better as a set rather than a list.
Scanning a list will not be faster than a membership test of a set (unless the list of words is very small).
Since the list is in order of most common use, the code detects incorrect passwords slightly faster if you preserve the order. :bikeshed:
Oh I missed that. Sorry!
Not sure how much a difference it makes, but it seems better to store this in Python rather than having to read from a text file. Worth it to make the file location customizable? If so, it might be nice to make "common passwords" a separate package so we don't have to include that list in Django. I guess users might not care for the additional setup tasks though.
excellent handling of congestion control
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
This also needs bounds checking and (maybe) retries if the ASG isn't yet available.
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
This can be converted to return True. No need of new variable retry_request
Are these put/post/delete/patch/update methods used anywhere? I don't see uses of them.
Please change these vars to ansible Host vars rather OS env vars.
@annikulin it needed more work than I initially thought to get httpapi plugin host var working. Probably we can commit this code as is and I will commit a followup PR to make it configurable using ansible host vars
Does the data model / json of the scaleways api ever change? A chained set of accessors like that tends to be a little fragile if the server response change. Could potentially use some defense against that. Afaict, KeyErrors or IndexErrors here would not be caught elsewhere and would cause a fatal error instead of a semi-graceful json_fail.
For the get('organization') request response? I'm assuming the list of 'organizations' and 'users' will always have a single item? (at least as used with a oauth token as used here). Will that depend on the type of api_token? (ie, if there is something like a organization or group level api_key, would the results be different? If so, could be useful to explain in the docs)
`AnsibleConnectionFailure` is a generic error, it does not mean the resource is missing. It should be more like an err 500.
wouldn't hurt to include some details about the error here. The original exception error message for example. But thats not a blocking suggestion.
a try/except LookupError with a fail_json around this should be sufficient to handle any potential api response data changes gracefully.
catchall exception handling isn't very useful as it hides whatever might have gone wrong from sight. I'd probably just call core(module) here. If there is a specific class of errors that you are expecting, then catching those and returning a more informative error message could be appropriate. I often see the following pattern used which isn't so bad: ``` python import traceback from ansible.module_utils._text import to_native try: core(module) except SomeException as e: module.fail_json(msg="Helpful error message: %s" % to_native(e), exception=traceback.format_exc()) ```
Here I think we just should just default to `json.dumps` if no encoder is specified. No need for an extra setting.
I can see a decent reason to use the `DjangoJSONEncoder` as the default as it gives us `Decimal` and `datetime` support for free. These are very common use cases for django sites (especially datetime!).
chop trailing newline (check code with flake8 to see a couple other minor formatting things)
Does this need to be a separate method? Seems unnecessary to me.
It does the right thing for me. I whipped up a quick container page: ``` $ cat out/index.rst .. automodule:: ansible.plugins.connection :members: ``` and when I generated it with sphinx-build I got this output for the exec_command sample: https://toshio.fedorapeople.org/ansible/test-autodoc/#ansible.plugins.connection.ConnectionBase.exec_command
@annikulin it needed more work than I initially thought to get httpapi plugin host var working. Probably we can commit this code as is and I will commit a followup PR to make it configurable using ansible host vars
Please change these vars to ansible Host vars rather OS env vars.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
This isn't needed, since we now only support 2.6+ anyway.
I can see a decent reason to use the `DjangoJSONEncoder` as the default as it gives us `Decimal` and `datetime` support for free. These are very common use cases for django sites (especially datetime!).
Here I think we just should just default to `json.dumps` if no encoder is specified. No need for an extra setting.
chop trailing newline (check code with flake8 to see a couple other minor formatting things)
It does the right thing for me. I whipped up a quick container page: ``` $ cat out/index.rst .. automodule:: ansible.plugins.connection :members: ``` and when I generated it with sphinx-build I got this output for the exec_command sample: https://toshio.fedorapeople.org/ansible/test-autodoc/#ansible.plugins.connection.ConnectionBase.exec_command
It's fine if it's a placeholder for future capabilities.
This can be converted to return True. No need of new variable retry_request
You will need to include the OAUTH_TOKEN environment variable also.
Please change these vars to ansible Host vars rather OS env vars.
@annikulin it needed more work than I initially thought to get httpapi plugin host var working. Probably we can commit this code as is and I will commit a followup PR to make it configurable using ansible host vars
I see that the requirements lists python-2.6 as the minimum python version. Unfortunately, format strings in python-2.6 are more limited than in python-2.7 and later. So you need to be explicit about the position in the format args list you are looking at like this: ``` python response = rest.get('floating_ips/{0}/actions/{1}'.format(ip, action_id)) ``` There's a few other places with format strings that have to be fixed as well.
no blank line needed
Django should automatically validate `max_length` without a custom method: ``` from django import forms class MyForm(forms.Form): f = forms.CharField(max_length=1) >>> form = MyForm({'f': '12'}) >>> form.errors {'f': ['Ensure this value has at most 1 character (it has 2).']} ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I meant all branches.
FWIW in #9383 this is handled by the `RenameField` operation -- no `AlterField` operation will be generated by the auto-detector just like none are generated on a model and `to` rename.
I'd drop it, and reintroduce the test later on in branches where we'll be able to backport #9383.
You should use `module.add_cleanup_file()` (somewhere above) instead of manually trying to clean up.
You need to make sure `tf` doesn't write a unicode string. tf.write(u("#!/bin/sh/\necho %s" % quote(self.passphrase)).encode("utf-8")) Ansible may have a utility function to do this.
In the "Generate privatekey6 - standard - with non-ASCII passphrase" test, this fails with `'ascii' codec can't decode byte 0xc3 in position 16: ordinal not in range(128)`. Stacktrace: ``` File "/tmp/ansible_openssh_keypair_payload_kCnMbD/__main__.py", line 193, in generate tf.write("#!/bin/sh\necho %s" % quote(self.passphrase)) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 706, in write return self.writer.write(data) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 369, in write data, consumed = self.encode(object, self.errors) ``` It seems to fail in all Python 2 tests; in Python 3 it seems to work.
Minor, but both places that use this could avoid assigning a value by always using the contents of `else` block if this were initialized to `0`. ``` Int cnt = termFreqMap.get(term); if (cnt == null) { cnt = new Int(); termFreqMap.put(term, cnt); } cnt.x += freq; ```
yep, that is what I meant, basically make sure new code is pep8 compliant
this module is marked as "legacy" and therefore the CI is not complaining about pep8 here, but it would help if we try to be pep8 compliant.
The issue with this is the timestamp in the filename and the actual timestamp for file changed might not be the same. Thus returning the wrong timestamp within the filename. To avoid this you can check if configurable backup path is given as input to the module. Refer https://github.com/ansible/ansible/pull/50801
This will break unicode strings under python 2.
this creates race condition. there is a time between remove and move that the file is unavailable. I see original code did same, but we should just allow move to work as it will be an atomic operation
This will cause the `overwrite=always` setting to never work, since the module exits before checking the value of `overwrite`. Please delete this line.
IMHO this statement is hard to read. Woul rewrite to simple if else and for loop.
maybe you add the type `type='path'`
This else part could be removed
Do you need that step? `clients = client_list` should be enough
Use the Python 2 and low 3 Time Machine: `'url too short: %s' % (video_pre_parts, )` or: `'url too short: %(video_pre_parts)s' % {'video_pre_parts': video_pre_parts, }` or: `'url too short: {video_pre_parts}'.format(video_pre_parts=video_pre_parts)` or: `'url too short: {0}'.format(video_pre_parts)` No doubt there are other ways (eg `....format(**locals())`
Generally, I'd have put the static regex directly as the first argument of `re.search()`.
chop newline for consistency with other tests
please include the trailing comma on the last item in a dictionary so if more items are added we don't have to modify this line again
Prefer this indent style: ``` response = self.client.post(reverse('...'), { .... }) ```
It sounds like maybe index_type could be used as the suffix and this method doesn't need that argument. I guess the question is whether index_type should be limited to 3 characters or if truncating the first 3 characters of "index_type" as the suffix is okay.
The hexdigest will always be a fixed length so this only happens if the provided suffix is too long, correct? In that case, I think it would be better to raise an error that the provided suffix is too long.
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
If this test won't be implemented it should be removed.
use the `flatten_dict` method from module_utils.network.nxos.utils.utils.py
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
Need a colon at the end here
It doesn't look like you need an OrderedDict here. A regular dict will do (and be faster and take up less memory).
In python, this is probably better represented as a global dictionary in a separate module (as outlined above). Something like file serviceprovider.py: SPKEYS = { 'ID': 'id', 'NAME': 'name', [..] } ```
Better to do this like this: ``` python return list(merged_items.values()) ``` Using list there will make sure that it is a list on python 3 (rather than a DictView).
You probably want to compare this to None specifically instead of just doing a check of whether ```item[ignored_key]``` is a false value. Otherwise you'll also catch 0, False, and empty containers here. Comparing to None would look like this: ``` if ignored_key in item and item[ignored_key] is None: ```
There's a lot of copying going on here as well. Both on this line and on line 456. Copying is slow so you want to eliminate any that aren't needed.
If you're unfamiliar with why that is, you should probably google it. It has to do with python processing the function declaration once when the function is declared and therefore there's only one copy of the default value which is used every time the function is called. If you have a mutable container as a default value, it will not be recreated between invocations so it may not be empty the second time you call the function.
You don't modify ignore_when_null in this function so it's probably harmless to use [] as its default value but it's a bad habit to get into. You should try to always use a immutable as a default value. In this case, you can do: ```ignore_when_null=tuple()```.
use the `flatten_dict` method from module_utils.network.nxos.utils.utils.py
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
Need a colon at the end here
It doesn't look like you need an OrderedDict here. A regular dict will do (and be faster and take up less memory).
In python, this is probably better represented as a global dictionary in a separate module (as outlined above). Something like file serviceprovider.py: SPKEYS = { 'ID': 'id', 'NAME': 'name', [..] } ```
Better to do this like this: ``` python return list(merged_items.values()) ``` Using list there will make sure that it is a list on python 3 (rather than a DictView).
You probably want to compare this to None specifically instead of just doing a check of whether ```item[ignored_key]``` is a false value. Otherwise you'll also catch 0, False, and empty containers here. Comparing to None would look like this: ``` if ignored_key in item and item[ignored_key] is None: ```
There's a lot of copying going on here as well. Both on this line and on line 456. Copying is slow so you want to eliminate any that aren't needed.
If you're unfamiliar with why that is, you should probably google it. It has to do with python processing the function declaration once when the function is declared and therefore there's only one copy of the default value which is used every time the function is called. If you have a mutable container as a default value, it will not be recreated between invocations so it may not be empty the second time you call the function.
You don't modify ignore_when_null in this function so it's probably harmless to use [] as its default value but it's a bad habit to get into. You should try to always use a immutable as a default value. In this case, you can do: ```ignore_when_null=tuple()```.
```suggestion the I(verification_method) will be updated and validation data (if applicable) will be returned. ```
```suggestion - There is a small delay (typically about 5 seconds, but can be as long as 60 seconds) before obtaining the random values when requesting a validation ```
```suggestion - There is a small delay (typically about 5 seconds, but can be as long as 30 seconds) before obtaining the random values when requesting a validation. ```
```suggestion description: Can be used to request certificates from ECS, with C(provider=entrust). ```
```suggestion sample: true ```
```suggestion returned: success and I(ov_eligible) is C(true) and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING). ```
```suggestion description: Whether the domain is eligible for submission of "EV" certificates. Will never be C(true) if I(ov_eligible) is C(false) ```
```suggestion sample: true ```
```suggestion description: Whether the domain is eligible for submission of "OV" certificates. Will never be C(false) if I(ov_eligible) is C(true) ```
```suggestion returned: success and I(ev_eligible) is C(true) and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING). ```
`parallell` => `parallel` `get_upodated_models` => `get_updated_models`
This line is related to the `save` method, and should sit beneath it, rather than the new `get_updated_model` method.
chop trailing newline (check code with flake8 to see a couple other minor formatting things)
Arf, this is also not optimal either. `pre_save` can have side-effects, like `django.db.models.fields.files.FileField.pre_save` does ðŸ˜• We probably don't want to trigger those here. I mean, serendipitously it would work for the `FileField` because even if the returned value is still the same (so we don't add the `field.name` to `updated_fields`), we actually triggered the side-effect committing the file ðŸ˜‚ However, that seems pretty brittle ðŸ˜… I'm not sure what the cleanest/Djangoest approach would be here ðŸ¤” We could add an attribute on the Field class, like `Field.has_pre_save: bool`, but that creates a precedent and users/libs must update their code accordingly. But at least, we would know _for sure_ which fields need to be added and which don't. Any other suggestion is very welcome!
I tried a similar approach while working on acfaec3db5ba39de52f6e607e74343dccf72fba1 and came to the conclusion that this approach can't work (due to something like module caching). As far as I know, you'll have to register the admin to a separate `AdminSite`.
I think there isn't much organization there. Using an existing site should be fine.
First we should verify this passes before we toggle `is_active` to False.
Would there be any utility to sorted() on files/sub_folders? AFAICT, os.walk will use the os.listdir() results directly for filenames. os.listdir() is doc'ed as 'arbitrary order' (iirc, more or less up to fs). Wondering if sorting those results would make for more reproducible behavior when trying to replicate errors. It shouldn't matter at all if there are no bugs, but it could make tracking down odd failures easier.
Omit these lines please.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
`parallell` => `parallel` `get_upodated_models` => `get_updated_models`
Remove this. But do add: ```yaml choices: [ absent, present ] ```
No quote around list members.
following task -> the following task
New modules only go into `devel`, so this will only be available in Ansible 2.4
Not strictly necessary as the default for parameter is that they're not required.
You can delete these empty lines.
What about to insert new line in front of the `if`? It would make nice visual separation between the variable definitions and the condition. The same at any place bellow.
This should be indented by 2 spaces. The same bellow.
```suggestion description: Can be used to request certificates from ECS, with C(provider=entrust). ```
`parallell` => `parallel` `get_upodated_models` => `get_updated_models`
`always_text` is gone.
Use single quotes consistently.
a OrderedSet, like @timgraham suggested
a nice line to see gone
Not strictly necessary as the default for parameter is that they're not required.
Appreciate what you've done with the tests but I think they're harder to comprehend now. As a Django dev I can jump in and read model classes, but the helpers are obscuring things a bit to me. Also you now have multiple tests per test method now - ideally there should just be a handful of classes per test and some assertions. I guess you can split based on overriding, non-overriding, etc.
I'm pretty sure you can make the abstract bases once in the test case class level: ``` class ...Tests(...): class SomeBase(models.Model): ... ``` Then reuse them appropriately in the individual test methods, which should only need to create the "bottom" classes? I grepped for `\btype\(` and found a few uses, but none are for "quick" building. I personally think the "tonnes of vertical scrolling" is a small concern compared to having to grok how `type()` works.
I think it's fine. Separate test cases are normally a good thing since they can fail individually.
It would have been more appropriate if `run_checks()` took an `Apps` instance as a parameter instead of a list of `AppConfig`s.
Would there be any utility to sorted() on files/sub_folders? AFAICT, os.walk will use the os.listdir() results directly for filenames. os.listdir() is doc'ed as 'arbitrary order' (iirc, more or less up to fs). Wondering if sorting those results would make for more reproducible behavior when trying to replicate errors. It shouldn't matter at all if there are no bugs, but it could make tracking down odd failures easier.
I would remove all aliases if possible.
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
(Same for the related options.)
This should really be: ``` - Set custom DNS search domains. (Use I(dns_search) with C('') if you don't wish to set the search domain.) ```
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
likely on s2016 as well. Maybe just remver this line since its effectively 'all versions of windows that ansible works on', so perhaps a bit redundant now.
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
(Similarly, mark any string that you are going to call decode on with as a b"string".)
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
It probably makes sense to test that the exception reason also matches expectations
You're checking two separate properties here. This should be in a separate test.
Match the error message
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
Yes, that's more helpful for me.
Perhaps this could be a docstring? You might elaborate a bit more -- as someone not familiar with MySQL, it's not clear to me what "improved" means.
This line looks backwards... (If I add a `required` to the select, the HTML validator will trigger on the lack of the empty value.)
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
This test doesn't fail either in current code. I suggest to simply drop it. No need to update the pull request, I'm going to commit the patch soon.
Inner functions are slow, especially for pypy - best to extract this!
Just a thought -- would an `expression.flatten()` method be useful? The check below would then be `any(expr for expr in expression.flatten() if isinstance(expr, F))`. Might need to look more closely at other places in the ORM to figure out if it'd be a useful construct.
I think this would be a bit more readable: ``` return ( '%(func)s %(op)s %(dist)s' % {'func': sql, 'op': self.op, 'dist': dist_sql}, params + dist_params ) ```
I _think_ that `Ref` will also need to return True, since it is a named reference to an existing `Col`.
F() expressions aren't the only ones that can refer to other columns in the query. How about Q(other_field__isnull=True). Also, expressions are free to resolve columns of the query without using F-expressions. We need some other way to know if the expression refers to columns of the query. Maybe we could first resolve the expression, the check for Col references? That might be better. The check should be done in the expression, so that the expression tells Django if it is referring to any columns. Making the compiler guess this is the wrong way in my opinion.
```python if step_size is not None: self.validators.append(validators.StepValueValidator(step_size)) ``` Use `django.core.validators.MinValueValidator` as a blueprint and create a `StepValueValidator`. Put the validation code from `to_python` in there.
Have a look at the `IntegerField` how it's done there. This has the advantage, that your `StepSizeValidator` can be reused. And it shall be reused, because `step` can also be applied to an `IntegerField`.
I'll take the time to write one later today but we could use a `@property`: ``` python @property def widget(self): if self.localize: return TextInput else: return NumberInput ``` One of the con here is that `NumberField.widget` will be an instance of `property`. We could also write a descriptor to maintain backward compatibilty: ``` python class WidgetDescriptor(object): def __init__(self, widget, localized_widget): self.widget = widget self.localized_widget = localized_widget def __get__(self, instance, owner): if instance and instance.localize: return self.localized_widget return self.widget class IntegerField(Field): widget = WidgetDescriptor(NumberInput, TextInput) ``` Maybe I'm just over-complicating this whole thing.
And this can be reverted.
`items = value.split(self.delimiter) if value else []` is slightly faster.
What if the prepared value contains unicode? (As I said before, you should use a list comprehension instead of a generator, it'll be about 2 times faster here)
Ditto about the `for`/`else` construct.
I suggest you use the `for`/`else` construct here. ``` python for validator in validators: if isinstance(validator, validators.MinValueValidator) and validator.limit_value <= min_value: break else: validators.append(validators.MinValueValidator(min_value)) ```
Simply return `validators`.
consider assertRaisesMessage to make the test a bit more specific.
That's weird -- `timezone.utc` is a `tzinfo` instance, not a subclass, like `timezone.UTC` was.
No strong feelings! Whatever works.
It seems consistent with the changes below but it's still weird...
(Similarly, mark any string that you are going to call decode on with as a b"string".)
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
Technically since we're doing this in a fixture, either `monkeypatch` should be used to insert these into `globals()`, or you should return `mod`, and then just use `mod.Template` and `mod.AnsibleNativeEnvironment` in your tests.
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
It probably makes sense to test that the exception reason also matches expectations
You're checking two separate properties here. This should be in a separate test.
docstring with example input/output would be really helpful
```suggestion (?:www\.)?(?:safaribooksonline|learning\.oreilly)\.com/ ``` This would handle also new URLs starting with learning.oreilly.
No such meta field.
No such meta field.
No such meta field.
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
```suggestion 'url': 'https://learning.oreilly.com/learning-paths/learning-path-python/9781788996396', ``` I assume it would be good to include one link starting with "learning.oreilly.com" in tests.
not finding formats should be an error, and it should be raised directly from `_sort_formats`.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
My only question is if this skip logic is still correctly applied (i.e. none of the other classes that inherited `ExtractorTests` require it)? If you verify that, ship it.
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
You probably should just exit here with `changed=False`
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
having `main()` call `run_module()` feels a bit redundant, I see no reason to not put all of the main logic in `main` (and splitting actions into functions where it makes sense)
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
I would also detect if the value is a bool and set it accordingly to `on`/`off`.
ditto as `required_if`
I think for this case, this should work: ``` python required_if = [('state', 'present', ('value', 'value_type'))] ``` But haven't tested that...
If config_source is only used in combo with direct, whichs looks to be the case, it may make sense to collapse it into one arg (config_source). Then GConfTool2.__init__ could: ``` python if config_source: self.base_command += ["--direct", "--config-source", config_source] ``` That assumes there won't be a scenario where it makes sense to call 'gconftool-2 --direct' or 'gconftool-2 --config-souce some_config_source' independently.
Ok, cool. :thumbsup:
@azaghal I would leave it as you have it, with the explicit check for 'present'
no need to check for present as it is default state
You are not returning ansible_facts here. This will break backward compatibility.
Thanks for that note ewen. I learned something!
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
required=False is not needed. You can remove that. `state=dict(type='str', choices=['absent', 'present'], default='present'),` should be good enough.
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
Might want to use simple quote here.
FWIW using `monkeypatch` is preferable because it's pytest-native unless I need something like mocks or spies.
yeah I think it would be worthwhile to at least test a single JOIN scenario.
You can find it as the leader of the offset partition in ZK, though you will need to have a ZK connection to do that.
We should also test the nonexistent time with `is_dst=True` and `is_dst=False`
One empty line above, please.
As this may need to be updated, consider 1. make the value a class var `_USER_AGENT` 2. import utils.std_headers and let non-null `std_headers['User_Agent']` override this value, so that `--user-agent ... ` or `--add-headers "User-Agent: ..."` take precedence (allows cli work-around if the site needs a new UA).
+1 on making configurable as well (it may very well change from one use case to the other.. this is a good default set however)
Again: ```suggestion return ('params={0}'.format(encrypted_params), headers) ```
with -> width
width, height, and offset
comma after tuple
Might want to use simple quote here.
+1 on making configurable as well (it may very well change from one use case to the other.. this is a good default set however)
Due to hanging indent, `).first()` should be on the next line.
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
single line docstring is okay if it fits
Remove the blank lines.
```suggestion msg_format="Error scaling {0} - %s".format(service.name)) ```
Would it be the acme of foolishness to suggest that we just always "load-two"? Then, we wouldn't have to maintain the extra lists of tests at the top of the file, and we wouldn't need this condition here. And also, we'd eliminate a difference among the benchmarks, since the brokers would be loaded with the same dataset in all cases (even if one of them is unused).
Alternatively, we can get rid of those lists by just matching on whether the test name ends with "join" (we match on table name elsewhere in this PR)
Ah, I didn't realize load takes that long, but in retrospect, it should have been obvious. Then, of course we shouldn't always load both, and my suggestion is just to match on the test name.
Why shouldn't a try/except block not be used ? I'm intrigued.
I would change this whole block with something like: ```python units = list('b', 'kb', 'mb', 'gb', 'tb', 'pb', 'eb', 'zb', 'yb') try: multiplier = 1000**units.index(unit) except ValueError: units = list(None, 'kib', 'mib', 'gib', 'tib', 'pib', 'eib', 'zib', 'yib') try: multiplier = 1024**units.index(unit) except ValueError: e = get_exception() module.fail_json(msg="No valid size unit specified. %s" % e) ```
Both are fine IMO, there's nothing wrong with doing presence-checks either. I'll let you decide.
The convention in other connection plugins seems to be to raise an `AnsibleError` instead of `IOError`.
prefer hanging indent style (allows longer lines, indentation always multiple of 4): ``` rasterheader = ( 1, 0, ... ) ```
Please add `type='str'`
```suggestion msg_format="Error scaling {0} - %s".format(service.name)) ```
Would it be the acme of foolishness to suggest that we just always "load-two"? Then, we wouldn't have to maintain the extra lists of tests at the top of the file, and we wouldn't need this condition here. And also, we'd eliminate a difference among the benchmarks, since the brokers would be loaded with the same dataset in all cases (even if one of them is unused).
Alternatively, we can get rid of those lists by just matching on whether the test name ends with "join" (we match on table name elsewhere in this PR)
Ah, I didn't realize load takes that long, but in retrospect, it should have been obvious. Then, of course we shouldn't always load both, and my suggestion is just to match on the test name.
Why shouldn't a try/except block not be used ? I'm intrigued.
I would change this whole block with something like: ```python units = list('b', 'kb', 'mb', 'gb', 'tb', 'pb', 'eb', 'zb', 'yb') try: multiplier = 1000**units.index(unit) except ValueError: units = list(None, 'kib', 'mib', 'gib', 'tib', 'pib', 'eib', 'zib', 'yib') try: multiplier = 1024**units.index(unit) except ValueError: e = get_exception() module.fail_json(msg="No valid size unit specified. %s" % e) ```
Both are fine IMO, there's nothing wrong with doing presence-checks either. I'll let you decide.
The convention in other connection plugins seems to be to raise an `AnsibleError` instead of `IOError`.
prefer hanging indent style (allows longer lines, indentation always multiple of 4): ``` rasterheader = ( 1, 0, ... ) ```
Please add `type='str'`
`try/finally` should be a bit smaller. If there's an exception before `form.save()` the photo won't exist so there's no need to cleanup. I'll make this change and merge it.
I don't think this is working correctly, did you test it for all possible inputs ? - First of all the size_unit is only correct if it is a known unit. - And if that's not true, and it's not a digit, you have to escalate. - If it is a digit, you likely won't have a unit and then you could assume bytes.
That's up to you. There is no requirement to have a 1:1 mapping between the command and the Ansible interface. And I personally prefer to always have units (e.g. the AIX LV interface assumes no unit to mean MB, not MiB so...)
There is a much easier way to do this, by using the index in the list. Something like: ```python unit = size[-1] units = list('b', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y') try: multiplier = 1024**units.index(unit) except ValueError:: e = get_exception() module.fail_json(msg="No valid size unit specified. %s" % e) ```
You could make the unit mandatory (which is what I would do), or you could allow no unit (which means in bytes) and in that case you will have to consider that case as well. (i.e.if unit is a digit)
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
The mixup to me is that people using incorrect units will silently be using bytes, whereas it should report this as an error.
This means that if you'd be using 16m, it will be 16 bytes too. You will have to do a lot more to avoid mixups here.
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
`try/finally` should be a bit smaller. If there's an exception before `form.save()` the photo won't exist so there's no need to cleanup. I'll make this change and merge it.
That's up to you. There is no requirement to have a 1:1 mapping between the command and the Ansible interface. And I personally prefer to always have units (e.g. the AIX LV interface assumes no unit to mean MB, not MiB so...)
You could make the unit mandatory (which is what I would do), or you could allow no unit (which means in bytes) and in that case you will have to consider that case as well. (i.e.if unit is a digit)
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
I don't think this is working correctly, did you test it for all possible inputs ? - First of all the size_unit is only correct if it is a known unit. - And if that's not true, and it's not a digit, you have to escalate. - If it is a digit, you likely won't have a unit and then you could assume bytes.
The mixup to me is that people using incorrect units will silently be using bytes, whereas it should report this as an error.
This means that if you'd be using 16m, it will be 16 bytes too. You will have to do a lot more to avoid mixups here.
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
This doesn't seem right, size is an integer at this point.
`try/finally` should be a bit smaller. If there's an exception before `form.save()` the photo won't exist so there's no need to cleanup. I'll make this change and merge it.
You could make the unit mandatory (which is what I would do), or you could allow no unit (which means in bytes) and in that case you will have to consider that case as well. (i.e.if unit is a digit)
That's up to you. There is no requirement to have a 1:1 mapping between the command and the Ansible interface. And I personally prefer to always have units (e.g. the AIX LV interface assumes no unit to mean MB, not MiB so...)
There is a much easier way to do this, by using the index in the list. Something like: ```python unit = size[-1] units = list('b', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y') try: multiplier = 1024**units.index(unit) except ValueError:: e = get_exception() module.fail_json(msg="No valid size unit specified. %s" % e) ```
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
I don't think this is working correctly, did you test it for all possible inputs ? - First of all the size_unit is only correct if it is a known unit. - And if that's not true, and it's not a digit, you have to escalate. - If it is a digit, you likely won't have a unit and then you could assume bytes.
This means that if you'd be using 16m, it will be 16 bytes too. You will have to do a lot more to avoid mixups here.
The mixup to me is that people using incorrect units will silently be using bytes, whereas it should report this as an error.
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
`try/finally` should be a bit smaller. If there's an exception before `form.save()` the photo won't exist so there's no need to cleanup. I'll make this change and merge it.
You could make the unit mandatory (which is what I would do), or you could allow no unit (which means in bytes) and in that case you will have to consider that case as well. (i.e.if unit is a digit)
That's up to you. There is no requirement to have a 1:1 mapping between the command and the Ansible interface. And I personally prefer to always have units (e.g. the AIX LV interface assumes no unit to mean MB, not MiB so...)
There is a much easier way to do this, by using the index in the list. Something like: ```python unit = size[-1] units = list('b', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y') try: multiplier = 1024**units.index(unit) except ValueError:: e = get_exception() module.fail_json(msg="No valid size unit specified. %s" % e) ```
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
I don't think this is working correctly, did you test it for all possible inputs ? - First of all the size_unit is only correct if it is a known unit. - And if that's not true, and it's not a digit, you have to escalate. - If it is a digit, you likely won't have a unit and then you could assume bytes.
This means that if you'd be using 16m, it will be 16 bytes too. You will have to do a lot more to avoid mixups here.
The mixup to me is that people using incorrect units will silently be using bytes, whereas it should report this as an error.
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
It's better to actually say that there's no file in place or it's inaccessible.
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
This isn't needed, since we now only support 2.6+ anyway.
These parens aren't necessary for unpacking the return values.
no restructured text (:class:) in docstrings please
Is there a typo? I'm not sure what "hub" means in this sentence.
I guess some tests might be needed for the router stuff.
chop trailing ", "
Put the } on the next line and add a trailing comma on this line. That's our convention to ease later adding more items to a dictionary, tuple, etc., if needed.
It's not required here. It was used in f179113e6cbc8ba0a8d4e87e1d4410fb61d63e75 because we were handling a possible `IntegrityError`.
I'd use rename_forwards/backwards for consistency with other methods like database_forwards.
There's no need to wrap the strings like this. Our project lint settings accept up to 160 characters wide. ```suggestion result['warnings'].append('Some configuration commands were unmanaged, review unmanaged list') if result.get('invalid'): result['warnings'].append('Some configuration commands were invalid, review invalid list') ```
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int'` remove the `isdigit` check from `check_params`
For integers please: use `type='int'` remove the `isdigit` check from `check_params`
We should leave here `for_update_part = None` (in the same line as it was previously) and calculate it only in `not combinator` branch, because it's not used for combined queries.
It's not used only for combined queries so we should call it only in `not combinator` branch.
Clarified version of what I had in mind: ``` # Helper method to provide a way to access this without caching it. # For example, admin checks run before the app cache is ready and we # need to be able to lookup fields before we cache the final result. ```
It looks like only `IndexError` is exercised in tests.
You're right. Thanks for the clarification :+1:
Do we need this branch? :thinking: All built-in handlers use non-evaluated querysets returned by `related_objects()`, so tests work without it: ```python # update fields for (field, value), instances_list in self.field_updates.items(): updates = reduce(or_, instances_list) updates.update(**{field.name: value}) ```
~~I changed this to an assertion for the only file that is affected by the second round of post-processing i.e. `cached/relative.css`.~~
Please remove this blank line as requested by Paolo.
A simpler version might be setting the become sudo_pass magic var and letting the lines above (385) deal with it
There's no need to wrap the strings like this. Our project lint settings accept up to 160 characters wide. ```suggestion result['warnings'].append('Some configuration commands were unmanaged, review unmanaged list') if result.get('invalid'): result['warnings'].append('Some configuration commands were invalid, review invalid list') ```
I guess some tests might be needed for the router stuff.
chop trailing ", "
Put the } on the next line and add a trailing comma on this line. That's our convention to ease later adding more items to a dictionary, tuple, etc., if needed.
It's not required here. It was used in f179113e6cbc8ba0a8d4e87e1d4410fb61d63e75 because we were handling a possible `IntegrityError`.
I'd use rename_forwards/backwards for consistency with other methods like database_forwards.
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int'` remove the `isdigit` check from `check_params`
For integers please: use `type='int'` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int'` remove the `isdigit` check from `check_params`
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
Please ignore, my suggestion is invalid syntax.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Is this change intentional? Seems like the metavar should be something like "RELEASE_BRANCH".
As long as they both equals it shouldn't make a difference. By that I mean you should have a test asserts `Index('foo') == Index(F('foo'))` An alternative could be to defer that to `create_sql`.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Minor but I'd move this control flow block after the `weights` one to match the args order.
For the sake of consistency, can we convert `model_name` to lower in here. All occurrences are calling `.lower()` explicitly.
Yes, this should be taken care of before.
I'd put the trailing `}` in a new line.
could switch to single quotes for consistency
You should be able to use direct attribute access here: `remote_field.through`
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
As with `@raise_deprecation`, this should be a module-level utility, not a method on the class.
add trailing comma
include trailing ,
docstring with example input/output would be really helpful
Please ignore, my suggestion is invalid syntax.
@smithdc1 it does thanks!
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
This means that if you'd be using 16m, it will be 16 bytes too. You will have to do a lot more to avoid mixups here.
The mixup to me is that people using incorrect units will silently be using bytes, whereas it should report this as an error.
This doesn't seem right, size is an integer at this point.
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
```suggestion raise InvalidCacheBackendError('Could not find backend {backend!r}: {e}') from e ```
Maybe rename this to substitute_crypto_req since it's replacing the one from requirements.txt with the one we autodetect rather than simply appending.
```suggestion params = self.settings[alias].copy() ```
Nitpick, you can use a list comprehension here instead of calling list on a generator expression.
Kind of a nitpick since this is setup.py... but API-wise I prefer returning a small dict with the dynamic values and letting the caller merge it/copy the static dict if necessary.
I think we should just let this exception propagate. We aren't adding any value by catching it here (and might be removing value if the traceback were useful). This isn't code in ansible itself so we don't have to create a nicer error message either.
We should avoid lambdas and filters most of the time. Use list comprehensions and generator expressions instead. It's both faster and more pythonic to use those idioms.
Style note, I finally convinced people a year or so ago that ```{}``` is superior to ```dict()```. So new code should use the literal syntax.
Same here, seems a ValueError would be cleaner.
Can you name this a little more verbosely? I can't unsee "get best"
I'd go for `try/except IOError` in order to make a point that variable initialized above is in fact a fallback.
```suggestion params = self.settings[alias].copy() ```
```suggestion raise InvalidCacheBackendError('Could not find backend {backend!r}: {e}') from e ```
There's a few things that I'd change about this function. But I think the toplevel concern is that it's doing too much. It doesn't need to take req. It should just decide whether we're using the pycrypto or cryptography backend, format and return that one dependency. The calling code can then substitute the value.
Maybe rename this to substitute_crypto_req since it's replacing the one from requirements.txt with the one we autodetect rather than simply appending.
Nitpick, you can use a list comprehension here instead of calling list on a generator expression.
We should avoid lambdas and filters most of the time. Use list comprehensions and generator expressions instead. It's both faster and more pythonic to use those idioms.
I think we should just let this exception propagate. We aren't adding any value by catching it here (and might be removing value if the traceback were useful). This isn't code in ansible itself so we don't have to create a nicer error message either.
Kind of a nitpick since this is setup.py... but API-wise I prefer returning a small dict with the dynamic values and letting the caller merge it/copy the static dict if necessary.
Style note, I finally convinced people a year or so ago that ```{}``` is superior to ```dict()```. So new code should use the literal syntax.
The docstring should explain why such proxy is needed.
```suggestion raise InvalidCacheBackendError('Could not find backend {backend!r}: {e}') from e ```
I'd go for `try/except IOError` in order to make a point that variable initialized above is in fact a fallback.
Maybe rename this to substitute_crypto_req since it's replacing the one from requirements.txt with the one we autodetect rather than simply appending.
Nitpick, you can use a list comprehension here instead of calling list on a generator expression.
We should avoid lambdas and filters most of the time. Use list comprehensions and generator expressions instead. It's both faster and more pythonic to use those idioms.
I think we should just let this exception propagate. We aren't adding any value by catching it here (and might be removing value if the traceback were useful). This isn't code in ansible itself so we don't have to create a nicer error message either.
Style note, I finally convinced people a year or so ago that ```{}``` is superior to ```dict()```. So new code should use the literal syntax.
There's a few things that I'd change about this function. But I think the toplevel concern is that it's doing too much. It doesn't need to take req. It should just decide whether we're using the pycrypto or cryptography backend, format and return that one dependency. The calling code can then substitute the value.
Kind of a nitpick since this is setup.py... but API-wise I prefer returning a small dict with the dynamic values and letting the caller merge it/copy the static dict if necessary.
This looks a bit odd to me. I wonder if something like this could be a bit more readable? Up to you. ``` return ( (sql_alter % {"column": self.quote_name(new_field.column), "type": new_type}, []), [], ) ```
Test failure is because this is missing `geography=True` on PostGIS.
You don't like f-strings at all, do you? :-)
Can we check that the constraint actually exists? ```python with connection.cursor() as cursor: constraints = connection.introspection.get_constraints( cursor, Neighborhood._meta.db_table, ) self.assertIn(constraint_name, constraints) ```
This can instead be `continue` and let the `else` unnest.
I would use kwargs ```suggestion check=models.Q(geom__within=poly), ```
Please rewrite as ``` if __name__ == '__main__': main() ```
Generally, I'd have put the static regex directly as the first argument of `re.search()`.
Use the Python 2 and low 3 Time Machine: `'url too short: %s' % (video_pre_parts, )` or: `'url too short: %(video_pre_parts)s' % {'video_pre_parts': video_pre_parts, }` or: `'url too short: {video_pre_parts}'.format(video_pre_parts=video_pre_parts)` or: `'url too short: {0}'.format(video_pre_parts)` No doubt there are other ways (eg `....format(**locals())`
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Passing `lead` and `trail` to the `trim_punctuation()` looks unnecessary. This method is called only here so we always pass `('', word, '')`. Maybe: ```suggestion lead, middle, trail = self.trim_punctuation(word) ``` and ```python def trim_punctuation(self, word): """ Trim trailing and wrapping punctuation from `word`. Return the items of the new state. """ lead, middle, trail = '', word, '' ... return lead, middle, trail ```
```suggestion return ''.join([ self.handle_word(word) for word in words ]) ``` I changed `handle_word` to return `word` in remaining cases.
Please, at most one alias, even better none. Having a long list of aliases is really bad UX IMO.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
required=False is the default, so you can omit that, same for any of the following. `name_regex=dict()`.
This should have exception handling around it. Once you update to AnsibleAWSModule you can do: ``` except (BotoCoreError, ClientError) as e: module.fail_json_aws(e, msg="Unable to list clusters") ``` and it will use e.response and add the traceback for you.
I double people will update this script correctly. We can only hope, that release managers verify this before sending the email... As an alternative, we can also wildcard this, and let release manger insert those manually. Similar to `<DETAILS OF THE CHANGES>` above.
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
This can be updated to use AnsibleAWSModule from ansible.module_utils.aws.core. Then you will be able to remove the check for HAS_BOTO3, the exception handling around the client, and the use of get_aws_connection_info and boto3_conn (as well as any of the imports for those things) and do `client = module.client('emr')` instead.
`of the it's last` -> `of its last`
nit add `a {@link Named} config`
Same could be said of newCost and newModel
Given that `model` also exists, it is weird that `field` refers to `original_model`. (Also where is `original_model` defined -- if nowhere then certainly tests seems to be missing). Maybe also rename to source & target model
nit: remove empty link
`KeyValueStore` -> `TimestampedKeyValueStore`
as above (more often below -- please fit all)
Because there is not `CogroupedKStream#reduce()` method this sentence is not useful and should be removed.
This overload does not take `Materialized` parameter
Note to self: if this is not discussed elsewhere in the docs, we should add it.
Both cases test the same thing, don't repeat yourself. Test things based on their properties. Here you'd probably want to test iterables, not just sets.
No need to parametrize with just one case.
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
It probably makes sense to test that the exception reason also matches expectations
```suggestion vault_data(), ```
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
(Similarly, mark any string that you are going to call decode on with as a b"string".)
You should probably expect unicode strings
IMO it's enough to test that `CreateExtension` honor `allow_migrate()`, creating extension is already tested in `postgres_tests`.
Maybe add an empty line above this one and give it a name, it seems like the `assert` task is just a parameter of the `openssl_certificate_info` one.
s/HashMap<String, Object> expected/Map<String, Object> expected
a list of dictionaries
required=False is the default, so you can omit that, same for any of the following. `name_regex=dict()`.
This should have exception handling around it. Once you update to AnsibleAWSModule you can do: ``` except (BotoCoreError, ClientError) as e: module.fail_json_aws(e, msg="Unable to list clusters") ``` and it will use e.response and add the traceback for you.
Looks like a split line.
This can be updated to use AnsibleAWSModule from ansible.module_utils.aws.core. Then you will be able to remove the check for HAS_BOTO3, the exception handling around the client, and the use of get_aws_connection_info and boto3_conn (as well as any of the imports for those things) and do `client = module.client('emr')` instead.
This line seems to have been split into two.
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
This is undoing the example.
It wouldn't validate the following: - http://.com - http://. - http://.. - http://../ - http://.www.foo.bar/ - http://.www.foo.bar./ It would indeed validate the following URL (but they are actually valid): - http://example - http://example. All the others are about leading and trailing hyphens, if we really want to filter them out despite the increased complexity then I suggest we break the pattern into multiple variable for readability: https://gist.github.com/386830e46e8d2aca9dcb Regarding formal grammar, it's spread out among a bunch of RFCs, I doubt it's worth the effort.
`localhost` or rather `localhost.` is a FQDN, that shouldn't require a special case.
Typo in `aggressive`
Typo in `aggressive`
Looks like a split line.
This line seems to have been split into two.
Please modify the generic function description statement.
s/HashMap<String, Object> expected/Map<String, Object> expected
Please update the generic function description
This can instead be `continue` and let the `else` unnest.
`del` is a builtin, not a function. These parens don't have to be here
ACk. Just FYI OS discovery can be done after CLICONF is implemented.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Passing `lead` and `trail` to the `trim_punctuation()` looks unnecessary. This method is called only here so we always pass `('', word, '')`. Maybe: ```suggestion lead, middle, trail = self.trim_punctuation(word) ``` and ```python def trim_punctuation(self, word): """ Trim trailing and wrapping punctuation from `word`. Return the items of the new state. """ lead, middle, trail = '', word, '' ... return lead, middle, trail ```
Ids must stay intact.
```suggestion return ''.join([ self.handle_word(word) for word in words ]) ``` I changed `handle_word` to return `word` in remaining cases.
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
This can instead be `continue` and let the `else` unnest.
`del` is a builtin, not a function. These parens don't have to be here
ACk. Just FYI OS discovery can be done after CLICONF is implemented.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Passing `lead` and `trail` to the `trim_punctuation()` looks unnecessary. This method is called only here so we always pass `('', word, '')`. Maybe: ```suggestion lead, middle, trail = self.trim_punctuation(word) ``` and ```python def trim_punctuation(self, word): """ Trim trailing and wrapping punctuation from `word`. Return the items of the new state. """ lead, middle, trail = '', word, '' ... return lead, middle, trail ```
That's completely different videos.
```suggestion return ''.join([ self.handle_word(word) for word in words ]) ``` I changed `handle_word` to return `word` in remaining cases.
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Maybe add an empty line above this one and give it a name, it seems like the `assert` task is just a parameter of the `openssl_certificate_info` one.
s/HashMap<String, Object> expected/Map<String, Object> expected
a list of dictionaries
required=False is the default, so you can omit that, same for any of the following. `name_regex=dict()`.
This should have exception handling around it. Once you update to AnsibleAWSModule you can do: ``` except (BotoCoreError, ClientError) as e: module.fail_json_aws(e, msg="Unable to list clusters") ``` and it will use e.response and add the traceback for you.
Looks like a split line.
This can be updated to use AnsibleAWSModule from ansible.module_utils.aws.core. Then you will be able to remove the check for HAS_BOTO3, the exception handling around the client, and the use of get_aws_connection_info and boto3_conn (as well as any of the imports for those things) and do `client = module.client('emr')` instead.
This line seems to have been split into two.
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
Typo in `aggressive`
Typo in `aggressive`
required=False is the default, so you can omit that, same for any of the following. `name_regex=dict()`.
s/HashMap<String, Object> expected/Map<String, Object> expected
This should have exception handling around it. Once you update to AnsibleAWSModule you can do: ``` except (BotoCoreError, ClientError) as e: module.fail_json_aws(e, msg="Unable to list clusters") ``` and it will use e.response and add the traceback for you.
Looks like a split line.
This line seems to have been split into two.
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
Please modify the generic function description statement.
This is undoing the example.
Typo in `aggressive`
Typo in `aggressive`
Looks like a split line.
This line seems to have been split into two.
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
Please modify the generic function description statement.
s/HashMap<String, Object> expected/Map<String, Object> expected
Please update the generic function description
This is undoing the example.
Typo in `aggressive`
Typo in `aggressive`
Looks like a split line.
This line seems to have been split into two.
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
Please modify these generic description statements.
Please modify the generic function description statement.
s/HashMap<String, Object> expected/Map<String, Object> expected
Wouldn't this approach fix the slicable fields to just these types? A mixin along with `slicable = True/False` on `F` may be an approach. Besides, I believe the policy is to keep `django.contrib.postgres` as separate as possible.
I think `get_internal_type` is better to use.
I think a mixin is a bit complicated. The attribute sounds simpler - it could be a function instead of just a boolean, then it could return the transform if applicable, else `None` for non-sliceable fields (default implementation).
Both of these are currently failing for me in a local virtualbox cluster. The first looks like a legit failure since the data doesn't seem to match, the second looks like maybe it's failing to create a topic perhaps because the previous test didn't tear down properly? The log for the second one indicates the topic already exists. ``` ====================================================================================================================================================================================================================================================================================================================================================================================================================================== SESSION REPORT (ALL TESTS) session_id: 2015-08-19--004 run time: 1 minute 40.540 seconds tests run: 2 passed: 0 failed: 2 ====================================================================================================================================================================================================================================================================================================================================================================================================================================== test_id: 2015-08-19--004.kafkatest.sanity_checks.test_mirror_maker.TestMirrorMakerService.test_end_to_end status: FAIL run time: 1 minute 2.808 seconds Traceback (most recent call last): File "/Users/ewencp/confluent/ducktape.git/ducktape/tests/runner.py", line 81, in run_all_tests result.data = self.run_single_test() File "/Users/ewencp/confluent/ducktape.git/ducktape/tests/runner.py", line 130, in run_single_test return self.current_test_context.function(self.current_test) File "/Users/ewencp/kafka.git/tests/kafkatest/sanity_checks/test_mirror_maker.py", line 94, in test_end_to_end assert len(self.consumer.messages_consumed[1]) == self.num_messages AssertionError -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- test_id: 2015-08-19--004.kafkatest.sanity_checks.test_mirror_maker.TestMirrorMakerService.test_lifecycle status: FAIL run time: 37.728 seconds Traceback (most recent call last): File "/Users/ewencp/confluent/ducktape.git/ducktape/tests/runner.py", line 78, in run_all_tests self.setup_single_test() File "/Users/ewencp/confluent/ducktape.git/ducktape/tests/runner.py", line 122, in setup_single_test self.current_test.setUp() File "/Users/ewencp/kafka.git/tests/kafkatest/sanity_checks/test_mirror_maker.py", line 58, in setUp self.k2.start() File "/Users/ewencp/kafka.git/tests/kafkatest/services/kafka.py", line 55, in start self.create_topic(topic_cfg) File "/Users/ewencp/kafka.git/tests/kafkatest/services/kafka.py", line 115, in create_topic node.account.ssh(cmd) File "/Users/ewencp/confluent/ducktape.git/ducktape/cluster/remoteaccount.py", line 79, in ssh return self._ssh_quiet(self.ssh_command(cmd), allow_fail) File "/Users/ewencp/confluent/ducktape.git/ducktape/cluster/remoteaccount.py", line 206, in _ssh_quiet raise e CalledProcessError: Command 'ssh vagrant@worker4 -o 'HostName 127.0.0.1' -o 'Port 2202' -o 'UserKnownHostsFile /dev/null' -o 'StrictHostKeyChecking no' -o 'PasswordAuthentication no' -o 'IdentityFile /Users/ewencp/kafka.git/.vagrant/machines/worker4/virtualbox/private_key' -o 'IdentitiesOnly yes' -o 'LogLevel FATAL' '/opt/kafka/bin/kafka-topics.sh --zookeeper worker2:2181 --create --topic topic --partitions 1 --replication-factor 1'' returned non-zero exit status 1 -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ```
This ended up printing out ``` AssertionError: num_produced: 1000, num_consumed: 869 ``` So I went and checked the console consumer log. Sure enough, I found this was the first message reported: ``` [DEBUG - 2015-08-20 16:44:53,298 - console_consumer - _worker - lineno:177]: consumed a message: 131 ``` So for some reason consumption didn't start with the first message. I ran the test and collected the `VerifiableProducer` log and it claims all were acked, they all went to partition 0, etc. I also logged the number of messages the `VerifiableProducer` claims were acked and that is 1000 as well. It seems something is still funky with this. How many times have you tried running to reproduce this? It sometimes passes for me, but maybe 50-60% of the time is failing. (By the way, it's kinda weird that the console consumer's per-message log goes to the test log, but verifiable producer is sent to a separate log file...)
single line looks more readable here
Looks like dead code here
Yeah, we still can't use dict comprehensions until 2.6 is formally dropped, sorry.
```suggestion Test that the returned value for timezone consists of only uppercase ```
That makes sense, let's keep it in that sense. EDIT: Actually, I'm wondering that if the `monitor` would always grep the same log4j entry in the outside verification or it always try to grep the new lines after the inner verification? If it's the first case, then the outside verification would always be redundant as we are doomed to just grep the same lines.
All methods only used once should be explicitly inlined.
Read: coding conventions, optional fields.
Do not use underscore as prefix.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
Read: coding conventions, mandatory fields.
I've provided **clear working** piece of code that you must just copy paste. Instead you introduced mess with base URL.
1. `_sort_formats`. 2. Must not break if any of these keys is missing.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
I think we have a problem here. The version that is supposed to be supplied as a parameter should only consist of major.minor version like `2.0` (so that all 2.x version go into the same repository) - this one is `2.0.0-beta1` though.
there is a helper in `AnsibleModule` for mutually exclusive params: ~~~diff module = AnsibleModule( argument_spec=argument_spec, + mutually_exclusive=(('positional_args', 'named_args'),), supports_check_mode=True, ) ~~~
All methods only used once should be explicitly inlined.
Do not use underscore as prefix.
Read: coding conventions, optional fields.
URLs must not be `None`.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
Read: coding conventions, mandatory fields.
I've provided **clear working** piece of code that you must just copy paste. Instead you introduced mess with base URL.
This intermediate dict is completely pointless. Build formats directly.
1. `_sort_formats`. 2. Must not break if any of these keys is missing.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Please change to: `options: {}`
Maybe C(.pem)? I'm not sure you need to specify that .pem is a file type since it's just the extension. @gundalow might know.
Use `C` formatting function for the sample list.
Use `U()` for `acme`.
Use `C` formatting function for URL.
typo: ot -> to
Again, suggest rewording this as suggested for win_http_proxy.
YYYY-MM-DD isn't a module option, so I think you can remove the I and parentheses
Suggest adding a combined example that also uses win_credential.
s/fo/of/ + ('security' or 'distribution')
Do not use underscore as prefix.
All methods only used once should be explicitly inlined.
`fatal=False` will print error wen failed. Instead it should be optional and not print any error.
Read: coding conventions, optional fields.
What the hell are you doing? `urljoin(url, '/api/v1/videos/%s' % video_id)`. All.
1. `_sort_formats`. 2. Must not break if any of these keys is missing.
Read: coding conventions, mandatory fields.
Similarly, ```if tc['skip'].get('i')```
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
typo: ot -> to
Again, suggest rewording this as suggested for win_http_proxy.
Please change to: `options: {}`
Use `U()` for `acme`.
this is a catchall, we normally frown on those. also note that for all of your `% e` they should be `% to_native(e)` to ensure proper character conversions
This doesn't look like it belongs in here
Use `C` formatting function for the sample list.
Suggest adding a combined example that also uses win_credential.
From what I can see here, these EXAMPLES are not valid YAML, and would not be operational if someone were to copy paste them. They should be laid out in pure/full YAML such as: ``` - name: Test Vlan - Create a vlan, name it cnos_vlan: host: "{{ inventory_hostname }}" username: "{{ hostvars[inventory_hostname]['username'] }}" ```
s/fo/of/ + ('security' or 'distribution')
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
Ditto. ```suggestion # Check empty result with non-execution optimization. with self.assertNumQueries(0): ```
I missed where you're doing the .decode.... the new version of the line should look like this: ``` python dumped += to_text(yaml.dump(abridged_result, allow_unicode=True, width=1000, Dumper=AnsibleDumper, default_flow_style=False)) ```
I don't see any need for this attribute.
Can we display a similar message with the test results? It can be helpful when you have plenty of errors, .e.g. ``` ... FAILED (errors=206, skipped=1094, expected failures=4) Used shuffle seed: 556738431 (generated) ```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Shouldn't this be `return isinstance(x, collections.Iterator)` instead? Actually we should replace all reference to this function by a `collection.Iterator` instance check.
I think there's an extra 'if so' here.
chop trailing newline (check code with flake8 to see a couple other minor formatting things)
These don't appear to be "public" attributes, so we should stay consistent with indicating their privateness by prepending their names with an `_` character.
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
Such an extensive docstring is not necessary, IMO.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Simplify error reporting as below, or skip this check and just let the `_html_search_regex()` calls raise the exception? ```suggestion for token in ('apiToken', 'widgetId'): if token not in rf_token_js: raise ExtractorError( 'Unable to fetch ' + token, expected=True) ```
The (non-empty) matches succeeded (or an exception would have been raised) so the values are both truthy: ```suggestion headers['rfApiProfileId'] = api_token headers['rfWidgetId'] = widget_id ```
```suggestion r'''widgetId:\s+["'](\w+)''', rf_token_js, 'widgetId') ```
```suggestion r'''apiToken:\s+["'](\w+)''', rf_token_js, 'apiToken') ```
use basic.py's `missing_required_lib` function as it gives details on host and python to avoid user confusion
```suggestion fallback=(env_fallback, ['ANSIBLE_HWC_USER']), ```
```suggestion fallback=(env_fallback, ['ANSIBLE_HWC_PASSWORD']), ```
```suggestion fallback=(env_fallback, ['ANSIBLE_HWC_DOMAIN']), ```
Please don't use such generic env vars. ```suggestion fallback=(env_fallback, ['ANSIBLE_HWC_IDENTITY_ENDPOINT']), ```
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
I'd do: ``` kwargs['separators'] = kwargs.get('separators', (',', ':')) ``` On Wed, Aug 21, 2013 at 8:06 PM, Tim Graham notifications@github.comwrote: > In django/contrib/messages/storage/session.py: > > > ``` > > else: > > self.request.session.pop(self.session_key, None) > > return [] > > ``` > > > > + > > - def serialize_messages(self, messages): > > - encoder = MessageEncoder(separators=(',', ':')) > > look ok? https://gist.github.com/timgraham/dc1cc1abe202d3830eab > > â€” > Reply to this email directly or view it on GitHubhttps://github.com/django/django/pull/1488/files#r5903355 > .
I'd move the `separators=(',', ':')` into `__init__` of `MessageEncoder`, so we always get "efficient" (having '__json_message' as key doesn't look to efficient ;)) json without having to specify it everywhere. But we can do this in a 1.6 cleanup commit after committing this.
I think so, btw please do `resolver.kwargs.copy()` to leave the original kwargs in place on the resolver object.
You should fetch the arguments and url name from `request.resolver_match` here to ensure that we redirect to the same view, if someone hooks up `password_reset_confirm` with a different name you'd get an error here.
do we really need that `is not None` check? `check_token` should return `False` for None tokens.
yeah, `request.session.get` would return none for the token and this wouldn't pass the comparision (which would be perfectly fine)
immediatelly -> immediately
@romgar If you find the time that would be great!
It might be smarter to validate the token first and only modify the session + redirect if it's valid. Otherwise it makes it really easy to create a session just by GET'ing a url (possible DoS vector). It also means you can't pass `accounts/password_reset` as the token and take advantage of our `request.path.replace()` code. It probably means validating the token twice, which is slightly slower. Seems fine to me if an invalid token gets leaked.
@collinanderson That is a good point, at first I wanted to be safe against leaking anything (just to be on the safe side), but the DOS argument is more important.
Simplify error reporting as below, or skip this check and just let the `_html_search_regex()` calls raise the exception? ```suggestion for token in ('apiToken', 'widgetId'): if token not in rf_token_js: raise ExtractorError( 'Unable to fetch ' + token, expected=True) ```
The (non-empty) matches succeeded (or an exception would have been raised) so the values are both truthy: ```suggestion headers['rfApiProfileId'] = api_token headers['rfWidgetId'] = widget_id ```
```suggestion r'''widgetId:\s+["'](\w+)''', rf_token_js, 'widgetId') ```
```suggestion r'''apiToken:\s+["'](\w+)''', rf_token_js, 'apiToken') ```
use basic.py's `missing_required_lib` function as it gives details on host and python to avoid user confusion
```suggestion fallback=(env_fallback, ['ANSIBLE_HWC_USER']), ```
```suggestion fallback=(env_fallback, ['ANSIBLE_HWC_PASSWORD']), ```
```suggestion fallback=(env_fallback, ['ANSIBLE_HWC_DOMAIN']), ```
Please don't use such generic env vars. ```suggestion fallback=(env_fallback, ['ANSIBLE_HWC_IDENTITY_ENDPOINT']), ```
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
`result` is empty and no capability is returned. Is this expected? You might want to call `get_device_info` here to get the device capabilities.
consider assertRaisesMessage to make the test a bit more specific.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
Use another lookup instead of `epoch` e.g. `second`.
True, I mixed up sth when switching branches. Nevertheless they work on all databases except PostgreSQL, so we need to fix this.
Cases from lines 361 and 363 work with the previous implementation.
Please use a single quote.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Is there a reason to do deepcopies here (and at the top of a few other methods)? deepcopies are slow and it doesn't look like the data is being modified (for which you might want to have a pristine copy and a modified copy) so you can probably discard the deepcopy to improve your speed.
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
`result` is empty and no capability is returned. Is this expected? You might want to call `get_device_info` here to get the device capabilities.
consider assertRaisesMessage to make the test a bit more specific.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
Use another lookup instead of `epoch` e.g. `second`.
True, I mixed up sth when switching branches. Nevertheless they work on all databases except PostgreSQL, so we need to fix this.
Cases from lines 361 and 363 work with the previous implementation.
Please use a single quote.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Is there a reason to do deepcopies here (and at the top of a few other methods)? deepcopies are slow and it doesn't look like the data is being modified (for which you might want to have a pristine copy and a modified copy) so you can probably discard the deepcopy to improve your speed.
Same here. `self.api_client` instead of `client`
* Please use `assertRaisesMessage` here. * `flake8` will require whitespace after the `:` in the lambda.
```suggestion self.run_on_commit.append((set(self.savepoint_ids), func, True)) ```
When we register the function to run robustly on commit in an atomic bloc, should we also try/catch it when it is actually run? Meaning that in `db.backends.base.base.BaseDatabaseWrapper.run_and_clear_commit_hooks` we should take into account this info
When calling `on_commit` there are basically two modes: - there is no transaction in progress, so we execute the function right away - we are in an atomic block, so we register the function to execute it later (`self.run_on_commit.append(`) The first case is handled by the PR, but not the second one. And I'd think that we would need to handle a robust execution in the second case too. Does that make it clearer? :)
These last four lines are duplicated in both conditions, should therefore come after the if block.
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
I'm not sure it's useful to implement _set_container in the base class, as the path seems different depending on stream.
I'm not sure I see a good use case for it. In any case, this check could be relaxed (or moved to the outer `get_response`, outside the middleware chain) later as a separate change (maybe after the old middleware system is gone, which would reduce the complexity of the change).
Same as above; let's leave it alone for now.
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
`result` is empty and no capability is returned. Is this expected? You might want to call `get_device_info` here to get the device capabilities.
consider assertRaisesMessage to make the test a bit more specific.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
Use another lookup instead of `epoch` e.g. `second`.
True, I mixed up sth when switching branches. Nevertheless they work on all databases except PostgreSQL, so we need to fix this.
Cases from lines 361 and 363 work with the previous implementation.
Please use a single quote.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Is there a reason to do deepcopies here (and at the top of a few other methods)? deepcopies are slow and it doesn't look like the data is being modified (for which you might want to have a pristine copy and a modified copy) so you can probably discard the deepcopy to improve your speed.
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
`result` is empty and no capability is returned. Is this expected? You might want to call `get_device_info` here to get the device capabilities.
consider assertRaisesMessage to make the test a bit more specific.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
Use another lookup instead of `epoch` e.g. `second`.
True, I mixed up sth when switching branches. Nevertheless they work on all databases except PostgreSQL, so we need to fix this.
Cases from lines 361 and 363 work with the previous implementation.
Please use a single quote.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Is there a reason to do deepcopies here (and at the top of a few other methods)? deepcopies are slow and it doesn't look like the data is being modified (for which you might want to have a pristine copy and a modified copy) so you can probably discard the deepcopy to improve your speed.
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
`result` is empty and no capability is returned. Is this expected? You might want to call `get_device_info` here to get the device capabilities.
consider assertRaisesMessage to make the test a bit more specific.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
Use another lookup instead of `epoch` e.g. `second`.
True, I mixed up sth when switching branches. Nevertheless they work on all databases except PostgreSQL, so we need to fix this.
Cases from lines 361 and 363 work with the previous implementation.
Please use a single quote.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Is there a reason to do deepcopies here (and at the top of a few other methods)? deepcopies are slow and it doesn't look like the data is being modified (for which you might want to have a pristine copy and a modified copy) so you can probably discard the deepcopy to improve your speed.
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
`result` is empty and no capability is returned. Is this expected? You might want to call `get_device_info` here to get the device capabilities.
consider assertRaisesMessage to make the test a bit more specific.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
Use another lookup instead of `epoch` e.g. `second`.
True, I mixed up sth when switching branches. Nevertheless they work on all databases except PostgreSQL, so we need to fix this.
Cases from lines 361 and 363 work with the previous implementation.
Please use a single quote.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Is there a reason to do deepcopies here (and at the top of a few other methods)? deepcopies are slow and it doesn't look like the data is being modified (for which you might want to have a pristine copy and a modified copy) so you can probably discard the deepcopy to improve your speed.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
Must not return `None`.
Here you pass the variable `label`. You have never defined it, though (you defined `ch_label` -- maybe you meant that one?). That's the first of the sanity checks which fails. You might already get that output by running `flake8` on your module.
No such meta field.
No such meta field.
No such meta field.
No such meta field.
```suggestion current_channels = get_current_channels(module, command) if state == 'present': ``` Since it is common to both `absent` and `present`
Basically @gundalow's point is that you don't need to repeat the work that Ansible is already doing enforcing required parameters
m3u8 is also available.
I don't see any need for this attribute.
Can we display a similar message with the test results? It can be helpful when you have plenty of errors, .e.g. ``` ... FAILED (errors=206, skipped=1094, expected failures=4) Used shuffle seed: 556738431 (generated) ```
You have some unmerged lines here
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
I realize this predates your PR, but why is 'unused' not used? It appears to be the target field provided by the SQLUpdateCompiler during as_sql(). This seems like a really good spot to allow a backend to hook in and CAST the expression to a type that is allowed by the target column.
I think [`_extract_m3u8_formats`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L847) does the work better. If the existing method does not fit the need, feel free to modify it.
``` for i, video_url in enumerate(video_urls): ```
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
(And round-tripping of the messages is already tested in other tests)
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
Good catch about the filter name! (And I like the extra spacing around `|` :) ) How about keeping the parantheses? I think it makes it easier to understand for the less jinja2-fluent what happens here. ```suggestion multi_group: (group_names | intersect(['alpha', 'beta', 'omega'])) | length >= 2 ```
not a show stopper, but the code might be clearer if we just add the '-n' and '%s'/dir_arg in the `if/else` and just execute `run_command` at the end
you can use `state` to avoid the 'or' to the user
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
Might want to use simple quote here.
I think this should also check for a condition on the constraint, since UniqueConstraints without conditions are always supported.
convention for the name of the first argument of a classmethod is "cls". If the parameter isn't used at all, make it a @staticmethod instead and remove the parameter altogether.
Ah, I see it now. yeah, so just change self to be cls to match convention when using `@classmethod` and it should be fine.
I find this code a bit confusing -- where is the `FieldError` expected to be raised? `expression.input_field.output_field`? Could you use an if statement for that check rather than try/except or maybe move the code that's not expected to raise into an else block of this try/except? .Not an expert here, so maybe it's fine as is.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
Would be fine I guess. I'm not too sure about the safety margin aspect in the first place, but it's SQLite so whatever works, I guess.
you can use `state` to avoid the 'or' to the user
You can remove the `?` - 2.4 is likely, and if not that can be updated :)
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
You may want to specify 'type: str/list/bool/dict' for some of these.
not a show stopper, but the code might be clearer if we just add the '-n' and '%s'/dir_arg in the `if/else` and just execute `run_command` at the end
you can use `state` to avoid the 'or' to the user
both are valid tests, i don't see why you need to eliminate the existing one
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
yeah I think it would be worthwhile to at least test a single JOIN scenario.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
Similarly, ```if tc['skip'].get('i')```
Might want to use simple quote here.
```suggestion item, fields=fields, using=self.db, ```
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
as a tuple
prefer hanging indent style (allows longer lines, indentation always multiple of 4): ``` rasterheader = ( 1, 0, ... ) ```
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
This method isn't necessary.
It doesn't look like you need an OrderedDict here. A regular dict will do (and be faster and take up less memory).
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
You need to make sure `tf` doesn't write a unicode string. tf.write(u("#!/bin/sh/\necho %s" % quote(self.passphrase)).encode("utf-8")) Ansible may have a utility function to do this.
In the "Generate privatekey6 - standard - with non-ASCII passphrase" test, this fails with `'ascii' codec can't decode byte 0xc3 in position 16: ordinal not in range(128)`. Stacktrace: ``` File "/tmp/ansible_openssh_keypair_payload_kCnMbD/__main__.py", line 193, in generate tf.write("#!/bin/sh\necho %s" % quote(self.passphrase)) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 706, in write return self.writer.write(data) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 369, in write data, consumed = self.encode(object, self.errors) ``` It seems to fail in all Python 2 tests; in Python 3 it seems to work.
```suggestion module.fail_json(msg="value must be of type string, integer or dict") ```
docstring with example input/output would be really helpful
you can use `state` to avoid the 'or' to the user
Please ignore, my suggestion is invalid syntax.
yeah I think it would be worthwhile to at least test a single JOIN scenario.
Might want to use simple quote here.
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
After looking at this some more, I don't see a good reason to open the file in binary mode only to convert each line to native string type for further manipulation. ```suggestion with open(self.USER_ATTR, 'r') as file_handler: ```
If the file isn't open in binary mode, there is no reason to decode here. ```suggestion line = line.strip() ```
Shouldn't this be `return isinstance(x, collections.Iterator)` instead? Actually we should replace all reference to this function by a `collection.Iterator` instance check.
Rather than use a `lamdba` here, it would be better to use a generator expression for clarity ```suggestion lines = line.split('::::')[1].split(';') tmp = dict(x.split('=') for x in lines) ```
This method isn't necessary.
Might want to check `rc` status before return output.
Sorry, my bad. I didn't see the implementation of `_run`.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
prefer hanging indent style (allows longer lines, indentation always multiple of 4): ``` rasterheader = ( 1, 0, ... ) ```
This can instead be `continue` and let the `else` unnest.
This method isn't necessary.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Eliminate all methods that is only used once.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
No point in base class.
You will add it when there will be a playlist support. For now it's completely useless.
I've already pointed out: use `display_id` until you get real id.
No `id` extracted.
Rather than use a `lamdba` here, it would be better to use a generator expression for clarity ```suggestion lines = line.split('::::')[1].split(';') tmp = dict(x.split('=') for x in lines) ```
Maybe `argon2_hash` -> `rest`
This method isn't necessary.
Sorry, my bad. I didn't see the implementation of `_run`.
Might want to check `rc` status before return output.
Might want to use simple quote here.
`enumerate` on for range.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
`if it encounter` => `if it encounters`
`mentionned` => `mentioned`
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
Missing `=dict` on this and the next few lines
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Please rewrite as ``` if __name__ == '__main__': main() ```
missing space between `,` and `and`
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
`if it encounter` => `if it encounters`
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
`mentionned` => `mentioned`
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
Maybe use `self.TEST_SIZE` to avoid storing the large list of authors.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Maybe the following will be more explicit: ```python first_author_books = result[0].books.all() self.assertEqual([self.book1], [first_author_books]) self.assertEqual([self.reader], [first_author_books[0].read_by.all()])) ```
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
Would be fine I guess. I'm not too sure about the safety margin aspect in the first place, but it's SQLite so whatever works, I guess.
flake8 complains about missing spaces around `*`
`if it encounter` => `if it encounters`
`mentionned` => `mentioned`
It's not actually a comprehension - this could just use a tuple literal.
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
For integers please: use `type='int` remove the `isdigit` check from `check_params`
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
missing space between `,` and `and`
Please rewrite as ``` if __name__ == '__main__': main() ```
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
chop trailing newline (check code with flake8 to see a couple other minor formatting things)
I can see a decent reason to use the `DjangoJSONEncoder` as the default as it gives us `Decimal` and `datetime` support for free. These are very common use cases for django sites (especially datetime!).
Here I think we just should just default to `json.dumps` if no encoder is specified. No need for an extra setting.
2.6 or 2.7? Also you `requirements` listed here and the modules.
set the safe
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
Can you use `assertContains()` here to prevent errors due to attribute order in the HTML tags.
IIRC get_cols() was renamed to get_group_by_cols() some time ago - no need to have both.
okay, but would be helpful to say _why_ we need to always return True.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
chop trailing newline (check code with flake8 to see a couple other minor formatting things)
I can see a decent reason to use the `DjangoJSONEncoder` as the default as it gives us `Decimal` and `datetime` support for free. These are very common use cases for django sites (especially datetime!).
Here I think we just should just default to `json.dumps` if no encoder is specified. No need for an extra setting.
2.6 or 2.7? Also you `requirements` listed here and the modules.
set the safe
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
Can you use `assertContains()` here to prevent errors due to attribute order in the HTML tags.
IIRC get_cols() was renamed to get_group_by_cols() some time ago - no need to have both.
okay, but would be helpful to say _why_ we need to always return True.
I'd rather raise an exception to account for the fact that assertions are ignored with `python -O` unless there's some reason to prefer the assert.
class attribute `output_field = IntegerField()` * fairly sure that's acceptable Then drop the `__init__`
chop "one of" add comma before "or"
put closing parenthesis on the next line
Okay, so to implement the examples like this: ``` with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ``` We'll want to take the dict from terms instead of from kwargs. terms will hold the dict that the user passed in so you should be able to drop in replace it in your constructor like so: ```CyberarkPassword(**terms)```
class attribute `output_field = IntegerField()` * fairly sure that's acceptable Then drop the `__init__`
Defining `__init__()` to specify a default `output_field` is required until we re-arrange `runtests.py` to avoid importing settings dependant modules before calling `django.setup()`.
It seems I was wrong - there are so many examples in the code that override init just to force the output field it's not worth worrying about here. Maybe in the future we can make it easier, but not necessary for this patch.
class attribute `output_field = FloatField()` * fairly sure that's acceptable Then drop the `__init__`
Perhaps "Cannot convert DateField '%s' to DateTimeField." (I'm not certain it's an issue here, but we're favoring '%s' over %r until we drop Python 2 support to avoid possible u'' prefixes.)
Okay, so @cyberark-bizdev take out my first example. The last two are what we're shooting for. (The middle one is the same as bcoca's example, just in compact yaml instead of "readable" yaml ;-)
@pilou-, cyberark-bizdev I had a chance to talk to bcoca today and he said the best way to do this is to pass a list of dicts to the lookup. Then either of these playbook constructs can be valid: ``` with_cyberaskpassword: - {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - {'appid': 'Application2', 'query': 'safe=Safe2;Folder=root;Object=User2', 'output': 'password,passprops.username,passprops.address'} with_cyberarkpassword: {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - debug: msg='{{ lookup('cyberark_password', {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'}) }}' ``` Implementing that will take a little restructuring. I'll go into that in the run method below.
not a list of dicts, just pass a dict to the with: ```yaml with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ```
I'd rather raise an exception to account for the fact that assertions are ignored with `python -O` unless there's some reason to prefer the assert.
chop "one of" add comma before "or"
check_output is not python2.6 compatible
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
put closing parenthesis on the next line
Okay, so to implement the examples like this: ``` with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ``` We'll want to take the dict from terms instead of from kwargs. terms will hold the dict that the user passed in so you should be able to drop in replace it in your constructor like so: ```CyberarkPassword(**terms)```
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
double interpolation is very hard to read. And this looks wrong. Should probably be: `'%s%s%s%s%s' % (prefix, sep, gname, sep, gval)` (rather than `gname, prefix, gval, prefix, sep` which would generate `EnvironmenttagDevtag_`)
```suggestion self._module.fail_json(msg="value must be of type string or dict") ```
This will make the syntax Python 2.6 compatible: ```suggestion items = dict((key, _get_item(item)) for key, item in items.items()) ``` The requirement for Python 2.6 syntax compatibility will be going away, but in the meantime this will fix the CI failure.
I think we should add an `allow_overwrite` or similar param.
Need to ensure that the group actually exists - currently when using this, the inventory plugin fails here because the previous code containing ``` if group_name not in self.inventory.groups: self.inventory.add_group(group_name) ``` has not been added here.
As discussed on IRC: no.
```suggestion raise AnsibleError('Unrecognized type <{0}> for playbook parameter <{1}>'.format(option_type, key)) ```
Please open a ticket to track the bug (all non-trivial changes should have a ticket).
happy new year ! yeah effectively finally this is required for python < 3.5 as you explain (I just tested it in an interactive session). I'm happy to open a PR with this specific change so.. should not be too long to come..
hmmm.. now I think I'm wrong, you want to only have the first partial func.. `func.func` shouldn't anyway be another partial or else that would be problematic I think, as views have to return something else.. Sorry for disturbance ;)
double interpolation is very hard to read. And this looks wrong. Should probably be: `'%s%s%s%s%s' % (prefix, sep, gname, sep, gval)` (rather than `gname, prefix, gval, prefix, sep` which would generate `EnvironmenttagDevtag_`)
```suggestion self._module.fail_json(msg="value must be of type string or dict") ```
This will make the syntax Python 2.6 compatible: ```suggestion items = dict((key, _get_item(item)) for key, item in items.items()) ``` The requirement for Python 2.6 syntax compatibility will be going away, but in the meantime this will fix the CI failure.
I think we should add an `allow_overwrite` or similar param.
Need to ensure that the group actually exists - currently when using this, the inventory plugin fails here because the previous code containing ``` if group_name not in self.inventory.groups: self.inventory.add_group(group_name) ``` has not been added here.
As discussed on IRC: no.
```suggestion raise AnsibleError('Unrecognized type <{0}> for playbook parameter <{1}>'.format(option_type, key)) ```
Please open a ticket to track the bug (all non-trivial changes should have a ticket).
happy new year ! yeah effectively finally this is required for python < 3.5 as you explain (I just tested it in an interactive session). I'm happy to open a PR with this specific change so.. should not be too long to come..
hmmm.. now I think I'm wrong, you want to only have the first partial func.. `func.func` shouldn't anyway be another partial or else that would be problematic I think, as views have to return something else.. Sorry for disturbance ;)
Just a nitpick: The test name looks already readable to me so I'd drop the docstring.
I couldn't see a reason not to use `super` here, otherwise LGTM.
Never use bare except.
Use `utils.parse_duration` instead.
This looks similar to `utils.decode_packed_codes`.
use basic.py's `missing_required_lib` function as it gives details on host and python to avoid user confusion
```suggestion fallback=(env_fallback, ['ANSIBLE_HWC_DOMAIN']), ```
```suggestion fallback=(env_fallback, ['ANSIBLE_HWC_USER']), ```
```suggestion fallback=(env_fallback, ['ANSIBLE_HWC_PASSWORD']), ```
Please don't use such generic env vars. ```suggestion fallback=(env_fallback, ['ANSIBLE_HWC_IDENTITY_ENDPOINT']), ```
avoid "we" to simplify, e.g. "Copy the subquery because it'll be modified."
I needed to add a `set_source_expressions` method which sets `self.subquery = source_expressions` here to get relabelling to work.
Constructing the entire string within the as_sql method departs from how other functions work. Is it possible to do something like: ``` class BaseCaseExpression(Func): function = None template = 'CASE %(simple)s %(conditions)s ELSE %(default)s END' ``` Then build up the dict required to fill in that template, and construct/return at the end? It may flow nicer, and allow 3rd party backends to modify the template without overriding the entire method.
1 line is okay --- we prefer longer lines up to 119 characters when it helps readability.
Subtests can also be used here.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
I prefer that the module would check the connection itself as well, without actually sending the message. If the API supports stub messages (or empty messages?), use that. Otherwise just test the authentication/connection some other way.
extra space after ,
"manual" is a recent occurrence, for older versions it was 'unmarkauto' iirc
I'd use a name like `assertBackendInSession`.
An example of using the `mocker` fixture. ```suggestion def mock_NamedTemporaryFile(mocker, **args): mock_ntf = mocker.MagicMock() mock_ntf.write = mocker.MagicMock() mock_ntf.close = mocker.MagicMock() ```
Please use assertRaisesMessage to verify this is the ValueError we expect.
```suggestion Test that the returned value for timezone consists of only uppercase ```
I know, was just wondering if it's intended that it works that way.
last loaded wins, but iirc, we reverse search on handlers list
Match the error message
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
No need for get(key, None) as None is the default fix also for following get()
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
I would change this to a `NotSupportedError`, e.g. ```python raise NotSupportedError( 'Bulk create a multi-table inherited model is not supported ' 'on this database backend.' ) ```
Are we doing singleton `__new__`/`__call__` magic internally on some of the callback plugin types? Seems like this would never be `False` otherwise...
This implies that constructing some callbacks doesn't result in an instance, but also doesn't raise an Exception? That doesn't sound right...
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
I would change this to a `NotSupportedError`, e.g. ```python raise NotSupportedError( 'Bulk create a multi-table inherited model is not supported ' 'on this database backend.' ) ```
Are we doing singleton `__new__`/`__call__` magic internally on some of the callback plugin types? Seems like this would never be `False` otherwise...
This implies that constructing some callbacks doesn't result in an instance, but also doesn't raise an Exception? That doesn't sound right...
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
I would change this to a `NotSupportedError`, e.g. ```python raise NotSupportedError( 'Bulk create a multi-table inherited model is not supported ' 'on this database backend.' ) ```
Are we doing singleton `__new__`/`__call__` magic internally on some of the callback plugin types? Seems like this would never be `False` otherwise...
This implies that constructing some callbacks doesn't result in an instance, but also doesn't raise an Exception? That doesn't sound right...
I would re-write to something like this: ```python def _attached_sd_service(self, storage_domain): dc_name = self._module.params['data_center'] # Find the DC, where the storage we want to remove reside: if not dc_name and self._module.params['state'] == 'absent': dcs_service = self._connection.system_service().data_centers_service() dc = search_by_attributes(dcs_service, storage=self.param('name'), status='up') if dc is None: raise Exception("Can't remove storage, because dataceneter bla bla') dc_name = dc.name attached_sds_service = self._attached_sds_service(dc_name) attached_sd_service = attached_sds_service.storage_domain_service(storage_domain.id) return attached_sd_service ```
This can be changed to look for only one space now that #57209 is merged. Thanks again for writing tests. Much appreciated.
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
I prefer putting the closing ) on the next line
I'm not sure whether it should be considered part of a separate clean up across all cases of this in the code base or whether we can handle this here for all of the adjacent lines, but it would be nice to fix the indentation: ```python self.management_form_html + ( '<tr><th>Choice:</th><td><input type="text" name="choices-0-choice" value="Calexico"></td></tr>' '<tr><th>Votes:</th><td><input type="number" name="choices-0-votes" value="100"></td></tr>' ) ```
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
No need to parametrize with just one case.
Plz also use `match` arg here
If we're just testing broker compatibility I don't think we even need this part of the test.
This should verify the output.
I don't see a reason for modifying a source exception, you can use: ```python raise ValueError(...) from e ``` Also `blew up` is not a appropriate wording and a new exception is not more informative because it refers to the field class `... the field <django.db.models.fields.CharField>` instead of `<app_label>.<model_name>.<field_name>`, maybe: ```python raise ValueError('Error during %s serializing: %s' % (field, e)) from e ``` I don't have a quick answer how to get a field path.
Thanks for updates :+1: > ... but I had trouble getting the field name, any ideas on that one? Unfortunately not, moreover I'm afraid that we will not be able to get `<app label>.<model name>.<field name>` or even `<app label>.<model name>` in a reliable way. We serialize `field` from `django.db.models` not a model attribute, that's why it's complicated or even not feasible. Each approach doesn't work in some cases, e.g. constructing messages in the `FunctionTypeSerializer` will not work for `lambda`s defined in the module: ``` Error during serializing test_one.models.<lambda>: ... ``` or imported from other modules: ``` ValueError: Error during serializing test_one.utils.<lambda>: ... ``` I think we should close this as wontfix :disappointed:
No it's not, without this patch it raises, e.g. ``` ValueError: Cannot serialize function: lambda ``` with this patch: ``` ValueError: Error during <django.db.models.fields.IntegerField> serializing: Cannot serialize function: lambda ``` It's not more descriptive, IMO. You can have hundreds of `IntegerField`s in dozens of apps. I would expect: ``` ValueError: Error during 'test_app.models.MyModel.field_1.default' serializing: Cannot serialize function: lambda ``` I don't see much value in this change if it's not feasible to get `<app label>.<model name>.<field name>.<parameter>` or at least `<app label>.<model name>.<field name>`. Maybe we should fix this in `FunctionTypeSerializer`, it should be doable to get at least `test_app.models.MyModel` from `__qualname__` and `__module__` :detective:
I'm sorry but it's still the same :shrug: . You can manipulate with tests to create an unreachable state where you will get an expected message but that's not the correct solution. Please check an [attached project ](https://code.djangoproject.com/raw-attachment/ticket/25370/ticket_25370.tar.gz) and try to run: ``` $> python manage.py makemigrations ``` it raises: ``` ValueError: Error during <django.db.models.fields.IntegerField> serializing: Cannot serialize function: lambda ```
This assertion is not related with this fix so I would move it to a separate commit.
You should also add an assertion for the imports provided in the second value of the tuple.
We need to consume the entire iterator: ``` Exception ignored in: <posix.ScandirIterator object at 0x7f5a9e95de10> ResourceWarning: unclosed scandir iterator <posix.ScandirIterator object at 0x7f5a9e95de10> ```
`assertNotIn` may pass from many reasons. I think it is better to check field value with `self._get_field_values()` hook, e.g. ```python self.assertEqual(self._get_field_values(child_data, 'parent_m2m'), []) ```
`frozenset` is missing.
falback to a static URL.
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
Could you add at the top of Apply the following line. This will create an ems log event for users with auto support turned on. netapp_utils.ems_log_event("na_ontap_ldap", self.server)
Maybe we could test that `name_color_uniq` is also in the message? ```suggestion with self.assertRaisesMessage(ValidationError, 'name_color_uniq'): ```
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
so this assertion looks incorrect, i would expect and empty string as the ssh args
no, if the variable is set but empty, you should empty out the options
`value` could be used instead of `argv`. `argk` is only required when `argv is not None` and could be moved in the `if` block.
Good catch, I will remove it before final squash.
Could use `bulk_create` ```suggestion Product.objects.bulk_create([ Product(name='p1'), Product(name='p1', color='red'), ]) ```
Actually someone can sent for example '123213321321321', which isn't valid name nor ID, so it will fail with HTTP 404.
Check this: https://github.com/ansible/ansible/pull/29175/files If you will do it the same we can close that PR.
I would re-write to something like this: ```python def _attached_sd_service(self, storage_domain): dc_name = self._module.params['data_center'] # Find the DC, where the storage we want to remove reside: if not dc_name and self._module.params['state'] == 'absent': dcs_service = self._connection.system_service().data_centers_service() dc = search_by_attributes(dcs_service, storage=self.param('name'), status='up') if dc is None: raise Exception("Can't remove storage, because dataceneter bla bla') dc_name = dc.name attached_sds_service = self._attached_sds_service(dc_name) attached_sd_service = attached_sds_service.storage_domain_service(storage_domain.id) return attached_sd_service ```
This can be changed to look for only one space now that #57209 is merged. Thanks again for writing tests. Much appreciated.
I would rephrase this as "Only set this to yes when you want private information about this key to leave the remote machine", the way it's written now I read "don't" instead of "do want"
I'm not sure whether it should be considered part of a separate clean up across all cases of this in the code base or whether we can handle this here for all of the adjacent lines, but it would be nice to fix the indentation: ```python self.management_form_html + ( '<tr><th>Choice:</th><td><input type="text" name="choices-0-choice" value="Calexico"></td></tr>' '<tr><th>Votes:</th><td><input type="number" name="choices-0-votes" value="100"></td></tr>' ) ```
In python we tend to add a trailing comma, also for the last item in a multi-line dict or list. This is explicitly allowed by PEP8 and ensures that if any items are added, only one new line is added (and not the previous line needs a change). So it keeps the origin of lines (blame) clean. And it's also much more convenient.
Please leave this defaulting to `no` and let users opt-in to transmitting unencrypted keys to a remote machine instead. There's far too much risk of private keys ending up in logs etc. and there's very little you can do with raw private keys anyways (the more widely used thing would probably be the associated public key).
Use `self.url_result(inner_url, 'Generic')` instead.
Well, basically that's the problem of these people not us. We don't care whether one can read regexp or not. Moreover most likely next time this code is read by someone is when extractor breaks due to layout change. Chances are this snippet is already irrelevant by that time.
It looks that `test_sqlmigrate_replaced_second_migration()` and `test_sqlmigrate_replaced_migration()` are redundant. Please remove one of them.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
We run `createsuperuser` in non-interactive mode so `@mock_inputs` is unnecessary.
Manager will raise a different `IntegrityError`, e.g. _"The row in table 'auth_tests_customuserwithfk' with primary key '1' has an invalid foreign key: auth_tests_customuserwithfk.group_id contains a value '-1' that does not have a corresponding value in auth_group.id."._
This looks unnecessary.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
`call_command()` raises an exception so these lines are unreachable.
Passing `group` to the `call_command()` is unnecessary because it's already passed via `DJANGO_SUPERUSER_GROUP`.
I removed the NotIn assertions in my edits because they are brittle since a typo in the message means they would inadvertently pass. I suppose if error messages were class attributes that would make it more robust, but it seems unlikely to me that a regression could be introduced such that both messages are displayed.
`call_command()` raises an exception so these lines are unreachable.
We can remove this block since Python 2.6 is not supported.
Please explain this in doc.
I think a mixin is a bit complicated. The attribute sounds simpler - it could be a function instead of just a boolean, then it could return the transform if applicable, else `None` for non-sliceable fields (default implementation).
`client.start_execution` is idempotent but `start_execution` always returns `changed=True` if `action=start`. We probably want to `client.list_executions` to see if the execution is already running before proceeding.
I haven't actually used stepfunctions myself so I'm just going off the docs. AIUI, `ExecutionAlreadyExists` would only be returned on a running execution if the `execution_input` is different, not in a case where the input is the same. As an ansible user who is new to step functions, I feel like if I gave the same input and ran the playbook twice I would expect the 2nd time to be `changed=false` if the first execution is still running. But that doesn't mean that's how AWS users familiar with step functions will expect it.
Wouldn't this approach fix the slicable fields to just these types? A mixin along with `slicable = True/False` on `F` may be an approach. Besides, I believe the policy is to keep `django.contrib.postgres` as separate as possible.
```suggestion module.exit_json(changed=True,**camel_dict_to_snake_dict(res))) ```
I think `get_internal_type` is better to use.
single line looks more readable here
Yeah, we still can't use dict comprehensions until 2.6 is formally dropped, sorry.
For a gzip file, I believe this will cause the browser to unzip it. ``` $ python3 >>> import mimetypes >>> mimetypes.guess_type('f.tar.gz') ('application/x-tar', 'gzip') ``` But I'm guessing the user wants the mime type to be `application/gzip` (or similar) with no encoding. [MDN Docs](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Encoding) > The Content-Encoding entity header is used to compress the media-type. When present, its value indicates which encodings were applied to the entity-body. It lets the client know, how to decode in order to obtain the media-type referenced by the Content-Type header. I know of at least one third party Django app that has this issue: https://github.com/johnsensible/django-sendfile/issues/38
The pylint error is occurring because this `MockSelector` class shadows the one on line 59.
You don't need to mock, it will return `False` for a bad file descriptor.
it should also check if it can write there
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
And `\r` or `\n` -- I dunno if Django protects from header injection
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
Download archive behavior must not change, it must only take place after success of the actual download and post processing.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Download archive behavior must not change, it must only take place after success of the actual download and post processing.
```IOError``` and ```OSError``` may occur here, too. For example, if I attempt to write to ```/root``` with as a non-user user: ``` $ youtube-dl -v --skip-download test:xiami:song -o "/root/%(id)s.%(ext)s" --write-sub [debug] System config: [] [debug] User config: [] [debug] Custom config: [] [debug] Command-line args: ['-v', '--skip-download', 'test:xiami:song', '-o', '/root/%(id)s.%(ext)s', '--write-sub'] [debug] Encodings: locale UTF-8, fs utf-8, out UTF-8, pref UTF-8 [debug] youtube-dl version 2017.04.28 [debug] Git HEAD: b5c39537b [debug] Python version 3.6.1 - Linux-4.10.11-1-ARCH-x86_64-with-arch [debug] exe versions: ffmpeg 3.3, ffprobe 3.3, rtmpdump 2.4 [debug] Proxy map: {} [TestURL] Test URL: http://www.xiami.com/song/1775610518 [xiami:song] 1775610518: Downloading JSON metadata [info] Writing video subtitles to: /root/1775610518.origin.lrc Traceback (most recent call last): File "<string>", line 23, in <module> File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 465, in main _real_main(argv) File "/home/yen/Projects/youtube-dl/youtube_dl/__init__.py", line 455, in _real_main retcode = ydl.download(all_urls) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1897, in download url, force_generic_extractor=self.params.get('force_generic_extractor', False)) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 832, in process_ie_result extra_info=extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 771, in extract_info return self.process_ie_result(ie_result, download, extra_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 825, in process_ie_result return self.process_video_result(ie_result, download=download) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1539, in process_video_result self.process_info(new_info) File "/home/yen/Projects/youtube-dl/youtube_dl/YoutubeDL.py", line 1717, in process_info with io.open(encodeFilename(sub_filename), 'wb') as subfile: PermissionError: [Errno 13] Permission denied: '/root/1775610518.origin.lrc' ``` It should report an error ```Cannot write subtitles file``` in this case.
chop "should" (just state the behavior)
Nitpick, you can use a list comprehension here instead of calling list on a generator expression.
I think we should just let this exception propagate. We aren't adding any value by catching it here (and might be removing value if the traceback were useful). This isn't code in ansible itself so we don't have to create a nicer error message either.
Maybe rename this to substitute_crypto_req since it's replacing the one from requirements.txt with the one we autodetect rather than simply appending.
We should avoid lambdas and filters most of the time. Use list comprehensions and generator expressions instead. It's both faster and more pythonic to use those idioms.
Kind of a nitpick since this is setup.py... but API-wise I prefer returning a small dict with the dynamic values and letting the caller merge it/copy the static dict if necessary.
Style note, I finally convinced people a year or so ago that ```{}``` is superior to ```dict()```. So new code should use the literal syntax.
This fails for me: ``` AssertionError: <MagicMock name='WatchmanReloader()' id='140033332456584'> is not an instance of <class 'django.utils.autoreload.StatReloader'> ``` perhaps that's the failure that Jenkins encountered.
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
```suggestion pkg_version = line.partition(':')[-1].strip() ```
It doesn't work if pkg_provides contain a version: ``` max@mde-oxalide % pacman -Qqei perl | grep Provide Provides : perl-archive-tar=2.30 perl-attribute-handlers=1.01 perl-autodie=2.29 perl-autoloader=5.74 perl-autouse=1.11 perl-b-debug=1.26 perl-base=2.27 perl-bignum=0.49 perl-carp=1.50 perl-compress-raw-bzip2=2.074 perl-compress-raw-zlib=2.076 perl-config-perl-v=0.29 perl-constant=1.33 perl-cpan-meta-requirements=2.140 perl-cpan-meta-yaml=0.018 perl-cpan-meta=2.150010 perl-cpan=2.20 perl-data-dumper=2.170 perl-db_file=1.840 perl-devel-ppport=3.40 perl-devel-selfstubber=1.06 perl-digest-md5=2.55 perl-digest-sha=6.01 perl-digest=1.17_01 perl-dumpvalue=1.18 perl-encode=2.97 perl-encoding-warnings=0.13 perl-env=1.04 perl-experimental=0.019 perl-exporter=5.73 perl-extutils-cbuilder=0.280230 perl-extutils-constant=0.25 perl-extutils-install=2.14 perl-extutils-makemaker=7.34 perl-extutils-manifest=1.70 perl-extutils-parsexs=3.39 perl-file-fetch=0.56 perl-file-path=2.15 perl-file-temp=0.2304 perl-filter-simple=0.95 perl-filter-util-call=1.58 perl-getopt-long=2.5 perl-http-tiny=0.070 perl-i18n-collate=1.02 perl-i18n-langtags=0.43 perl-if=0.0608 perl-io-compress=2.074 perl-io-socket-ip=0.39 perl-io-zlib=1.10 perl-io=1.39 perl-ipc-cmd=1.00 perl-ipc-sysv=2.07 perl-json-pp=2.97001 perl-lib=0.64 perl-libnet=3.11 perl-locale-codes=3.56 perl-locale-maketext-simple=0.21_01 perl-locale-maketext=1.29 perl-math-bigint-fastcalc=0.5006 perl-math-bigint=1.999811 perl-math-bigrat=0.2613 perl-math-complex=1.5901 perl-memoize=1.03_01 perl-mime-base64=3.15 perl-module-corelist=5.20180622 perl-module-load-conditional=0.68 perl-module-load=0.32 perl-module-loaded=0.08 perl-module-metadata=1.000033 perl-net-ping=2.62 perl-params-check=0.38 perl-parent=0.236 perl-pathtools=3.74 perl-perl-ostype=1.010 perl-perlfaq=5.021011 perl-perlio-via-quotedprint=0.08 perl-pod-checker=1.73 perl-pod-escapes=1.07 perl-pod-parser=1.63 perl-pod-perldoc=3.2801 perl-pod-simple=3.35 perl-pod-usage=1.69 perl-podlators=5.006 perl-safe=2.40 perl-scalar-list-utils=1.50 perl-search-dict=1.07 perl-selfloader=1.25 perl-socket=2.027 perl-storable=3.08 perl-sys-syslog=0.35 perl-term-ansicolor=4.06 perl-term-cap=1.17 perl-term-complete=1.403 perl-term-readline=1.17 perl-test-harness=3.42 perl-test-simple=1.302133 perl-test=1.31 perl-text-abbrev=1.02 perl-text-balanced=2.03 perl-text-parsewords=3.30 perl-text-tabs=2013.0523 perl-thread-queue=3.12 perl-thread-semaphore=2.13 perl-threads-shared=1.58 perl-threads=2.22 perl-tie-file=1.02 perl-tie-refhash=1.39 perl-time-hires=1.9759 perl-time-local=1.25 perl-time-piece=1.3204 perl-unicode-collate=1.25 perl-unicode-normalize=1.26 perl-version=0.9923 perl-xsloader=0.30 ```
Is it possible ? For me it would mean that it exist a package without version.
Why don't you just encode suffix like `u'-\u00c5\u00d1\u015a\u00cc\u03b2\u0141\u00c8'.encode('utf-8')` and for the no_temp_unicode override just do `suffix = b'-ansible'`? No need for this check which is arguably more confusing.
@thomwiggers `line.partition(':')[2]` yes, I have no problem with that.
```suggestion pkg_name = line.partition(':')[-1].strip() ``` idem, no need to lower.
If you're going to do partition, I'd rather use the fact that it'll always return a 3-tuple instead of using `-1` to index the third element. i.e.: ```python line.partition(':')[2] ```
the `f.close()` is implied as part of the with block. Also, I don't think this should be in the module since it looks like a debug statement.
If you're just raising, you can skip the try/except since it's handled in the caller.
Most of the time if you want a list and you're using map() you can convert to a list comprehension for a more pythonic (and faster, although that's almost never an issue) implementation: ``` python current_rules = [(x['priority'], x['rule_name']) for x in get_rules(api, name)] ```
Looks like this could be a single line.
Same. The `filter` doesn't make sense to me
Same question for dropping lambda here as well.
`self.each_context` actually already contains a fully populated app list, under `available_apps`. We could make this more efficient by extracting `app_list` from `available_apps` rather than calculating it twice. ``` context = self.each_context(request) app_list = context['available_apps'].get(app_label) if not app_list: raise Http404('The requested admin page does not exist.') context.update({'app_list': [app_List], ...}) ```
the shell itself would have done it before. but might have done it slightly differently.
I don't think this is safe. If someone is calling this function without unsafe_shell they probably have not quoted the arguments to prevent this sort of expansion.
Okay, looking further down, I see that you're just moving this around though...
`objectID` does not match the id from `AmericasTestKitchenIE`.
- use the more simpler agolia Search index endpoint. - use `query` argument. - fetch only the attributes needed for extraction.
Looks like this could be a single line.
Shouldn't this be `return isinstance(x, collections.Iterator)` instead? Actually we should replace all reference to this function by a `collection.Iterator` instance check.
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
I would chop blank lines in this test.
Chop `Ensure that`.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
use ```from ansible.module_utils.vmware import get_cluster```
I meant `from ansible.module_utils.vmware import find_cluster_by_name`
My bad. Could you please add this to vmware.py
use ```from ansible.module_utils.vmware import get_parent_datacenter```
Looks like this could be a single line.
I am split on my feelings about this function. It doesn't work the same as it did as part of `AnsibleModule`. Before, it would error on the first time that `count > 1`, whereas we aren't doing so now, we are collecting them all, but then not using them all. Additionally, this is more like `list_mutually_exclusive`. I think this function should raise some form of Exception, instead of just returning a list of the mutually exclusive args that were provided by the user.
This code isn't correct. (2, 7) is the check you want, and then with >= if I am not mistaken.
Shouldn't this be `return isinstance(x, collections.Iterator)` instead? Actually we should replace all reference to this function by a `collection.Iterator` instance check.
As discussed on IRC: no.
I would chop blank lines in this test.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
Chop `Ensure that`.
use ```from ansible.module_utils.vmware import get_cluster```
I meant `from ansible.module_utils.vmware import find_cluster_by_name`
it should also check if it can write there
Might want to use simple quote here.
Might want to check `rc` status before return output.
Sorry, my bad. I didn't see the implementation of `_run`.
This method isn't necessary.
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
You should use `module.add_cleanup_file()` (somewhere above) instead of manually trying to clean up.
In the "Generate privatekey6 - standard - with non-ASCII passphrase" test, this fails with `'ascii' codec can't decode byte 0xc3 in position 16: ordinal not in range(128)`. Stacktrace: ``` File "/tmp/ansible_openssh_keypair_payload_kCnMbD/__main__.py", line 193, in generate tf.write("#!/bin/sh\necho %s" % quote(self.passphrase)) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 706, in write return self.writer.write(data) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 369, in write data, consumed = self.encode(object, self.errors) ``` It seems to fail in all Python 2 tests; in Python 3 it seems to work.
You need to make sure `tf` doesn't write a unicode string. tf.write(u("#!/bin/sh/\necho %s" % quote(self.passphrase)).encode("utf-8")) Ansible may have a utility function to do this.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Use _ (underline) instead of webpage if the value is not used.
When there is no `topicTitle`, `categories` will be `[None]` that does not make any sense.
Use _download_webpage is urlh is not used. And note should be meaningful for typical users.
remove the register line, kind of confused
Are all of these necessary? I think youtube-dl defaults suffice.
urlencode_postdata should be better than urlencode + encode
It's a function in ```..utils```. For example: ```data = urlencode_postdata({'foo': 'bar'})```. Basically it does the same thing as ```urlencode({'foo': 'bar'}).encode('ascii')```, just a more meaningful name.
Use ```data``` and ```headers``` parameters of _download_webpage instead.
strip_jsonp should work here
Use json.dumps instead
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
When there is no `topicTitle`, `categories` will be `[None]` that does not make any sense.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Is there a typo? I'm not sure what "hub" means in this sentence.
no restructured text (:class:) in docstrings please
Any problem with: ``` @property def media(self): ```
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
This isn't needed, since we now only support 2.6+ anyway.
I think something like `SETTING_BOTH` will be fine. No need to memorialize the bug number.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Since Ansible subscribes to unicode sandwich, this should be the callers' (actually, where the data enters the system) responsibility.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
2.6 or 2.7? Also you `requirements` listed here and the modules.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
I can't think of any reason off the top of my head that we'd need to customize the module object creation, so just having create_module return `None` *should* be fine. IIRC `exec_module` is literally just about populating the module (already created by create_module or the base import machinery) from the code; the 3.x import machinery takes care of the transactional insertion/removal from `sys.modules` on fresh imports and reloads, so we should probably never mess with that in the 3.x loader code at all. It's been awhile, but I think it basically just calls create_module (and does the stock creation if we didn't return something), provisionally inserts the module to `sys.modules`, calls exec_module under an exception handler, and removes the module on an error if it inserted it.
Add trailing comma.
`required: false` is the default so not required
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
Use hanging indentation (the same in the second test).
Please use a single quote.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Use another lookup instead of `epoch` e.g. `second`.
You could fix #19781 real quick right here :D
I'll take the time to write one later today but we could use a `@property`: ``` python @property def widget(self): if self.localize: return TextInput else: return NumberInput ``` One of the con here is that `NumberField.widget` will be an instance of `property`. We could also write a descriptor to maintain backward compatibilty: ``` python class WidgetDescriptor(object): def __init__(self, widget, localized_widget): self.widget = widget self.localized_widget = localized_widget def __get__(self, instance, owner): if instance and instance.localize: return self.localized_widget return self.widget class IntegerField(Field): widget = WidgetDescriptor(NumberInput, TextInput) ``` Maybe I'm just over-complicating this whole thing.
Remove blank line (and below).
Remove blank line (and below).
Personally, I think `new_key` would be a more commonly used choice.
Undo unrelated change (and below).
There is a minor behaviour change here. Previously calling `decr()` with `delta=0` would call `self._cache.decr()`, but now it'll call `self._cache.incr()` instead. In theory this shouldn't be a problem, but am highlighting it.
@wrouesnel CI failure due to PEP 8 issue: ``` 2017-01-28 07:26:38 ERROR: PEP 8: lib/ansible/modules/cloud/amazon/ec2.py:930:1: W293 blank line contains whitespace (legacy) ```
if min_count <=
`items = value.split(self.delimiter) if value else []` is slightly faster.
What if the prepared value contains unicode? (As I said before, you should use a list comprehension instead of a generator, it'll be about 2 times faster here)
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
So if I update some parameter+ change state to running, it won't start, IIUC
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Use another lookup instead of `epoch` e.g. `second`.
Yeah, but does this code works well, when updating some attribute via that? As far as I read it correctly it doesn't.
I am fine. It's just that it's `if` with too many and\or's are just too much for me :) I just preffer straighforward code with no complex if's, if they are not really needed.
I see now. Quite complex.. If we want to say user he can't stop/start non-existing VM, wouldn't it be simpler to just say that(one simple condition) and instead of saying that he sent empty parameters tell him, which parameters are required(returned from backend) this would very simplify all of this and I don't think we really need this. But if you insist this is better I am ok.
Not critical, but `for num_started, node in enumerate(consumer.nodes, 1)` would probably be more idiomatic.
Does replication factor matter in this test at all? I can't think of why it would matter for the replication factor, so it seems weird to make it 2 here.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
I think this would be a bit more readable: ``` return ( '%(func)s %(op)s %(dist)s' % {'func': sql, 'op': self.op, 'dist': dist_sql}, params + dist_params ) ```
I would catch all exceptions here, so that the output is clean JSON. Please use this common code: ```python except: e = get_exception() module.fail_json(msg=str(e)) ```
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
Why don't you run `netstat -lnptu` instead and parse everything in one go ? Also beware that if this is run as a normal user, you may not get pid information.
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
Ah, yes, I miss read the code. Nothing to change here.
Should be ``self.weight``
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I think this would be a bit more readable: ``` return ( '%(func)s %(op)s %(dist)s' % {'func': sql, 'op': self.op, 'dist': dist_sql}, params + dist_params ) ```
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
Ah, yes, I miss read the code. Nothing to change here.
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
colors should all be configurable
excellent handling of congestion control
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Should be ``self.weight``
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
I think this would be a bit more readable: ``` return ( '%(func)s %(op)s %(dist)s' % {'func': sql, 'op': self.op, 'dist': dist_sql}, params + dist_params ) ```
Please don't make lines longer! There was nothing really wrong with this line before
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
This 'default_chains' doesn't seem to be used anywhere.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Should be ``self.weight``
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
I think this would be a bit more readable: ``` return ( '%(func)s %(op)s %(dist)s' % {'func': sql, 'op': self.op, 'dist': dist_sql}, params + dist_params ) ```
Please don't make lines longer! There was nothing really wrong with this line before
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
This 'default_chains' doesn't seem to be used anywhere.
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
Please don't make lines longer! There was nothing really wrong with this line before
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Seems nicer to infer private_zone if vpc_id is set. But not a blocker
I was able to fix this locally by changing to `if r == self.payload:`. No idea if there's some case this doesn't work for though. At scale ansible might not be the right tool. However I have no desire to deal with adding another tool just for updating a single record.
excellent handling of congestion control
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
Good catch, I will remove it before final squash.
Please use shorter name (less than 30 chars), e.g. `deferrable_pink_constraint`.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
This should be: ``params = config_params + params``
Should be ``self.weight``
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
I think this would be a bit more readable: ``` return ( '%(func)s %(op)s %(dist)s' % {'func': sql, 'op': self.op, 'dist': dist_sql}, params + dist_params ) ```
Please don't make lines longer! There was nothing really wrong with this line before
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Please don't make lines longer! There was nothing really wrong with this line before
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
You should emphasize that the module can and will not do any idempotence checking for this.
(In general, I don't think modules should have such options.)
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
excellent handling of congestion control
```not (foo is None)``` => ```foo is not None```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
since `list_type` can only be `top` or `global` you can change it to `else:` and safely remove `jsonp_url = ""` (which should have been set to `None`).
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
Since we expect the name to start with 'http://' or 'https://' I would so this: ```python if name.startswith('http://') or name.startswith('https://'): ... ```
Good catch, I will remove it before final squash.
Please use shorter name (less than 30 chars), e.g. `deferrable_pink_constraint`.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
This should be: ``params = config_params + params``
Should be ``self.weight``
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
I think this would be a bit more readable: ``` return ( '%(func)s %(op)s %(dist)s' % {'func': sql, 'op': self.op, 'dist': dist_sql}, params + dist_params ) ```
Please don't make lines longer! There was nothing really wrong with this line before
`target` is only available for `Col`, so it crashes for expressions, see `test_slicing_of_f_expression_with_annotated_expression`.
This will not work for `OuterRef()` :disappointed: because we don't resolve it properly, so it generates: ```sql EXISTS ( SELECT (1) AS "a" FROM "expressions_company" U0 WHERE SUBSTRING(U0."name", 8, 1) = (SUBSTRING(U0."name", 3, 1)) LIMIT 1 ) ``` instead of ```sql EXISTS ( SELECT (1) AS "a" FROM "expressions_company" U0 WHERE SUBSTRING(U0."name", 8, 1) = (SUBSTRING("expressions_company"."name", 3, 1)) LIMIT 1 ) ``` see `test_slicing_of_outerref`.
We allow up to 119 characters, so this doesn't need to be wrapped. ```suggestion def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False): ```
Wouldn't this approach fix the slicable fields to just these types? A mixin along with `slicable = True/False` on `F` may be an approach. Besides, I believe the policy is to keep `django.contrib.postgres` as separate as possible.
I think it will be more readable to keep `int` and `slice` in separate branches, e.g.: ```python def __init__(self, f_obj, slice_obj): if isinstance(slice_obj, int): if slice_obj < 0: raise ValueError('Negative indexing is not supported.') self.low = slice_obj self.length = 1 elif isinstance(slice_obj, slice): if ( (slice_obj.start is not None and slice_obj.start < 0) or (slice_obj.stop is not None and slice_obj.stop < 0) ): raise ValueError('Negative indexing is not supported.') if slice_obj.step is not None: raise ValueError('Step argument is not supported.') self.low = 1 if slice_obj.start is None else int(slice_obj.start) + 1 self.length = None if slice_obj.stop is None else int(slice_obj.stop) - self.low + 1 else: raise TypeError('Argument to slice must be either int or slice instance.') self.expression = f_obj ```
Casting `int` to `int` is not necessary.
We can remove this check after fixing the `Field.slice_expression()`.
Chop blank line.
Please use f-strings as Python 3.6+ is now the requirement More information is available including some benchmarks. https://cito.github.io/blog/f-strings/
I think `get_internal_type` is better to use.
Can we display a similar message with the test results? It can be helpful when you have plenty of errors, .e.g. ``` ... FAILED (errors=206, skipped=1094, expected failures=4) Used shuffle seed: 556738431 (generated) ```
I don't see any need for this attribute.
OTOH, this does nothing since the module is already cached by the time this gets executed. So it's a no-op.
Having an import in a test function is not the best idea since it mutates the whole test runtime process. You'd probably need to do the whole test in a subprocess to properly isolate it.
I don't see a reason we can't use Python's `hash()` builtin, which is even faster and cached on strings I also don't think we need a class here - a single function to do the shuffling would do.
(And round-tripping of the messages is already tested in other tests)
You have some unmerged lines here
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
I don't think another var is necessary. If the first iteration to start the processes succeeds and won't interrupt the control flow, all of them will be active. ```suggestion processes = [ Process(target=_run_threads, args=(_,)) for _ in range(1, 6) ] for t in processes: t.daemon = True t.start() for t in processes: t.join() ```
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
`form_class` is defined in `RangeField.formfield()` so this is redundant.
Note that format strings changed in python-2.7 So things like "{} {}".format() needs to change to "{0} {1}".format() for python-2.6 compatibility.
okay, but would be helpful to say _why_ we need to always return True.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
Need a colon at the end here
`form_class` is defined in `RangeField.formfield()` so this is redundant.
okay, but would be helpful to say _why_ we need to always return True.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
`form_class` is defined in `RangeField.formfield()` so this is redundant.
Same as above, I'd revert to the simpler version. The main idea behind `decode` for me is a consistent interface where it makes sense, we don't have to bend backwards just to use it.
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
remove "0" in {0}
`if it encounter` => `if it encounters`
`mentionned` => `mentioned`
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
Please rewrite as ``` if __name__ == '__main__': main() ```
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
You can skip the blank lines in these tests as three lines long isn't too difficult too read with spaces.
I'd use a name like `assertBackendInSession`.
Please use assertRaisesMessage to verify this is the ValueError we expect.
We should avoid lambdas and filters most of the time. Use list comprehensions and generator expressions instead. It's both faster and more pythonic to use those idioms.
I was thinking to assign the group permissions at the beginning of the test case so you can check all three together and not need the second round of tests along with setting the user back to `is_active=True`. Also, `codename='test_(user|group)'` would be helpful.
plz don't use EOL escaping, wrap with braces instead.
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
Match the error message
Nitpick, you can use a list comprehension here instead of calling list on a generator expression.
here too: pysopenssl --> pyopenssl
> Umh... this is a breaking change. @phihag should we add a hidden backwards compatibility `--write-srt`, or just specify the change in release notes? > Please stay backwards-compatible.
As Collin said, slightly longer lines are okay rather than awkward wrappings like this.
I'd revert this as I don't think it is better and it isn't making this more consistent with code elsewhere. ```suggestion if isinstance(stored_backend, RemoteUserBackend): ```
Will break the extraction if there is no title attribute. This should be optional.
This workaround is not necessary with my proposition ```suggestion ('ar', []), ```
It was removed from ParseField in #17933. It also shouldn't be added here.
I'm pretty sure camelCase shouldn't be supported any more.
+1 but I would check with @clintongormley whether we are happy making this breaking change in 5.0. However, we haven't actually formally removed camelCase in ParseField IIRC so adding this extra name here might be unnecessary anyway
Ok great, thanks for the correction @rjernst
The usual style is to put the closing parenthesis on the next line and include a trailing comma on this line.
Please ignore, my suggestion is invalid syntax.
with -> width
comma after tuple
width, height, and offset
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
`enumerate` on for range.
Useless with timestamp available.
Don't capture groups you don't use. Unused captured group.
like diff = load_config(self._module, config_xml, [])
`id` isn't used, it is sufficient to iterate on keys.
User of this inventory plugin should be able to use `v6_main_ip` for value of `ansible_host` (i don't know if it makes sens to be able to use `internal_ip`) or no `ansible_host` at all (meaning that `server['label']` will be used instead).
CI failure due to python 2.4 syntax error: ``` 2017-02-03 01:32:20 Compiling ./lib/ansible/module_utils/cloudera.py ... 2017-02-03 01:32:20 File "./lib/ansible/module_utils/cloudera.py", line 149 2017-02-03 01:32:20 dictionary = { value:key for key,value in dictionary.items() } 2017-02-03 01:32:20 ^ 2017-02-03 01:32:20 SyntaxError: invalid syntax ```
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
`e` isn't used: remove it or use it.
It might be worth including the bridgename in the error message.
`Check the configuration files` seems vague, I propose: `Check inventory file and vultr configuration files`.
When `hostname_preference` is equal to `name`, there is no need to define `ansible_host`.
use a single loop? ~~~python for server in _retrieve_servers(api_key): server = Vultr.normalize_result(server, SCHEMA) .... ~~~ ~~~
In Python3, `super()` is enough.
I think you meant `get_ellipsized_page_range` here. Use `number` instead of `page_num` which matches the argument name used by `Paginator.get_page()`.
There is no need to provide a value for `max_pages_num` - I think it should be calculated automatically: ```python (self.on_each_side + self.on_ends) * 2 ``` Otherwise developers can provide values that are incompatible with each other...
A test is missing for this change.
If it doesn't provide any useful functionality, I'd omit it.
Is `test_doesnt_work_with_init_subclass` meant to test this change? I still don't see any test failures if this change is reverted.
a misspell? SQLFuncMixn -> SQLFuncMixin
Maybe @felixxm or @carltongibson can guide, but I believe it'd be good practice to use a `warnings.warn` in `__init__`, although a deprecation timeline has not been determined for `django.contrib.postgres.field.JSONField`.
```suggestion params = self.settings[alias].copy() ```
Dict literals are preferred (0d74c41981687598d3fa0a7eb9712ce4c387ca19).
Can this fail with KeyError? It looks like that was transformed to InvalidCacheBackendError in the old code.
You can skip the blank lines in these tests as three lines long isn't too difficult too read with spaces.
```suggestion params = self.settings[alias].copy() ```
I don't that we should do this. `BaseBackend` contains only basic methods.
... also we cannot use `User` in the `BaseBackend` so it will be hard to return something consistent.
Please use assertRaisesMessage to verify this is the ValueError we expect.
```suggestion raise InvalidCacheBackendError('Could not find backend {backend!r}: {e}') from e ```
I'd use a name like `assertBackendInSession`.
This is not covered by tests, also raising an exceptions in user-defined functions is not really helpful for users: ```python django.db.utils.OperationalError: user-defined function raised exception ``` I think we should return `None` instead.
here too: pysopenssl --> pyopenssl
Sorry, I had a typo in https://github.com/django/django/pull/12675#discussion_r418965162 -- what I ment to say was: keep `work factor` in safe summary (since it's for display and already translated) and `work_factor` in `decode`. This will also require less changes in the tests.
When there is no `topicTitle`, `categories` will be `[None]` that does not make any sense.
`enumerate` on for range.
No such meta field.
No such meta field.
I guess more of the preparation activities could be moved outside making the tests cleaner: ```suggestion DOCKER_IMAGES_CALLS = [ call( '', ['images', 'quay.io/ansible/centos7-test-container', '--format', '{{json .}}'], capture=True, always=True), call( '', ['images', 'quay.io/ansible/centos7-test-container', '--format', 'json'], capture=True, always=True), ] @pytest.fixture def docker_command_patch_kwargs(docker_images, mocker, request): patch_kwargs = request.param if not patch_kwargs: return mocker.patch( 'ansible_test._internal.docker_util.docker_command', **patch_kwargs ) @pytest.mark.parametrize( ('returned_items_count', 'patched_dc_stdout', dc_calls_num), ( (3, {'return_value': (DOCKER_OUTPUT_MULTIPLE, '')}), (2, {'return_value': (PODMAN_OUTPUT, '')}), (0, {'return_value': ('', '')}), ), indirect=('ansible_module_args', ), ids=('docker JSONL', 'podman JSON sequence', 'empty output'), ) @pytest.mark.usefixtures('docker_command_patch_kwargs') def test_docker_images(returned_items_count, patched_dc_stdout, dc_calls_num, ansible_test): ret = docker_images('', 'quay.io/ansible/centos7-test-container') assert len(ret) == returned_items_count ansible_test._internal.docker_util.docker_command.assert_has_calls( DOCKER_IMAGES_CALLS[:1], ) def test_podman_fallback(ansible_test, docker_images, subprocess_error, mocker): '''Test podman >2 && <2.2 fallback''' cmd = ['docker', 'images', 'quay.io/ansible/centos7-test-container', '--format', '{{json .}}'] docker_command_results = [ subprocess_error(cmd, status=1, stderr='function "json" not defined'), (PODMAN_OUTPUT, ''), ] mocker.patch( 'ansible_test._internal.docker_util.docker_command', side_effect=docker_command_results) ret = docker_images('', 'quay.io/ansible/centos7-test-container') ansible_test._internal.docker_util.docker_command.assert_has_calls(DOCKER_IMAGES_CALLS) assert len(ret) == 2 ```
Here you pass the variable `label`. You have never defined it, though (you defined `ch_label` -- maybe you meant that one?). That's the first of the sanity checks which fails. You might already get that output by running `flake8` on your module.
Don't capture groups you don't use. Unused captured group.
Useless with timestamp available.
Same as above, I'd revert to the simpler version. The main idea behind `decode` for me is a consistent interface where it makes sense, we don't have to bend backwards just to use it.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
`enumerate` on for range.
This should be extracted very first.
Don't capture groups you don't use. Unused captured group.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
No brackets needed.
This is no longer actual.
This is useless at the end.
Useless with timestamp available.
Referring `url` from `url` looks like nonsense. Provide rationale.
Instead, please set `self.index = json.load(f)`. The JSON library uses the `.load` function to stream straight from files without needing to `.read()` first. This also avoids pesky encoding issues with Python2/3.
CI failure due to trailing whitespace (PEP 8 check): ``` 2017-02-08 14:44:54 ERROR: PEP 8: lib/ansible/modules/cloud/amazon/ec2_win_password.py:167:10: W291 trailing whitespace (legacy) ```
use same indent style as previous item
But it must be in this if.
It would be good to wrap this in a try/except botocore.exceptions.ClientError as e
If you use a catchall exception handler like this then you should also provide the traceback like this: ``` python import traceback from ansible.module_utils._text import to_native [...] module.fail_json(msg='Unanticipated error running atomic: %s' % to_native(e), exception=traceback.format_exc()) ```
A message string would good to say that image is not preset or something similar.
It looks like just setting `instance_monitoring` to `true` now won't really do anything, since basic monitoring is on by default and the value of `advanced_instance_monitoring` would default to `False`. This seems confusing.
Running the legacy ec2_asg integration tests that use this module, I think this line should be `if block_device_mapping:` because it doesn't look like it can be None or an empty list. The error: ``` An exception occurred during task execution. To see the full traceback, use -vvv. The error was: Missing required parameter in BlockDeviceMappings[0]: "DeviceName" fatal: [localhost]: FAILED! => {"changed": false, "failed": true, "module_stderr": "Traceback (most recent call last):\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 436, in <module>\n main()\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 430, in main\n create_launch_config(connection, module)\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 326, in create_launch_config\n connection.create_launch_configuration(**launch_config)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 253, in _api_call\n return self._make_api_call(operation_name, kwargs)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 517, in _make_api_call\n api_params, operation_model, context=request_context)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 572, in _convert_to_request_dict\n api_params, operation_model)\n File \"/Library/Python/2.7/site-packages/botocore/validate.py\", line 270, in serialize_to_request\n raise ParamValidationError(report=report.generate_report())\nbotocore.exceptions.ParamValidationError: Parameter validation failed:\nMissing required parameter in BlockDeviceMappings[0]: \"DeviceName\"\n", "module_stdout": "", "msg": "MODULE FAILURE", "rc": 0} ```
add a message or use ``` python self.assertIn('/dev/loop9', lsblk_uuids) self.assertIn('/dev/sda1', lsblk_uuids) ```
Instead, please set `self.index = json.load(f)`. The JSON library uses the `.load` function to stream straight from files without needing to `.read()` first. This also avoids pesky encoding issues with Python2/3.
CI failure due to trailing whitespace (PEP 8 check): ``` 2017-02-08 14:44:54 ERROR: PEP 8: lib/ansible/modules/cloud/amazon/ec2_win_password.py:167:10: W291 trailing whitespace (legacy) ```
If you use a catchall exception handler like this then you should also provide the traceback like this: ``` python import traceback from ansible.module_utils._text import to_native [...] module.fail_json(msg='Unanticipated error running atomic: %s' % to_native(e), exception=traceback.format_exc()) ```
It would be good to wrap this in a try/except botocore.exceptions.ClientError as e
A message string would good to say that image is not preset or something similar.
Can you name this a little more verbosely? I can't unsee "get best"
It looks like just setting `instance_monitoring` to `true` now won't really do anything, since basic monitoring is on by default and the value of `advanced_instance_monitoring` would default to `False`. This seems confusing.
Running the legacy ec2_asg integration tests that use this module, I think this line should be `if block_device_mapping:` because it doesn't look like it can be None or an empty list. The error: ``` An exception occurred during task execution. To see the full traceback, use -vvv. The error was: Missing required parameter in BlockDeviceMappings[0]: "DeviceName" fatal: [localhost]: FAILED! => {"changed": false, "failed": true, "module_stderr": "Traceback (most recent call last):\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 436, in <module>\n main()\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 430, in main\n create_launch_config(connection, module)\n File \"/var/folders/by/k8_fbl593dlctgqmwq5wzl2c0000gn/T/ansible_g_YCzK/ansible_module_ec2_lc.py\", line 326, in create_launch_config\n connection.create_launch_configuration(**launch_config)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 253, in _api_call\n return self._make_api_call(operation_name, kwargs)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 517, in _make_api_call\n api_params, operation_model, context=request_context)\n File \"/Library/Python/2.7/site-packages/botocore/client.py\", line 572, in _convert_to_request_dict\n api_params, operation_model)\n File \"/Library/Python/2.7/site-packages/botocore/validate.py\", line 270, in serialize_to_request\n raise ParamValidationError(report=report.generate_report())\nbotocore.exceptions.ParamValidationError: Parameter validation failed:\nMissing required parameter in BlockDeviceMappings[0]: \"DeviceName\"\n", "module_stdout": "", "msg": "MODULE FAILURE", "rc": 0} ```
`dict((k, v) for k, v in launch_configs[0].items() if k not in ...)` is probably a bit more readable.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Also need `m/alacarta`: ```suggestion _VALID_URL = r'https?://(?:www\.)?rtve\.es/(?P<kind>(?:playz?|(?:m/)?alacarta)/(?:audios|videos)|filmoteca)/[^/]+/[^/]+/(?P<id>\d+)' ```
We should also test the nonexistent time with `is_dst=True` and `is_dst=False`
You might want to specify a different error handler here. The default handler is surrogate_or_replace. When non-utf8 byte sequences are encountered, it either uses surrogateescape if available or replace. This can munge output on python2. Since we're sending this to json.loads (rather than displaying it to the user) it might be better to use surrogate_or_strict as the error handler here.
If something on the managed machine messes up, this could be in some other encoding. Also, there's always the chance that the way the client machine handles non-utf-8 bytes will lead to those bytes coming through unescaped. Basically, the connection plugin is a boundary between Ansible and another system. So we need to be a little more paranoid in sanitizing our data in this area.
Connection plugins return bytes for stdout and stderr (the callers are responsible for transforming to text or not). So this needs to remain with to_bytes().
I remember having to add the to_bytes()... it could be that an old version of winrm returned text here. I do not remember for sure if that was fixed in the lowest winrm version that we support. Two other notes: 1) This needs to return bytes on python3 as well as python2 2) to_bytes() is a no-op (overhead of the function call and a couple conditionals but no encoding is done) if the string is already a byte string.
I'm not about the URL. **Do not touch** the global `std_headers`.
Due to hanging indent, `).first()` should be on the next line.
The convention in other connection plugins seems to be to raise an `AnsibleError` instead of `IOError`.
**Never ever** use `eval` on data you don't control.
Also need `m/alacarta`: ```suggestion _VALID_URL = r'https?://(?:www\.)?rtve\.es/(?P<kind>(?:playz?|(?:m/)?alacarta)/(?:audios|videos)|filmoteca)/[^/]+/[^/]+/(?P<id>\d+)' ```
We should also test the nonexistent time with `is_dst=True` and `is_dst=False`
You might want to specify a different error handler here. The default handler is surrogate_or_replace. When non-utf8 byte sequences are encountered, it either uses surrogateescape if available or replace. This can munge output on python2. Since we're sending this to json.loads (rather than displaying it to the user) it might be better to use surrogate_or_strict as the error handler here.
If something on the managed machine messes up, this could be in some other encoding. Also, there's always the chance that the way the client machine handles non-utf-8 bytes will lead to those bytes coming through unescaped. Basically, the connection plugin is a boundary between Ansible and another system. So we need to be a little more paranoid in sanitizing our data in this area.
Connection plugins return bytes for stdout and stderr (the callers are responsible for transforming to text or not). So this needs to remain with to_bytes().
I remember having to add the to_bytes()... it could be that an old version of winrm returned text here. I do not remember for sure if that was fixed in the lowest winrm version that we support. Two other notes: 1) This needs to return bytes on python3 as well as python2 2) to_bytes() is a no-op (overhead of the function call and a couple conditionals but no encoding is done) if the string is already a byte string.
I'm not about the URL. **Do not touch** the global `std_headers`.
Due to hanging indent, `).first()` should be on the next line.
The convention in other connection plugins seems to be to raise an `AnsibleError` instead of `IOError`.
**Never ever** use `eval` on data you don't control.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
Not strictly necessary as the default for parameter is that they're not required.
Please remove this. It is wrong. Don't add a default in this case.
required=False is the default, so you can omit that, same for any of the following. `name_regex=dict()`.
Should have been module.fail_json
This can be updated to use AnsibleAWSModule from ansible.module_utils.aws.core. Then you will be able to remove the check for HAS_BOTO3, the exception handling around the client, and the use of get_aws_connection_info and boto3_conn (as well as any of the imports for those things) and do `client = module.client('emr')` instead.
according to doc spec, there should be an alias: ~~~diff - ip=dict(), + ip=dict(aliases=['network']), ~~~
Not sure why we would return the provided information. The user provided it, so it already knows the values.
Since we expect the name to start with 'http://' or 'https://' I would so this: ```python if name.startswith('http://') or name.startswith('https://'): ... ```
I think something like this will be better (please test it to ensure you do get an error if the param/ENV isn't set) `api_token=dict(fallback=(env_fallback, ['CLOUDSCALE_API_TOKEN']), no_log=True, required=True),` Then you can delete ``` api_token = module.params['api_token'] or os.environ.get('CLOUDSCALE_API_TOKEN') if not api_token: module.fail_json... ```
Again, this could be a class level attribute.
Do we need to return module_path? I don't see that we modify it anywhere and the calling code already has it.
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Use built-in method? Also, should it crash (raise) if `json.loads()` fails, or just return an empty dict again? ```suggestion try: return self._download_json( url, song_id, data=data.encode('ascii'), headers=headers) except ExtractorError as e: if type(e.cause) in (ValueError, TypeError): # JSON load failure raise except Exception: pass return {} ```
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
Again: ```suggestion ['{0}={1}'.format(k, v if v is not None else 'undefined') ```
As this may need to be updated, consider 1. make the value a class var `_USER_AGENT` 2. import utils.std_headers and let non-null `std_headers['User_Agent']` override this value, so that `--user-agent ... ` or `--add-headers "User-Agent: ..."` take precedence (allows cli work-around if the site needs a new UA).
Again: ```suggestion return ('params={0}'.format(encrypted_params), headers) ```
Condition the value? ```suggestion bitrate = int_or_none(details.get('bitrate')) or 999000 ```
flake8 complains about missing spaces around `*`
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
with -> width
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
width, height, and offset
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
Ditto. ```suggestion # Check empty result with non-execution optimization. with self.assertNumQueries(0): ```
It's better to use `compat_urllib_parse.urlencode` in this line. One of the benefits is that there is no need to escape ep in `generate_ep()` anymore.
Gotcha, okay I think this is acceptable.
I think all the calls to `render()` can be removed (it worked for me in this test at least)
This should go to `announce@apache.org` as well, that's actually the most critical one as that makes it "official".
Please chop unnecessary blank lines.
```not (foo is None)``` => ```foo is not None```
I double people will update this script correctly. We can only hope, that release managers verify this before sending the email... As an alternative, we can also wildcard this, and let release manger insert those manually. Similar to `<DETAILS OF THE CHANGES>` above.
Chop this docstring.
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
This should go to `announce@apache.org` as well, that's actually the most critical one as that makes it "official".
comma after tuple
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
plz don't use EOL escaping, wrap with braces instead.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
Match the error message
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
I needed to add a `set_source_expressions` method which sets `self.subquery = source_expressions` here to get relabelling to work.
```suggestion return '-' + value if neg else value ```
```suggestion return '-' + value if neg else int(value) ```
the shell itself would have done it before. but might have done it slightly differently.
Okay, looking further down, I see that you're just moving this around though...
I don't think this is safe. If someone is calling this function without unsafe_shell they probably have not quoted the arguments to prevent this sort of expansion.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
What if `default` is not a constant but a field reference? e.g. `F('integer')`
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
I needed to add a `set_source_expressions` method which sets `self.subquery = source_expressions` here to get relabelling to work.
Notice that, conceptually, the `set(a) | set(b)` operation is different than `a + b`. The former performs set union meaning that duplicates are removed, whereas the latter is a simple concatenation with the possibility for the same item to appear twice. If these are considered "set operations", then the else part should be replaced by something like: ``` c = a + filter(lambda x: x not in a, b) ```
the shell itself would have done it before. but might have done it slightly differently.
Okay, looking further down, I see that you're just moving this around though...
I don't think this is safe. If someone is calling this function without unsafe_shell they probably have not quoted the arguments to prevent this sort of expansion.
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
I needed to add a `set_source_expressions` method which sets `self.subquery = source_expressions` here to get relabelling to work.
Notice that, conceptually, the `set(a) | set(b)` operation is different than `a + b`. The former performs set union meaning that duplicates are removed, whereas the latter is a simple concatenation with the possibility for the same item to appear twice. If these are considered "set operations", then the else part should be replaced by something like: ``` c = a + filter(lambda x: x not in a, b) ```
the shell itself would have done it before. but might have done it slightly differently.
Okay, looking further down, I see that you're just moving this around though...
I don't think this is safe. If someone is calling this function without unsafe_shell they probably have not quoted the arguments to prevent this sort of expansion.
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
are you sure about this point of "consist of only A-z0-9-_=", as opposed to those characters may not appear in the separator? (not too familiar with this restriction myself) I don't know that the complexity of a `setter` is necessary. I'd just modify the test to initialize a `Signer` each time.
I think this should be: ``` python values = map(lambda x: x.strip(), tags['Value'].split(',')) ``` Basically the same as it was before this change but with `tags['Value']` instead.
may be blind, but think this will never get fired if v is empty. you create an empty list, which will be skipped in the for loop.
If I got it right, resourse unpacking happens every time `tr` is called. Have you measured the overhead imposed by this approach? Probably it would be better to unpack it once to temp dir on start and cleanup on exit.
Could you clarify what's happening by adding an example 2-line CSV file? I think that would improve the docs.
Probably better to write this as ``` python if self.args.refresh_cache or not self.is_cache_valid(): self.update_cache() ```
No need for this to be a private method. rename to get_inventory().
This method doesn't seem to do much. I would merge it into the run() method. * read_settings and parse_cli_args don't seem related. They don't operate on the same instance attributes, for instance. * It's a bit funny to have a private method (_read_settings()) calling public methods (read_settings() and parse_cli_args()) that aren't usable by anything else. * If this is merged into run(), it will only add one more line there and will remove 5 lines overall.
What's the point of this? `set-cookie` headers are handled internally.
This should probably be the last method in the class.
Use the context manager version of `open()` so you don't need to worry about closing it manually.
I envisioned something like this: ``` python content = None with open(path, read_mode) as f: try: content = f.read() except UnicodeDecodeError: # If mimetype suggests the file is text but it's actually binary, # read() will raise a UnicodeDecodeError on Python 3. pass # If the previous read in text mode failed, try binary mode. if content is None: with open(path, 'rb') as f: content = f.read() mimetype = DEFAULT_ATTACHMENT_MIME_TYPE ```
Nitpick, you can use a list comprehension here instead of calling list on a generator expression.
Maybe rename this to substitute_crypto_req since it's replacing the one from requirements.txt with the one we autodetect rather than simply appending.
We should avoid lambdas and filters most of the time. Use list comprehensions and generator expressions instead. It's both faster and more pythonic to use those idioms.
Style note, I finally convinced people a year or so ago that ```{}``` is superior to ```dict()```. So new code should use the literal syntax.
Kind of a nitpick since this is setup.py... but API-wise I prefer returning a small dict with the dynamic values and letting the caller merge it/copy the static dict if necessary.
I think we should just let this exception propagate. We aren't adding any value by catching it here (and might be removing value if the traceback were useful). This isn't code in ansible itself so we don't have to create a nicer error message either.
chop "should" (just state the behavior)
This fails for me: ``` AssertionError: <MagicMock name='WatchmanReloader()' id='140033332456584'> is not an instance of <class 'django.utils.autoreload.StatReloader'> ``` perhaps that's the failure that Jenkins encountered.
```suggestion ({b'str'.encode(DEFAULT_ENCODING): b'str'.encode(DEFAULT_ENCODING)}, {u'str': u'str'}), ```
I think these should be `is True` since the `.assertTrue` being replaced would fail if on something like `"foo"` which is truthy. So the `assert "foo"` would pass where `self.assertTrue("foo")` wouldn't.
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
since the brackets on their own lines add whitespace, I think you could omit the blank line between queries/assertions in most cases to make this file a bit shorter.
This test should include updating a decimal field that includes decimal points: ``` Value(Decimal("1.1")) ```
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
If you have time, we also try to include on trailing comma on the last kwarg so if more items are added later, we don't have to modify that line again.
move ) to next line for consistency
you could also write this as: h_item = search_obj_in_list(w_item['member'], h, key='member') or {}
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
Similarly, ```if tc['skip'].get('i')```
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
use a single line or use hanging indent (we avoid non-4 space indents)
you can use `state` to avoid the 'or' to the user
single line as above
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
I think all the calls to `render()` can be removed (it worked for me in this test at least)
Gotcha, okay I think this is acceptable.
You can find it as the leader of the offset partition in ZK, though you will need to have a ZK connection to do that.
[...] remove matching rows **for rel_b**.
Hmm, I thought we'd have `LATEST_0_10_1` and `LATEST_0_10_0` instead of `LATEST_0_10` (which doesn't make sense because we have multiple feature releases in the `0_10` line.
Ideally it's best if we can avoid `sleep` calls Is there a reason why you can't use `stop_node` without sleeping? This should block until the process is gone and the background thread has finished processing output from the consumer
Hopefully we can remove this `sleep` as well
Nit: maybe there should be no default for `should_fail` since we always pass a value.
"if all childs refer same children", I'm not too sure what it means. Also maybe the `num_childs` var could be changed to `num_children`.
Probably worth extracting this variable. It seems like it would be easy to update the one in `wait_until` (maybe because we have transient failures) while forgetting to update this one (causing confusing logs).
I think it's fine. Separate test cases are normally a good thing since they can fail individually.
What concerns me is the implication (not directly stated in the docs, at least that I can find) that you can't add/modify a Lightsail instance's keypair after creation (short of ssh'ing to the instance with an existing key and editing auth keys by hand). **If** this is correct, I feel like we should anticipate users having a problem here and make keypair a required option for create actions. It may not be boto-y, but it's still something we can easily anticipate and make nicer for our users. If that's not correct and I'm just missing something about Lightsail key management, then maybe keypair management or a lightsail_key module would be good future improvements (but outside the scope of this PR).
Personal niggle: boto3 doesn't list keyPairName as a 'required' argument. I have some use cases where I bake the keys into the AMI and don't want to pass an additional key to the instances...
I don't think Lightsail allows custom AMIs, at least from what I've seen in the docs and prodding at the console a bit. There is a question of if we should have a default to use the region default key when state=present instead of requiring a user specification, generally the way lightsail does keys is different from the way ec2 does keys though and I'm still familiarizing myself with the differences.
I was going off my knowledge of EC2, I've not worked with Lightsail. Fair enough if you can't use custom AMIs (today). At the same time, adding additional 'requirements' on top of boto3's only risks breaking use cases you might not have thought of.
`del` is a builtin, not a function. These parens don't have to be here
This can instead be `continue` and let the `else` unnest.
"its equally-named replacements" is confusing, since "it" seems to refer to this module, rather than the individual classes that used to be in it. I would re-word this error as: ``` django.contrib.contenttypes.generic is deprecated and will be removed in Django 1.9. Its contents have been moved to the fields, forms, and admin submodules of django.contrib.contenttypes. ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Why not use a list comprehension here? It's probably more efficient: `return [self.sanitize_keys(i) for i in data]`
Instead of `list`, use `Sequence`. Similarly, replace `dict` above by `Mapping`. (You'll need `from ansible.module_utils.common._collections_compat import Mapping, Sequence` to be able to do that.)
may be blind, but think this will never get fired if v is empty. you create an empty list, which will be skipped in the for loop.
If the values can contain a colon we'll want to change the second line.split() so that we don't truncate the value. Something like this: ``` python smbios_dict = dict((line.split(':')[0].strip(), line.split(':', 1)[1].strip()) for line in out.splitlines() if ':' in line ) ```
Please use more descriptive variable names for at least `_h` and `_k`, and probably `k` and `v` also. I think it's fine to use some single letter variables in list/dict comprehensions or simple loops, but once there is some additional logic I think readability is improved by using longer names. Also, it should be fine to reassign the lowercase version of `k` (or whatever it ends up being) to itself since the original value isn't used anywhere else and the type hasn't changed.
Upon further reflection, I think these three lines along with `check_mutually_exclusive` are erroneous for subspecs. We don't currently have a way to specify `required_if/one_of/together` or `mutually_exclusive` for subspecs, so in every case *but* the networking "aggregate" thing, this is severely broken (as you'd be making nonsensical checks in most cases)
`self._options_context` stores the context of parent options in case of nested suboptions. As the `elif` block does not have a recursive call to `_handle_options` context handling is not required here. However, context information can be added as part of error message in case this argument is part of nested sub-options (similar to other error messages in sub-option handling).
Wouldn't this approach fix the slicable fields to just these types? A mixin along with `slicable = True/False` on `F` may be an approach. Besides, I believe the policy is to keep `django.contrib.postgres` as separate as possible.
Is there a reason to sort this? Since we're just putting it into a set (to uniquify the list I assume) it doesn't seem necessary to sort. Since we're just iterating over sd_instances, a frozenset is more appropriate than a set.
I think `get_internal_type` is better to use.
chop trailing newline (check code with flake8 to see a couple other minor formatting things)
Here I think we just should just default to `json.dumps` if no encoder is specified. No need for an extra setting.
I can see a decent reason to use the `DjangoJSONEncoder` as the default as it gives us `Decimal` and `datetime` support for free. These are very common use cases for django sites (especially datetime!).
`always_text` is gone.
It's not Meraki dashboard, but Meraki API that didn't return JSON compatible data. We have to get rid of any relation to Dashboard IMO. Dashboard is the GUI.
So, I think on failure this output would be useful as well. So maybe it's better to create a new dictionary `debug_result` which you can then provide to fail_json calls related to fetch_url failing, and you can merge it with result before exit_json. ```python debug_result = dict( url=url, method=module.params['method'].upper(), headers=headers, payload=payload, ) ``` Then add the response and status: ```python debug_result['status'] = info['status'] debug_result['response'] = resp.read() ``` Then call fail_json using it: ```python module.fail_json(msg=str(e), **debug_result) ``` And exit_json: ```python if module.params['output_level'] == 'debug': result.update(debug_result) module.exit_json(**result) ```
Ansible 2.7 dropped support for Py2.6
Is this something we still need to specify? We dropped support for python < 2.6 several releases ago now.
Presumably you want `It *will* always use`, or something to the effect
Can the native ElementTree api be used here. ```from xml.etree.ElementTree import fromstring``` This will reduce module dependency on `lxml`.
Code duplication 80-86, 89-94.
Try except blocks should be as small as possible. In this case, if is only needed around the calls to modify and delete. Perhaps put the try except around the if state conditional to limit the statements that it surrounds
`call_command()` raises an exception so these lines are unreachable.
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
don't do this by default, set from cli `self._options` does not exist or is None
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
its new in 2.4 so it doesn't apply to older versions
this whole section is not needed, just use `self._plugin_options[<option name>]`
missing ini option for host
~you don't seem to use 'cli' why import it? also the base class does this already.~ v1 compatibility it seems.
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This is not fixed.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
I think we don't need it. but lets @felixxm decide about it. Thanks for the patch :+1:
Do we really need this test? seems it doesn't relate to this change.
You can skip the blank lines in these tests as three lines long isn't too difficult too read with spaces.
Please use assertRaisesMessage to verify this is the ValueError we expect.
I'd use a name like `assertBackendInSession`.
> Is there any specific reason why we would prefer using the operation in this case? Yes, because we have it. Using a RAW SQL is the last option, we're developing the ORM in order not to use them.
Why we're adding a constraint manually and not with `AddConstraintNotValid()`? Also, please use hanging indentation.
This message is different on PostgreSQL 13+, it should be enough to check that a constraint name is in the message.
redundant, remove ```suggestion ```
```suggestion query=dict(type='list', elements='str'), ```
This seems suboptimal for a number of reasons: - It's doing `len(text)` twice. - If `len(text) == length` it's slicing unnecessarily instead of returning `text`. - Using `fill_text * length` is too much as we'll only ever need `lengrh - len(text)`. Maybe the following would be better: ```suggestion delta = length - len(text) if delta == 0: return text if delta < 0: return text[:length] return (fill_text * delta)[:delta] + text ``` If `fill_text` is more than one character, `fill_text * delta` still generates too much, so we still need to `[:delta]`. Offhand, I'm not sure I can think of anything better that wouldn't be slower.
> I think this isn't a great trade-off to make, the special branch for "is the length exactlyt the same" will cost more than it saves across a range of different input string types. Only a few will match it. Yeah, I wondered whether the exact length check might be detrimental. > Regardless this is a rarely used DB function, this is too much time spent optimizing it already :) ðŸ˜„ Yup. Happy with your solution.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
This is not fixed.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
Inline everything used only once.
Sorry, I had a typo in https://github.com/django/django/pull/12675#discussion_r418965162 -- what I ment to say was: keep `work factor` in safe summary (since it's for display and already translated) and `work_factor` in `decode`. This will also require less changes in the tests.
`enumerate` on for range.
Same as above, I'd revert to the simpler version. The main idea behind `decode` for me is a consistent interface where it makes sense, we don't have to bend backwards just to use it.
Don't capture groups you don't use. Unused captured group.
Useless with timestamp available.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
No brackets needed.
This is no longer actual.
This is useless at the end.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Please revert unrelated cosmetic changes to keep the diff clean.
Yes, please rebase the branch and remove the try/fail pattern as done in 6729b96d8a15048b2295c916c5b881a59d9417a0. If you're unfamiliar with the process you might find https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/working-with-git/#rebasing-branches helpful.
this should be in `finally` just in case the commands before throw an exception
It should be `if self._module.params.get('sparse') is not None`, because if `sparse` is `False` it won't send `sparse=False`
@chrisvanheuveln and I chatted about this. No further changes needed here since we avoid the 400 error with the order change and the command is blocking.
I think this option is deprecated - https://www.vmware.com/support/developer/converter-sdk/conv51_apireference/vim.vm.RelocateSpec.Transformation.html
Single quotes please. Also, can we use `size` instead of `dims` for consistency with the other tests? ```suggestion size = images.get_image_dimensions('missing.png') self.assertEqual(size, (None, None)) ```
This seems like it would break galaxy which needed expand_paths
Never mind, I now see the change to galaxy further down.
Once we have a client, I don't think we need to handle this exception again.
Yes, end users would only ever read this object, hence the lack of a friendly `Builder`. I think it's OK to leave as-is.
This loop seems strange to say the least - all but last result are overwritten by the looping.
No need to construct a new `dict` and call `dict.update()` here. Also the key ought to exist in the map or something has gone drastically wrong, so no need to use `dict.get()`. ```python return_dict[key_map[key]] = value ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Plz also use `match` arg here
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
```suggestion vault_data(), ```
```suggestion assert ansible_json_encoder.default(test_input) == {'__ansible_vault': expected} ```
Same as above: unnecessary fixture test.
```suggestion assert ansible_json_encoder.default(test_input) == expected ```
If this test won't be implemented it should be removed.
Check for the exception in the test body instead of using a decorator. Also, since you're testing for an exception, you only need to define the input and not compare against expected: ```suggestion with pytest.raises(TypeError): ansible_json_encoder.default(test_input) ```
Yeah, so none of the tests failed if I forced either the multiple or single argument form of the function. That can't be right.
I'd split this line in two
```suggestion query=dict(type='list', elements='str'), ```
Please remove this, `AnsibleModule` already prevents this.
Would it maybe be better to only check the beginning of the query? Assume that the query is `SELECT * FROM USER_UPDATES`; in that case, because `INSERT` appears in `q.upper()` and `cursor.rowcount > 0` (assuming the table isn't empty), the module will say something was changed.
How about ```suggestion q = q.lstrip()[0:max_keyword_len].upper() ``` and removing the `.upper()` calls below? Will be more efficient :)
Why we're adding a constraint manually and not with `AddConstraintNotValid()`? Also, please use hanging indentation.
> Is there any specific reason why we would prefer using the operation in this case? Yes, because we have it. Using a RAW SQL is the last option, we're developing the ORM in order not to use them.
redundant, remove ```suggestion ```
This message is different on PostgreSQL 13+, it should be enough to check that a constraint name is in the message.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Similarly, ```if tc['skip'].get('i')```
Note: this is usually expressed as: ``` python if not full_version: ```
`self.real_apps` is always a set, `set()` is unnecessary (here and in many other lines).
To catch this as a `IntegrityError` on Oracle we need to add `ORA-00001` to the wrapper: ```diff --git a/django/db/backends/oracle/base.py b/django/db/backends/oracle/base.py index 4d2f5b51f1..3908aebda1 100644 --- a/django/db/backends/oracle/base.py +++ b/django/db/backends/oracle/base.py @@ -73,7 +73,7 @@ def wrap_oracle_errors(): # _C00102056) violated - parent key not found' # Convert that case to Django's IntegrityError exception. x = e.args[0] - if hasattr(x, 'code') and hasattr(x, 'message') and x.code == 2091 and 'ORA-02291' in x.message: + if hasattr(x, 'code') and hasattr(x, 'message') and x.code == 2091 and ('ORA-02291' in x.message or 'ORA-00001' in x.message): raise IntegrityError(*tuple(e.args)) raise ```
I'd suggest this for the indentation: ```python return [ force_text( capi.get_field_name(capi.get_field_defn(self._layer._ldefn, i)), self.encoding, strings_only=True, ) for i in range(self.num_fields) ] ``` Any concern about having no tests for the `self.encoding, strings_only=True` bit? I'm not sure we would expect non-string values in the first place? Also, at least on my system, there are no failures with it removed here or in `layer_name` above.
I don't see a need for string interpolation in cases like this.
Please use shorter name (less than 30 chars), e.g. `deferrable_pink_constraint`.
UnqiueConstraint is suggested instead of unique_togather in newer versions [2.2+]
```suggestion type: list suboptions: ```
Good catch, I will remove it before final squash.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
Similarly, ```if tc['skip'].get('i')```
I don't see any need for this attribute.
Can we display a similar message with the test results? It can be helpful when you have plenty of errors, .e.g. ``` ... FAILED (errors=206, skipped=1094, expected failures=4) Used shuffle seed: 556738431 (generated) ```
You have some unmerged lines here
Same here, not following order `(value, expected)`
(And round-tripping of the messages is already tested in other tests)
Same here, not following order `(value, expected)`
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
Guess it's better to use `self.assertGreater(len(para), 0)` instead
Same here, not following order `(value, expected)`
Docstring shouldn't have a trailing space. Docstring sentences should end with a period.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
variable names like ```json_obj``` is not good. It doesn't describe what's the purpose of this variable.
For long query strings, it's better to use ```query``` parameter here.
```not (foo is None)``` => ```foo is not None```
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
I don't see any need for this attribute.
with -> width
Can we display a similar message with the test results? It can be helpful when you have plenty of errors, .e.g. ``` ... FAILED (errors=206, skipped=1094, expected failures=4) Used shuffle seed: 556738431 (generated) ```
width, height, and offset
comma after tuple
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
You have some unmerged lines here
Casting `int` to `int` is not necessary.
I think it will be more readable to keep `int` and `slice` in separate branches, e.g.: ```python def __init__(self, f_obj, slice_obj): if isinstance(slice_obj, int): if slice_obj < 0: raise ValueError('Negative indexing is not supported.') self.low = slice_obj self.length = 1 elif isinstance(slice_obj, slice): if ( (slice_obj.start is not None and slice_obj.start < 0) or (slice_obj.stop is not None and slice_obj.stop < 0) ): raise ValueError('Negative indexing is not supported.') if slice_obj.step is not None: raise ValueError('Step argument is not supported.') self.low = 1 if slice_obj.start is None else int(slice_obj.start) + 1 self.length = None if slice_obj.stop is None else int(slice_obj.stop) - self.low + 1 else: raise TypeError('Argument to slice must be either int or slice instance.') self.expression = f_obj ```
We allow up to 119 characters, so this doesn't need to be wrapped. ```suggestion def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False): ```
Please use f-strings as Python 3.6+ is now the requirement More information is available including some benchmarks. https://cito.github.io/blog/f-strings/
We can remove this check after fixing the `Field.slice_expression()`.
Chop blank line.
Close, but not quite! ðŸ˜‰ ```suggestion def _sqlite_log(base, x): if base is None or x is None: return None # Arguments reversed to match SQL standard. return log(x, base) ```
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
it should also check if it can write there
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
I think [`_extract_m3u8_formats`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py#L847) does the work better. If the existing method does not fit the need, feel free to modify it.
> Speaking of which, I should submit a PR to add Python 3.5 to tox.ini and .travis.yml #12627.
The code is the same, just the data differs, so I think it would be clearer to move the loop outside the if. ``` python output_data = self.py3_output_data if sys.version_info[0] >= 3 else self.py2_output_data for msg, param in output_data.items(): ... ```
Yeah, a name can either be a global or a local in a scope, and Python determines that during compilation time by looking for certain statement types (such as assignments or imports). You could do this at the top level: ``` python try: # Python 3 from importlib import reload except ImportError: # Python 2 pass ``` or use `six.moves.reload_module`. (Hm, [importlib.reload](https://docs.python.org/3/library/importlib.html#importlib.reload) appeared in Python 3.4, before that it was [imp.reload](https://docs.python.org/3/library/imp.html#imp.reload). I suppose it's fine to require Python 3.4+ for Ansible -- people who don't have it can fall back to 2.7.)
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
``` for i, video_url in enumerate(video_urls): ```
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
For some reason this causes ``` {aliases: [attr]} {choices: [all, 'no', none, safe, urllib2, 'yes']} {default: [200]} ``` while non-sequence values for `default` yield ``` default: 30 ``` without the `{...}`.
`is` tests identity in python so if statement always produces a change. We need to use `!=` here to compare equality instead of identity.
This would be a nice place to use tuple expansion `action, index = index_of_matching_route(....)`
Yeah, you'd need `return None, None` at the end of index_of_matching_route()
surround only the part that will threw the exception.
Missing conditional on execution of `main`: ```python if __name__ == '__main__': main() ```
```suggestion for _ in range(3): ```
Use the [boto3 exception guidelines](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/amazon/GUIDELINES.md#boto3-2)
need to catch BotoCoreError here too.
`InvalidInternetID` doesn't seem to exist according to botocore's source
```suggestion (u'1', u'PB', 1125899906842624), (u'1E', 1152921504606846976), (u'1EX', 1152921504606846976), (u'1Z', 1180591620717411303424), (u'1ZB', 1180591620717411303424), (u'1Y', 1208925819614629174706176), (u'1YB', 1208925819614629174706176), ```
last loaded wins, but iirc, we reverse search on handlers list
I know, was just wondering if it's intended that it works that way.
Match the error message
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
Having a class for just one test method is unnecessary.
You're checking two separate properties here. This should be in a separate test.
here too: pysopenssl --> pyopenssl
plz don't use EOL escaping, wrap with braces instead.
```suggestion ({b'str'.encode(DEFAULT_ENCODING): b'str'.encode(DEFAULT_ENCODING)}, {u'str': u'str'}), ```
For some reason this causes ``` {aliases: [attr]} {choices: [all, 'no', none, safe, urllib2, 'yes']} {default: [200]} ``` while non-sequence values for `default` yield ``` default: 30 ``` without the `{...}`.
`is` tests identity in python so if statement always produces a change. We need to use `!=` here to compare equality instead of identity.
This would be a nice place to use tuple expansion `action, index = index_of_matching_route(....)`
Yeah, you'd need `return None, None` at the end of index_of_matching_route()
surround only the part that will threw the exception.
Missing conditional on execution of `main`: ```python if __name__ == '__main__': main() ```
```suggestion for _ in range(3): ```
Use the [boto3 exception guidelines](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/amazon/GUIDELINES.md#boto3-2)
need to catch BotoCoreError here too.
`InvalidInternetID` doesn't seem to exist according to botocore's source
`return '%s-%s-%s' % (y or 0, m or 0, d or 0)` can be moved here.
We should pass `using` from the queryset ```suggestion ct_id = self.get_content_type(model=model_cls, using=ct_queryset.db).pk ```
`if it encounter` => `if it encounters`
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
`mentionned` => `mentioned`
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Drop the `as e` since it's not used, and is not compatible with python 2.4.
Another map that can be a list comprehension: ``` content = [x.strip() for x in m.group(3).split(" and ")] ```
Instead of including a stacktrace in the message, better to put it into the exception field: ``` module.fail_json(msg="Xpath %s causes a failure: %s\n -- tree is %s" % (xpath, to_native(e), etree.tostring(tree, pretty_print=True)), exception=traceback.format_exc()) ```
Instead of child.encode(), use to_bytes(child, errors='surrogate_or_strict"). to_bytes() protect against calling encode() on something that is already a byte string. (calling encode on a byte string can sometimes traceback).
Not necessary but this map and the one on 690 are better written as: ``` python elif LooseVersion('.'.join(to_native(ver_field) for ver_field in etree.LXML_VERSION)) < LooseVersion('2.3.0'):
This applies to the other functions where a list is the default value as well.
Shouldn't use a mutable container as default arg in python. If you ever add to the container you then find that you're sharing a single opy of that container everytime the function in python. You can do one of these two instead: ``` python # If you don't need to add to the container now. This just prevents doing something stupid when the code is changed in the future. def finish(module, tree, xpath, namespaces, changed=False, msg="", hitcount=0, matches=tuple()): # if matches needs to be a mutable container. This creates a fresh list everytime the function is called. def finish(module, tree, xpath, namespaces, changed=False, msg="", hitcount=0, matches=None): if matches is None: matches = [] ```
This needs to be 'rb' so that it's bytes just like the BytesIO above.
In general, it's better to use``` collections.MutableMapping``` in isinstance as that allows duck-typing.
Not necessary but exception handling (since this is ansible-2.4+) cna be written as: ``` except etree.XMLSyntaxError as e: module.fail_json(msg="Error while parsing path: %s" % to_native(e)) ```
is it the intention to have `getCurrentStepKey` return the "NEXT_*_SETTING", while there exists a "CURRENT_*_SETTING" that can be misunderstood to be just that, the current setting? seems like it is more a "previous" setting
oh, woops. thought I counted right. sry
```suggestion 'url': 'https://learning.oreilly.com/learning-paths/learning-path-python/9781788996396', ``` I assume it would be good to include one link starting with "learning.oreilly.com" in tests.
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
So if I update some parameter+ change state to running, it won't start, IIUC
Same here about multi-line toString methods
Here again I think we should use `builder.timeField` to handle this
Can you make this non-pretty, it's always weird when you log things and then end up being multi-line
For human readable-ness, an additional field should be added, we shouldn't replace the field with a human readable version. You should be able to do ```java builder.timeField("modified_millis", "modified", modifiedDate); ``` (replacing the field names with the fields we want to use) and then you don't have to check the human readable flag yourself
should this check be in the same place where we make the actual transition? then it doesn't look a `LIFECYCLE_FORCED_PHASE` would have to be set as a signal back to this execution.
This can be a separate ticket, but this message probably needs to be revised now that migrations are compulsory for all apps in 1.9.
Migrations plans with both forwards and backwards migrations are not supported.
I don't like how this entire if block looks. Is `plan` allowed to be `None` considering the check/assignment on line 73 and 74? I'd prefer to call out that it can not be forwards and backwards by doing something like: ``` if all_forwards == all_backards: raise.. # this may not be correct if `plan` can be `None` elif all_forwards: migrate_forwards() else: migrate_backwards() ``` I feel this highlights the invariant, and avoids potential problems of misreading the bool combinations in the original code.
That code is marginally longer, but I think it's clearer. I'll leave it up to what you think is best though.
Please format this like this: ```python skip_lines = [ '+----------------------------------------------------------+', ' Available Repositories in /etc/yum.repos.d/redhat.repo' ] ```
Instead of child.encode(), use to_bytes(child, errors='surrogate_or_strict"). to_bytes() protect against calling encode() on something that is already a byte string. (calling encode on a byte string can sometimes traceback).
Instead of including a stacktrace in the message, better to put it into the exception field: ``` module.fail_json(msg="Xpath %s causes a failure: %s\n -- tree is %s" % (xpath, to_native(e), etree.tostring(tree, pretty_print=True)), exception=traceback.format_exc()) ```
This applies to the other functions where a list is the default value as well.
Shouldn't use a mutable container as default arg in python. If you ever add to the container you then find that you're sharing a single opy of that container everytime the function in python. You can do one of these two instead: ``` python # If you don't need to add to the container now. This just prevents doing something stupid when the code is changed in the future. def finish(module, tree, xpath, namespaces, changed=False, msg="", hitcount=0, matches=tuple()): # if matches needs to be a mutable container. This creates a fresh list everytime the function is called. def finish(module, tree, xpath, namespaces, changed=False, msg="", hitcount=0, matches=None): if matches is None: matches = [] ```
In general, it's better to use``` collections.MutableMapping``` in isinstance as that allows duck-typing.
should this be super()
`None` as a default value to `get()` is not required here. I would also just do `if method` since `None` is falsey it's really a nitpick.
I believe this was meant to be a `references` check instead of a `reduce` check? Since `reduce` returns operations, not a boolean. Something like "`not op.references(other)`" so that we can ensure that `other` can properly be pulled forwards
Ah I see. I wasn't aware of the different signatures available for this.
I don't think this is safe. If someone is calling this function without unsafe_shell they probably have not quoted the arguments to prevent this sort of expansion.
Okay, looking further down, I see that you're just moving this around though...
the shell itself would have done it before. but might have done it slightly differently.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
As discussed on IRC: no.
errors should go to stderr, you can just use sys.exit('message") to do both in one statement
This can be a separate ticket, but this message probably needs to be revised now that migrations are compulsory for all apps in 1.9.
Migrations plans with both forwards and backwards migrations are not supported.
I don't like how this entire if block looks. Is `plan` allowed to be `None` considering the check/assignment on line 73 and 74? I'd prefer to call out that it can not be forwards and backwards by doing something like: ``` if all_forwards == all_backards: raise.. # this may not be correct if `plan` can be `None` elif all_forwards: migrate_forwards() else: migrate_backwards() ``` I feel this highlights the invariant, and avoids potential problems of misreading the bool combinations in the original code.
That code is marginally longer, but I think it's clearer. I'll leave it up to what you think is best though.
Please format this like this: ```python skip_lines = [ '+----------------------------------------------------------+', ' Available Repositories in /etc/yum.repos.d/redhat.repo' ] ```
Instead of child.encode(), use to_bytes(child, errors='surrogate_or_strict"). to_bytes() protect against calling encode() on something that is already a byte string. (calling encode on a byte string can sometimes traceback).
Instead of including a stacktrace in the message, better to put it into the exception field: ``` module.fail_json(msg="Xpath %s causes a failure: %s\n -- tree is %s" % (xpath, to_native(e), etree.tostring(tree, pretty_print=True)), exception=traceback.format_exc()) ```
This applies to the other functions where a list is the default value as well.
Shouldn't use a mutable container as default arg in python. If you ever add to the container you then find that you're sharing a single opy of that container everytime the function in python. You can do one of these two instead: ``` python # If you don't need to add to the container now. This just prevents doing something stupid when the code is changed in the future. def finish(module, tree, xpath, namespaces, changed=False, msg="", hitcount=0, matches=tuple()): # if matches needs to be a mutable container. This creates a fresh list everytime the function is called. def finish(module, tree, xpath, namespaces, changed=False, msg="", hitcount=0, matches=None): if matches is None: matches = [] ```
In general, it's better to use``` collections.MutableMapping``` in isinstance as that allows duck-typing.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
++ thanks for changing this :)
nit: formatting, add some whitespaces
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: cannot create point collection with empty set of points at __randomizedtesting.SeedInfo.seed([9959A23819CF838B:6FE2182D9705AE]:0) at org.elasticsearch.common.geo.builders.ShapeBuilder.<init>(ShapeBuilder.java:97) at org.elasticsearch.common.geo.builders.MultiPointBuilder.<init>(MultiPointBuilder.java:46) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseMultiPoint(GeoWKTShapeParserTests.java:82) ... ```
colors should all be configurable
nit: formatting, add some whitespaces
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: cannot create point collection with empty set of points at __randomizedtesting.SeedInfo.seed([3BC798163FFF812D:918E10886BC43EC1]:0) at org.elasticsearch.common.geo.builders.ShapeBuilder.<init>(ShapeBuilder.java:97) at org.elasticsearch.common.geo.builders.LineStringBuilder.<init>(LineStringBuilder.java:49) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseMultiLineString(GeoWKTShapeParserTests.java:112) ... ```
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: Invalid number of points in LineString (found 1 - must be 0 or >= 2) at __randomizedtesting.SeedInfo.seed([3BC798163FFF812D:3A8577712E4A2AD2]:0) at com.vividsolutions.jts.geom.LineString.init(LineString.java:102) at com.vividsolutions.jts.geom.LineString.<init>(LineString.java:93) at com.vividsolutions.jts.geom.GeometryFactory.createLineString(GeometryFactory.java:539) at com.vividsolutions.jts.geom.GeometryFactory.createLineString(GeometryFactory.java:531) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseLineString(GeoWKTShapeParserTests.java:99) ... ```
You should be able to use `self.vmware_test_platform` here.
This check isn't quite right since it would just check lexicographically (e.g. `'2.9.0' > '2.11.0'`, but the former is a smaller version than the latter). For now this will probably work fine since we don't hit minor version numbers that large, but eventually this won't work (e.g. major version numbers go > 9). (Similarly, the sort of tags isn't quite right either.)
Remove the blank lines.
single line docstring is okay if it fits
It is also OK as-is if you like it; I just wanted to make it clear you don't *have* to obey the consistency if you see a better way. [I think I was in a bike-shedding mood when I did this review... Feel free to push back :-)]
Perhaps `required_if` can help.
I think this validation can be dropped too. it's only `name` which is required=True. (plus we wouldn't even reach this function without finding a definition by name)
OK, I see what you're doing here :+1:, I think there is slightly better way. Just suggestions, your call. Just removing these is not perfect. Ideally, omitting say `associations` in module args would behave same as writing explicit `associations: {}`. But currently explicit `associations: {}` won't match what's left after you remove empty values from API, causing unnecessary request and changed=True. - Args side can be normalized for you if you make attributes & associations first-class options, or suboptions, defaulting to `{}`. - Note that only recently manageiq master started sending empty values; nulls were omitted before. Not sure that's relevant Ã¢Â€Â” do you get Nones or empty hashes here? Anyway :+1: to normalizing API results in one direction or another Ã¢Â€Â” don't *rely* on getting empty values. - A pattern that works & reads nicely is defining a normalizing function and applying it on both sides of comparison. `if normalize(current) == normalized(desired)`. - IMHO, splitting logic between `has_field` and here is harder to follow, and it could be simple enough to inline, perhaps something like this: ``` current_properties = dynamic_resource_definition.get('properties', {}) current_name = dynamic_resource_definition.get('name') if (current_name == name and current_properties.get('attributes', {}) == properties.get('attributes', {}) and current_properties.get('associations', {}) == properties.get('associations', {}): ```
I think this is a typo - `changed` should be `changes`
no, this refers to the standard `changed=True` / `changed=False` and `msg="..."` results returned by ansible modules.
++ thanks for changing this :)
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
Trailing space in string not required.
please fail if required stuff is null
FWIW using `monkeypatch` is preferable because it's pytest-native unless I need something like mocks or spies.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
```suggestion from ansible.release import __version__ expected_version_line = 'ansible {ver!s}{scm_info!s}'.format(ver=__version__, scm_info=mocked_version) assert expected_version_line == version_lines[0], 'Incorrect ansible version line in "ansible --version" output' ```
colors should all be configurable
++ thanks for changing this :)
please fail if required stuff is null
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
Can you make this into a couple statements for easier readability? Something like: ``` invalid_entries = [x for x in statement['Principal']['AWS'] if not x.startswith('arn:aws:iam::)] if clean_invalid_entries and len(invalid_entries): for entry in invalid_entries: statement['Principal']['AWS'].remove(entry) ``` Or there's always: ``` original_size = len(statement['Principal']['AWS']) statement['Principal']['AWS'] = [x for x in statement['Principal']['AWS'] if x.startswith('arn:aws:iam::)] have_invalid_entries = (original_size == len(statement['Principal']['AWS'])) ```
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: cannot create point collection with empty set of points at __randomizedtesting.SeedInfo.seed([3BC798163FFF812D:918E10886BC43EC1]:0) at org.elasticsearch.common.geo.builders.ShapeBuilder.<init>(ShapeBuilder.java:97) at org.elasticsearch.common.geo.builders.LineStringBuilder.<init>(LineStringBuilder.java:49) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseMultiLineString(GeoWKTShapeParserTests.java:112) ... ```
nit: formatting, add some whitespaces
Add here that the `key_alias` or `key_arn` are both ways to provide it.
You don't need this conditional, since Ansible enforces that these are the only choices.
This must be in a separate try-except so that it does not break renaming to correct name if any of these statements fails.
Remove unnecessary whitespace.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
@charettes Thanks :+1: I removed unnecessary connector, see #15511. As far as I'm aware we now prefer non-kwargs constructions for internal usage, see 9662193aea2ee982bc8e553c62499aca5e606755 and #14699,
Minor but this could have likely be simplified by using `reduce` to avoid the private `_connector` usage ```python condition = reduce( (Q(app_label=app_label, model__in=models) for app_label, models in needed_models) , operator.or_) ``` In all cases `Q(("app_label", app_label), ("model__in", models), _connector=Q.AND)` can be simplified to `Q(app_label=app_label, model__in=models)` since `_connector` defaults to `Q.AND`.
You could use `subtest()` for the loop.
Would this be clearer? ``` max_query_params = connection.features.max_query_params if max_query_params is None or max_query_params >= len(numbers): ```
I would use `choices` in `labels` and `values` to simplify implementation, e.g. ```python @property def labels(cls): return [label for _, label in cls.choices] @property def values(cls): return [value for value, _ in cls.choices] ```
I think we can use `in cls` instead of `in cls.__members__.values()` (here and in `names`).
There could be a check in here to prevent both label and label_id from being specified if state is absent.
Remove unnecessary whitespace.
This must be in a separate try-except so that it does not break renaming to correct name if any of these statements fails.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Extract human readable title from the `webpage`.
These formats should not be removed.
This check does not make any sense. If there are no formats extraction should stop immediately.
You need to make sure `tf` doesn't write a unicode string. tf.write(u("#!/bin/sh/\necho %s" % quote(self.passphrase)).encode("utf-8")) Ansible may have a utility function to do this.
In the "Generate privatekey6 - standard - with non-ASCII passphrase" test, this fails with `'ascii' codec can't decode byte 0xc3 in position 16: ordinal not in range(128)`. Stacktrace: ``` File "/tmp/ansible_openssh_keypair_payload_kCnMbD/__main__.py", line 193, in generate tf.write("#!/bin/sh\necho %s" % quote(self.passphrase)) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 706, in write return self.writer.write(data) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 369, in write data, consumed = self.encode(object, self.errors) ``` It seems to fail in all Python 2 tests; in Python 3 it seems to work.
You should use `module.add_cleanup_file()` (somewhere above) instead of manually trying to clean up.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Remove unnecessary whitespace.
This must be in a separate try-except so that it does not break renaming to correct name if any of these statements fails.
No, `set_fs_attributes_if_different` respects `module.check`.
This gets called in check mode too and might change attributes on disk
You need to make sure `tf` doesn't write a unicode string. tf.write(u("#!/bin/sh/\necho %s" % quote(self.passphrase)).encode("utf-8")) Ansible may have a utility function to do this.
In the "Generate privatekey6 - standard - with non-ASCII passphrase" test, this fails with `'ascii' codec can't decode byte 0xc3 in position 16: ordinal not in range(128)`. Stacktrace: ``` File "/tmp/ansible_openssh_keypair_payload_kCnMbD/__main__.py", line 193, in generate tf.write("#!/bin/sh\necho %s" % quote(self.passphrase)) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 706, in write return self.writer.write(data) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 369, in write data, consumed = self.encode(object, self.errors) ``` It seems to fail in all Python 2 tests; in Python 3 it seems to work.
You should use `module.add_cleanup_file()` (somewhere above) instead of manually trying to clean up.
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Minor, but both places that use this could avoid assigning a value by always using the contents of `else` block if this were initialized to `0`. ``` Int cnt = termFreqMap.get(term); if (cnt == null) { cnt = new Int(); termFreqMap.put(term, cnt); } cnt.x += freq; ```
Minor consistency nit- our stuff seems to be using `[action]_[backend]()`, where these aliased imports have that reversed. I'd vote for `[action]_[backend]()` over this- makes the code a little easier to read...
```suggestion params = self.settings[alias].copy() ```
```suggestion raise InvalidCacheBackendError('Could not find backend {backend!r}: {e}') from e ```
Maybe rename this to substitute_crypto_req since it's replacing the one from requirements.txt with the one we autodetect rather than simply appending.
Nitpick, you can use a list comprehension here instead of calling list on a generator expression.
It's probably not a very common use case, but if a cache table is created and then another one is later added, running `createcachetable` again will throw an error because the first table already exists. I think it would be nice to handle that case.
Maybe add an empty line above this one and give it a name, it seems like the `assert` task is just a parameter of the `openssl_certificate_info` one.
Can you name this a little more verbosely? I can't unsee "get best"
```suggestion description: Returns a dictionary for every extension OID ```
Same here, seems a ValueError would be cleaner.
Usually, testing private interfaces doesn't make sense.
It doesn't matter whether it's a method or a function. A private function is related to the module scope, a private method is related to the class. Still, both are private, it's just a different level of namespacing. If a module name starts with an underscore it'd be also private.
For algorithmic code, it can make sense to test private methods and private functions in isolation from the rest of the code. This does seem to be a place where that could be justified. The code being tested is functional (meaning it operates via parameters and return values rather than callbacks) and it plugs into a larger framework which is outside of our control. What I'll sometimes do is push all the permutations of data that I care about at the private function and then push a small subset at the public interface to make sure that the interaction between the public and private code is working as expected.
```suggestion assert isinstance(wrap_var(b'foo'), type(b'')) ```
This can be changed to look for only one space now that #57209 is merged. Thanks again for writing tests. Much appreciated.
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
It probably makes sense to test that the exception reason also matches expectations
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
No need to parametrize with just one case.
Both cases test the same thing, don't repeat yourself. Test things based on their properties. Here you'd probably want to test iterables, not just sets.
Do we need this check? All tests pass without it.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
`form_class` is defined in `RangeField.formfield()` so this is redundant.
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
```suggestion Kwargs: ```
Add here that the `key_alias` or `key_arn` are both ways to provide it.
You don't need this conditional, since Ansible enforces that these are the only choices.
Wouldn't this approach fix the slicable fields to just these types? A mixin along with `slicable = True/False` on `F` may be an approach. Besides, I believe the policy is to keep `django.contrib.postgres` as separate as possible.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
I think a mixin is a bit complicated. The attribute sounds simpler - it could be a function instead of just a boolean, then it could return the transform if applicable, else `None` for non-sliceable fields (default implementation).
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
Just in case it's probably a good idea to use `.get()` instead of dict access syntax in case the key is unset.
I think `get_internal_type` is better to use.
It'd be good to include a default sleep here so this loop doesn't continuously hit the oVirt endpoint. Even just a one or two second sleep would be fine between tries.
`InvalidInternetID` doesn't seem to exist according to botocore's source
we want want -> we want
You'll want to branch off `< 3.6`.
Might make sense to raise exception in this case: ``` class Test: @cached_property def a(self): pass b = a ```
@graingert Probably it doesn't worth it, but still doable :-) ``` In [85]: class desc: ...: def __init__(self, f): ...: self.name = f.__name__ ...: ...: def __get__(self, instance, cls): ...: if instance is None: ...: return self ...: ...: for cls in cls.__mro__: ...: name = '_%s%s' % (cls.__name__, self.name) ...: if cls.__dict__.get(name) is self: ...: return name ...: ...: class A: ...: @desc ...: def __x(self): ...: pass ...: ...: ...: class B(A): ...: pass ...: In [86]: a = A() ...: a._A__x ...: Out[86]: '_A__x' In [87]: b = B() ...: b._A__x ...: Out[87]: '_A__x' ```
I think it's the right class: ``` In [38]: class desc: ...: def __get__(self, instance, cls): ...: if instance is None: ...: return self ...: return instance.__dict__['_%s__mangled' % cls.__name__] In [39]: class A: ...: d = desc() ...: ...: def __init__(self): ...: self.__mangled = 42 ...: In [40]: A().d Out[40]: 42 ```
@graingert `cls` is passed here.
I'll push my edits tomorrow.
``` class A: __print = cached_property(print, '__print') ``` This will not work and we can easily detect it too.
Might make sense to check explicitly set name too, because '__name' obviously will not work.
Can you just do `name.startswith('__') and not name.endswith('__')`? Simpler is better
I'd say `on Python < 3.6`
```python kwargs['max_value'] = min(value, kwargs.get('max_value', value)) ```
`items = value.split(self.delimiter) if value else []` is slightly faster.
And this can be reverted.
Ditto about the `for`/`else` construct.
I suggest you use the `for`/`else` construct here. ``` python for validator in validators: if isinstance(validator, validators.MinValueValidator) and validator.limit_value <= min_value: break else: validators.append(validators.MinValueValidator(min_value)) ```
consider assertRaisesMessage to make the test a bit more specific.
What if the prepared value contains unicode? (As I said before, you should use a list comprehension instead of a generator, it'll be about 2 times faster here)
Simply return `validators`.
~~A list comprehension would be a little faster.~~ EDIT: Forgive me, I misread the code.
Tests are missing for some validators, e.g. `DecimalValidator` and codes `invalid`, `max_digits`, `max_whole_digits`, and `max_decimal_places`.
I would chop blank lines in this test.
Chop `Ensure that`.
Technically since we're doing this in a fixture, either `monkeypatch` should be used to insert these into `globals()`, or you should return `mod`, and then just use `mod.Template` and `mod.AnsibleNativeEnvironment` in your tests.
Tests are missing for some validators, e.g. `DecimalValidator` and codes `invalid`, `max_digits`, `max_whole_digits`, and `max_decimal_places`.
FWIW using `monkeypatch` is preferable because it's pytest-native unless I need something like mocks or spies.
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
Match the error message
plz don't use EOL escaping, wrap with braces instead.
```python kwargs['max_value'] = min(value, kwargs.get('max_value', value)) ```
`items = value.split(self.delimiter) if value else []` is slightly faster.
And this can be reverted.
Ditto about the `for`/`else` construct.
I suggest you use the `for`/`else` construct here. ``` python for validator in validators: if isinstance(validator, validators.MinValueValidator) and validator.limit_value <= min_value: break else: validators.append(validators.MinValueValidator(min_value)) ```
consider assertRaisesMessage to make the test a bit more specific.
What if the prepared value contains unicode? (As I said before, you should use a list comprehension instead of a generator, it'll be about 2 times faster here)
Simply return `validators`.
~~A list comprehension would be a little faster.~~ EDIT: Forgive me, I misread the code.
Tests are missing for some validators, e.g. `DecimalValidator` and codes `invalid`, `max_digits`, `max_whole_digits`, and `max_decimal_places`.
Try to minimize the test that demonstrates the regression. I think this part isn't important -- assigning a value when creating the model should work just as well.
Instead of infinite checking, please cap the amount of time to some (can be long) value. 5 or 10 minutes would be *plenty* for CFN to generate a changeset.
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
Thank you for making this a warning
When running check mode and a stack does not yet exist, this call fails. If you catch botocore.exceptions.ClientError in a separate `except` you can check if the error is that the stack doesn't exist, then return None and show the state as "changed"
Also, this would break while deleting a stack without providing a template.
It'd be better here to catch boto exceptions & other exceptions separately, since generic exceptions (like IOError if the network fails, for example) don't have status codes and other boto-isms.
stack will not be created
Well, I suppose you *could* install Solaris in a VM, but I do see why youÃ¢Â€Â™d consider than the problem of someone who cares enough about Solaris to already have some Solaris. ThatÃ¢Â€Â™s cool. Frankly IÃ¢Â€Â™m impressed you tried to take this on in the first place. So, then, letÃ¢Â€Â™s merge what we have. The tests all pass, now, and itÃ¢Â€Â™s a step in the right direction.
No, `pkgutil` is the OpenCSW package manager. These are typically the only packages that it manages.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
```suggestion item, fields=fields, using=self.db, ```
single line as above
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
Similarly, ```if tc['skip'].get('i')```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
you can use `state` to avoid the 'or' to the user
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
Similarly, ```if tc['skip'].get('i')```
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Change the following: ```python import hashlib ``` Into: ```python from hashlib import sha1, sha224, sha256, sha384, sha512 ``` Then: ```suggestion return sha1(text.encode()).hexdigest() ``` And for `_sqlite_sha224()`, etc.
Hah. Had the same thought before I got here. See the caveats mentioned above.
Unlike `_sqlite_lpad()`, I'm not sure we can really improve this. `fill_text * length` is likely to be too long in most cases, and even worse if `fill_text` is multiple characters, but any alternative will probably be slower in general.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
I'm not sure this is really worth it. Perhaps we can just add `step = list.append` to each of the four aggregates below and make them inherit from `list` directly? It might be clearer keeping `step` and `finalize` together, but I'm easy either way.
I think a better wording would be: "Backwards compatibility for storage backends without support for `max_length` will be removed in Django 2.0."
Unlike an "app", I wouldn't say that a model is "installed". Two solutions: - check if the app is installed and adjust the error message accordingly: "app X isn't installed" or "app X doesn't provide model Y" - just rephrase do "which doesn't exist"
Would there be any utility to sorted() on files/sub_folders? AFAICT, os.walk will use the os.listdir() results directly for filenames. os.listdir() is doc'ed as 'arbitrary order' (iirc, more or less up to fs). Wondering if sorting those results would make for more reproducible behavior when trying to replicate errors. It shouldn't matter at all if there are no bugs, but it could make tracking down odd failures easier.
Omit 0/1. There was a past commit that removed all usage of that since it just adds verbosity.
we want want -> we want
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
It would have been more appropriate if `run_checks()` took an `Apps` instance as a parameter instead of a list of `AppConfig`s.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
lines can be longer
I'm not sure the docstring adds any value here.
Honestly, I just failed to invent a better name
Having it upstream would be nice, but I doubt we can change our minimum required Jinja2 version anytime soon.
JFYI In the internal slack channel we came up with an idea: https://github.com/ansible/ansible/pull/39924 So it might get into devel before you merge this
I'd write this loop as ``` for pos, prefix in enumerate(prefix_gen()): if prefix not in self.subq_aliases: self.alias_prefix = prefix break if pos > local_recursion limit: raise Runtime... ```
Are there situations in which this can happen other than "too many subqueries"? If not, I'd suggest an error message that might be more helpful to the end user would be "Maximum recursion depth exceeded: too many subqueries."
Yep, simply ignoring all other options when `state == 'absent'` is the easiest solution (that's what most other modules do, too).
This module fails in check mode since it is not calling `module.exit_json()`. Since this module does not look at the state and compare it before making changes, I would suggest removing check mode support from the module currently.
I would expect a module to issue warnings only for something which the user needs to be warned about. If the user wants something to be absent and it already is absent, that's not something I would expect a warning about. You also don't issue a warning if the user has `state == 'present'` and the sequence is already there.
This should not result in a warning, but simply result in `changed == False`.
This must be in a separate try-except so that it does not break renaming to correct name if any of these statements fails.
Remove unnecessary whitespace.
```suggestion # Run masked_action when enabled param is not provided or service is supposed to be enabled/started. if not module.check_mode and module.params['enabled'] in [None, True]): ```
You could use RenameMethodsBase.
`mentionned` => `mentioned`
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
`if it encounter` => `if it encounters`
Missing `=dict` on this and the next few lines
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Does replication factor matter in this test at all? I can't think of why it would matter for the replication factor, so it seems weird to make it 2 here.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
You can find it as the leader of the offset partition in ZK, though you will need to have a ZK connection to do that.
`del` is a builtin, not a function. These parens don't have to be here
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
If we're just testing broker compatibility I don't think we even need this part of the test.
Please rewrite as ``` if __name__ == '__main__': main() ```
Remove unnecessary whitespace.
This must be in a separate try-except so that it does not break renaming to correct name if any of these statements fails.
```suggestion # Run masked_action when enabled param is not provided or service is supposed to be enabled/started. if not module.check_mode and module.params['enabled'] in [None, True]): ```
@Ian-Foote thanks for the clarification I always mix up the two terms.
It's not actually a comprehension - this could just use a tuple literal.
`mentionned` => `mentioned`
`if it encounter` => `if it encounters`
Missing `=dict` on this and the next few lines
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
This must be in a separate try-except so that it does not break renaming to correct name if any of these statements fails.
Remove unnecessary whitespace.
```suggestion # Run masked_action when enabled param is not provided or service is supposed to be enabled/started. if not module.check_mode and module.params['enabled'] in [None, True]): ```
You could use RenameMethodsBase.
`mentionned` => `mentioned`
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
`if it encounter` => `if it encounters`
Missing `=dict` on this and the next few lines
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
with -> width
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
width, height, and offset
comma after tuple
put closing parenthesis on the next line
You could use lambda to save some lines defining the function
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
`client.start_execution` is idempotent but `start_execution` always returns `changed=True` if `action=start`. We probably want to `client.list_executions` to see if the execution is already running before proceeding.
I haven't actually used stepfunctions myself so I'm just going off the docs. AIUI, `ExecutionAlreadyExists` would only be returned on a running execution if the `execution_input` is different, not in a case where the input is the same. As an ansible user who is new to step functions, I feel like if I gave the same input and ran the playbook twice I would expect the 2nd time to be `changed=false` if the first execution is still running. But that doesn't mean that's how AWS users familiar with step functions will expect it.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Does replication factor matter in this test at all? I can't think of why it would matter for the replication factor, so it seems weird to make it 2 here.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
You can find it as the leader of the offset partition in ZK, though you will need to have a ZK connection to do that.
`del` is a builtin, not a function. These parens don't have to be here
Is there specific value in doing this for all variants here? In particular, does the security protocol variation get us something that existing tests don't? Another way of asking this is, beyond the failures we would catch with existing tests where brokers generally fail to communicate with each other when using specific security protocols, is there a way we think reassignment specifically would fail that would be related to the security protocol? E.g. is there a different inter-broker connection that's being made that otherwise wouldn't be made? I ask because I think we already test a lot of stuff with variations of security protocols and I'm a bit worried about test bloat due to parametrizing tests being relatively easy (and leading to much longer test runs).
If we're just testing broker compatibility I don't think we even need this part of the test.
Please rewrite as ``` if __name__ == '__main__': main() ```
`test_jsonfield` looks good to me (since this is InspectDBTests, _introspection seems redundant).
Ã°ÂŸÂ‘Â to not needing this
This is usually not needed
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
It's usually better to use raw-strings for regexps: ```suggestion assert re.match(r'ansible [0-9.a-z]+ .*$', version_lines[0]), 'Incorrect ansible version line in "ansible --version" output' ``` (I'm pretty sure Python 3.6+ will emit warnings if you don't)
Plz also use `match` arg here
FWIW using `monkeypatch` is preferable because it's pytest-native unless I need something like mocks or spies.
`del` is a builtin, not a function. These parens don't have to be here
If we're just testing broker compatibility I don't think we even need this part of the test.
I think these should be `is True` since the `.assertTrue` being replaced would fail if on something like `"foo"` which is truthy. So the `assert "foo"` would pass where `self.assertTrue("foo")` wouldn't.
Indeed, you are right.
I'd omit the blank line since it's hard to get confused in 3 lines of code. Also the commit message could describe the issue being fixed instead of the implementation of the fix.
I typically use something like the following: ```python return str(val).lower() in {'1', 't', 'y', 'on', 'true', 'yes'} ``` This works whether you pass a boolean, integer or string, although we are only expecting a string from the environment anyway. If we wanted to be strict, we should reject invalid values: ```python if str(val).lower() in {'1', 't', 'y', 'on', 'true', 'yes'}: return True if str(val).lower() in {'0', 'f', 'n', 'off', 'false', 'no'}: return False raise ValueError('Non-boolean string provided.') ```
This is missing other likely candidates: `y` and `yes` I'd also argue it should be simplified to be case-insensitive so that `TRUE` == `true` (which requires you actually assert the incoming `val` is actuall stringy enough to have `.lower()`, rather than simply doing an equality match against any of the values)
If you're just raising, you can skip the try/except since it's handled in the caller.
the `f.close()` is implied as part of the with block. Also, I don't think this should be in the module since it looks like a debug statement.
Can you name this a little more verbosely? I can't unsee "get best"
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
This isn't needed, since we now only support 2.6+ anyway.
It seems `AMQPConnectionError` could be used instead of `Exception`.
can be ignored
hm maybe that's what I've searched for
boto3_conn now handles region problems, no need to do it in the module
This logic seems ignore the use case of removing all tags.
can be ignored
still: https://github.com/ansible/ansible/pull/44070#discussion_r212981844 https://github.com/ansible/ansible/pull/44070#discussion_r213508181
I'm not a huge fan of this, we should be able to build the result set from module.params and the response.
when using dict you can just do `dict(msg=to_text(body), message_count=....`.
Same order, type first.
It seems `AMQPConnectionError` could be used instead of `Exception`.
hm maybe that's what I've searched for
can be ignored
boto3_conn now handles region problems, no need to do it in the module
can be ignored
Not required with AnsibleAWSModule
Same order, type first.
still: https://github.com/ansible/ansible/pull/44070#discussion_r212981844 https://github.com/ansible/ansible/pull/44070#discussion_r213508181
when using dict you can just do `dict(msg=to_text(body), message_count=....`.
I'm not a huge fan of this, we should be able to build the result set from module.params and the response.
Is this possible? If so, it will be good to cover this scenario with tests.
I fixed this issue in fe0ddbc84e4d8836ce8d27a1218d360c5482c2be.
I think this should use the `request['path']`.
To anyone curious, it's likely a problem of when (or rather, where) `connection.features` gets evaluated. ```python await a_getattr(connection.features, "supports_json_field") ``` is executed in the following order: ```python foo = connection.features bar = a_getattr(foo, "supports_json_field") await bar ``` Evaluating `connection.features` does the following: 1. Since `connection` is a `ConnectionProxy` instance, it first finds a default connection 2. It takes that connection and returns a `DatabaseFeatures` instance bound to it That first step is tricky because it uses the `_connections` object as a cache. `_connections` is an instance of `asgiref.local.Local`, which is thread-local by design. So the same cache instance will contain different attributes depending on where (which thread) it's accessed from. Now, when `connection.features` is evaluated in the main thread, it returns a `DatabaseFeatures` instance bound to the connection object created in the main thread (and only safe to use within that main thread). The last puzzle piece is `sync_to_async`, which creates a one-off worker thread to complete a synchronous function call without blocking the event loop. The `getattr` call executed within `sync_to_async` runs in a new thread. This new thread attempts to use the features object passed from the main thread. Which, in turn, tries to use the connection object created in the main thread (to check if the connection is to a MariaDB server).
Diff reproducing on SQLite: ``` diff --git a/tests/async/tests.py b/tests/async/tests.py index 66eece4b97..0610b6f1b0 100644 --- a/tests/async/tests.py +++ b/tests/async/tests.py @@ -1,13 +1,15 @@ +import _thread import asyncio import os from unittest import mock -from asgiref.sync import async_to_sync +from asgiref.sync import async_to_sync, sync_to_async from django.core.cache import DEFAULT_CACHE_ALIAS, caches from django.core.exceptions import ImproperlyConfigured, SynchronousOnlyOperation +from django.db import connection from django.http import HttpResponse -from django.test import SimpleTestCase +from django.test import SimpleTestCase, TestCase from django.utils.asyncio import async_unsafe from django.views.generic.base import View @@ -25,13 +27,22 @@ class CacheTest(SimpleTestCase): self.assertIs(cache_1, cache_2) -class DatabaseConnectionTest(SimpleTestCase): +class DatabaseConnectionTest(TestCase): """A database connection cannot be used in an async context.""" async def test_get_async_connection(self): with self.assertRaises(SynchronousOnlyOperation): list(SimpleModel.objects.all()) + async def test_validate_thread_sharing(self): + def func(connection): + print(connection.features.supports_json_field) + + await sync_to_async(func)(connection) + + a_getattr = sync_to_async(getattr) + print(await a_getattr(connection.features, "supports_json_field")) + class AsyncUnsafeTest(SimpleTestCase): """ ``` The first version works, wrapping the whole feature check in `sync_to_async`. The second, wrapping `getattr` causes the error.
Hey @patrys â€” Nice. ðŸ‘ > The last puzzle piece is... I'd gotten to just before this, but was missing this, so ðŸ‘¯ â€” Thanks! We need to have a little think about how we structure calls to not run into this â€” especially if they're nested somewhere inside a context manager, as @felixxm hit for `CaptureQueriesContext` â€” but tomorrow... ðŸ›ï¸ ðŸŽ
Using `connection` inside `sync_to_async` correctly finds a `default` connection in `_connections` (which is a `Local` instance). However, using `connection` outside of `sync_to_async` doesn't find a `default` connection in `_connections` and creates a new one. In both cases, `_connections` is the same object according to its `ID`. :exploding_head:
+1 for adding `name:` lines
Note to self: if this is not discussed elsewhere in the docs, we should add it.
Typically the keywords for state we use are `started`, `stopped`, `present`, and `absent` since these are states rather than commands.
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
I realize this predates your PR, but why is 'unused' not used? It appears to be the target field provided by the SQLUpdateCompiler during as_sql(). This seems like a really good spot to allow a backend to hook in and CAST the expression to a type that is allowed by the target column.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
okay, but would be helpful to say _why_ we need to always return True.
```suggestion template_name = 'forms_tests/form_snippet.html' ```
Oh, I just meant to suggest the `value` to `values` (plural) renaming, not more...
```suggestion ``` I don't think you should do this, case is meaningful: ``` max@mde-oxalide % pacman -Q libreoffice-en-US libreoffice-fresh 6.1.3-1 max@mde-oxalide % pacman -Q libreoffice-en-us error: package 'libreoffice-en-us' was not found ```
If you're going to do partition, I'd rather use the fact that it'll always return a 3-tuple instead of using `-1` to index the third element. i.e.: ```python line.partition(':')[2] ```
The following properties indicate if
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
I realize this predates your PR, but why is 'unused' not used? It appears to be the target field provided by the SQLUpdateCompiler during as_sql(). This seems like a really good spot to allow a backend to hook in and CAST the expression to a type that is allowed by the target column.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
okay, but would be helpful to say _why_ we need to always return True.
```suggestion ``` I don't think you should do this, case is meaningful: ``` max@mde-oxalide % pacman -Q libreoffice-en-US libreoffice-fresh 6.1.3-1 max@mde-oxalide % pacman -Q libreoffice-en-us error: package 'libreoffice-en-us' was not found ```
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
I realize this predates your PR, but why is 'unused' not used? It appears to be the target field provided by the SQLUpdateCompiler during as_sql(). This seems like a really good spot to allow a backend to hook in and CAST the expression to a type that is allowed by the target column.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Please use a single quote.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
```suggestion template_name = 'forms_tests/form_snippet.html' ```
```suggestion ``` I don't think you should do this, case is meaningful: ``` max@mde-oxalide % pacman -Q libreoffice-en-US libreoffice-fresh 6.1.3-1 max@mde-oxalide % pacman -Q libreoffice-en-us error: package 'libreoffice-en-us' was not found ```
If you're going to do partition, I'd rather use the fact that it'll always return a 3-tuple instead of using `-1` to index the third element. i.e.: ```python line.partition(':')[2] ```
The following properties indicate if
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
I realize this predates your PR, but why is 'unused' not used? It appears to be the target field provided by the SQLUpdateCompiler during as_sql(). This seems like a really good spot to allow a backend to hook in and CAST the expression to a type that is allowed by the target column.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
okay, but would be helpful to say _why_ we need to always return True.
You could update the verbs tense to the convention used elsewhere: "Handle", "set", etc. rather than the "ing" versions.
```suggestion pkg_version = line.partition(':')[-1].strip() ```
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
This can be single lined: ```python '%s\n%s' % (combined_list[prev], combined_list[k]), ```
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
```suggestion errors_on_separate_row=True, ```
Please use a single quote.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
Unless there's a significant difference, a single RTL language seems fine... ðŸ¤”
```suggestion template_name = 'forms_tests/form_snippet.html' ```
```suggestion ({b'str'.encode(DEFAULT_ENCODING): b'str'.encode(DEFAULT_ENCODING)}, {u'str': u'str'}), ```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
Having a class for just one test method is unnecessary.
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
(Similarly, mark any string that you are going to call decode on with as a b"string".)
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
You should probably expect unicode strings
You're checking two separate properties here. This should be in a separate test.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
(Similarly, mark any string that you are going to call decode on with as a b"string".)
Having a class for just one test method is unnecessary.
```suggestion ({b'str'.encode(DEFAULT_ENCODING): b'str'.encode(DEFAULT_ENCODING)}, {u'str': u'str'}), ```
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
It probably makes sense to test that the exception reason also matches expectations
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
You should probably expect unicode strings
You're checking two separate properties here. This should be in a separate test.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Both [`exit_json`](https://github.com/ansible/ansible/blob/ebd08d2a01d96d41f8bc1ccf27bc1bbf91060a44/lib/ansible/module_utils/basic.py#L2254) and [`fail_json`](https://github.com/ansible/ansible/blob/ebd08d2a01d96d41f8bc1ccf27bc1bbf91060a44/lib/ansible/module_utils/basic.py#L2261) methods call `sys.exit`: the old code checked that `fail_json` has been called and would have failed if `exit_json` were called in `pip.main`.
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
Plz also use `match` arg here
(Similarly, mark any string that you are going to call decode on with as a b"string".)
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
You should probably expect unicode strings
```suggestion ({b'str'.encode(DEFAULT_ENCODING): b'str'.encode(DEFAULT_ENCODING)}, {u'str': u'str'}), ```
Having a class for just one test method is unnecessary.
You're checking two separate properties here. This should be in a separate test.
1. No umask respected. 2. There is no `os.chmod` in python 3.2 according to python [docs](https://docs.python.org/3/library/os.html#os.chmod). 3. flake8.
Maybe `files_total`? We already break things, so...
Might be nice for demonstration purposes if the two records actually have different keys. Maybe: ```suggestion aTopic.pipeInput(1, "999-alpha"); bTopic.pipeInput(999, "beta"); ```
bump (it looks like this didn't get changed)
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
In the "Generate privatekey6 - standard - with non-ASCII passphrase" test, this fails with `'ascii' codec can't decode byte 0xc3 in position 16: ordinal not in range(128)`. Stacktrace: ``` File "/tmp/ansible_openssh_keypair_payload_kCnMbD/__main__.py", line 193, in generate tf.write("#!/bin/sh\necho %s" % quote(self.passphrase)) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 706, in write return self.writer.write(data) File "/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/codecs.py", line 369, in write data, consumed = self.encode(object, self.errors) ``` It seems to fail in all Python 2 tests; in Python 3 it seems to work.
You need to make sure `tf` doesn't write a unicode string. tf.write(u("#!/bin/sh/\necho %s" % quote(self.passphrase)).encode("utf-8")) Ansible may have a utility function to do this.
You should use `module.add_cleanup_file()` (somewhere above) instead of manually trying to clean up.
In theory a locale dependant pattern could be provided and `locale.setlocale` be called but this is really unlikely.
One thing that worries me here is that it'll discard any `kwargs` so if `output_field` is provided, since we allow it to be passed as a positional argument it will be lost. e.g. `Subquery(queryset, models.BooleanField())`.
use tags.items() here, no need for iteritems import
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
Same question for dropping lambda here as well.
I had to change this to `template_params['subquery'], sql_params = self.subquery.query.get_compiler(connection=connection).as_sql()` in order to compile correctly when using a database other than the `DEFAULT_DB_ALIAS`.
with -> width
width, height, and offset
comma after tuple
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
I don't see a need for string interpolation in cases like this.
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
need to x2 check tmrow, but this should be an error, we don't allow for circular parenting
Shouldn't this be `When I(containers) is C(yes)`? (Same for the others.)
colors should all be configurable
```suggestion if self.client.module.params['disk_usage']: ```
The leading underscore in the '_meta' key is missing here.
Add a task ```.yaml - debug: var: result.docker_host_facts ``` And similar tasks after the other examples.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
It probably makes sense to test that the exception reason also matches expectations
Use the Python 2 and low 3 Time Machine: `'url too short: %s' % (video_pre_parts, )` or: `'url too short: %(video_pre_parts)s' % {'video_pre_parts': video_pre_parts, }` or: `'url too short: {video_pre_parts}'.format(video_pre_parts=video_pre_parts)` or: `'url too short: {0}'.format(video_pre_parts)` No doubt there are other ways (eg `....format(**locals())`
need to x2 check tmrow, but this should be an error, we don't allow for circular parenting
colors should all be configurable
The leading underscore in the '_meta' key is missing here.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
It probably makes sense to test that the exception reason also matches expectations
Use the Python 2 and low 3 Time Machine: `'url too short: %s' % (video_pre_parts, )` or: `'url too short: %(video_pre_parts)s' % {'video_pre_parts': video_pre_parts, }` or: `'url too short: {video_pre_parts}'.format(video_pre_parts=video_pre_parts)` or: `'url too short: {0}'.format(video_pre_parts)` No doubt there are other ways (eg `....format(**locals())`
need to x2 check tmrow, but this should be an error, we don't allow for circular parenting
colors should all be configurable
The leading underscore in the '_meta' key is missing here.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
``` for key, label in (('play_addr_lowbr', 'Low'), ('play_addr', 'Normal'), ('download_addr', 'Download')): format_url = url_or_none(try_get(data, lambda x: x['video'][key]['url_list'])) if not format_url: continue formats.append(...) ```
Should contain `quality` key.
`enumerate` on for range.
Must not break extraction if missing.
Don't capture groups you don't use. Unused captured group.
This is total mess. Should not match https://www.tiktok.com/@leenabhushanvideo6748451240264420610, https://www.tiktok.com/embed/video/6567659045795758085 and so on.
No brackets needed.
This is useless at the end.
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
Useless with timestamp available.
```suggestion # Run masked_action when enabled param is not provided or service is supposed to be enabled/started. if not module.check_mode and module.params['enabled'] in [None, True]): ```
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
Missing `=dict` on this and the next few lines
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Please rewrite as ``` if __name__ == '__main__': main() ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
missing space between `,` and `and`
```suggestion # Run masked_action when enabled param is not provided or service is supposed to be enabled/started. if not module.check_mode and module.params['enabled'] in [None, True]): ```
Maybe if it's only one line to create a function (and the call is not really complicated), you can remove this function and use directly `nb_enpoint.create()` instead, like you do with `nb_endpoint.delete()`.
You can drop `int` here now (you already are doing it in `decode`), same for other hashers/methods probably.
Missing `=dict` on this and the next few lines
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Please rewrite as ``` if __name__ == '__main__': main() ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
missing space between `,` and `and`
This doesn't look like it is tested.
Upon further reflection, I think these three lines along with `check_mutually_exclusive` are erroneous for subspecs. We don't currently have a way to specify `required_if/one_of/together` or `mutually_exclusive` for subspecs, so in every case *but* the networking "aggregate" thing, this is severely broken (as you'd be making nonsensical checks in most cases)
`self._options_context` stores the context of parent options in case of nested suboptions. As the `elif` block does not have a recursive call to `_handle_options` context handling is not required here. However, context information can be added as part of error message in case this argument is part of nested sub-options (similar to other error messages in sub-option handling).
This can instead be `continue` and let the `else` unnest.
Python 3.5 does not have `unicode`. Take a look at the [ansible text helper methods](https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/_text.py) for alternatives. Only noting the first occurrence.
Wouldn't this approach fix the slicable fields to just these types? A mixin along with `slicable = True/False` on `F` may be an approach. Besides, I believe the policy is to keep `django.contrib.postgres` as separate as possible.
I think a mixin is a bit complicated. The attribute sounds simpler - it could be a function instead of just a boolean, then it could return the transform if applicable, else `None` for non-sliceable fields (default implementation).
this isn't needed, as ordereddict from pypi only supports python2.4 - python2.6
I think `get_internal_type` is better to use.
Please rewrite as ``` if __name__ == '__main__': main() ```
This doesn't look like it is tested.
Upon further reflection, I think these three lines along with `check_mutually_exclusive` are erroneous for subspecs. We don't currently have a way to specify `required_if/one_of/together` or `mutually_exclusive` for subspecs, so in every case *but* the networking "aggregate" thing, this is severely broken (as you'd be making nonsensical checks in most cases)
`self._options_context` stores the context of parent options in case of nested suboptions. As the `elif` block does not have a recursive call to `_handle_options` context handling is not required here. However, context information can be added as part of error message in case this argument is part of nested sub-options (similar to other error messages in sub-option handling).
This can instead be `continue` and let the `else` unnest.
Python 3.5 does not have `unicode`. Take a look at the [ansible text helper methods](https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/_text.py) for alternatives. Only noting the first occurrence.
Wouldn't this approach fix the slicable fields to just these types? A mixin along with `slicable = True/False` on `F` may be an approach. Besides, I believe the policy is to keep `django.contrib.postgres` as separate as possible.
I think a mixin is a bit complicated. The attribute sounds simpler - it could be a function instead of just a boolean, then it could return the transform if applicable, else `None` for non-sliceable fields (default implementation).
this isn't needed, as ordereddict from pypi only supports python2.4 - python2.6
I think `get_internal_type` is better to use.
Please rewrite as ``` if __name__ == '__main__': main() ```
I think `get_internal_type` is better to use.
This should probably be the last method in the class.
this is unsafe and can lead to race conditions and data corruption, you should write to a temp file and use `basic`'s `atomic_move` method to finalize changes.
`is` tests identity in python so if statement always produces a change. We need to use `!=` here to compare equality instead of identity.
This can instead be `continue` and let the `else` unnest.
I think a mixin is a bit complicated. The attribute sounds simpler - it could be a function instead of just a boolean, then it could return the transform if applicable, else `None` for non-sliceable fields (default implementation).
need to catch BotoCoreError here too.
This would be a nice place to use tuple expansion `action, index = index_of_matching_route(....)`
Yeah, you'd need `return None, None` at the end of index_of_matching_route()
Wouldn't this approach fix the slicable fields to just these types? A mixin along with `slicable = True/False` on `F` may be an approach. Besides, I believe the policy is to keep `django.contrib.postgres` as separate as possible.
I think `get_internal_type` is better to use.
This should probably be the last method in the class.
this is unsafe and can lead to race conditions and data corruption, you should write to a temp file and use `basic`'s `atomic_move` method to finalize changes.
`is` tests identity in python so if statement always produces a change. We need to use `!=` here to compare equality instead of identity.
This can instead be `continue` and let the `else` unnest.
I think a mixin is a bit complicated. The attribute sounds simpler - it could be a function instead of just a boolean, then it could return the transform if applicable, else `None` for non-sliceable fields (default implementation).
need to catch BotoCoreError here too.
This would be a nice place to use tuple expansion `action, index = index_of_matching_route(....)`
Yeah, you'd need `return None, None` at the end of index_of_matching_route()
Wouldn't this approach fix the slicable fields to just these types? A mixin along with `slicable = True/False` on `F` may be an approach. Besides, I believe the policy is to keep `django.contrib.postgres` as separate as possible.
Instead of infinite checking, please cap the amount of time to some (can be long) value. 5 or 10 minutes would be *plenty* for CFN to generate a changeset.
I think `get_internal_type` is better to use.
It'd be better here to catch boto exceptions & other exceptions separately, since generic exceptions (like IOError if the network fails, for example) don't have status codes and other boto-isms.
This can instead be `continue` and let the `else` unnest.
need to catch BotoCoreError here too.
For python 2.6 compatibility, this and the following four strings have to look like `cmd = "ping {0} {1}".format(vrf, dest)`
This doesn't seem right to me - you may not need the `catch_extra_error_codes`
Please add check-mode support (and if possible also diff support).
`InvalidInternetID` doesn't seem to exist according to botocore's source
I don't think this is really needed (and doesn't seem to be used)
this is unsafe and can lead to race conditions and data corruption, you should write to a temp file and use `basic`'s `atomic_move` method to finalize changes.
`is` tests identity in python so if statement always produces a change. We need to use `!=` here to compare equality instead of identity.
This can instead be `continue` and let the `else` unnest.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
Chop `Ensure that`.
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
Yeah, you'd need `return None, None` at the end of index_of_matching_route()
This would be a nice place to use tuple expansion `action, index = index_of_matching_route(....)`
Please add check-mode support (and if possible also diff support).
Should also be "path"
this is unsafe and can lead to race conditions and data corruption, you should write to a temp file and use `basic`'s `atomic_move` method to finalize changes.
`is` tests identity in python so if statement always produces a change. We need to use `!=` here to compare equality instead of identity.
This can instead be `continue` and let the `else` unnest.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
Chop `Ensure that`.
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
Yeah, you'd need `return None, None` at the end of index_of_matching_route()
This would be a nice place to use tuple expansion `action, index = index_of_matching_route(....)`
Please add check-mode support (and if possible also diff support).
Should also be "path"
```suggestion assert expected == "exception" ```
Ahh true, sorry for the noise. No changes are required.
I removed it.
```suggestion assert expected == "exception" ```
It looks like a duplicate of `test_save_unsaved_instance_on_generic_foreign_key` :thinking:
> Shall I also remove `test_target_model_is_unsaved_save` and `test_target_model_is_unsaved_bulk_create`? Yes, they are redundant.
is this test needed? I'd suspect it's already tested.
```suggestion assert expected == "exception" ```
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Similarly, ```if tc['skip'].get('i')```
Maybe `subTest` to make tests less repetitive e.g.: ```python msg = 'Geotransform must consist of 6 numeric values.' for geotransform in ([1, 2], [1, 2, 3, 4, 5, 'foo'], [1, 2, 3, 4, 5, 6, 'foo']): with self.subTest(geotransform=geotransform): with self.assertRaisesMessage(ValueError, msg): rsmem.geotransform = geotransform ```
(Similarly, mark any string that you are going to call decode on with as a b"string".)
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
TBH I don't much value in testing Python's error messages, `self.assertRaises(TypeError)` should be enough.
```suggestion assert expected == "exception" ```
Both cases test the same thing, don't repeat yourself. Test things based on their properties. Here you'd probably want to test iterables, not just sets.
I would chop blank lines in this test.
Chop `Ensure that`.
It probably makes sense to test that the exception reason also matches expectations
This module fails in check mode since it is not calling `module.exit_json()`. Since this module does not look at the state and compare it before making changes, I would suggest removing check mode support from the module currently.
Don't we usually subscribe to a "unicode sandwich" strategy where functions pass around unicode strings and the function converts to bytes or native strings when it crosses an external boundary? I've also seen the `b_` or `n_` prefix being used if a function specifically accepts a byte or native string but not sure if that's an actual guideline we have going forward.
If `date` and `time` is not added as part of a file name (in case of a configurable backup path option) `date` and `time` logic can be changed to fetch from the file information probably using os.stat()
```suggestion self.assertTrue(self.temp_dir.joinpath(name).exists()) ```
Same here, don't we usually use byte strings for file operations.
```suggestion display.vvv(u"No connection ot reset: %s" % to_text(stderr)) ``` ```suggestion display.vvv(u"No connection to reset: %s" % to_text(stderr)) ```
yep, that is what I meant, basically make sure new code is pep8 compliant
this module is marked as "legacy" and therefore the CI is not complaining about pep8 here, but it would help if we try to be pep8 compliant.
Oops, I misread the diff and see that you only modified the existing archives. Still an explanation of exactly what going on would be nice as it's not obvious to me.
An example of using the `mocker` fixture. ```suggestion def mock_NamedTemporaryFile(mocker, **args): mock_ntf = mocker.MagicMock() mock_ntf.write = mocker.MagicMock() mock_ntf.close = mocker.MagicMock() ```
IMO there is no need to check a file content: ```suggestion msg = '...' with self.assertRaisesMessage(CommandError, msg): call_command( 'squashmigrations', 'migrations', '0001', '0002', squashed_name='initial', interactive=False, verbosity=0, ) ```
I'm not sure it's useful to implement _set_container in the base class, as the path seems different depending on stream.
These last four lines are duplicated in both conditions, should therefore come after the if block.
I'm curious if this should be a `BaseException` so that it'd work on `Ctrl+C` (`SIGINT`) or `SystemExit` (`SIGTERM`).
Omit these lines please.
This behavior doesn't exist in the current implementation of `HttpResponse`.
It's better to actually say that there's no file in place or it's inaccessible.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Is this even needed, we will be dropping py2 and `to_text` does not call `__unicode__`. You may as well just put this in `__str__`.
These parens aren't necessary for unpacking the return values.
this is already done by argspec when param is defined as boolean, all redundant
This doesn't look right. This basically allows Python 3.0.1+, not 3.0.0+ as you probably intended. But even with that, it's best to just compare the major version and use "py2 or not py2" because py3+ also means py4+. OTOH we vendor `six` which contains proper constants already. So just import that: ```python from ansible.module_utils import six if six.PY2: ... else: ... ```
I am split on my feelings about this function. It doesn't work the same as it did as part of `AnsibleModule`. Before, it would error on the first time that `count > 1`, whereas we aren't doing so now, we are collecting them all, but then not using them all. Additionally, this is more like `list_mutually_exclusive`. I think this function should raise some form of Exception, instead of just returning a list of the mutually exclusive args that were provided by the user.
```suggestion 'module': ('async_wrapper', ), 'cache': ('base', ), ```
`IGNORE[xxx]` is used as something iterable, and I don't think any of the calling code wants a list of letters :)
It Python, you should use a proper camel case for classes: ```suggestion class TestJsonEncodeFallback: ```
It doesn't matter whether it's a method or a function. A private function is related to the module scope, a private method is related to the class. Still, both are private, it's just a different level of namespacing. If a module name starts with an underscore it'd be also private.
For algorithmic code, it can make sense to test private methods and private functions in isolation from the rest of the code. This does seem to be a place where that could be justified. The code being tested is functional (meaning it operates via parameters and return values rather than callbacks) and it plugs into a larger framework which is outside of our control. What I'll sometimes do is push all the permutations of data that I care about at the private function and then push a small subset at the public interface to make sure that the interaction between the public and private code is working as expected.
Usually, testing private interfaces doesn't make sense.
The reason it's very niche is that it is specially adapted to only return values which we can remove from output for no_log. ie: if there was a parameter like secure=[True|False] and that was marked no_log in the argument_spec for some reason, we don't want to hide True or False in all of our output (that could actually tell an attacker what the value of secure was as they could see whether True or False was being hidden in other parameters)
We should probably rename this. Revisiting this code... this is a pretty special-case function as well. I wonder if it should be a private function inside of parameters.py (basic.py can make it public as part of its API but the idea would be that we don't expect any new code to use it.)
I think we prefer the closing paren on a newline
this is too aggressive as it removes all ACLs, not just the ones we added take into account that directories can have 'default acl' to be added to all new files in a directory, this would wipe those along with the one we added to copy the file into place
```suggestion display.vvv(u"No connection ot reset: %s" % to_text(stderr)) ``` ```suggestion display.vvv(u"No connection to reset: %s" % to_text(stderr)) ```
That was true until surrogateescape was added: ``` python >>> a = b'\xff'.decode('utf8', errors='surrogateescape') >>> a '\udcff' >>> '\udcff'.encode('utf-8') Traceback (most recent call last): File "<stdin>", line 1, in <module> UnicodeEncodeError: 'utf-8' codec can't encode character '\udcff' in position 0: surrogates not allowed >>> '\udcff'.encode('utf-8', errors='surrogateescape') b'\xff' >>> '\udcff'.encode('utf-8', errors='replace') b'?' ```
Use `errors='replace'` so that we don't traceback in case of invalid utf-8. Since this is a logging function, it's okay to display a mangled string.
I think we should add an `allow_overwrite` or similar param.
two blank lines...
Please do not assume a model only inherits from `django.db.models.Model` I inherit from standard python classes as well. Add check for `issubclass(model, django.db.models.Model)`.
The pylint error is occurring because this `MockSelector` class shadows the one on line 59.
Ok, cool. :thumbsup:
Based on my reading of the MSDN doc this should just result in returning the default value, not throwing an Exception. "Returns the last element of an observable sequence that matches the predicate, or a default value if no value is found." http://msdn.microsoft.com/en-us/library/hh228948(v=vs.103).aspx It should return the last value that matches the predicate, but if nothing matches then it should return the default value.
This whole block should be changed into: ```python module = AnsibleModule( argument_spec=dict( name=dict(type='str', required=True), remote=dict(type='str'), state=dict(type='str', default="present", choices=['absent', 'present']) ), supports_check_mode=True, ) ``` Assuming that **name** is always required, and **repo** is required when **state == 'present'**. - Put the type-option first, as this one is usually always added - Lines in Ansible can be 160 chars wide, so there's usually no need to split it a line
No need to split the line.
Not sure if this piece is doing exactly what you expect in all cases.
This whole block should be changed into: ```python module = AnsibleModule( argument_spec=dict( name=dict(type='str', required=True), remote=dict(type='str'), state=dict(type='str', default="present", choices=['absent', 'present']) ), supports_check_mode=True, required_if = ( ('state', 'present', ('repo')), ), ) ``` Assuming that **name** is always required, and **repo** is required when **state == 'present'**
No it's part of python and should be available.
Why have both `schema` and `newschema`? I would assume that if I specify another value for `schema`, that the schema will be changed.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
I wouldn't do this, why no just write **module.params** below. It seems an needless abstraction with little merit, and more confusion.
What about to separate the `if` blocks here as well as you did above? The same bellow.
#66872 seems to think this would be 1.10.0, from experience I'd suspect it's actually a botocore version that's required...
It requires `botocore>=1.13.21`
Rather than running this test inside create_autoscaling_group I'd recommend running it in main() prior to any possible changes happening.
Thanks for that note ewen. I learned something!
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
this whole section is not needed, just use `self._plugin_options[<option name>]`
its new in 2.4 so it doesn't apply to older versions
don't do this by default, set from cli `self._options` does not exist or is None
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
naming nit: singular `endpoint` is one of `raw_endpoints` from playbook, plural `endpoints` is in "connection_configurations" format. can you find less confusing names? perhaps rename `endpoints` list to `configurations`? (also consider s/build_endpoints/build_configurations/)
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
Do we need to call `list(fields)` here? :thinking:
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Ditto for `[]` â†’ `None` and `ON_CONFLICTS_NONE` â†’ `None`.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
```suggestion item, fields=fields, using=self.db, ```
It would have been more appropriate if `run_checks()` took an `Apps` instance as a parameter instead of a list of `AppConfig`s.
This should be botocore.exceptions.NoCredentialsError.
Catch botocore.exceptions.ClientError instead of Exception here too.
If this is set to type='int' this should avoid the later type conversion
It would be good to have a try/except around this call.
This will always return changed even if there is failure in deleting hook.
``` "Unable to get hooks from repository %s: %s" % to_native(err) ``` to ``` Unable to get hooks from repository :%s" % to_native(err) ```
~I do not think so `check mode` is required for facts module.~
Make sense to me. Thanks for info.
use `missing_required_lib` from `ansible.module_utils.basic`
Too few format values ``` Unable to get hooks from repository : %s" % to_native(err) ```
This check is only necessary in `URLResolver._populate()`, since `URLPattern._populate()` can never be called recursively.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
I don't think you should re-number the existing tests.
According to the DEP, this should be usable as a class decorator, but I don't see this here... Something like ```python def register_converter(converter, typename=None): if typename is None and isinstance(converter, str): # We're used as a decorator return functools.partial(register_converter, typename=converter) else: ... # current body ``` Alternatively, make `typename` an attribute of the converter class -- then `register_converter` takes a single argument and can trivially be used as a decorator. Making the name accessible in the converter is probably better anyway, for error-reporting in any non-trivial `to_python()` or `to_url()` method.
Any problem with: ``` @property def media(self): ```
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
```python return {**DEFAULT_CONVERTERS, **REGISTERED_CONVERTERS} ``` Python 3 FTW
Not supported on Python 3.4.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
I think that we only want to remove a single `'g'` from the end of the string and continue to allow things like `'gg'`, `'1gg'`, `'g2'`, and `'g3g'` to raise an error as they would have before. ```suggestion if arg.endswith('g'): arg = arg[:-1] force_grouping = True ```
Omit these lines please.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
Can you name this a little more verbosely? I can't unsee "get best"
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
This isn't needed, since we now only support 2.6+ anyway.
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Breaks once `userData` is not first.
Carry long lines. Bother to finally read coding conventions.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
if you put it like that, you can undo c08cd8b
sure, nothing bad happens, but it might still be a user configuration error (user configured initalTimeout, but forgot to set maxTimeout). I think it's nicer to just throw an exception and report it.
I understand what you're trying to do here, but managing both parent as child objects in a single module is going to be problematic IMO. This can only work if your childs are always fully listed (and replaced/updated).
Looking for outside counsel on this one ;-)
In the metric system, 'kilo' is abbreviated as 'k'. I guess this is wrong in the UCS interface though.
I expected that much.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
++ thanks for changing this :)
please fail if required stuff is null
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
looks like there are two levels of indentation instead of one
Missing space after the `for`
colors should all be configurable
nit: formatting, add some whitespaces
I get occasional failures here if run frequently (100 iters), e.g for this seed: ``` java.lang.IllegalArgumentException: Invalid number of points in LineString (found 1 - must be 0 or >= 2) at __randomizedtesting.SeedInfo.seed([3BC798163FFF812D:3A8577712E4A2AD2]:0) at com.vividsolutions.jts.geom.LineString.init(LineString.java:102) at com.vividsolutions.jts.geom.LineString.<init>(LineString.java:93) at com.vividsolutions.jts.geom.GeometryFactory.createLineString(GeometryFactory.java:539) at com.vividsolutions.jts.geom.GeometryFactory.createLineString(GeometryFactory.java:531) at org.elasticsearch.common.geo.GeoWKTShapeParserTests.testParseLineString(GeoWKTShapeParserTests.java:99) ... ```
```suggestion # Note: These examples do not set authentication details, see the AWS Guide for details. ``` It's helpful to hint the AWS Guide here so folks know about it.
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
add 'python library' , then the 'pip install' becomes redundant, some people cannot use pip to install libraries
i would have single 'msg' and then a 'type' option, this follows the pattern of other modules.
Could you please use `AnsibleAWSModule` instead? We didn't have a lot of the aws utils we do now when this was first PRd, it will save having to port it later on. https://docs.ansible.com/ansible/devel/dev_guide/platforms/aws_guidelines.html#creating-new-aws-modules
Missing boto3 will be automatically handled by AnsibleAWSModule, this should just be: ``` try: import botocore.exceptions except ImportError: pass ```
`aws_common_argument_spec` is currently unused, `get_aws_connection_info` and `boto3_conn` shouldn't be necessary.
These calls need to be wrapped in try/except to handle exceptions, otherwise the exception will just bubble up to the user. We provide a decorator in the EC2 module utils that handles boto3 exceptions and does retries for you. https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ec2.py#L71
This should use the standard client setup, which handles exceptions internally: ```self.conn = module.client('iam')```
This (and the other exception handling) should use the standard convenience function, which will format the exception for you: ``` self.module.fail_json_aws(e, msg='Error getting provider ARN for {0}'.format(name)) ```
Chop blank line.
Both cases test the same thing, don't repeat yourself. Test things based on their properties. Here you'd probably want to test iterables, not just sets.
Add trailing comma.
No need to parametrize with just one case.
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
Use hanging indentation (the same in the second test).
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
It probably makes sense to test that the exception reason also matches expectations
You could use lambda to save some lines defining the function
Chop blank line.
Both cases test the same thing, don't repeat yourself. Test things based on their properties. Here you'd probably want to test iterables, not just sets.
Add trailing comma.
No need to parametrize with just one case.
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
Mark any string that you are going to call encode on as a ```u"string"```. Otherwise you can run into problems on Python2.
Use hanging indentation (the same in the second test).
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
It probably makes sense to test that the exception reason also matches expectations
You could use lambda to save some lines defining the function
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
`form_class` is defined in `RangeField.formfield()` so this is redundant.
This is actually useful, since a `.get('volumes', [])` can still return `None` if the value `volumes` is explicitly None, as opposed to nonexistent.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
`# If a list of input formats from one of the format_modules was retrieved, make sure...`
This code is almost the same as in `replace_unnamed_groups()`, the only difference is that the beginning of non-capturing group is longer i.e. `'(?:'` instead of `'('`. We could add an internal hook and use it in both places, e.g. ```python def _find_groups(pattern, group_matcher): group_indices = [ (m.start(0), m.end()) for m in non_capturing_group_matcher.finditer(pattern) ] # Loop over the groups. for start, end in unnamed_group_indices: ... for idx, val in enumerate(pattern[end:]): ... if unmatched_open_brackets == 0: group_indices.append((start, end + idx + 1)) break # Remove unnamed group matches inside other unnamed capture groups. group_start_end_indices = [] prev_end = None for start, end in group_indices: if prev_end and start > prev_end or not prev_end: group_start_end_indices.append((start, end)) prev_end = end return group_start_end_indices ``` Moreover, with some boolean flags (e.g. `named=True/False`) this could also be reused in `replace_named_groups()` :thinking: .
I would also detect if the value is a bool and set it accordingly to `on`/`off`.
use the `flatten_dict` method from module_utils.network.nxos.utils.utils.py
You can also add a deprecation warning for usages like `from django.forms.extras.widgets import SelectDateWidget`.
This will also have to take into account `filter(m2m__field=val1, criteria=m2m__otherfield=val2)` != `filter(m2m__field=val1).filter(m2m__otherfield=val2)` as explained in [spanning multi-valued relationships](https://docs.djangoproject.com/en/1.11/topics/db/queries/#spanning-multi-valued-relationships)
I think chaining `filter`/`exclude` should be an `AND` not an `OR`. It makes more sense to me and I'm pretty sure that what queryset `filter`/`exclude` do.
Just in case it's probably a good idea to use `.get()` instead of dict access syntax in case the key is unset.
we want want -> we want
`None` in `getattr` is already default.
you don't need `nulls_last=True` here because it's a PK you're ordering by, which is non-nullable
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
using `assertCountEqual` would save all the fuss with ordering here
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
we want want -> we want
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Please ignore, my suggestion is invalid syntax.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
Ditto. ```suggestion # Check empty result with non-execution optimization. with self.assertNumQueries(0): ```
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Ditto. ```suggestion # Check empty result with non-execution optimization. with self.assertNumQueries(0): ```
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Ditto. ```suggestion # Check empty result with non-execution optimization. with self.assertNumQueries(0): ```
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
Could you also change this line for me? ```diff - Cast(expression, FloatField()) if not isinstance(expression.output_field, FloatField) + Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField) ``` Don't forget to import `IntegerField`.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Ditto. ```suggestion # Check empty result with non-execution optimization. with self.assertNumQueries(0): ```
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
use to_native (module_utils._text) instead of str, it deals with py2/py3 compatiblity
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
Clarify 'we', if possible.
`form_class` is defined in `RangeField.formfield()` so this is redundant.
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Check line length (79) chop "we" "If it's not there or if l10n is disabled, fall back.."
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
`# If a list of input formats from one of the format_modules was retrieved, make sure...`
You can also add a deprecation warning for usages like `from django.forms.extras.widgets import SelectDateWidget`.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
This join generation concerns me - not that it won't work just that it's kinda magical and ugly. It would be awesome if we could use the relationship name somewhere. Perhaps `SubQuery(rel_name, qs=BLAH)` which is a similar API to `Prefetch`? I don't know how easy that would be to get to work as the `rel` object would probably need to do some of the transformations. It may allow a wider variety of rel objects to work though - e.g. subquery on a M2M field.
iirc we decided to allow configure commands as part of cli_command to support config commands that result in a command prompt
So I would get rid of these.
Rename DO to DigitalOcean to avoid acronyms
Please don't add `default: null`, that's implicit and adds no value. (Especially since it is a required parameter)
Empty aliases is not needed. Keep it simple.
Default needs adding to docs
Okay, so to implement the examples like this: ``` with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ``` We'll want to take the dict from terms instead of from kwargs. terms will hold the dict that the user passed in so you should be able to drop in replace it in your constructor like so: ```CyberarkPassword(**terms)```
okay, but would be helpful to say _why_ we need to always return True.
following task -> the following task
Please use a single quote.
set the safe
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Missing `=dict` on this and the next few lines
If you're just raising, you can skip the try/except since it's handled in the caller.
You should emphasize that the module can and will not do any idempotence checking for this.
(In general, I don't think modules should have such options.)
it seems this and other parameters are missing from docs
the `f.close()` is implied as part of the with block. Also, I don't think this should be in the module since it looks like a debug statement.
chop trailing newline (check code with flake8 to see a couple other minor formatting things)
Please use a single quote.
This can be single lined: ```python '%s\n%s' % (combined_list[prev], combined_list[k]), ```
set the safe
```suggestion - "mas-cli (U(https://github.com/mas-cli/mas)) 1.5.0+ available as C(mas) in the bin path" ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Missing `=dict` on this and the next few lines
```suggestion to iterate use a C(with_) directive. ```
(In general, I don't think modules should have such options.)
You should emphasize that the module can and will not do any idempotence checking for this.
Indeed, you are right.
These don't appear to be "public" attributes, so we should stay consistent with indicating their privateness by prepending their names with an `_` character.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
```suggestion description: Can be used to request certificates from ECS, with C(provider=entrust). ```
```suggestion description: Whether the domain is eligible for submission of "EV" certificates. Will never be C(true) if I(ov_eligible) is C(false) ```
```suggestion returned: success and I(ov_eligible) is C(true) and I(domain_status) is C(APPROVED), C(RE_VERIFICATION) or C(EXPIRING). ```
```suggestion sample: true ```
```suggestion description: Whether the domain is eligible for submission of "OV" certificates. Will never be C(false) if I(ov_eligible) is C(true) ```
is the `any` required here - `if metrics_to_disable:` should suffice, I'd think
@webknjaz we have no gaurantees that the artifacts are being built from a git checkout.
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
Thanks for that note ewen. I learned something!
I wouldn't do this, why no just write **module.params** below. It seems an needless abstraction with little merit, and more confusion.
This whole block should be changed into: ```python module = AnsibleModule( argument_spec=dict( name=dict(type='str', required=True), remote=dict(type='str'), state=dict(type='str', default="present", choices=['absent', 'present']) ), supports_check_mode=True, required_if = ( ('state', 'present', ('repo')), ), ) ``` Assuming that **name** is always required, and **repo** is required when **state == 'present'**
not a list of dicts, just pass a dict to the with: ```yaml with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ```
@pilou-, cyberark-bizdev I had a chance to talk to bcoca today and he said the best way to do this is to pass a list of dicts to the lookup. Then either of these playbook constructs can be valid: ``` with_cyberaskpassword: - {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - {'appid': 'Application2', 'query': 'safe=Safe2;Folder=root;Object=User2', 'output': 'password,passprops.username,passprops.address'} with_cyberarkpassword: {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - debug: msg='{{ lookup('cyberark_password', {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'}) }}' ``` Implementing that will take a little restructuring. I'll go into that in the run method below.
Okay, so @cyberark-bizdev take out my first example. The last two are what we're shooting for. (The middle one is the same as bcoca's example, just in compact yaml instead of "readable" yaml ;-)
Okay, so to implement the examples like this: ``` with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ``` We'll want to take the dict from terms instead of from kwargs. terms will hold the dict that the user passed in so you should be able to drop in replace it in your constructor like so: ```CyberarkPassword(**terms)```
Based on the change above to use `**self._options` in `RedisCache._cache()`, we should replace `options` with keys that are expected. As it stands at the moment a key could be misspelled in `OPTIONS` and passed here but not used. Based on the changes requested above, we should at least have `pickle_protocol=None` here.
And this would be... ```suggestion self._serializer = PickleSerializer(pickle_protocol) ```
Can we make `key` set to `None` by default to avoid needing to pass `None` explicitly below? ```suggestion def get_client(self, key=None, write=False): ```
We could flatten this? ```suggestion return default if value is None else self._serializer.loads(value) ```
~~Do we want to use [`.setex()`](https://redis-py.readthedocs.io/en/stable/#redis.Redis.setex)?~~ ```suggestion client.setex(key, timeout, self._serializer.dumps(value)) ``` ~~This is just an observation that can be ignored. Maybe it is better to keep `.set()` as for `.add()` above where there is no combination of `.setex()` and `.setnx()`, requiring the use of `.set()` instead.~~ **Edit:** Definitely ignore this as I read the following [here](https://redis.io/commands/set): > Note: Since the SET command options can replace SETNX, SETEX, PSETEX, GETSET, it is possible that in future versions of Redis these commands will be deprecated and finally removed.
Ah, yes. Good observation. ðŸ™‚
Rather than passing all of `OPTIONS` here we should only pass the protocol: ```suggestion def __init__(self, protocol=None): self._protocol = pickle.HIGHEST_PROTOCOL if protocol is None else protocol def dumps(self, value): return pickle.dumps(value, self._protocol) ```
We don't need the extra variable here. ```suggestion client.mset({k: self._serializer.dumps(v) for k, v in data.items()}) ```
Shouldn't this be writable? ```suggestion client = self.get_client(None, write=True) ```
No need, I will commit the patch soon.
Based on the change above to use `**self._options` in `RedisCache._cache()`, we should replace `options` with keys that are expected. As it stands at the moment a key could be misspelled in `OPTIONS` and passed here but not used. Based on the changes requested above, we should at least have `pickle_protocol=None` here.
And this would be... ```suggestion self._serializer = PickleSerializer(pickle_protocol) ```
Can we make `key` set to `None` by default to avoid needing to pass `None` explicitly below? ```suggestion def get_client(self, key=None, write=False): ```
Rather than passing all of `OPTIONS` here we should only pass the protocol: ```suggestion def __init__(self, protocol=None): self._protocol = pickle.HIGHEST_PROTOCOL if protocol is None else protocol def dumps(self, value): return pickle.dumps(value, self._protocol) ```
We could flatten this? ```suggestion return default if value is None else self._serializer.loads(value) ```
~~Do we want to use [`.setex()`](https://redis-py.readthedocs.io/en/stable/#redis.Redis.setex)?~~ ```suggestion client.setex(key, timeout, self._serializer.dumps(value)) ``` ~~This is just an observation that can be ignored. Maybe it is better to keep `.set()` as for `.add()` above where there is no combination of `.setex()` and `.setnx()`, requiring the use of `.set()` instead.~~ **Edit:** Definitely ignore this as I read the following [here](https://redis.io/commands/set): > Note: Since the SET command options can replace SETNX, SETEX, PSETEX, GETSET, it is possible that in future versions of Redis these commands will be deprecated and finally removed.
Ah, yes. Good observation. ðŸ™‚
The current coding style prefers single quotes. Please fix throughout, but new code only. ```suggestion if 'PICKLE_VERSION' in options: ``` _(Eventually it will be double quotes when we can use `black`.)_
No need, I will commit the patch soon.
Rename to `BaseSequenceSerializer`, make the `_format()` raise a `NotImplementedError` similar to the `BaseSerializer`. Then add a `ListSerializer` along `TupleSerializer` etc. that implements the `_format()` method. ``` python class BaseSequenceSerializer(BaseSerializer): def _format(self): raise ... class ListSerializer(BaseSequenceSerializer): def _format(self): return "[%s]" class TupleSerializer(BaseSequenceSerializer): # as already implemented ```
No need, I will commit the patch soon.
Based on the change above to use `**self._options` in `RedisCache._cache()`, we should replace `options` with keys that are expected. As it stands at the moment a key could be misspelled in `OPTIONS` and passed here but not used. Based on the changes requested above, we should at least have `pickle_protocol=None` here.
And this would be... ```suggestion self._serializer = PickleSerializer(pickle_protocol) ```
Can we make `key` set to `None` by default to avoid needing to pass `None` explicitly below? ```suggestion def get_client(self, key=None, write=False): ```
We could flatten this? ```suggestion return default if value is None else self._serializer.loads(value) ```
~~Do we want to use [`.setex()`](https://redis-py.readthedocs.io/en/stable/#redis.Redis.setex)?~~ ```suggestion client.setex(key, timeout, self._serializer.dumps(value)) ``` ~~This is just an observation that can be ignored. Maybe it is better to keep `.set()` as for `.add()` above where there is no combination of `.setex()` and `.setnx()`, requiring the use of `.set()` instead.~~ **Edit:** Definitely ignore this as I read the following [here](https://redis.io/commands/set): > Note: Since the SET command options can replace SETNX, SETEX, PSETEX, GETSET, it is possible that in future versions of Redis these commands will be deprecated and finally removed.
Ah, yes. Good observation. ðŸ™‚
Rather than passing all of `OPTIONS` here we should only pass the protocol: ```suggestion def __init__(self, protocol=None): self._protocol = pickle.HIGHEST_PROTOCOL if protocol is None else protocol def dumps(self, value): return pickle.dumps(value, self._protocol) ```
The current coding style prefers single quotes. Please fix throughout, but new code only. ```suggestion if 'PICKLE_VERSION' in options: ``` _(Eventually it will be double quotes when we can use `black`.)_
Shouldn't this be writable? ```suggestion client = self.get_client(None, write=True) ```
No need, I will commit the patch soon.
Based on the change above to use `**self._options` in `RedisCache._cache()`, we should replace `options` with keys that are expected. As it stands at the moment a key could be misspelled in `OPTIONS` and passed here but not used. Based on the changes requested above, we should at least have `pickle_protocol=None` here.
Can we make `key` set to `None` by default to avoid needing to pass `None` explicitly below? ```suggestion def get_client(self, key=None, write=False): ```
We could flatten this? ```suggestion return default if value is None else self._serializer.loads(value) ```
And this would be... ```suggestion self._serializer = PickleSerializer(pickle_protocol) ```
Ah, yes. Good observation. ðŸ™‚
~~Do we want to use [`.setex()`](https://redis-py.readthedocs.io/en/stable/#redis.Redis.setex)?~~ ```suggestion client.setex(key, timeout, self._serializer.dumps(value)) ``` ~~This is just an observation that can be ignored. Maybe it is better to keep `.set()` as for `.add()` above where there is no combination of `.setex()` and `.setnx()`, requiring the use of `.set()` instead.~~ **Edit:** Definitely ignore this as I read the following [here](https://redis.io/commands/set): > Note: Since the SET command options can replace SETNX, SETEX, PSETEX, GETSET, it is possible that in future versions of Redis these commands will be deprecated and finally removed.
Rather than passing all of `OPTIONS` here we should only pass the protocol: ```suggestion def __init__(self, protocol=None): self._protocol = pickle.HIGHEST_PROTOCOL if protocol is None else protocol def dumps(self, value): return pickle.dumps(value, self._protocol) ```
Shouldn't this be writable? ```suggestion client = self.get_client(None, write=True) ```
The current coding style prefers single quotes. Please fix throughout, but new code only. ```suggestion if 'PICKLE_VERSION' in options: ``` _(Eventually it will be double quotes when we can use `black`.)_
No need, I will commit the patch soon.
Based on the change above to use `**self._options` in `RedisCache._cache()`, we should replace `options` with keys that are expected. As it stands at the moment a key could be misspelled in `OPTIONS` and passed here but not used. Based on the changes requested above, we should at least have `pickle_protocol=None` here.
Can we make `key` set to `None` by default to avoid needing to pass `None` explicitly below? ```suggestion def get_client(self, key=None, write=False): ```
We could flatten this? ```suggestion return default if value is None else self._serializer.loads(value) ```
~~Do we want to use [`.setex()`](https://redis-py.readthedocs.io/en/stable/#redis.Redis.setex)?~~ ```suggestion client.setex(key, timeout, self._serializer.dumps(value)) ``` ~~This is just an observation that can be ignored. Maybe it is better to keep `.set()` as for `.add()` above where there is no combination of `.setex()` and `.setnx()`, requiring the use of `.set()` instead.~~ **Edit:** Definitely ignore this as I read the following [here](https://redis.io/commands/set): > Note: Since the SET command options can replace SETNX, SETEX, PSETEX, GETSET, it is possible that in future versions of Redis these commands will be deprecated and finally removed.
Ah, yes. Good observation. ðŸ™‚
And this would be... ```suggestion self._serializer = PickleSerializer(pickle_protocol) ```
Rather than passing all of `OPTIONS` here we should only pass the protocol: ```suggestion def __init__(self, protocol=None): self._protocol = pickle.HIGHEST_PROTOCOL if protocol is None else protocol def dumps(self, value): return pickle.dumps(value, self._protocol) ```
Shouldn't this be writable? ```suggestion client = self.get_client(None, write=True) ```
The current coding style prefers single quotes. Please fix throughout, but new code only. ```suggestion if 'PICKLE_VERSION' in options: ``` _(Eventually it will be double quotes when we can use `black`.)_
No need, I will commit the patch soon.
Based on the change above to use `**self._options` in `RedisCache._cache()`, we should replace `options` with keys that are expected. As it stands at the moment a key could be misspelled in `OPTIONS` and passed here but not used. Based on the changes requested above, we should at least have `pickle_protocol=None` here.
Can we make `key` set to `None` by default to avoid needing to pass `None` explicitly below? ```suggestion def get_client(self, key=None, write=False): ```
We could flatten this? ```suggestion return default if value is None else self._serializer.loads(value) ```
~~Do we want to use [`.setex()`](https://redis-py.readthedocs.io/en/stable/#redis.Redis.setex)?~~ ```suggestion client.setex(key, timeout, self._serializer.dumps(value)) ``` ~~This is just an observation that can be ignored. Maybe it is better to keep `.set()` as for `.add()` above where there is no combination of `.setex()` and `.setnx()`, requiring the use of `.set()` instead.~~ **Edit:** Definitely ignore this as I read the following [here](https://redis.io/commands/set): > Note: Since the SET command options can replace SETNX, SETEX, PSETEX, GETSET, it is possible that in future versions of Redis these commands will be deprecated and finally removed.
Ah, yes. Good observation. ðŸ™‚
And this would be... ```suggestion self._serializer = PickleSerializer(pickle_protocol) ```
We don't need the extra variable here. ```suggestion client.mset({k: self._serializer.dumps(v) for k, v in data.items()}) ```
Rather than passing all of `OPTIONS` here we should only pass the protocol: ```suggestion def __init__(self, protocol=None): self._protocol = pickle.HIGHEST_PROTOCOL if protocol is None else protocol def dumps(self, value): return pickle.dumps(value, self._protocol) ```
Shouldn't this be writable? ```suggestion client = self.get_client(None, write=True) ```
No need, I will commit the patch soon.
Based on the change above to use `**self._options` in `RedisCache._cache()`, we should replace `options` with keys that are expected. As it stands at the moment a key could be misspelled in `OPTIONS` and passed here but not used. Based on the changes requested above, we should at least have `pickle_protocol=None` here.
Can we make `key` set to `None` by default to avoid needing to pass `None` explicitly below? ```suggestion def get_client(self, key=None, write=False): ```
We could flatten this? ```suggestion return default if value is None else self._serializer.loads(value) ```
~~Do we want to use [`.setex()`](https://redis-py.readthedocs.io/en/stable/#redis.Redis.setex)?~~ ```suggestion client.setex(key, timeout, self._serializer.dumps(value)) ``` ~~This is just an observation that can be ignored. Maybe it is better to keep `.set()` as for `.add()` above where there is no combination of `.setex()` and `.setnx()`, requiring the use of `.set()` instead.~~ **Edit:** Definitely ignore this as I read the following [here](https://redis.io/commands/set): > Note: Since the SET command options can replace SETNX, SETEX, PSETEX, GETSET, it is possible that in future versions of Redis these commands will be deprecated and finally removed.
Ah, yes. Good observation. ðŸ™‚
And this would be... ```suggestion self._serializer = PickleSerializer(pickle_protocol) ```
We don't need the extra variable here. ```suggestion client.mset({k: self._serializer.dumps(v) for k, v in data.items()}) ```
Shouldn't this be writable? ```suggestion client = self.get_client(None, write=True) ```
Rather than passing all of `OPTIONS` here we should only pass the protocol: ```suggestion def __init__(self, protocol=None): self._protocol = pickle.HIGHEST_PROTOCOL if protocol is None else protocol def dumps(self, value): return pickle.dumps(value, self._protocol) ```
No need, I will commit the patch soon.
Based on the change above to use `**self._options` in `RedisCache._cache()`, we should replace `options` with keys that are expected. As it stands at the moment a key could be misspelled in `OPTIONS` and passed here but not used. Based on the changes requested above, we should at least have `pickle_protocol=None` here.
Ah, yes. Good observation. ðŸ™‚
~~Do we want to use [`.setex()`](https://redis-py.readthedocs.io/en/stable/#redis.Redis.setex)?~~ ```suggestion client.setex(key, timeout, self._serializer.dumps(value)) ``` ~~This is just an observation that can be ignored. Maybe it is better to keep `.set()` as for `.add()` above where there is no combination of `.setex()` and `.setnx()`, requiring the use of `.set()` instead.~~ **Edit:** Definitely ignore this as I read the following [here](https://redis.io/commands/set): > Note: Since the SET command options can replace SETNX, SETEX, PSETEX, GETSET, it is possible that in future versions of Redis these commands will be deprecated and finally removed.
Can we make `key` set to `None` by default to avoid needing to pass `None` explicitly below? ```suggestion def get_client(self, key=None, write=False): ```
We could flatten this? ```suggestion return default if value is None else self._serializer.loads(value) ```
And this would be... ```suggestion self._serializer = PickleSerializer(pickle_protocol) ```
Shouldn't this be writable? ```suggestion client = self.get_client(None, write=True) ```
Rather than passing all of `OPTIONS` here we should only pass the protocol: ```suggestion def __init__(self, protocol=None): self._protocol = pickle.HIGHEST_PROTOCOL if protocol is None else protocol def dumps(self, value): return pickle.dumps(value, self._protocol) ```
The current coding style prefers single quotes. Please fix throughout, but new code only. ```suggestion if 'PICKLE_VERSION' in options: ``` _(Eventually it will be double quotes when we can use `black`.)_
No need, I will commit the patch soon.
Based on the change above to use `**self._options` in `RedisCache._cache()`, we should replace `options` with keys that are expected. As it stands at the moment a key could be misspelled in `OPTIONS` and passed here but not used. Based on the changes requested above, we should at least have `pickle_protocol=None` here.
And this would be... ```suggestion self._serializer = PickleSerializer(pickle_protocol) ```
Can we make `key` set to `None` by default to avoid needing to pass `None` explicitly below? ```suggestion def get_client(self, key=None, write=False): ```
We could flatten this? ```suggestion return default if value is None else self._serializer.loads(value) ```
~~Do we want to use [`.setex()`](https://redis-py.readthedocs.io/en/stable/#redis.Redis.setex)?~~ ```suggestion client.setex(key, timeout, self._serializer.dumps(value)) ``` ~~This is just an observation that can be ignored. Maybe it is better to keep `.set()` as for `.add()` above where there is no combination of `.setex()` and `.setnx()`, requiring the use of `.set()` instead.~~ **Edit:** Definitely ignore this as I read the following [here](https://redis.io/commands/set): > Note: Since the SET command options can replace SETNX, SETEX, PSETEX, GETSET, it is possible that in future versions of Redis these commands will be deprecated and finally removed.
Ah, yes. Good observation. ðŸ™‚
Rather than passing all of `OPTIONS` here we should only pass the protocol: ```suggestion def __init__(self, protocol=None): self._protocol = pickle.HIGHEST_PROTOCOL if protocol is None else protocol def dumps(self, value): return pickle.dumps(value, self._protocol) ```
The current coding style prefers single quotes. Please fix throughout, but new code only. ```suggestion if 'PICKLE_VERSION' in options: ``` _(Eventually it will be double quotes when we can use `black`.)_
Shouldn't this be writable? ```suggestion client = self.get_client(None, write=True) ```
Based on the change above to use `**self._options` in `RedisCache._cache()`, we should replace `options` with keys that are expected. As it stands at the moment a key could be misspelled in `OPTIONS` and passed here but not used. Based on the changes requested above, we should at least have `pickle_protocol=None` here.
And this would be... ```suggestion self._serializer = PickleSerializer(pickle_protocol) ```
Rather than passing all of `OPTIONS` here we should only pass the protocol: ```suggestion def __init__(self, protocol=None): self._protocol = pickle.HIGHEST_PROTOCOL if protocol is None else protocol def dumps(self, value): return pickle.dumps(value, self._protocol) ```
Can we make `key` set to `None` by default to avoid needing to pass `None` explicitly below? ```suggestion def get_client(self, key=None, write=False): ```
We could flatten this? ```suggestion return default if value is None else self._serializer.loads(value) ```
Ah, yes. Good observation. ðŸ™‚
~~Do we want to use [`.setex()`](https://redis-py.readthedocs.io/en/stable/#redis.Redis.setex)?~~ ```suggestion client.setex(key, timeout, self._serializer.dumps(value)) ``` ~~This is just an observation that can be ignored. Maybe it is better to keep `.set()` as for `.add()` above where there is no combination of `.setex()` and `.setnx()`, requiring the use of `.set()` instead.~~ **Edit:** Definitely ignore this as I read the following [here](https://redis.io/commands/set): > Note: Since the SET command options can replace SETNX, SETEX, PSETEX, GETSET, it is possible that in future versions of Redis these commands will be deprecated and finally removed.
The current coding style prefers single quotes. Please fix throughout, but new code only. ```suggestion if 'PICKLE_VERSION' in options: ``` _(Eventually it will be double quotes when we can use `black`.)_
No need, I will commit the patch soon.
Rename to `BaseSequenceSerializer`, make the `_format()` raise a `NotImplementedError` similar to the `BaseSerializer`. Then add a `ListSerializer` along `TupleSerializer` etc. that implements the `_format()` method. ``` python class BaseSequenceSerializer(BaseSerializer): def _format(self): raise ... class ListSerializer(BaseSequenceSerializer): def _format(self): return "[%s]" class TupleSerializer(BaseSequenceSerializer): # as already implemented ```
Nitpick, you can use a list comprehension here instead of calling list on a generator expression.
Maybe rename this to substitute_crypto_req since it's replacing the one from requirements.txt with the one we autodetect rather than simply appending.
I'd revert this as I don't think it is better and it isn't making this more consistent with code elsewhere. ```suggestion if isinstance(stored_backend, RemoteUserBackend): ```
Same here, seems a ValueError would be cleaner.
Can you name this a little more verbosely? I can't unsee "get best"
It seems appropriate to remove `and callable(backend.get_user)` then.
This one is fine as `auth.load_backend()` is used in `clean_username()` above. ```suggestion request.session.get(auth.BACKEND_SESSION_KEY, ''), ```
I'd go for `try/except IOError` in order to make a point that variable initialized above is in fact a fallback.
I don't think there's a use case for `settings.AUTHENTICATION_BACKENDS` without any backends that have a `get_user()` method.
Yes, I think Django would be obviously broken in such a configuration anyway.
This is a "set" method called from redfish_config (which doesn't pass in the systems_uri param). So need to remove that param here and just use self.system_uris[0] below.
Use single quotes for strings, unless there's a nested single quote. ([Python style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style))
Should check for a list.
`if not content_url:`.
I think `if props is not None` is more conventional :smile:
This is unreachable code.
I meant, `return` as `module.fail_json` will return anyways.
```suggestion module.fail_json_aws(e, msg="Boto failure") ``` Boto/API exceptions should use fail_json_aws
There are 3 AWS partitions: aws, aws-cn, and aws-us-gov. Organizations is not yet available in China but it is in GovCloud, so we should also check for `arn:aws-us-gov:organizations:`.
single line looks more readable here
And this would be... ```suggestion self._serializer = PickleSerializer(pickle_protocol) ```
Should check for a list.
Check for `compat_str` also.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
This will only get the ip address for non fixed ip's? Another way of getting ip's from libvirt is ```python domain.interfaceAddresses(libvirt.VIR_DOMAIN_INTERFACE_ADDRESSES_SRC_AGENT, 0) {'lo': {'hwaddr': '00:00:00:00:00:00', 'addrs': [{'prefix': 8, 'type': 0, 'addr': '127.0.0.1'}, {'prefix': 128, 'type': 1, 'addr': '::1'}]}, 'eth0': {'hwaddr': '52:54:00:03:b1:0b', 'addrs': [{'prefix': 24, 'type': 0, 'addr': '192.168.122.2'}, {'prefix': 64, 'type': 1, 'addr': 'fe80::c2de:d88c:1c2f:21c5'}]}} ```
single line looks more readable here
```suggestion module.fail_json_aws(e, msg="Failed to delete organizational unit") ```
```suggestion module.fail_json_aws(e, msg="Boto failure") ``` Boto/API exceptions should use fail_json_aws
```suggestion module.fail_json_aws(e, msg="Failed to create organizational unit") ``` and here
There are 3 AWS partitions: aws, aws-cn, and aws-us-gov. Organizations is not yet available in China but it is in GovCloud, so we should also check for `arn:aws-us-gov:organizations:`.
Nitpick, you can use a list comprehension here instead of calling list on a generator expression.
Maybe rename this to substitute_crypto_req since it's replacing the one from requirements.txt with the one we autodetect rather than simply appending.
I'd revert this as I don't think it is better and it isn't making this more consistent with code elsewhere. ```suggestion if isinstance(stored_backend, RemoteUserBackend): ```
Same here, seems a ValueError would be cleaner.
Can you name this a little more verbosely? I can't unsee "get best"
It seems appropriate to remove `and callable(backend.get_user)` then.
This one is fine as `auth.load_backend()` is used in `clean_username()` above. ```suggestion request.session.get(auth.BACKEND_SESSION_KEY, ''), ```
I'd go for `try/except IOError` in order to make a point that variable initialized above is in fact a fallback.
Yes, I think Django would be obviously broken in such a configuration anyway.
I don't think there's a use case for `settings.AUTHENTICATION_BACKENDS` without any backends that have a `get_user()` method.
Nitpick, you can use a list comprehension here instead of calling list on a generator expression.
Maybe rename this to substitute_crypto_req since it's replacing the one from requirements.txt with the one we autodetect rather than simply appending.
I'd revert this as I don't think it is better and it isn't making this more consistent with code elsewhere. ```suggestion if isinstance(stored_backend, RemoteUserBackend): ```
Same here, seems a ValueError would be cleaner.
Can you name this a little more verbosely? I can't unsee "get best"
It seems appropriate to remove `and callable(backend.get_user)` then.
This one is fine as `auth.load_backend()` is used in `clean_username()` above. ```suggestion request.session.get(auth.BACKEND_SESSION_KEY, ''), ```
I'd go for `try/except IOError` in order to make a point that variable initialized above is in fact a fallback.
Yes, I think Django would be obviously broken in such a configuration anyway.
I don't think there's a use case for `settings.AUTHENTICATION_BACKENDS` without any backends that have a `get_user()` method.
Based on the change above to use `**self._options` in `RedisCache._cache()`, we should replace `options` with keys that are expected. As it stands at the moment a key could be misspelled in `OPTIONS` and passed here but not used. Based on the changes requested above, we should at least have `pickle_protocol=None` here.
And this would be... ```suggestion self._serializer = PickleSerializer(pickle_protocol) ```
Rather than passing all of `OPTIONS` here we should only pass the protocol: ```suggestion def __init__(self, protocol=None): self._protocol = pickle.HIGHEST_PROTOCOL if protocol is None else protocol def dumps(self, value): return pickle.dumps(value, self._protocol) ```
Can we make `key` set to `None` by default to avoid needing to pass `None` explicitly below? ```suggestion def get_client(self, key=None, write=False): ```
The current coding style prefers single quotes. Please fix throughout, but new code only. ```suggestion if 'PICKLE_VERSION' in options: ``` _(Eventually it will be double quotes when we can use `black`.)_
We could flatten this? ```suggestion return default if value is None else self._serializer.loads(value) ```
Ah, yes. Good observation. ðŸ™‚
~~Do we want to use [`.setex()`](https://redis-py.readthedocs.io/en/stable/#redis.Redis.setex)?~~ ```suggestion client.setex(key, timeout, self._serializer.dumps(value)) ``` ~~This is just an observation that can be ignored. Maybe it is better to keep `.set()` as for `.add()` above where there is no combination of `.setex()` and `.setnx()`, requiring the use of `.set()` instead.~~ **Edit:** Definitely ignore this as I read the following [here](https://redis.io/commands/set): > Note: Since the SET command options can replace SETNX, SETEX, PSETEX, GETSET, it is possible that in future versions of Redis these commands will be deprecated and finally removed.
No need, I will commit the patch soon.
Shouldn't this be writable? ```suggestion client = self.get_client(None, write=True) ```
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
There are multiple formats, some may have `file` some may not. If some new hq format is introduced with different rendition your approach will break downloading by default while proper URL handling will not. Finally read coding conventions.
Whether it has changed or not does not mean there should be a format with invalid URL.
Must not be `None`.
Is that error "the port is in use by something" or "port is already in the state you asked for"? If it's the former (as I suspect), it's arguably incorrect to silently succeed (as the port will not be in the requested state since it's a member of the wrong broadcast domain).
This is also not properly idempotent- you're not comparing to the existing port list, so always returning "changed: True" even if it's already in the right state.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
I'm also making the rash assumption that the underlying API is itself idempotent (eg that it will succeed if you ask it to set a state that it's already in)- if not, this is a much bigger problem.
I think this is a big usability problem that users will be upset about, as it's inconsistent with pretty much every other Ansible module (and not very usable in a declarative system), but it's your module. Very difficult to change that behavior later without a separate mode switch or breaking change.
This sentence ``` "To validate an individual application's models rather than all applications' models, call ``self.check(app_configs)`` from ``handle()``, where ``app_configs`` is the list of application's configuration provided by the app registry." ``` is still valid. I will restore it.
```suggestion to iterate use a C(with_) directive. ```
(In general, I don't think modules should have such options.)
You should emphasize that the module can and will not do any idempotence checking for this.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
(Same for the related options.)
I would remove this `device` here, and similar the option name in the examples for `device_read_bps`, `device_read_iops` etc.
A general remark: you should always use complete sentences. So this should end with a period.
Please use formatting like `C(<device-on-host>[:<device-on-container>][:<permissions>])`, and `(e.g. device C(/dev/sdc:/dev/xvdc:rwm))` in the line below.
What kind of values are valid here? I would expect this to be `float` based on the `0.0` in the description.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Co-locate this with Lag and the base class rather than defining alphabetically. Same as First/LastValue.
`arity = 1`
class attribute `output_field = IntegerField()` * fairly sure that's acceptable Then drop the `__init__`
class attribute `output_field = IntegerField()` * fairly sure that's acceptable Then drop the `__init__`
Defining `__init__()` to specify a default `output_field` is required until we re-arrange `runtests.py` to avoid importing settings dependant modules before calling `django.setup()`.
class attribute `output_field = FloatField()` * fairly sure that's acceptable Then drop the `__init__`
It seems I was wrong - there are so many examples in the code that override init just to force the output field it's not worth worrying about here. Maybe in the future we can make it easier, but not necessary for this patch.
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
lines can be longer
Also here, I don't know why you are splitting lines. It does not make it easier to read. Usually it is done out of necessity (e.g. PEP8 rules require it), but in this case there is no need to make this weird.
Various modules allow to specify the name of the executable in order to use a binary from a non-standard location (i.e. a binary not located in the PATH). Look at e.g. the isoextract on how to do this.
I don't think Lightsail allows custom AMIs, at least from what I've seen in the docs and prodding at the console a bit. There is a question of if we should have a default to use the region default key when state=present instead of requiring a user specification, generally the way lightsail does keys is different from the way ec2 does keys though and I'm still familiarizing myself with the differences.
What concerns me is the implication (not directly stated in the docs, at least that I can find) that you can't add/modify a Lightsail instance's keypair after creation (short of ssh'ing to the instance with an existing key and editing auth keys by hand). **If** this is correct, I feel like we should anticipate users having a problem here and make keypair a required option for create actions. It may not be boto-y, but it's still something we can easily anticipate and make nicer for our users. If that's not correct and I'm just missing something about Lightsail key management, then maybe keypair management or a lightsail_key module would be good future improvements (but outside the scope of this PR).
Personal niggle: boto3 doesn't list keyPairName as a 'required' argument. I have some use cases where I bake the keys into the AMI and don't want to pass an additional key to the instances...
I was going off my knowledge of EC2, I've not worked with Lightsail. Fair enough if you can't use custom AMIs (today). At the same time, adding additional 'requirements' on top of boto3's only risks breaking use cases you might not have thought of.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
`client.start_execution` is idempotent but `start_execution` always returns `changed=True` if `action=start`. We probably want to `client.list_executions` to see if the execution is already running before proceeding.
I haven't actually used stepfunctions myself so I'm just going off the docs. AIUI, `ExecutionAlreadyExists` would only be returned on a running execution if the `execution_input` is different, not in a case where the input is the same. As an ansible user who is new to step functions, I feel like if I gave the same input and ran the playbook twice I would expect the 2nd time to be `changed=false` if the first execution is still running. But that doesn't mean that's how AWS users familiar with step functions will expect it.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Unfortunately this doesn't work well for formsets. I get `order.OrdersProducts.ordersproducts_set-15-product`.
Just trash it IMO, and the corresponding one in gis. The converters changes already completely removed/renamed two (private) methods on DatabaseOperations and Compiler
Unfortunately annotation names can contain LOOKUP_SEP - that is the reason why the ugly refs_aggregate method was added. For example qs.annotate(Max('id')) will create an annotation named max__id, and that can't be referred if the code checks for len(field_list) == 1.
This seems problematic. What happens if you do this: ``` reused_f = F('pk') qs1 = Model1.objects.filter(pk=reused_f) qs2 = Model2.objects.filter(pk=reused_f) ``` By my reading the reused_f will be first prepared for qs1, getting Model1.pk as col. Then the same object is reused and prepared for qs2, this time the reused_f's col is overwritten with Model2.pk. The ExpressionNodes must be either stateless, or they must be copied on prepare(). Doing the latter will be easier. A basic copy method could do the following: ``` def copy(self): copy = Empty() copy.__class__ = self.__class__ copy.__dict__ = self.__dict__.copy() return copy ```
`if it encounter` => `if it encounters`
`mentionned` => `mentioned`
extra space after ,
I realize this predates your PR, but why is 'unused' not used? It appears to be the target field provided by the SQLUpdateCompiler during as_sql(). This seems like a really good spot to allow a backend to hook in and CAST the expression to a type that is allowed by the target column.
The backend should be given a chance to figure out what should happen in this situation. Possibly by adding DatabaseOperations.resolve_expression_source(expression, sources)
`target` is only available for `Col`, so it crashes for expressions, see `test_slicing_of_f_expression_with_annotated_expression`.
This will not work for `OuterRef()` :disappointed: because we don't resolve it properly, so it generates: ```sql EXISTS ( SELECT (1) AS "a" FROM "expressions_company" U0 WHERE SUBSTRING(U0."name", 8, 1) = (SUBSTRING(U0."name", 3, 1)) LIMIT 1 ) ``` instead of ```sql EXISTS ( SELECT (1) AS "a" FROM "expressions_company" U0 WHERE SUBSTRING(U0."name", 8, 1) = (SUBSTRING("expressions_company"."name", 3, 1)) LIMIT 1 ) ``` see `test_slicing_of_outerref`.
```suggestion raise ValueError('Slice stop must be greater than slice start.') ```
I'd move this line to the top of `__init__()` so it isn't lost below all the conditional logic.
We allow up to 119 characters, so this doesn't need to be wrapped. ```suggestion def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False): ```
It would be neat if `randomize_list` also accepted a similar `seed` arg for repeatable (pseudo-random) ordering.
```suggestion module.exit_json(changed=True,**camel_dict_to_snake_dict(res))) ```
You can achieve this same result output with `module.exit_json(changed=True,**camel_dict_to_snake_dict(execution))` which can be imported from `ansible.module_utils.ec2`. That will automatically cover if AWS ever starts returning additional keys from this API and is generally easier to maintain.
I haven't actually used stepfunctions myself so I'm just going off the docs. AIUI, `ExecutionAlreadyExists` would only be returned on a running execution if the `execution_input` is different, not in a case where the input is the same. As an ansible user who is new to step functions, I feel like if I gave the same input and ran the playbook twice I would expect the 2nd time to be `changed=false` if the first execution is still running. But that doesn't mean that's how AWS users familiar with step functions will expect it.
`client.start_execution` is idempotent but `start_execution` always returns `changed=True` if `action=start`. We probably want to `client.list_executions` to see if the execution is already running before proceeding.
Match the error message
This is failing because `match` takes a regular expression, which is interpreting the `()` as grouping characters, not as literal parenthesis. Another issue here is the error message can be two different values. ```suggestion with pytest.raises(TypeError, match=r'(no ordering relation is defined for complex numbers)|(unsupported operand type\(s\) for /)'): ```
```suggestion - There is a small delay (typically about 5 seconds, but can be as long as 30 seconds) before obtaining the random values when requesting a validation. ```
```suggestion serial_port=dict(type='int', required=True), ```
I think all the `serial_` prefixes are repetitive. I think this module should be renamed `cpm_serial_port` then you can just remove all the `serial_` prefixes. This would also remove confusion on whether this module is configuring serial or network ports.
can we rename this to make clear that this has nothing to do with git, but is only about s3? `--upload-to-s3` seems a bit long
Similarly, ```if tc['skip'].get('i')```
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
I was also confused by this logic for a while and I got an optimization idea from the [Raft dissertation](http://wcl.cs.rpi.edu/pilots/library/papers/consensus/RAFTOngaroPhD.pdf) about this, The sentence below is taken from section "5.4.2 Committing entries from previous terms": ``` There are some situations where a leader could safely conclude that an older log entry is committed (for example, if that entry is stored on every server), but Raft takes a more conservative approach for simplicity. ``` so we can also update commitIndex(highWatermark) if logEndOffset of all followers have passed the highWatermark. I don't think this is a good idea since it makes the logic opaque but will not necessarily really optimize any performance, so I just mention it here and feel free to ignore it Ã°ÂŸÂ™Âˆ.
aws_ip_ranges -> aws_service_ip_ranges
I don't think boto_params will include access_key or secret_key if a profile is being used (which might be through passing `profile` parameter or with `AWS_PROFILE` set) Looking at https://github.com/jmenga/requests-aws-sign the way to do it is to use ``` session = session.Session() credentials = session.get_credentials() ``` but even there, we'd need to cope with the `profile` parameter.
You probably should add yourself to the authors list as well.
This seems random and can lead to flaky tests. So there is no way to measure latency while Trogdor task is running? For the rate limit test below we do wait for task to run.
this is a catchall, we normally frown on those. also note that for all of your `% e` they should be `% to_native(e)` to ensure proper character conversions
nit: doc how the output of `iperf` looks like. helps with understanding this code and regex.
Just an empty line, could be removed for cleaner code
In `check_mode: yes` the returned query will always be `SELECT 1 FROM 'tablename'`. But this is not what I expected. Normally I like to see what query will be executed for that change. Please change this behavior to always return the query which will be executed even if it is not in `check_mode: yes`
nit: add a newline here too.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
use a single line or use hanging indent (we avoid non-4 space indents)
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
single line as above
```suggestion item, fields=fields, using=self.db, ```
You probably should add yourself to the authors list as well.
You should really have at least one example with `state=absent`.
I see the old tests do this, but (AFAICS) there's no reason to to store `post_data` on `self`. (It's not accessed outside this test case.)
Tripe `"""` for a docstring. Missing period at the end.
Maybe this should be a class docstring :thinking:
should be displayed
We'd only put the ticket number for a particularly tricky ticket. I don't think it's necessary here.
I don't think we should go so deep into validation, we opt out from numbers but at the same time we allow the whole unicode range. Unicode numbers like `à¹‘` would happily validate therefore it's an uphill battle. I'd opt for a vastly simplified regex to validate FQDN: `'(?:[a-z0-9\u00a1-\uffff-]+\.?)+'`. Sure it'll let some invalid segments go through (e.g. leading/trailing hyphens) but at least it doesn't pretend of being exhaustive. Proper validation requires a parser anyway.
It wouldn't validate the following: - http://.com - http://. - http://.. - http://../ - http://.www.foo.bar/ - http://.www.foo.bar./ It would indeed validate the following URL (but they are actually valid): - http://example - http://example. All the others are about leading and trailing hyphens, if we really want to filter them out despite the increased complexity then I suggest we break the pattern into multiple variable for readability: https://gist.github.com/386830e46e8d2aca9dcb Regarding formal grammar, it's spread out among a bunch of RFCs, I doubt it's worth the effort.
For the cases where I've wanted to use a nose test generator, I've ended up just making the test class inherit from object. You lose the TestCase methods (various self.assert*) but usually test generators methods have pretty simple assertions so not too much of a issue. To me it is simpler than injecting the support into a TestCase subclass. (an example is https://github.com/alikins/ansible/blob/6e8a3d0c25fdeaa349351c6636b062376c30b0a6/test/units/plugins/lookup/test_ini.py)
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
with -> width
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
width, height, and offset
comma after tuple
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
++ thanks for changing this :)
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
please fail if required stuff is null
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
with -> width
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
width, height, and offset
comma after tuple
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
++ thanks for changing this :)
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
please fail if required stuff is null
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
with -> width
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
width, height, and offset
comma after tuple
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
++ thanks for changing this :)
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
please fail if required stuff is null
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
with -> width
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
width, height, and offset
comma after tuple
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
++ thanks for changing this :)
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
please fail if required stuff is null
This function is too complex. Could you please factor and avoid: - more than 2 level of condition - inner loop. You can probably break down the function in several small functions.
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
no restructured text (:class:) in docstrings please
with -> width
comma after tuple
width, height, and offset
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
This message is different on PostgreSQL 13+, it should be enough to check that a constraint name is in the message.
redundant, remove ```suggestion ```
I'm not sure why these tests raises a `ProgrammingError` :thinking:
```suggestion with self.assertRaises((OperationalError, ProgrammingError)): ```
```suggestion with self.assertRaises((OperationalError, ProgrammingError)): ```
```suggestion with self.assertRaises((OperationalError, ProgrammingError)): ```
Good catch, I will remove it before final squash.
Why we're adding a constraint manually and not with `AddConstraintNotValid()`? Also, please use hanging indentation.
> Is there any specific reason why we would prefer using the operation in this case? Yes, because we have it. Using a RAW SQL is the last option, we're developing the ORM in order not to use them.
I think these should be `is True` since the `.assertTrue` being replaced would fail if on something like `"foo"` which is truthy. So the `assert "foo"` would pass where `self.assertTrue("foo")` wouldn't.
This function is too complex. Could you please factor and avoid: - more than 2 level of condition - inner loop. You can probably break down the function in several small functions.
with -> width
Ok thanks Tim
comma after tuple
width, height, and offset
you might want to get the module/action as the include can be given a name that does not match `include:`
You shouldn't need the extra parentheses inside `extend()`, FYI.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
I think a better wording would be: "Backwards compatibility for storage backends without support for `max_length` will be removed in Django 2.0."
I would consider that as not working
I think it's the right class: ``` In [38]: class desc: ...: def __get__(self, instance, cls): ...: if instance is None: ...: return self ...: return instance.__dict__['_%s__mangled' % cls.__name__] In [39]: class A: ...: d = desc() ...: ...: def __init__(self): ...: self.__mangled = 42 ...: In [40]: A().d Out[40]: 42 ```
@graingert Probably it doesn't worth it, but still doable :-) ``` In [85]: class desc: ...: def __init__(self, f): ...: self.name = f.__name__ ...: ...: def __get__(self, instance, cls): ...: if instance is None: ...: return self ...: ...: for cls in cls.__mro__: ...: name = '_%s%s' % (cls.__name__, self.name) ...: if cls.__dict__.get(name) is self: ...: return name ...: ...: class A: ...: @desc ...: def __x(self): ...: pass ...: ...: ...: class B(A): ...: pass ...: In [86]: a = A() ...: a._A__x ...: Out[86]: '_A__x' In [87]: b = B() ...: b._A__x ...: Out[87]: '_A__x' ```
@graingert `cls` is passed here.
Ah. lambda is prettier though :). maybe we can run pep8 with ignore lambda error.
Might make sense to raise exception in this case: ``` class Test: @cached_property def a(self): pass b = a ```
I'd say `on Python < 3.6`
Keep using `self.module.fail_json()`, of course fine. Ã°ÂŸÂ‘Â Changing all `.format()` into `%`, great! Ã°ÂŸÂ‘Â I've never noticed that the module has so many `.format()`...my reviews must be too rough at that time Ã°ÂŸÂ˜Â“
Use `%` instead of `.format()` to support older version of Python. ```python self.abort('%s is not a supported option on target platform' % key) ``` (L557 too)
I would prefer to keep a slice logic with `start` and `stop` instead of `low` and `length`.
Chop blank line.
We can remove this check after fixing the `Field.slice_expression()`.
Chop blank line.
We allow up to 119 characters, so this doesn't need to be wrapped. ```suggestion def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False): ```
```suggestion return '-' + value if neg else value ```
```suggestion return '-' + value if neg else int(value) ```
Why do we need to create the intermediate tuple? ```suggestion for ie_key in set( map(lambda a: a[5:], filter( lambda x: callable(getattr(TestDownload, x, None)), filter( lambda t: re.match(r"test_.+(?<!(?:_all|.._\d|._\d\d|_\d\d\d))$", t), dir(TestDownload))))): ```
Is there any need for these separate methods? I could understand if it were required for testing, but that doesn't seem to be the case. If we are to keep them separate, could we not do the following to avoid the nested method calls? ```python is_ccw = property(_is_ccw_py if geos_version_tuple() < (3, 7) else _is_ccw_geos) ```
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
```suggestion assert isinstance(wrap_var(['foo']), list) ```
```suggestion assert wrap_var([None])[0] is None ```
```suggestion assert isinstance(wrap_var(set(['foo'])), set) ```
```suggestion assert item is None ```
```suggestion assert wrap_var(dict(foo=None))['foo'] is None ```
flake8 complains about missing spaces around `*`
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
with -> width
comma after tuple
width, height, and offset
> But I think it would be better if we didn't let the debug implementation slow the non-debug implementation more. Yeah. That did occur to me, but haven't had time to check the performance. It might still work out in concert with other ideas I've hadâ€¦ But those don't need to hold this up.
I think that you could move the slicing inside `DebugLexer._tag_re_split()` and then `DebugLexer.tokenize()` will be even closer to `Lexer.tokenize()`: ```suggestion for bit, position in self._tag_re_split(): ``` Maybe with these changes it makes sense to rename `DebugLexer._tag_re_split()` to something like `.split()` and add the same method to `Lexer` with something like: ```python def split(): yield from ((bit, None) for bit in tag_re.split(self.template_string)) ``` Then you should be able to ditch `DebugLexer.tokenize()` entirely and inherit it.
I think `get_internal_type` is better to use.
Wouldn't this approach fix the slicable fields to just these types? A mixin along with `slicable = True/False` on `F` may be an approach. Besides, I believe the policy is to keep `django.contrib.postgres` as separate as possible.
We can remove this check after fixing the `Field.slice_expression()`.
Chop blank line.
```suggestion # Windows registry may not be configured with correct # mimetypes. ```
Please use f-strings as Python 3.6+ is now the requirement More information is available including some benchmarks. https://cito.github.io/blog/f-strings/
This syntax is not available in Python 3. You can simply use ``` return [x^y for x, y in zip(data1, data2)] ```
```suggestion self.assertEqual(value, b'text/plain') ```
this module is marked as "legacy" and therefore the CI is not complaining about pep8 here, but it would help if we try to be pep8 compliant.
yep, that is what I meant, basically make sure new code is pep8 compliant
Plz use a context manager to have a safe resource closing ```suggestion with tarfile.open(tar_filepath, mode='w:gz') as tar_file: ```
Oops, I misread the diff and see that you only modified the existing archives. Still an explanation of exactly what going on would be nice as it's not obvious to me.
if a case is only used in 1 file like `FinderTestCase`, I'd put it there.
Yes please, I didn't audit for all instances.
I learned recently that you can use actual separate literals to improve readability: ```suggestion @pytest.mark.parametrize(['url', 'expected'], [ ```
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
maybe just ```suggestion part_boundary, b"--", ```
May I ask you to implement an exponential backoff algorithm here? https://en.wikipedia.org/wiki/Exponential_backoff
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
Yeah, it's fine.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
For future reference, Kafka uses relatively long lines: up to 100 is considered fine. I can fix the instances in this PR during the merge, but good to take into account in future contributions.
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
Yeah, it's fine.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
```suggestion item, fields=fields, using=self.db, ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
For future reference, Kafka uses relatively long lines: up to 100 is considered fine. I can fix the instances in this PR during the merge, but good to take into account in future contributions.
Careful - Value only works with basic types that the adapter itself knows how to convert to SQL. You could try passing the fields Field as the `output_field` param to `Value` which will get it to translate the value using the fields get_db_prep_* methods. Unfortunately Fields are not themselves Expressions, which is why you've chosen to wrap the field in a Value. We could consider the following options: 1. Implement the Expression API on fields directly 2. Add a FieldExpression that wraps a field and translates the Expressions API onto the Field API Personally, I've been thinking about 1 being useful a lot. Fields should just be another type of expression. If I were doing fields from scratch, they would be. If just passing the fields Field instance to output_field doesn't work for more complex types, then you might need to consider the options above.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Is there any cases of `not f.is_relation and f.many_to_many`? If not then `f.many_to_many` should be sufficient.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
This seems problematic. What happens if you do this: ``` reused_f = F('pk') qs1 = Model1.objects.filter(pk=reused_f) qs2 = Model2.objects.filter(pk=reused_f) ``` By my reading the reused_f will be first prepared for qs1, getting Model1.pk as col. Then the same object is reused and prepared for qs2, this time the reused_f's col is overwritten with Model2.pk. The ExpressionNodes must be either stateless, or they must be copied on prepare(). Doing the latter will be easier. A basic copy method could do the following: ``` def copy(self): copy = Empty() copy.__class__ = self.__class__ copy.__dict__ = self.__dict__.copy() return copy ```
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
This function is too complex. Could you please factor and avoid: - more than 2 level of condition - inner loop. You can probably break down the function in several small functions.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
flake8 complains about missing spaces around `*`
width, height, and offset
comma after tuple
with -> width
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
please fail if required stuff is null
WDYT about adding `or self.title` here? ``` python self.title = title or self.title or self.lookup_title ``` This would add support for the following: ``` python class MyStaffFilter(BooleanFieldListFilter): title = "By can haz admin" # no effect :( overwritten in __init__ @admin.register(User) class UserAdmin(UserAdminBase): list_filter = ( ("is_staff", MyStaffFilter), ) ```
What do you think about ```python def __repr__(self): template = "<%(name)s: fields='%(fields)s'%(fastupdate)s%(gin_pending_list_limit)s>" template_values = { 'name': self.__class__.__name__, 'fields': ', '.join(self.fields), 'fastupdate': '', 'gin_pending_list_limit': '', } if self.fastupdate is not None: template_values['fastupdate'] = ', fastupdate=%s' % ('True' if self.fastupdate else 'False') if self.gin_pending_list_limit is not None: template_values['gin_pending_list_limit'] = ', gin_pending_list_limit=%d' % self.gin_pending_list_limit return template % template_values```
Do you think we're adding value with such complicated `__repr__` methods? I've thought maybe we should ditch worrying about including of Index's params in them and just let `Index.__repr__` show the basics. Does the repr should up anywhere important? I can't think of anywhere.
Tests for this method seem missing. It seems like we need a better way to build these reprs that's not so complicated and repetitive for each index.
It's fine ðŸ‘
This could be shortened to: ```python if str(retry[1]).startswith('inf'): ```
I think either name should be mandatory or this should take a label selector.
How about moving this `if` below the next `if`? Then you don't need the inner `if`.
this is already done by argspec when param is defined as boolean, all redundant
Use `missing_required_lib` from `ansible.module_utils.basic`
```suggestion ``` I don't think you should do this, case is meaningful: ``` max@mde-oxalide % pacman -Q libreoffice-en-US libreoffice-fresh 6.1.3-1 max@mde-oxalide % pacman -Q libreoffice-en-us error: package 'libreoffice-en-us' was not found ```
```suggestion pkg_name = line.partition(':')[-1].strip() ``` idem, no need to lower.
@thomwiggers `line.partition(':')[2]` yes, I have no problem with that.
If you're going to do partition, I'd rather use the fact that it'll always return a 3-tuple instead of using `-1` to index the third element. i.e.: ```python line.partition(':')[2] ```
```suggestion pkg_version = line.partition(':')[-1].strip() ```
It doesn't work if pkg_provides contain a version: ``` max@mde-oxalide % pacman -Qqei perl | grep Provide Provides : perl-archive-tar=2.30 perl-attribute-handlers=1.01 perl-autodie=2.29 perl-autoloader=5.74 perl-autouse=1.11 perl-b-debug=1.26 perl-base=2.27 perl-bignum=0.49 perl-carp=1.50 perl-compress-raw-bzip2=2.074 perl-compress-raw-zlib=2.076 perl-config-perl-v=0.29 perl-constant=1.33 perl-cpan-meta-requirements=2.140 perl-cpan-meta-yaml=0.018 perl-cpan-meta=2.150010 perl-cpan=2.20 perl-data-dumper=2.170 perl-db_file=1.840 perl-devel-ppport=3.40 perl-devel-selfstubber=1.06 perl-digest-md5=2.55 perl-digest-sha=6.01 perl-digest=1.17_01 perl-dumpvalue=1.18 perl-encode=2.97 perl-encoding-warnings=0.13 perl-env=1.04 perl-experimental=0.019 perl-exporter=5.73 perl-extutils-cbuilder=0.280230 perl-extutils-constant=0.25 perl-extutils-install=2.14 perl-extutils-makemaker=7.34 perl-extutils-manifest=1.70 perl-extutils-parsexs=3.39 perl-file-fetch=0.56 perl-file-path=2.15 perl-file-temp=0.2304 perl-filter-simple=0.95 perl-filter-util-call=1.58 perl-getopt-long=2.5 perl-http-tiny=0.070 perl-i18n-collate=1.02 perl-i18n-langtags=0.43 perl-if=0.0608 perl-io-compress=2.074 perl-io-socket-ip=0.39 perl-io-zlib=1.10 perl-io=1.39 perl-ipc-cmd=1.00 perl-ipc-sysv=2.07 perl-json-pp=2.97001 perl-lib=0.64 perl-libnet=3.11 perl-locale-codes=3.56 perl-locale-maketext-simple=0.21_01 perl-locale-maketext=1.29 perl-math-bigint-fastcalc=0.5006 perl-math-bigint=1.999811 perl-math-bigrat=0.2613 perl-math-complex=1.5901 perl-memoize=1.03_01 perl-mime-base64=3.15 perl-module-corelist=5.20180622 perl-module-load-conditional=0.68 perl-module-load=0.32 perl-module-loaded=0.08 perl-module-metadata=1.000033 perl-net-ping=2.62 perl-params-check=0.38 perl-parent=0.236 perl-pathtools=3.74 perl-perl-ostype=1.010 perl-perlfaq=5.021011 perl-perlio-via-quotedprint=0.08 perl-pod-checker=1.73 perl-pod-escapes=1.07 perl-pod-parser=1.63 perl-pod-perldoc=3.2801 perl-pod-simple=3.35 perl-pod-usage=1.69 perl-podlators=5.006 perl-safe=2.40 perl-scalar-list-utils=1.50 perl-search-dict=1.07 perl-selfloader=1.25 perl-socket=2.027 perl-storable=3.08 perl-sys-syslog=0.35 perl-term-ansicolor=4.06 perl-term-cap=1.17 perl-term-complete=1.403 perl-term-readline=1.17 perl-test-harness=3.42 perl-test-simple=1.302133 perl-test=1.31 perl-text-abbrev=1.02 perl-text-balanced=2.03 perl-text-parsewords=3.30 perl-text-tabs=2013.0523 perl-thread-queue=3.12 perl-thread-semaphore=2.13 perl-threads-shared=1.58 perl-threads=2.22 perl-tie-file=1.02 perl-tie-refhash=1.39 perl-time-hires=1.9759 perl-time-local=1.25 perl-time-piece=1.3204 perl-unicode-collate=1.25 perl-unicode-normalize=1.26 perl-version=0.9923 perl-xsloader=0.30 ```
Is it possible ? For me it would mean that it exist a package without version.
```suggestion rcmd = "%s --sync --info %s" % (pacman_path, realname) ```
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
There is no need to pass `on_conflict`, `update_fields`, or `unique_fields` when `on_conflict` is None.
```suggestion item, fields=fields, using=self.db, ```
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
Can we always assume we are getting a list returned? I believe `SQLInsertCompiler.execute_sql()` always returns a list now? (Although we may need a note in the backward incompatibility section for database backends?) If all that were the case, we could possibly condense this whole section to something like the following: ```python returning_fields = None bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert if bulk_return and not ignore_conflicts: returning_fields = self.model._meta.db_returning_fields return list(itertools.chain.from_iterable( self._insert( item, fields=fields, using=self.db, returning_fields=returning_fields, ignore_conflicts=ignore_conflicts, ) for item in (objs[i:i + batch_size] for i in range(0, len(objs), batch_size)): )) ```
single line as above
use a single line or use hanging indent (we avoid non-4 space indents)
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
As with the old `_batched_insert` code, I think we should pass the db alias here `self._insert(objs_without_pk, fields, using=self.db)`, it defaults to that but passing it seems more explicit.
Would it be better to raise `NotImplementedError()` so backends don't overlook implementing it? They can always return an empty list if it's not supported.
This can be single-lined: ```suggestion return (matched_tags or not tags) and not test_tags.intersection(exclude_tags) ```
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
I would chop `does_`: ```suggestion if test_match_tags(test, tags, exclude_tags) ```
```suggestion I(minvalue), I(maxvalue), I(start), I(cache), I(cycle), I(rename_to), ```
Yep, simply ignoring all other options when `state == 'absent'` is the easiest solution (that's what most other modules do, too).
Why have both `schema` and `newschema`? I would assume that if I specify another value for `schema`, that the schema will be changed.
This should not result in a warning, but simply result in `changed == False`.
I would expect a module to issue warnings only for something which the user needs to be warned about. If the user wants something to be absent and it already is absent, that's not something I would expect a warning about. You also don't issue a warning if the user has `state == 'present'` and the sequence is already there.
```suggestion - If C(false) (NO CYCLE) is specified, any calls to nextval after the sequence ```
maybe should be get('properties', {}).get('ipConfigurations') so it will be shorter
these lines are a bit long... ``` m2m_field = models.ManyToManyField( VeryLongModelNamezzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz, related_name="rn1", ) ```
Note that ...
`related_manager_name` not `related_model_name`
WDYT about adding `or self.title` here? ``` python self.title = title or self.title or self.lookup_title ``` This would add support for the following: ``` python class MyStaffFilter(BooleanFieldListFilter): title = "By can haz admin" # no effect :( overwritten in __init__ @admin.register(User) class UserAdmin(UserAdminBase): list_filter = ( ("is_staff", MyStaffFilter), ) ```
version added requires quotes, otherwise it will be processed as number
Would it be better to raise `NotImplementedError()` so backends don't overlook implementing it? They can always return an empty list if it's not supported.
Either add `self` or make it `@staticmethod`
The hexdigest will always be a fixed length so this only happens if the provided suffix is too long, correct? In that case, I think it would be better to raise an error that the provided suffix is too long.
It sounds like maybe index_type could be used as the suffix and this method doesn't need that argument. I guess the question is whether index_type should be limited to 3 characters or if truncating the first 3 characters of "index_type" as the suffix is okay.
This check repeats 3 times and you've already done at least one typo in one of the instances. Please move it into a function.
Oh, I missed that the check is slightly different. but still such function could accept key + check type class and could be reused 3 times if you want to go for it :)
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
It doesn't make sense to turn a unicode string into a unicode string. It's just `str(str(''))`, which is probably not what you want. Maybe you wanted to turn bytes into text here? In such case check'd be against `binary_type` and the translation should use `to_text()` shim.
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Which state the entity should have after executing the action can be specified by passing `wait_condition` parameter. -> State which the entity should be in after execution of the action can be specified by `wait_condition` parameter.
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
```suggestion msg = ... with self.assertRaisesMessage(ValueError, msg): ```
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Omit these lines please.
As long as they both equals it shouldn't make a difference. By that I mean you should have a test asserts `Index('foo') == Index(F('foo'))` An alternative could be to defer that to `create_sql`.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
Shouldn't `w[0].message` be `w[0].category`? You could also deindent this line.
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
These parens aren't necessary for unpacking the return values.
```suggestion msg = ... with self.assertRaisesMessage(ValueError, msg): ```
Yea, as Sergey said above, there's a bug where such attributes aren't cached.
Ahh right just read through the whole discussion. I wonder if this could be an opportunity to solve the issue. e.g. ```python def __set_name__(self, owner, name): if self.name is None: self.name = name self.func = self.real_func elif name != self.name: # Re-assignment of a cached property to a class # creates a new instance. instance = type(self)(self.real_func, name) setattr(owner, name, instance) ``` That would make ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = is_postgresql_9_5 ``` The equivalent of ```python @cached_property def is_postgresql_9_5(self): return self.connection.pg_version >= 90500 has_select_for_update_skip_locked = cached_property( is_postgresql_9_5.real_func, 'has_select_for_update_skip_locked' ) ``` And address the bug preventing the `__dict__` level caching bug discovered by @sir-sigurd.
FWIW the `TypeError` raised here would be backward incompatible. Such a pattern is even used in Django core. https://github.com/django/django/blob/0ce2ad9ca4623cfd6dc2515430c0ae8a1717a607/django/db/backends/postgresql/features.py#L56-L74
Omit these lines please.
As long as they both equals it shouldn't make a difference. By that I mean you should have a test asserts `Index('foo') == Index(F('foo'))` An alternative could be to defer that to `create_sql`.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
Shouldn't `w[0].message` be `w[0].category`? You could also deindent this line.
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
These parens aren't necessary for unpacking the return values.
Indeed, you are right.
Such an extensive docstring is not necessary, IMO.
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
@graingert Probably it doesn't worth it, but still doable :-) ``` In [85]: class desc: ...: def __init__(self, f): ...: self.name = f.__name__ ...: ...: def __get__(self, instance, cls): ...: if instance is None: ...: return self ...: ...: for cls in cls.__mro__: ...: name = '_%s%s' % (cls.__name__, self.name) ...: if cls.__dict__.get(name) is self: ...: return name ...: ...: class A: ...: @desc ...: def __x(self): ...: pass ...: ...: ...: class B(A): ...: pass ...: In [86]: a = A() ...: a._A__x ...: Out[86]: '_A__x' In [87]: b = B() ...: b._A__x ...: Out[87]: '_A__x' ```
I think it's the right class: ``` In [38]: class desc: ...: def __get__(self, instance, cls): ...: if instance is None: ...: return self ...: return instance.__dict__['_%s__mangled' % cls.__name__] In [39]: class A: ...: d = desc() ...: ...: def __init__(self): ...: self.__mangled = 42 ...: In [40]: A().d Out[40]: 42 ```
Might make sense to raise exception in this case: ``` class Test: @cached_property def a(self): pass b = a ```
`always_text` is gone.
I'd say `on Python < 3.6`
I think we should create a hook similar to the `_field_should_be_indexed()`, that will allow 3rd-party backends to adjust this behavior, e.g. ```python class BaseDatabaseSchemaEditor: ... def _field_should_be_altered(self, old_field, new_field): # Don't alter when changing only a field name. return ( old_field.column != new_field.column or old_field.deconstruct()[1:] != new_field.deconstruct()[1:] ) ```
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
In this test `stdin=MockTTY()` is still required.
`stderr` -> `stdout`
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
This can be single-lined.
A `minimumInputLength` of zero is [the default for Select2](https://github.com/select2/select2/blob/120672dce79b6862cb17a966af06e0617a404b58/src/js/select2/defaults.js#L363).
Ensure that the new arguments are keyword only: ```suggestion def handle_word( self, word, *, safe_input, trim_url_limit=None, nofollow=False, autoescape=False, ): ```
I removed the NotIn assertions in my edits because they are brittle since a typo in the message means they would inadvertently pass. I suppose if error messages were class attributes that would make it more robust, but it seems unlikely to me that a regression could be introduced such that both messages are displayed.
chop "one of" add comma before "or"
this should be in `finally` just in case the commands before throw an exception
I would say `Deploy key has been updated` instead of `should have been updated`
move to finally
Same as for the updated, I'd rather say `has been deleted`
use the `missing_required_lib` function from `ansible.module_utils.basic`
use `missing_required_lib` from `ansible.module_utils.basic`
The new implementation should support signatures for a number of resolvelib versions. They could be defined dynamically in runtime.
Ah, right, got it :+1:
I think we should add an `allow_overwrite` or similar param.
```suggestion 'public_ip', ```
```suggestion SubnetIds=module.params.get('subnet_ids'), ``` and here as well
```suggestion SubnetIds=module.params.get('subnet_ids'), ``` Here this var needs to stay as you had it originally - ansible prefers snake cased but boto typically needs camelcase. `instance_parameters` will be passed into the boto connection so needs to match what the API expects, both here and later when you access the returned parameters. https://docs.aws.amazon.com/dms/latest/APIReference/API_CreateReplicationSubnetGroup.html#API_CreateReplicationSubnetGroup_RequestSyntax
Yes I think you might confuse something here. This is just creating a group with one host.
@CFSworks didn't got this, why are we creating a group with the Droplet ID, IP or name? I think you made this to keep the old behaviour but I'm afraid ansible will discard hostvars, which are using **_dest_** since they don't match **_do_id_** :/ I didn't tests this and maybe I'm confused.
Please don't make lines longer! There was nothing really wrong with this line before
no restructured text (:class:) in docstrings please
This check is also redundant.
Maybe add "ansible versions below 2.10" or something so it's clear this is a one-time problem, not that they can never upgrade `ansible` again...
This should set self._connected = True (not really needed right now but could have ramifications later) and also use display to print a message. For instance, this is what the docker connect plugin does: ``` def _connect(self, port=None): """ Connect to the container. Nothing to do """ super(Connection, self)._connect() if not self._connected: display.vvv(u"ESTABLISH DOCKER CONNECTION FOR USER: {0}".format( self.actual_user or '?'), host=self._play_context.remote_addr ) self._connected = True ```
It''s desirable to call to_bytes with errors='surrogate_or_strict' so that it raises an exception instead of replacing undecodable bytes with a "?". Also, map() is more pythonically expressed as a list comprehension. ``` python local_cmd = [to_bytes(i, errors='surrogate_or_strict') for i in local_cmd] ```
What is this supposed to achieve? Modules are cached on import, they'll be picked up from that cache regardless of the pointers being listed in `sys.modules`.
Unfortunately, shlex.split() needs a bit of code to make it compatible with both python-2.6 and python3.x On python-2.6, it only works on byte strings. In python3.x it only works with text strings. So you have to test for python version and then convert appropriately. Code like the following is what I use: ``` python from ansible.compat import six from ansible.module_utils._text import to_bytes, to_text [...] nspawn_args = self._play_context.nspawn_args if six.PY2: nspawn_args = shlex.split(to_bytes(nspawn_args, errors='surrogate_or_strict')) else: nspawn_args = shlex.split(to_text(nspawn_args, errors='surrogate_or_strict'))
if not handling become methods update the class variable to indicate this
not that, i'm talking about `become_methods` list class variable.
example that supports all defaults except 'su' ``` become_methods = frozenset(C.BECOME_METHODS).difference(('su',)) ```
su requires a tty, that is why most 'subprocess' plugins don't support it.
```python raise ValueError( "ISO week directive '%s' is incompatible with the year " "directive '%s'. Use the ISO year '%%G' instead." % ( week_format, year_format, ) ) ```
As long as they both equals it shouldn't make a difference. By that I mean you should have a test asserts `Index('foo') == Index(F('foo'))` An alternative could be to defer that to `create_sql`.
flake8 complains about missing spaces around `*`
I'd expect to see a test for this. Please audit test coverage carefully and make sure all lines are covered.
Maybe shorter `# The entire email address must be parsed.`.
I'd reverse this to `if data is None`, to match the structure below (and prevent unnecessary negation).
I think we can move this under `try ... except`, e.g. ```python try: token, rest = parser.get_mailbox(addr) if rest: # The entire address must be parsed. raise ValueError nm = token.display_name or '' localpart = token.local_part domain = token.domain or '' except (HeaderParseError, ValueError, IndexError): raise ValueError("Invalid address '{}'".format(addr)) ```
Nothing, it is just my personal preferences and it was just a suggestion.
IMO it will me more readable to move `base` outside, also we resolve `F()` expressions twice (line 28 & 29), so maybe, e.g. ```python if isinstance(expression, str): expression = F(expression) if isinstance(expression, F): expression = base.resolve_expression(query=query, simple_col=True) else: expression = expression.resolve_expression(query=query) ``` Moreover I think we don't need to iterate twice over the expressions: ```python for expression, operator in self.expressions: ... expressions.append('%s WITH %s' % (sql % params, operator) return expressions ```
comma after tuple
Whoaw, I knew there was an impact, but did not suspect this. This is ugly. cc @kbreit Opened a ticket for this. https://github.com/ansible/ansible/issues/52717
We tend to use HAS_REQUESTS as a standard way of doing this, but... We actually prefer that modules use *lib/ansible/module_utils/urls.py*, specifically **fetch_url()** or **open_url()** for anything HTTP/REST based.
```suggestion - requests (Python library) ```
```suggestion - FQDN or IP Address for the connection (vCenter or ESXi Host). ```
This should be an @staticmethod nad self removed.
Actually... Even better: use a defaultdict instead of this. This is how defaultdicts work: ``` python from collections import defaultdict foo = defaultdict(list) foo[k].append(v) ```
Same here, make this a regular function
Make this a regular function
This should be a @staticmethod and self removed.
Actually, better would be to move this to the toplevel as a regular function
> Shall I also remove `test_target_model_is_unsaved_save` and `test_target_model_is_unsaved_bulk_create`? Yes, they are redundant.
It looks like a duplicate of `test_save_unsaved_instance_on_generic_foreign_key` :thinking:
I removed it.
Ahh true, sorry for the noise. No changes are required.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
For python 2.6 compatibility, this and the following four strings have to look like `cmd = "ping {0} {1}".format(vrf, dest)`
Okay, so to implement the examples like this: ``` with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ``` We'll want to take the dict from terms instead of from kwargs. terms will hold the dict that the user passed in so you should be able to drop in replace it in your constructor like so: ```CyberarkPassword(**terms)```
not a list of dicts, just pass a dict to the with: ```yaml with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ```
@pilou-, cyberark-bizdev I had a chance to talk to bcoca today and he said the best way to do this is to pass a list of dicts to the lookup. Then either of these playbook constructs can be valid: ``` with_cyberaskpassword: - {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - {'appid': 'Application2', 'query': 'safe=Safe2;Folder=root;Object=User2', 'output': 'password,passprops.username,passprops.address'} with_cyberarkpassword: {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'} - debug: msg='{{ lookup('cyberark_password', {'appid': 'Application1', 'query': 'safe=Safe1;Folder=root;Object=User1', 'output': 'password,passprops.username,passprops.address'}) }}' ``` Implementing that will take a little restructuring. I'll go into that in the run method below.
Okay, so @cyberark-bizdev take out my first example. The last two are what we're shooting for. (The middle one is the same as bcoca's example, just in compact yaml instead of "readable" yaml ;-)
add trailing comma
I wonder if something like `serialize_result` might be a more descriptive name for this function.
Actually this should be `classmethod` for `InlineModelAdmin` and a regular method for `ModelAdmin`.
add trailing comma
```suggestion Convert the provided model object to a dictionary that is added to the results list. ```
It would be useful to tell the user which `key` is invalid.
^ that seems to be an expression not really a data type issue (sorting keys, this is another known json issue), in any case, there is also an existing `jsonify` in module_utils.
a simpler fix i was already working on: `+ display.display(json.dumps(text, cls=AnsibleJSONEncoder, sort_keys=True, indent=4))`
well, not die with unexpected exception .. tempted to say there is no real reason the type should be incorrect for any keys. So ending in an error should be fine, just not an unhandled one.
`form_class` is defined in `RangeField.formfield()` so this is redundant.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
This isn't needed, since we now only support 2.6+ anyway.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
Can you name this a little more verbosely? I can't unsee "get best"
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
Additional task can be executed before action on entity is executed by passing `pre_action` parameter to method. Another task can be executed after action is executed, by passing `post_action` parameter to method. -> Task executed before an action on entity can optionally be specified in `pre_action` parameter. Task executed after an action on entity can optionally be specified in `post_action` parameter.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
This isn't needed, since we now only support 2.6+ anyway.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
```suggestion old_item['aliases'] = sorted(old_item['aliases']) ```
Please create shallow copies to avoid the original data to be modified: ```suggestion new_item = dict(new_item) old_item = dict(old_item) # Sort the aliases ```
Please do not remove this trailing comma, this is our preference with dicts and lists to minimize future diffs when lines are changed.
```suggestion new_item['aliases'] = sorted(new_item['aliases']) ```
Maybe even better: ```suggestion new_item['aliases'] = sorted(new_item['aliases'] or []) ``` To convert `None` to an empty list.
Please also change it here: ```suggestion old_item['aliases'] = sorted(old_item['aliases'] or []) ```
For some reason this causes ``` {aliases: [attr]} {choices: [all, 'no', none, safe, urllib2, 'yes']} {default: [200]} ``` while non-sequence values for `default` yield ``` default: 30 ``` without the `{...}`.
I think `get_internal_type` is better to use.
I think a mixin is a bit complicated. The attribute sounds simpler - it could be a function instead of just a boolean, then it could return the transform if applicable, else `None` for non-sliceable fields (default implementation).
Wouldn't this approach fix the slicable fields to just these types? A mixin along with `slicable = True/False` on `F` may be an approach. Besides, I believe the policy is to keep `django.contrib.postgres` as separate as possible.
yeah I think it would be worthwhile to at least test a single JOIN scenario.
I think this test would be fine without the blank lines, it's fairly short.
single line looks okay here
Yes, good catch. We can delete one of the two functions, they are now identical.
netrics => metrics
`required: false` is the default and can be removed
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
Might want to use simple quote here.
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
Thanks for that note ewen. I learned something!
yeah I think it would be worthwhile to at least test a single JOIN scenario.
I think this test would be fine without the blank lines, it's fairly short.
single line looks okay here
Yes, good catch. We can delete one of the two functions, they are now identical.
netrics => metrics
`required: false` is the default and can be removed
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
Might want to use simple quote here.
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
Thanks for that note ewen. I learned something!
Since utilities functions can be called in tight loops it's best to minimize the number of Python function calls when readability doesn't suffer too much.
Should this be ``` python checked_list.sort(key=lambda x: sorted(list(x.items())) if isinstance(x, dict) else x) ``` tracebacks from test failures in CI make me suspect the view object from set(x.items()) fails because set() wants hashable items and the view object from x.items() is not. Does wrapping it in a list() or tuple() work? ``` 2017-06-26 16:43:33 The full traceback is: 2017-06-26 16:43:33 Traceback (most recent call last): 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_module_ecs_ecr.py", line 304, in run 2017-06-26 16:43:33 policy = sort_json_policy_dict(policy) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 603, in sort_json_policy_dict 2017-06-26 16:43:33 ordered_policy_dict[key] = value_is_list(value) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 595, in value_is_list 2017-06-26 16:43:33 checked_list.sort(key=lambda x: sorted(set(x.items())) if isinstance(x, dict) else x) 2017-06-26 16:43:33 File "/tmp/ansible_cLYNf7/ansible_modlib.zip/ansible/module_utils/ec2.py", line 595, in <lambda> 2017-06-26 16:43:33 checked_list.sort(key=lambda x: sorted(set(x.items())) if isinstance(x, dict) else x) 2017-06-26 16:43:33 TypeError: unhashable type: 'list' ```
Shouldn't this be `return isinstance(x, collections.Iterator)` instead? Actually we should replace all reference to this function by a `collection.Iterator` instance check.
I tried to fix these in #12645 but clearly that hasn't carried over to docstrings ðŸ¤”
I don't think boto_params will include access_key or secret_key if a profile is being used (which might be through passing `profile` parameter or with `AWS_PROFILE` set) Looking at https://github.com/jmenga/requests-aws-sign the way to do it is to use ``` session = session.Session() credentials = session.get_credentials() ``` but even there, we'd need to cope with the `profile` parameter.
Thanks :+1: , IMO a separate ticket is not necessary, we can `Refs #33348 -- ...`.
Can we deprecate passing `None` in `errors` in a separate PR? and do this in advance. There is already a lot of changes in this patch, that should make it more readable and easier to review. I can try to refactor this out if you don't have time to keep working on this.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
Why not also do this when `module.check_mode` is `True`? Then the check mode behavior is closer to the real thing.
If I'm not completely mistaken this will fail if the attacker requests a page where `csrfmiddlewaretoken` or `HTTP_X_CSRFTOKEN` is shorter than the `CSRF_KEY_LENGTH`.
Not convinced we need this example, particularly with `with_` going away.
2.6 or 2.7? Also you `requirements` listed here and the modules.
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
This should really be: ``` - Set custom DNS search domains. (Use I(dns_search) with C('') if you don't wish to set the search domain.) ```
Can you name this a little more verbosely? I can't unsee "get best"
If I'm not completely mistaken this will fail if the attacker requests a page where `csrfmiddlewaretoken` or `HTTP_X_CSRFTOKEN` is shorter than the `CSRF_KEY_LENGTH`.
Not convinced we need this example, particularly with `with_` going away.
2.6 or 2.7? Also you `requirements` listed here and the modules.
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
This should really be: ``` - Set custom DNS search domains. (Use I(dns_search) with C('') if you don't wish to set the search domain.) ```
Can you name this a little more verbosely? I can't unsee "get best"
I am not sure if this is the right regex, as it is imported from geometry.regex. I would say it is, thats why I used it here originally. But I am not certain and not very familiar with regexes. Maybe this option could be left away for now until a function to translate from WKB and a capi ptr is included. A related question is what wkb to use for rasters. As far as I know, the definition of a raster wkb is not as established as the WKBs for geometries. I would propose to use the PostGIS definition, but maybe there are others.
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
Similarly, ```if tc['skip'].get('i')```
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
I'm not sure why but other parts of the code are using "my" for the variable name for a yum_base object so we should keep doing that.
With this reinstated, it looks as though a test is needed as you didn't remove one...
`import botocore` will make this code a lot more consistent with most other boto3-based ansible modules
slightly simpler could be: "asking the user what ..."
could chop "Make sure to" without any loss of meaning.
I mean like: ``` python - package = fetch_rpm_from_url(spec, module) + package = function(spec, module) ``` But looking at this, I still think it should be a context manager rather than a pseudo-decorator. That would look something like this: ``` python from contextlib import contextmanager [...] @contextmanager def set_env_proxy(conf_file, installroot): old_environ = # [old values of the proxy env vars] try: my = yum_base(conf_file, installroot) # [set environment to the proxies that yum knows about] yield except: raise finally: # reset the environment to the values saved in old_environ [...] if '://' in spec: with set_env_proxy(conf_file, installroot): package = fetch_rpm_from_url(spec, module=module) [...] ```
If you were to do this as a decorator, you should be calling ```function``` here instead of hardcoding fetch_rpm_from_url.
I think there isn't much organization there. Using an existing site should be fine.
I tried a similar approach while working on acfaec3db5ba39de52f6e607e74343dccf72fba1 and came to the conclusion that this approach can't work (due to something like module caching). As far as I know, you'll have to register the admin to a separate `AdminSite`.
Put the } on the next line and add a trailing comma on this line. That's our convention to ease later adding more items to a dictionary, tuple, etc., if needed.
Minor but I'd move this control flow block after the `weights` one to match the args order.
It should not. See the description of the field.
Yes, fine with me.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Please remove this. It is wrong. Don't add a default in this case.
As `title` is not a required field for playlists, add `default=None`. Similarly for `uploader_id` below.
`title` must be mandatory I've already told about this.
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Hmm.. I think it should be ``` self.driver.stop() self.driver.wait() ``` instead as used in other places. Not sure why the test itself did not fail though, without calling `stop()` the `wait()` call should fail.
```suggestion self.headers = { 'Content-Type': "application/json" } ```
This can be moved to `check_dict` method.
```suggestion self.params[k] = 2 if v == 'dns' else 1 ```
I am not able to create a new monitor - ``` fatal: [localhost]: FAILED! => { "changed": false, "invocation": { "module_args": { "apikey": "VALUE_SPECIFIED_IN_NO_LOG_PARAMETER", "check_type": null, "monitorid": null, "name": "myMonitor_002", "state": "present", "url": "http://www.my-domain.com" } }, "msg": "Could not perform action newMonitor" } ``
```suggestion state = json.loads(to_text(state.read(), errors='surrogate_or_strict')) ``` This makes this call future proof.
Make a common method in class to get url and return json loads response.
I don't think it needs to be `synchronized` any more because we do all plugin building in one thread.
We didn't specify this in the KIP but I think `toStream()` and `toStream(mapper)` should also have overrides with`Named`
```suggestion * If the return value of {@link ValueTransformer#transform(Object) ValueTransformer#transform()} is {@code null}, no ``` Please also fix this on the original method
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
Note to self: if this is not discussed elsewhere in the docs, we should add it.
nit: remove empty link
as above (more often below -- please fit all)
+1 for adding `name:` lines
This overload does not take `Materialized` parameter
Perhaps say that the repartitioning will happen automatically, i.e., user doesn't need to do anything.
You could update the verbs tense to the convention used elsewhere: "Handle", "set", etc. rather than the "ing" versions.
So I noticed this long list of options without defaults or choices. Possibly this is exactly what is intended. However, in a lot of cases there is an implicit default that could be mentioned (even when it's not enforced when missing). If you initialize a new swarm I expect there are defaults set.
What are the default values for te below parameters if you do not specify it on creation ? - task_history_retention_limit - keep_old_snapshots - log_entries_for_slow_followers - heartbeat_tick - election_tick - dispatcher_heartbeat_period - node_cert_expiry - ca_force_rotate - autolock_managers These possibly require an additional entry in the description to state the defaults on creation. (So you can't add a real default value, because that may modify an existing entry)
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
FYI, in case you switch to working on whole config, `get_config.config` gives you the whole tree. (`get_config(:foo).config` is generally equivalent to `get_config.config[:foo]`.) Also, VMDB::Config is deprecated. May be better to use `::Settings.to_hash`.
Should match the variable registered in the preceding task. ```suggestion spot_price: "{{ spot_prices.ec2_spot_pricing_history.0.spot_price }}" ```
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
Wrong module name. ```suggestion - ec2_spot_pricing_history_info: ```
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
okay, but would be helpful to say _why_ we need to always return True.
nit: needs a comma after the `{@link ...}`
nit: remove empty line
I don't think we can do this. Also, I would only mention it, when we start to deprecate an API.
```suggestion * If the return value of {@link ValueTransformer#transform(Object) ValueTransformer#transform()} is {@code null}, no ``` Please also fix this on the original method
Is this key for the purpose of renaming an existing block? Usually how this works is some key is global (maybe name) and then if other parameters are changed (like description) the module will detect that the resource description is out of date and change it, so users don't have to manually tell the module what fields to update.
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
Note to self: if this is not discussed elsewhere in the docs, we should add it.
as above (more often below -- please fit all)
+1 for adding `name:` lines
This overload does not take `Materialized` parameter
I don't think it needs to be `synchronized` any more because we do all plugin building in one thread.
We didn't specify this in the KIP but I think `toStream()` and `toStream(mapper)` should also have overrides with`Named`
```suggestion * If the return value of {@link ValueTransformer#transform(Object) ValueTransformer#transform()} is {@code null}, no ``` Please also fix this on the original method
I reordered the method from "few parameter" to "more parameters" to make it easier to navigate within the file.
Note to self: if this is not discussed elsewhere in the docs, we should add it.
nit: remove empty link
as above (more often below -- please fit all)
+1 for adding `name:` lines
This overload does not take `Materialized` parameter
Perhaps say that the repartitioning will happen automatically, i.e., user doesn't need to do anything.
it should be `minor_vers = int(version[1])`
Note that `LooseVersion` could be used there: ``` from distutils.version import LooseVersion [...] return LooseVersion(version) < LooseVersion('3.2') ```
I liked generator approach more: it's more readable in terms of separating a distinct logical step from a higher-level flow.
This whole block could've been done with a nested list comprehension without any need for temporary variable introduction: ```python names = [ name for name_list in names for name in name_list.split(',') ] ``` Alternative FP approach would look like: ```python from itertools import partial, reduce ... names = reduce( lambda l, e: l + e, map( partial(str.split, sep=','), names ), [], ) ``` (but Pythonistas don't like such style in general)
Whenever you're doing more than simle parsing (simple parsing is usually, one command, like ```variable.split(",")```) you should give some examples of what the code is expecting as input and what it will give as output.
Lists also have .extend() which might be what you need here
`list`s have `.append()` method. You probably don't want to re-allocate a new instance on each iteration of the loop.
Nesting generator expressions and list comprehensions like this is bad style. It has the same problems as a run-on sentence in natural languages (Making it hard for other people to read and keep the entirety of the clause in their memory. Easy to misinterpret the meaning because of misreading one small piece of the grammar). In most cases, using a ```for``` loop with indentation for at least one of the loops is better. If I understand this correctly, You are trying to take input of this form: ``` names = ['one >1.0,<2.0', 'two', '>3.0', '<4.0'] ``` and flatten it so that it is in this form: ``` names = ['one', '>1.0', '<2.0', 'two', '>3.0', '<4.0'] ``` ? If so, it's not quite right as it will currently return ``` ['one >1.0', '<2.0', 'two', '>3.0', '<4.0'] ```
Here you can simply write: ``` if name_parts: ``` Python considers empty containers to be False-y.
Currently with this input: ``` names = ['one >1.0,<2.0', 'two', '>3.0', '<4.0'] ``` You would get this output: ``` distribution_names = ['one >1.0,<2.0', 'two,>3.0,<4.0'] ``` Note that one does not have a comma before the first version while two does.
```suggestion assert expected == "exception" ```
```suggestion assert expected == "exception" ```
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
```suggestion assert expected == "exception" ```
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
```suggestion assert expected == "exception" ```
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
chop "one of" add comma before "or"
Okay, so to implement the examples like this: ``` with_cyberark_password: appid: 'Application1' query: 'safe=...;' output: 'password ....' ``` We'll want to take the dict from terms instead of from kwargs. terms will hold the dict that the user passed in so you should be able to drop in replace it in your constructor like so: ```CyberarkPassword(**terms)```
put closing parenthesis on the next line
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
Trivial fix, but it would be good to say 2.12 instead of 2.12.1 here so that we get the latest patch release. There's one other place in the file that also needs to be updated.
colors should all be configurable
You don't need this conditional, since Ansible enforces that these are the only choices.
Since this isn't implemented, perhaps lets not mention it? I found it confusing
I think I'd keep the `fields` parameter as a list, perhaps passing `None` for `params` here.
That's already better, but there is even a cached_property decorator in django.utils.functional that does the job of caching the value.
If POSTGIS_TEMPLATE exists, it will be a string, not a tuple. So you'd better make the tuple in the execute method below instead.
I guess this could say "doesn't support all the lookups" in case gis_lookups is a list.
Yeah, it's fine.
use a single line or use hanging indent (we avoid non-4 space indents)
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
Should `template=None` be a local variable rather than an (unused) kwarg? I don't think the `as_vendor()` methods typically take any kwargs. Similarly for `function=None` for `as_postgresql/sqlite()`.
I'd keep this on one line for better readability
I think I'd trim it right here rather than in tests ```suggestion """.lstrip() # noqa: E501 ```
Cross-compability, backwards compatibility, command line compatibility, ease of discovery. * Other packaging modules support comma separated package entries. * The pip module itself supports comma separated package entries * pip supports single string entries as valid packages * comma separated list entries are the natural way to write lists on the commandline. * error messages if people were to violate these rules (for instance: ```ansible localhost -m pip -a 'names=django>=1.11.0,<1.12.0'``` wonn't point to the actual solution unless we implement this sort of detection for creating the error message... in which case we might as well do the right thing.
As far as I understand this is only necessary to support stuff like `djang>1.11.0,<1.12.0,bottle>0.10,<0.20,!=0.11`. Why should we support this? Using a list is to me the obviously better API and support this structure adds a lot of (to me unnecessary) complexity.
Functions which are side effect free and don't deal with external information like this one are excellent choices to unittest. You can give the function a wide range of potential inputs and check that they match up with the expected outputs far cheaper than you can with intergration tests.
Whenever you're doing more than simle parsing (simple parsing is usually, one command, like ```variable.split(",")```) you should give some examples of what the code is expecting as input and what it will give as output.
Can you give an example for the error message coming from this? As stated above I would rather remove support for construtcs, which can raise exceptions here.
Nesting generator expressions and list comprehensions like this is bad style. It has the same problems as a run-on sentence in natural languages (Making it hard for other people to read and keep the entirety of the clause in their memory. Easy to misinterpret the meaning because of misreading one small piece of the grammar). In most cases, using a ```for``` loop with indentation for at least one of the loops is better. If I understand this correctly, You are trying to take input of this form: ``` names = ['one >1.0,<2.0', 'two', '>3.0', '<4.0'] ``` and flatten it so that it is in this form: ``` names = ['one', '>1.0', '<2.0', 'two', '>3.0', '<4.0'] ``` ? If so, it's not quite right as it will currently return ``` ['one >1.0', '<2.0', 'two', '>3.0', '<4.0'] ```
Here you can simply write: ``` if name_parts: ``` Python considers empty containers to be False-y.
Currently with this input: ``` names = ['one >1.0,<2.0', 'two', '>3.0', '<4.0'] ``` You would get this output: ``` distribution_names = ['one >1.0,<2.0', 'two,>3.0,<4.0'] ``` Note that one does not have a comma before the first version while two does.
I usually pre-compile regexes into a global variable so that it only has to be done once. ``` _VCS_RE = re.compile(r'(svn|git|hg|bzr)\+') [...] def _is_vcs_url(name): """Test whether a name is a vcs url or not.""" return re.match(_VCS_RE, name) ```
`'>=', '<=', '!=', '==', '>', '<'` is duplicated with that operator map. you could do global map and use `that_map.keys()`
I think I'd keep the `fields` parameter as a list, perhaps passing `None` for `params` here.
`,` is fine here.
this should be done once(in `_real_initialize`).
the assumption was based on the fact that the cookie is set programatically and not using `Set-Cookie` headers, but as it's undefined wheather the value can change or not, i guess it's better to set the cookie value for every request.
```suggestion if gql_auth: ```
I think I'd trim it right here rather than in tests ```suggestion """.lstrip() # noqa: E501 ```
It's a standard practice to use separate args for different params. Also, autogenerated param ids aren't very readable when they are complex so in such cases it's better to assign them meaningful names (they are displayed in the pytest report): ```suggestion @pytest.mark.parametrize( ('returned_items_count', 'patched_dc_stdout'), ( (3, (DOCKER_OUTPUT_MULTIPLE, '')), (2, (PODMAN_OUTPUT, '')), (0, ('', '')), ), ids=('docker JSONL', 'podman JSON sequence', 'empty output'), ) def test_docker_images(docker_images, mocker, returned_items_count, patched_dc_stdout): mocker.patch( 'ansible_test._internal.docker_util.docker_command', return_value=patched_dc_stdout) ret = docker_images('', 'quay.io/ansible/centos7-test-container') assert len(ret) == returned_items_count ```
It is highly recommended to use `assert` statements in the pytest env because it integrates with its reporting better and more natively. ```suggestion assert len(ret) == 3 ```
It's probably not a good idea to make imports outside of the top of the module. Besides, only the first import will get evaluated while others will just load modules from the cache.
These 3 tests are actually one test. It should be parametrized rather than copy-pasted.
```suggestion assert expected == "exception" ```
```suggestion assert expected == "exception" ```
@CruxBox my point is that you are already returning in the case of an exception *and* in the `else` clause so the code is equivalent to ```python try: date_value = datetime.date(int(y), int(m), int(d)) except ValueError: # Return pseudo-ISO dates with zeros for any unselected values, # e.g. '2017-0-23'. return '%s-%s-%s' % (y or 0, m or 0, d or 0) date_value = datetime_safe.new_date(date_value) return date_value.strftime(input_format) ``` Again, this is minor but we usually avoid using `try/else` in the rest of the code base unless it's necessary.
Minor but the `else` clause wrapping is unnecessary here given you return in the `except`.
```suggestion assert expected == "exception" ```
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
```suggestion assert expected == "exception" ```
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
You could update the verbs tense to the convention used elsewhere: "Handle", "set", etc. rather than the "ing" versions.
Ensure that the new arguments are keyword only: ```suggestion def handle_word( self, word, *, safe_input, trim_url_limit=None, nofollow=False, autoescape=False, ): ```
I prefer putting the closing ) on the next line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
I think `enumerate` would work here
Use match arg instead: ```suggestion with pytest.raises(TypeError, match='Cannot json serialize'): ```
This will not work: in the case of exception, the control flow will not reach this line because the block execution will be interrupted one line above. ```suggestion assert "Cannot json serialize" in str(excinfo.value) ```
If you expect this invocation to emit an exception, `assert`ion doesn't make sense here: ```suggestion ansible_json_encoder.default(test_input) ```
Good catch, I will remove it before final squash.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
This should be: ``params = config_params + params``
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
I'd recommend a more explicit import, ie ``` python from ansible.module_utils.basic import AnsibleModule' ``` And using regular imports for 're', 'shlex', 'os', 'json', 'tempfile'
ipt_load_stderr doesnt appear to be defined anywhere.
This 'default_chains' doesn't seem to be used anywhere.
Good catch, I will remove it before final squash.
This could be extracted too. Also it's a pep8 warning on the latest pep8 - apparently you shouldn't assign lambdas and should use `def` instead.
@raphaelm This line is correct. We use `pk` twice for each object, once in a `WHEN id = ... THEN` clause and once in a `WHERE id IN (...)` clause to filter rows for update.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
This should be: ``params = config_params + params``
```suggestion ) ```
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
I'd recommend a more explicit import, ie ``` python from ansible.module_utils.basic import AnsibleModule' ``` And using regular imports for 're', 'shlex', 'os', 'json', 'tempfile'
ipt_load_stderr doesnt appear to be defined anywhere.
This 'default_chains' doesn't seem to be used anywhere.
Please use shorter name (less than 30 chars), e.g. `deferrable_pink_constraint`.
Good catch, I will remove it before final squash.
Should be ``self.weight``
Remove the two extra double-quotes here.
Please don't make lines longer! There was nothing really wrong with this line before
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
I prefer putting the closing ) on the next line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
++ thanks for changing this :)
please fail if required stuff is null
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
looks like there are two levels of indentation instead of one
Missing space after the `for`
nit: formatting, add some whitespaces
prefer hanging indent style with 1 arg per line
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
`band_input`, you don't get much by saving one char :-)
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
I prefer putting the closing ) on the next line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
I think `enumerate` would work here
Oh, I just meant to suggest the `value` to `values` (plural) renaming, not more...
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
`elif` might be clearer (I understand it's not necessary)
prefer hanging indent style with 1 arg per line
You could update the verbs tense to the convention used elsewhere: "Handle", "set", etc. rather than the "ing" versions.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
I prefer putting the closing ) on the next line
I think `enumerate` would work here
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
looks like there are two levels of indentation instead of one
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
I prefer putting the closing ) on the next line
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
prefer hanging indent style with 1 arg per line
I think `enumerate` would work here
please fail if required stuff is null
looks like there are two levels of indentation instead of one
I don't understand why this can be true when in L263 we use `assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0)` I investigated this error yesterday with Jim and he pushed the "assumeTrue" part, so this test shouldn't run into those cases anymore. Did you run the mentioned reproduction line on current master, because I could get it to fail using it on 77617c8e62f7fa1407c62bdf33e4892206d62952
colors should all be configurable
This doesn't need to be quoted.
The same like above.
This should also start on the line above if the other is moved.
You can delete these empty lines.
Since Ansible subscribes to unicode sandwich, this should be the callers' (actually, where the data enters the system) responsibility.
Capital letter in the value.
FYI, in case you switch to working on whole config, `get_config.config` gives you the whole tree. (`get_config(:foo).config` is generally equivalent to `get_config.config[:foo]`.) Also, VMDB::Config is deprecated. May be better to use `::Settings.to_hash`.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
Can you name this a little more verbosely? I can't unsee "get best"
I think that we only want to remove a single `'g'` from the end of the string and continue to allow things like `'gg'`, `'1gg'`, `'g2'`, and `'g3g'` to raise an error as they would have before. ```suggestion if arg.endswith('g'): arg = arg[:-1] force_grouping = True ```
"any other input"
2.6 or 2.7? Also you `requirements` listed here and the modules.
Capital letter in the value.
Since Ansible subscribes to unicode sandwich, this should be the callers' (actually, where the data enters the system) responsibility.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
FYI, in case you switch to working on whole config, `get_config.config` gives you the whole tree. (`get_config(:foo).config` is generally equivalent to `get_config.config[:foo]`.) Also, VMDB::Config is deprecated. May be better to use `::Settings.to_hash`.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
okay, but would be helpful to say _why_ we need to always return True.
```suggestion # Run masked_action when enabled param is not provided or service is supposed to be enabled/started. if not module.check_mode and module.params['enabled'] in [None, True]): ```
tempted to have no default and make it 'yes|no' boolean, use 'default' behaviour when `None`
`if it encounter` => `if it encounters`
`mentionned` => `mentioned`
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
Missing `=dict` on this and the next few lines
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
Could collections examples be added in? I know https://github.com/ansible/ansible/pull/25210 hasn't been merged yet, though if you know that structure that would have us from having to come back to this
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
missing space between `,` and `and`
```suggestion # Run masked_action when enabled param is not provided or service is supposed to be enabled/started. if not module.check_mode and module.params['enabled'] in [None, True]): ```
tempted to have no default and make it 'yes|no' boolean, use 'default' behaviour when `None`
`if it encounter` => `if it encounters`
`mentionned` => `mentioned`
Please remove choices for `type='bool'`, as `bool` accepts yes,1,true, etc.
Missing `=dict` on this and the next few lines
`load_config` doesn't return anything https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/ios.py#L121
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
There are some options to `AnsibleModule` which you may be able to use to validate options passed in. Look at existing modules for examples: ``` mutually_exclusive required_together required_one_of require_if ```
missing space between `,` and `and`
Minor but I'd move this control flow block after the `weights` one to match the args order.
Thanks. This PR is assigned to the 2.12 milestone, which we'll be going through after creating the stable-2.11 branch (which will be done when 2.11 RC1 comes out).
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
Please remove this. It is wrong. Don't add a default in this case.
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
Use another lookup instead of `epoch` e.g. `second`.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
Please use a single quote.
```suggestion Test that the returned value for timezone consists of only uppercase ```
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
```suggestion self.headers = { 'Content-Type': "application/json" } ```
This can be moved to `check_dict` method.
```suggestion self.params[k] = 2 if v == 'dns' else 1 ```
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Hmm.. I think it should be ``` self.driver.stop() self.driver.wait() ``` instead as used in other places. Not sure why the test itself did not fail though, without calling `stop()` the `wait()` call should fail.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
```suggestion state = json.loads(to_text(state.read(), errors='surrogate_or_strict')) ``` This makes this call future proof.
Make a common method in class to get url and return json loads response.
I am not able to create a new monitor - ``` fatal: [localhost]: FAILED! => { "changed": false, "invocation": { "module_args": { "apikey": "VALUE_SPECIFIED_IN_NO_LOG_PARAMETER", "check_type": null, "monitorid": null, "name": "myMonitor_002", "state": "present", "url": "http://www.my-domain.com" } }, "msg": "Could not perform action newMonitor" } ``
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
```suggestion self.headers = { 'Content-Type': "application/json" } ```
This can be moved to `check_dict` method.
```suggestion self.params[k] = 2 if v == 'dns' else 1 ```
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Hmm.. I think it should be ``` self.driver.stop() self.driver.wait() ``` instead as used in other places. Not sure why the test itself did not fail though, without calling `stop()` the `wait()` call should fail.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
```suggestion state = json.loads(to_text(state.read(), errors='surrogate_or_strict')) ``` This makes this call future proof.
Make a common method in class to get url and return json loads response.
I am not able to create a new monitor - ``` fatal: [localhost]: FAILED! => { "changed": false, "invocation": { "module_args": { "apikey": "VALUE_SPECIFIED_IN_NO_LOG_PARAMETER", "check_type": null, "monitorid": null, "name": "myMonitor_002", "state": "present", "url": "http://www.my-domain.com" } }, "msg": "Could not perform action newMonitor" } ``
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
```suggestion self.headers = { 'Content-Type': "application/json" } ```
This can be moved to `check_dict` method.
```suggestion self.params[k] = 2 if v == 'dns' else 1 ```
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Hmm.. I think it should be ``` self.driver.stop() self.driver.wait() ``` instead as used in other places. Not sure why the test itself did not fail though, without calling `stop()` the `wait()` call should fail.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
```suggestion state = json.loads(to_text(state.read(), errors='surrogate_or_strict')) ``` This makes this call future proof.
Make a common method in class to get url and return json loads response.
I am not able to create a new monitor - ``` fatal: [localhost]: FAILED! => { "changed": false, "invocation": { "module_args": { "apikey": "VALUE_SPECIFIED_IN_NO_LOG_PARAMETER", "check_type": null, "monitorid": null, "name": "myMonitor_002", "state": "present", "url": "http://www.my-domain.com" } }, "msg": "Could not perform action newMonitor" } ``
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
```suggestion self.headers = { 'Content-Type': "application/json" } ```
This can be moved to `check_dict` method.
```suggestion self.params[k] = 2 if v == 'dns' else 1 ```
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Hmm.. I think it should be ``` self.driver.stop() self.driver.wait() ``` instead as used in other places. Not sure why the test itself did not fail though, without calling `stop()` the `wait()` call should fail.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
```suggestion state = json.loads(to_text(state.read(), errors='surrogate_or_strict')) ``` This makes this call future proof.
Make a common method in class to get url and return json loads response.
I am not able to create a new monitor - ``` fatal: [localhost]: FAILED! => { "changed": false, "invocation": { "module_args": { "apikey": "VALUE_SPECIFIED_IN_NO_LOG_PARAMETER", "check_type": null, "monitorid": null, "name": "myMonitor_002", "state": "present", "url": "http://www.my-domain.com" } }, "msg": "Could not perform action newMonitor" } ``
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
```suggestion self.headers = { 'Content-Type': "application/json" } ```
This can be moved to `check_dict` method.
```suggestion self.params[k] = 2 if v == 'dns' else 1 ```
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Hmm.. I think it should be ``` self.driver.stop() self.driver.wait() ``` instead as used in other places. Not sure why the test itself did not fail though, without calling `stop()` the `wait()` call should fail.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
```suggestion state = json.loads(to_text(state.read(), errors='surrogate_or_strict')) ``` This makes this call future proof.
Make a common method in class to get url and return json loads response.
I am not able to create a new monitor - ``` fatal: [localhost]: FAILED! => { "changed": false, "invocation": { "module_args": { "apikey": "VALUE_SPECIFIED_IN_NO_LOG_PARAMETER", "check_type": null, "monitorid": null, "name": "myMonitor_002", "state": "present", "url": "http://www.my-domain.com" } }, "msg": "Could not perform action newMonitor" } ``
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
```suggestion self.headers = { 'Content-Type': "application/json" } ```
This can be moved to `check_dict` method.
```suggestion self.params[k] = 2 if v == 'dns' else 1 ```
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Hmm.. I think it should be ``` self.driver.stop() self.driver.wait() ``` instead as used in other places. Not sure why the test itself did not fail though, without calling `stop()` the `wait()` call should fail.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
```suggestion state = json.loads(to_text(state.read(), errors='surrogate_or_strict')) ``` This makes this call future proof.
Make a common method in class to get url and return json loads response.
I am not able to create a new monitor - ``` fatal: [localhost]: FAILED! => { "changed": false, "invocation": { "module_args": { "apikey": "VALUE_SPECIFIED_IN_NO_LOG_PARAMETER", "check_type": null, "monitorid": null, "name": "myMonitor_002", "state": "present", "url": "http://www.my-domain.com" } }, "msg": "Could not perform action newMonitor" } ``
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
```suggestion self.headers = { 'Content-Type': "application/json" } ```
This can be moved to `check_dict` method.
```suggestion self.params[k] = 2 if v == 'dns' else 1 ```
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Hmm.. I think it should be ``` self.driver.stop() self.driver.wait() ``` instead as used in other places. Not sure why the test itself did not fail though, without calling `stop()` the `wait()` call should fail.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
```suggestion state = json.loads(to_text(state.read(), errors='surrogate_or_strict')) ``` This makes this call future proof.
Make a common method in class to get url and return json loads response.
I am not able to create a new monitor - ``` fatal: [localhost]: FAILED! => { "changed": false, "invocation": { "module_args": { "apikey": "VALUE_SPECIFIED_IN_NO_LOG_PARAMETER", "check_type": null, "monitorid": null, "name": "myMonitor_002", "state": "present", "url": "http://www.my-domain.com" } }, "msg": "Could not perform action newMonitor" } ``
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
```suggestion self.headers = { 'Content-Type': "application/json" } ```
This can be moved to `check_dict` method.
```suggestion self.params[k] = 2 if v == 'dns' else 1 ```
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Hmm.. I think it should be ``` self.driver.stop() self.driver.wait() ``` instead as used in other places. Not sure why the test itself did not fail though, without calling `stop()` the `wait()` call should fail.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
Make a common method in class to get url and return json loads response.
```suggestion state = json.loads(to_text(state.read(), errors='surrogate_or_strict')) ``` This makes this call future proof.
I am not able to create a new monitor - ``` fatal: [localhost]: FAILED! => { "changed": false, "invocation": { "module_args": { "apikey": "VALUE_SPECIFIED_IN_NO_LOG_PARAMETER", "check_type": null, "monitorid": null, "name": "myMonitor_002", "state": "present", "url": "http://www.my-domain.com" } }, "msg": "Could not perform action newMonitor" } ``
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
```suggestion self.headers = { 'Content-Type': "application/json" } ```
This can be moved to `check_dict` method.
```suggestion self.params[k] = 2 if v == 'dns' else 1 ```
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
Hmm.. I think it should be ``` self.driver.stop() self.driver.wait() ``` instead as used in other places. Not sure why the test itself did not fail though, without calling `stop()` the `wait()` call should fail.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
```suggestion state = json.loads(to_text(state.read(), errors='surrogate_or_strict')) ``` This makes this call future proof.
Make a common method in class to get url and return json loads response.
I am not able to create a new monitor - ``` fatal: [localhost]: FAILED! => { "changed": false, "invocation": { "module_args": { "apikey": "VALUE_SPECIFIED_IN_NO_LOG_PARAMETER", "check_type": null, "monitorid": null, "name": "myMonitor_002", "state": "present", "url": "http://www.my-domain.com" } }, "msg": "Could not perform action newMonitor" } ``
is this really what was intended given compatibility? and do none of our tests rely on the existing behavior to wait until that message is logged? i would have expected some that use the REST API (e.g. pretty much anything distributed that submits a connector as one of its first post-start tasks) and would be flaky if we change the default to `INSTANT`
```suggestion self.params[k] = 2 if v == 'dns' else 1 ```
```suggestion self.headers = { 'Content-Type': "application/json" } ```
This can be moved to `check_dict` method.
Hmm.. I think it should be ``` self.driver.stop() self.driver.wait() ``` instead as used in other places. Not sure why the test itself did not fail though, without calling `stop()` the `wait()` call should fail.
@ewencp thanks for chiming in! Just double checked the code of `StreamsTestBaseService#stop_node()` is indeed calling with clean_shutdown to true, i.e. to wait for the pid to die out. So the second `wait` call should be removable.
`wait` should be used if the `Service` is expected to exit naturally based on previous commands: ``` Wait for the service to finish. This only makes sense for tasks with a fixed amount of work to do. For services that generate output, it is only guaranteed to be available after this call returns. ``` You generally do not need to use it in concert with `stop` as `stop` is generally implemented either to synchronously wait (on a clean shutdown via SIGTERM) or kills aggressively enough that you don't actually have to wait (i.e. SIGKILL). I suspect many of the uses of `wait`/`stop` with these `Service` classes are just redundant -- that is rare or non-existent elsewhere. Use of `wait` elsewhere is mostly limited to things like a console producer/consumer that are known to need to consume a limited amount of data.
Make a common method in class to get url and return json loads response.
```suggestion state = json.loads(to_text(state.read(), errors='surrogate_or_strict')) ``` This makes this call future proof.
I am not able to create a new monitor - ``` fatal: [localhost]: FAILED! => { "changed": false, "invocation": { "module_args": { "apikey": "VALUE_SPECIFIED_IN_NO_LOG_PARAMETER", "check_type": null, "monitorid": null, "name": "myMonitor_002", "state": "present", "url": "http://www.my-domain.com" } }, "msg": "Could not perform action newMonitor" } ``
preferred format is "#15346, #15573 - Issue description"
This line is related to the `save` method, and should sit beneath it, rather than the new `get_updated_model` method.
`parallell` => `parallel` `get_upodated_models` => `get_updated_models`
choosing a different string wouldn't hurt
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
this method is unused but it maybe useful in the future.
While this is technically correct I prefer ``` if not (many_to_many or foreign_key) ``` as it describes the intent better.
Can you please choose a new error code that is otherwise unused.
one more for the single line version
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
`band_input`, you don't get much by saving one char :-)
`has_key` is deprecated, use `if 'origin' in ds_input:` instead. Also please replace other occurrences in the patch.
We try to avoid line continuation backslashes in Django, and it's not a problem to overcome the 80-chars limit when it makes readability better.
I prefer putting the closing ) on the next line
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
We might instead replace other instances by `self._ptr`, to convey the idea of a private and internal variable. Just a suggestion.
prefer hanging indent style with 1 arg per line
`del` is a builtin, not a function. These parens don't have to be here
I think `enumerate` would work here
How about adding `self.assertIs(response.context['is_multipart'], True)` before the `assertContains()` (since those can be difficult to debug)? (Might be worth adding a similar assertion somewhere for the False case.)
I prefer putting the closing ) on the next line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
I think `enumerate` would work here
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
missing ini option for host
`field_preference` must be a list or a tuple.
Code duplication 80-86, 89-94.
I prefer putting the closing ) on the next line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
I think `enumerate` would work here
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
Code duplication 80-86, 89-94.
prefer hanging indent style with 1 arg per line
Actually I would probably keep most of those exceptions as a single line -- we prefer long erlines when it improves readability.
I prefer putting the closing ) on the next line
You don't need `required=False`, it's the default. And we tend to start with the parameter type.
I think `enumerate` would work here
@s-hertel using client exceptions rather than botocore exceptions is nicer when they actually exist (which they do here for a kms client): ``` except connection.exceptions.NotFoundException: return None except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e: # Legitimate failure module.fail_json_aws(e, ... ```
Technically, you could remove this line, and just use: ``` alias_results, self._legal_inputs = handle_aliases(spec, param) ```
After taking a look at the new function, I could see leaving the stub function in so that it can establish default values for arg_spec and params.
This is a private function (starts with an underscore) so I'd hope we could move this without having to leave a stub for backwards compatibility. If you want to remove the stub in a separate commit/PR just in case, though, I can live with that.
`result` is empty and no capability is returned. Is this expected? You might want to call `get_device_info` here to get the device capabilities.
`((app_label, model_name), [fields])` LGTM On April 1, 2015 3:46:12 PM GMT+02:00, Alex Hill notifications@github.com wrote: > > - from those operations and use them to provide a nicer error > > message. > > + > > - This will work for any function passed to > > lazy_related_operation that > > - has a keyword argument called 'field'. > > - """ > > + > > - def extract_field(operation): > > - # Expect a functools.partial with a kwarg called 'field' > > applied. > > - try: > > - return operation.func.keywords['field'] > > - except (AttributeError, KeyError): > > - return None > > + > > - extract_fields = lambda ops: list(filter(None, > > map(extract_field, ops))) > > + > > - # Generate pairs of (("app_label", "modelname), [fields]) > > Yes you do. The idea was to give an example of the literal contents of > the variable rather than its origins, hence the quotes surrounding each > element. I'm happy to go with `(app_label, model_name)`, without the > quotes. > > --- > > Reply to this email directly or view it on GitHub: > https://github.com/django/django/pull/4423/files#r27569838
missing quote after `modelname` which should also be `model_name`
Space before ).
I suggest to add a line break and list models one per line with 2 spaces indentation.
Should `template=None` be a local variable rather than an (unused) kwarg? I don't think the `as_vendor()` methods typically take any kwargs. Similarly for `function=None` for `as_postgresql/sqlite()`.
Same could be said of newCost and newModel
this is not a 1.0 callback, its using 2.0 API
The various `CALLBACK_` prefixed class attributes are missing. Take a look at the other callbacks for an example. In particular, this is needed: ``` python CALLBACK_NEEDS_WHITELIST = True ```
Does it actually matter if the sizes are different here? we have chosen to make them the same above but it doesn't break the algorithm if they are different so I don't think this assert is necessary.
`result` is empty and no capability is returned. Is this expected? You might want to call `get_device_info` here to get the device capabilities.
`((app_label, model_name), [fields])` LGTM On April 1, 2015 3:46:12 PM GMT+02:00, Alex Hill notifications@github.com wrote: > > - from those operations and use them to provide a nicer error > > message. > > + > > - This will work for any function passed to > > lazy_related_operation that > > - has a keyword argument called 'field'. > > - """ > > + > > - def extract_field(operation): > > - # Expect a functools.partial with a kwarg called 'field' > > applied. > > - try: > > - return operation.func.keywords['field'] > > - except (AttributeError, KeyError): > > - return None > > + > > - extract_fields = lambda ops: list(filter(None, > > map(extract_field, ops))) > > + > > - # Generate pairs of (("app_label", "modelname), [fields]) > > Yes you do. The idea was to give an example of the literal contents of > the variable rather than its origins, hence the quotes surrounding each > element. I'm happy to go with `(app_label, model_name)`, without the > quotes. > > --- > > Reply to this email directly or view it on GitHub: > https://github.com/django/django/pull/4423/files#r27569838
missing quote after `modelname` which should also be `model_name`
Space before ).
I suggest to add a line break and list models one per line with 2 spaces indentation.
Should `template=None` be a local variable rather than an (unused) kwarg? I don't think the `as_vendor()` methods typically take any kwargs. Similarly for `function=None` for `as_postgresql/sqlite()`.
Same could be said of newCost and newModel
this is not a 1.0 callback, its using 2.0 API
The various `CALLBACK_` prefixed class attributes are missing. Take a look at the other callbacks for an example. In particular, this is needed: ``` python CALLBACK_NEEDS_WHITELIST = True ```
Does it actually matter if the sizes are different here? we have chosen to make them the same above but it doesn't break the algorithm if they are different so I don't think this assert is necessary.
`re.sub` part can be put in `transform_source` parameter of `_parse_json`.
**Always** check code with flake8.
I think you can get rid of the rstrip('\n') here for the same reason as you got rid of it in _find_bind_mounts() (or alternatively, if rstrip is necessary here, then it's probably still needed in _find_bind_mounts() as well).
Right -- it shouldn't be needed because splitlines() will remove all "\n".
With this simplification, the pythonic way of generating a list is to use list comprehension. You can replace the whole function body with: ```python return [parse_to_obj(r) for r in parse_to_logical_rows(vlan_out)] ```
What is this supposed to achieve? Modules are cached on import, they'll be picked up from that cache regardless of the pointers being listed in `sys.modules`.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
Python 3.5 does not have `unicode`. Take a look at the [ansible text helper methods](https://github.com/ansible/ansible/blob/devel/lib/ansible/module_utils/_text.py) for alternatives. Only noting the first occurrence.
ipt_load_stderr doesnt appear to be defined anywhere.
Since this `int()` call is no longer inside a `try` `except`, we now get a stack trace if the checksum is an invalid base 16 value. ``` ValueError: invalid literal for int() with base 16: '541a1ef5373be3dc49fc542fd9a65177b664aec01c8d8608f99e6ec95577d8ci' ``` ```suggestion try: int(checksum, 16) except ValueError: module.fail_json(msg='The checksum format is invalid', **result) ```
Trick credit: https://twitter.com/raymondh/status/967927989752098816
Oh.. I missed the part about "more than two lines", so please post example output, so we could take a closer look at the issue together :)
It's usually better to use raw-strings for regexps: ```suggestion assert re.match(r'ansible [0-9.a-z]+ .*$', version_lines[0]), 'Incorrect ansible version line in "ansible --version" output' ``` (I'm pretty sure Python 3.6+ will emit warnings if you don't)
Need spaces around `+` sign.
Please note that in Python we don't use brackets unless really needed.
```suggestion from ansible.release import __version__ expected_version_line = 'ansible {ver!s}{scm_info!s}'.format(ver=__version__, scm_info=mocked_version) assert expected_version_line == version_lines[0], 'Incorrect ansible version line in "ansible --version" output' ```
Else part is not necessary since we are initialising `b_passwd` already.
FWIW using `monkeypatch` is preferable because it's pytest-native unless I need something like mocks or spies.
You can completely drop this conditional, since iterating over empty iterable produces the same effect.
you could also write this as: h_item = search_obj_in_list(w_item['member'], h, key='member') or {}
this get_field_by_name should be adjusted with formalized model meta API I guess
There should be a sane API through `schema` ( A SchemaEditor, I presume) to do this.
I don't think that SQLite can pass kwargs, so I think that this can be replaced by `if None in args`.
Ah. Yes, quite right. So this could be simplified to: ```python return None if None in args else func(*args, **kwargs) ```
Please add `@functools.wraps(func)` to this.
`strip()` removes also other whitespace, e.g. `\t`. I think we should move `strip()` to the next line, i.e. ```python value = re.sub(r'[^\w\s-]', '', value.lower()) return re.sub(r'[-\s]+', '-', value).strip('-_') ```
This looks similar to `utils.decode_packed_codes`.
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
not a blocker, I would probably not error out here, instead you could print out a warning message, up to your decision: ~~~python module.warn(warning='Cannot change type of an existing volume.') ~~~
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
2.6 or 2.7? Also you `requirements` listed here and the modules.
The original code made use of unpacking generalizations rather than excessive use of `list.append()` and `list.extend()`. Please restore this.
(In general, I don't think modules should have such options.)
You should emphasize that the module can and will not do any idempotence checking for this.
> would it make sense to publish this module as is (with dependency on unicon library and support for local connection type) as a short term solution and plan how to replace it with network_cli connection as a long-term plan and do it as a part of the next release? Yes, that should work IMO. FYI Ansible follows 4 version deprecation cycle that is if the code (options) available in a stable release is deprecated it can be removed after four releases, typical Ansible release cycle is around 4 to 6 months.
That does make sense. Thanks.
What kind of values are valid here? I would expect this to be `float` based on the `0.0` in the description.
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
A general remark: you should always use complete sentences. So this should end with a period.
I might be missing something but I can't come up with a way `escape_isnt_last_filter is True` when reaching this branch as the only way to get in there is when `seen_escape_filter is True` which is only set in the `isinstance(obj, EscapeData)` branch below where `escape_isnt_last_filter` is immediately set to `False`.
It's hard to assess the correctness of this change through visual inspection. I'll trust you (and the test suite) on this.
IMO, we can use `self.template_string` without an extra variable.
Does replication factor matter in this test at all? I can't think of why it would matter for the replication factor, so it seems weird to make it 2 here.
Not critical, but `for num_started, node in enumerate(consumer.nodes, 1)` would probably be more idiomatic.
I wonder if 25 should be defined in the [SQL constants module](https://github.com/django/django/blob/master/django/db/models/sql/constants.py)? I am afraid changing 25 in the code might not be changed here, so the test would silently become obsolete.
I would filter that first, via a generator. I think this might be more readable. ```python lists = (lst for lst in lists if lst) ``` or ```python lists = filter(None, lists) ```
You might want to consider returning a generator yourself. In which case you should just `yield from` the generator instead of returning a list.
instead of using `lst[0]` and `lst[1:]` I would recommend: ```python head, *tail = lst ````
Removing this 'if' and leaving the return doesn't result in any failures.
Why has this logic changed? I can't see what it has to do with removing the use of `_clone()`
No, I think you've changed both of them. I don't think they're any more similar now than they were, the logic is fundamentally the same and this sort of change is not needed.
This line is related to the `save` method, and should sit beneath it, rather than the new `get_updated_model` method.
First we should verify this passes before we toggle `is_active` to False.
`parallell` => `parallel` `get_upodated_models` => `get_updated_models`
2.6 or 2.7? Also you `requirements` listed here and the modules.
Has `refresh_from_db` ever worked properly? From my personal experience we found the built in function buggy. We tend to just reload via `Model.objects.get(id=old_object.id)` in our code.
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
Why has this logic changed? I can't see what it has to do with removing the use of `_clone()`
No, I think you've changed both of them. I don't think they're any more similar now than they were, the logic is fundamentally the same and this sort of change is not needed.
This line is related to the `save` method, and should sit beneath it, rather than the new `get_updated_model` method.
First we should verify this passes before we toggle `is_active` to False.
I tried a similar approach while working on acfaec3db5ba39de52f6e607e74343dccf72fba1 and came to the conclusion that this approach can't work (due to something like module caching). As far as I know, you'll have to register the admin to a separate `AdminSite`.
I think there isn't much organization there. Using an existing site should be fine.
`parallell` => `parallel` `get_upodated_models` => `get_updated_models`
2.6 or 2.7? Also you `requirements` listed here and the modules.
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I might be missing something but I can't come up with a way `escape_isnt_last_filter is True` when reaching this branch as the only way to get in there is when `seen_escape_filter is True` which is only set in the `isinstance(obj, EscapeData)` branch below where `escape_isnt_last_filter` is immediately set to `False`.
It's hard to assess the correctness of this change through visual inspection. I'll trust you (and the test suite) on this.
IMO, we can use `self.template_string` without an extra variable.
Does replication factor matter in this test at all? I can't think of why it would matter for the replication factor, so it seems weird to make it 2 here.
Not critical, but `for num_started, node in enumerate(consumer.nodes, 1)` would probably be more idiomatic.
I wonder if 25 should be defined in the [SQL constants module](https://github.com/django/django/blob/master/django/db/models/sql/constants.py)? I am afraid changing 25 in the code might not be changed here, so the test would silently become obsolete.
I would filter that first, via a generator. I think this might be more readable. ```python lists = (lst for lst in lists if lst) ``` or ```python lists = filter(None, lists) ```
You might want to consider returning a generator yourself. In which case you should just `yield from` the generator instead of returning a list.
instead of using `lst[0]` and `lst[1:]` I would recommend: ```python head, *tail = lst ````
Removing this 'if' and leaving the return doesn't result in any failures.
Why has this logic changed? I can't see what it has to do with removing the use of `_clone()`
No, I think you've changed both of them. I don't think they're any more similar now than they were, the logic is fundamentally the same and this sort of change is not needed.
This line is related to the `save` method, and should sit beneath it, rather than the new `get_updated_model` method.
First we should verify this passes before we toggle `is_active` to False.
I tried a similar approach while working on acfaec3db5ba39de52f6e607e74343dccf72fba1 and came to the conclusion that this approach can't work (due to something like module caching). As far as I know, you'll have to register the admin to a separate `AdminSite`.
I think there isn't much organization there. Using an existing site should be fine.
`parallell` => `parallel` `get_upodated_models` => `get_updated_models`
2.6 or 2.7? Also you `requirements` listed here and the modules.
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
This line is related to the `save` method, and should sit beneath it, rather than the new `get_updated_model` method.
First we should verify this passes before we toggle `is_active` to False.
I think there isn't much organization there. Using an existing site should be fine.
I tried a similar approach while working on acfaec3db5ba39de52f6e607e74343dccf72fba1 and came to the conclusion that this approach can't work (due to something like module caching). As far as I know, you'll have to register the admin to a separate `AdminSite`.
`parallell` => `parallel` `get_upodated_models` => `get_updated_models`
2.6 or 2.7? Also you `requirements` listed here and the modules.
Has `refresh_from_db` ever worked properly? From my personal experience we found the built in function buggy. We tend to just reload via `Model.objects.get(id=old_object.id)` in our code.
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Please do not revert this doc change. The old text is wrong in almost every important way.
`CANONICAL_RANGE_BOUNDS` is unnecessary: ```suggestion def __init__(self, *args, default_bounds='[)', **kwargs): ```
For consistency path pruning should be done the same as in the boilerplate test: https://github.com/ansible/ansible/blob/60a24bbdaae3a17bf20613810f347bfb8283f306/test/sanity/code-smell/boilerplate.py#L13-L14 https://github.com/ansible/ansible/blob/60a24bbdaae3a17bf20613810f347bfb8283f306/test/sanity/code-smell/boilerplate.py#L41-L42
```suggestion to iterate use a C(with_) directive. ```
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Such an extensive docstring is not necessary, IMO.
Use generators to avoid massive memory consumption when huge lists are being used: ``` for batch in (objs[i:i + batch_size] for i in xrange(0, len(objs), batch_size)): ```
This can be single lined: ```python '%s\n%s' % (combined_list[prev], combined_list[k]), ```
I wonder if 25 should be defined in the [SQL constants module](https://github.com/django/django/blob/master/django/db/models/sql/constants.py)? I am afraid changing 25 in the code might not be changed here, so the test would silently become obsolete.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
Please use a single quote.
```suggestion - If C(false) (NO CYCLE) is specified, any calls to nextval after the sequence ```
Why have both `schema` and `newschema`? I would assume that if I specify another value for `schema`, that the schema will be changed.
This can be single-lined: ```suggestion return (matched_tags or not tags) and not test_tags.intersection(exclude_tags) ```
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
I would chop `does_`: ```suggestion if test_match_tags(test, tags, exclude_tags) ```
multiple dictionary access: instead of ```python if required_config.get('rotation', None): rotation = required_config['rotation'] ``` use: ```python rotation = required_config('rotation') if rotation is not None: # do your stuff ``` use this rule for all dictionary access below
No need for get(key, None) as None is the default fix also for following get()
@charettes, any reply here? I guess we shouldn't block the patch about the issue with backwards migrations if we can't find a simple solution.
Having the migration name hard-coded in here doesn't strike me like the cleanest solution. I don't have an alternative right now, tho.
Ah, didn't know this existed yet. I see that this PR is mostly a "copy" of the ContentTypes one. Sounds alright for now, then.
what `_show_interface_dcb_ets(if_type, if_id)` fails: better to handle the case if the return value is None
```python if threshold_mode == "absolute": delimiter = ' ' else: delimiter = '%' min_val = int(min_threshold.split(delimiter)[0]) max_val= int(max_threshold.split(delimiter)[0]) congestion_control = dict( control=mode.lower(), threshold_mode=threshold_mode, min_threshold=min_val, max_threshold=max_val) ```
excellent handling of congestion control
```python if state == "disabled": if current_state == "enabled": self._commands.append('interface {0} {1} no traffic-class {2} congestion-control'.format(if_type, if_id, tc)) continue ```
```python if dcb_mode != current_dcb_mode or weight != current_weight: self._commands.append( 'interface {0} {1} traffic-class {2} dcb ets {3} {4}'.format(if_type, if_id, tc, dcb_mode, weight)) ```
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
You should emphasize that the module can and will not do any idempotence checking for this.
(In general, I don't think modules should have such options.)
What kind of values are valid here? I would expect this to be `float` based on the `0.0` in the description.
```suggestion by setting the ``REGISTRY_AUTH_FILE`` environment variable. ``export REGISTRY_AUTH_FILE=path`` ```
```suggestion - Path of the authentication file. Default is ``${XDG_RUNTIME_DIR}/containers/auth.json`` ```
Please use formatting like `C(<device-on-host>[:<device-on-container>][:<permissions>])`, and `(e.g. device C(/dev/sdc:/dev/xvdc:rwm))` in the line below.
(Same for the related options.)
A general remark: you should always use complete sentences. So this should end with a period.
I would remove this `device` here, and similar the option name in the examples for `device_read_bps`, `device_read_iops` etc.
2.6 or 2.7? Also you `requirements` listed here and the modules.
This method seems suspicious. It special-cases float, integer and decimal fields, but does nothing at all for other types of fields.
At a guess, this comes from this code: https://github.com/django/django/commit/e9103402c0fa873aea58a6a11dba510cd308cb84#diff-11 which was present before the converters refactor. This needs to use the same flow to get the converters as the normal queries do.
The expression should decide the output type. If the database generates something different, then Django needs to either convert the database value, or preferrably use different SQL for the backend to produce directly the correct value. What we need here is some machinery that decides what is the correct output type for field types A and B for given connector. The problem here is 3rd party fields. For example, how do we correctly guess the type of IntegerField / CustomCompositeField? In addition we need ability to produce different SQL per field type, per backend: TextField() + TextField() should produce || connected SQL. Finally we need the ability to ask the backend for a converter for the expression. These are non-trivial problems. I think we have to solve the situation in some simple way, and deal with the above mentioned problems later on.
FYI, in case you switch to working on whole config, `get_config.config` gives you the whole tree. (`get_config(:foo).config` is generally equivalent to `get_config.config[:foo]`.) Also, VMDB::Config is deprecated. May be better to use `::Settings.to_hash`.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
If you're just raising, you can skip the try/except since it's handled in the caller.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
the `f.close()` is implied as part of the with block. Also, I don't think this should be in the module since it looks like a debug statement.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Unlike an "app", I wouldn't say that a model is "installed". Two solutions: - check if the app is installed and adjust the error message accordingly: "app X isn't installed" or "app X doesn't provide model Y" - just rephrase do "which doesn't exist"
[...] remove matching rows **for rel_b**.
If I'm not completely mistaken this will fail if the attacker requests a page where `csrfmiddlewaretoken` or `HTTP_X_CSRFTOKEN` is shorter than the `CSRF_KEY_LENGTH`.
Single quotes please. Also, can we use `size` instead of `dims` for consistency with the other tests? ```suggestion size = images.get_image_dimensions('missing.png') self.assertEqual(size, (None, None)) ```
This looks like a bad-merge. The code has been changed since the first patch so we'll need to adjust.
Could use `assertSequenceEqual` to avoid the `itemgetter`
Instead of including a stacktrace in the message, better to put it into the exception field: ``` module.fail_json(msg="Xpath %s causes a failure: %s\n -- tree is %s" % (xpath, to_native(e), etree.tostring(tree, pretty_print=True)), exception=traceback.format_exc()) ```
Perhaps "Cannot convert DateField '%s' to DateTimeField." (I'm not certain it's an issue here, but we're favoring '%s' over %r until we drop Python 2 support to avoid possible u'' prefixes.)
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
chop blank line
Unlike an "app", I wouldn't say that a model is "installed". Two solutions: - check if the app is installed and adjust the error message accordingly: "app X isn't installed" or "app X doesn't provide model Y" - just rephrase do "which doesn't exist"
[...] remove matching rows **for rel_b**.
If I'm not completely mistaken this will fail if the attacker requests a page where `csrfmiddlewaretoken` or `HTTP_X_CSRFTOKEN` is shorter than the `CSRF_KEY_LENGTH`.
Single quotes please. Also, can we use `size` instead of `dims` for consistency with the other tests? ```suggestion size = images.get_image_dimensions('missing.png') self.assertEqual(size, (None, None)) ```
This looks like a bad-merge. The code has been changed since the first patch so we'll need to adjust.
Could use `assertSequenceEqual` to avoid the `itemgetter`
Instead of including a stacktrace in the message, better to put it into the exception field: ``` module.fail_json(msg="Xpath %s causes a failure: %s\n -- tree is %s" % (xpath, to_native(e), etree.tostring(tree, pretty_print=True)), exception=traceback.format_exc()) ```
Perhaps "Cannot convert DateField '%s' to DateTimeField." (I'm not certain it's an issue here, but we're favoring '%s' over %r until we drop Python 2 support to avoid possible u'' prefixes.)
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
chop blank line
The wording we usually use is "django.utils.translate.string_concat() is deprecated in favor of django.utils.text.format_lazy()."
It would be better to mention that in the release notes. :-)
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
Ah! Of course, sorry I missed that.
Won't this result in a confusing SQL-level error if you pass in `None` for `expression` or `pos` by accident? I'm assuming `length` is the only one we actually expect to possibly be `None`. If that's true, I think it would be better to do something like: ``` expressions = [expression, pos] if length is not None: expressions.append(length) ``` Or, if you prefer: `expressions = (expression, pos, length) if length is not None else (expression, pos)`
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
`test_jsonfield` looks good to me (since this is InspectDBTests, _introspection seems redundant).
Meanwhile, I merged a conflicting patch. I think you just need to remove the quotes from around the star.
flake8 doesn't like the hanging indent here.
I'm thinking these parameters shouldn't be filled in, since we're popping `stack_description` so if a user wanted a hard-fail for nonexistent stacks, they could do: ``` - cloudformation_facts: stack_name: not-real failed_when: not cloudformation['not-real'] ```
Also probably put that in the examples, for if users want that behavior but maybe don't know about `failed_when`.
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
camel2snake should indeed handle NotificationARNs properly (#25105)
Just checked, ec2_asg_facts is the only other module that references 'ar_ns', and that has code that copes either way.
Ooh, this is a nasty bug - if this wasn't in this PR,I wouldn't have spotted this. If this code happens elsewhere, it'll break when 2.4 goes out (because a now valid key is overwritten by the content of a now missing key)
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
The wording we usually use is "django.utils.translate.string_concat() is deprecated in favor of django.utils.text.format_lazy()."
It would be better to mention that in the release notes. :-)
Would it make sense to add a string version of this too? From a user's point of view, the name of the corresponding data type could be useful. GDAL Pixel data types in source: http://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4 ``` GDAL_PIXEL_TYPES = { 0: 'GDT_Unknown', # Unknown or unspecified type 1: 'GDT_Byte', # Eight bit unsigned integer 2: 'GDT_UInt16', # Sixteen bit unsigned integer 3: 'GDT_Int16', # Sixteen bit signed integer 4: 'GDT_UInt32', # Thirty two bit unsigned integer 5: 'GDT_Int32', # Thirty two bit signed integer 6: 'GDT_Float32', # Thirty two bit floating point 7: 'GDT_Float64', # Sixty four bit floating point 8: 'GDT_CInt16', # Complex Int16 9: 'GDT_CInt32', # Complex Int32 10: 'GDT_CFloat32', # Complex Float32 11: 'GDT_CFloat64' # Complex Float64 } ```
2.6 or 2.7? Also you `requirements` listed here and the modules.
I typically use something like the following: ```python return str(val).lower() in {'1', 't', 'y', 'on', 'true', 'yes'} ``` This works whether you pass a boolean, integer or string, although we are only expecting a string from the environment anyway. If we wanted to be strict, we should reject invalid values: ```python if str(val).lower() in {'1', 't', 'y', 'on', 'true', 'yes'}: return True if str(val).lower() in {'0', 'f', 'n', 'off', 'false', 'no'}: return False raise ValueError('Non-boolean string provided.') ```
This is missing other likely candidates: `y` and `yes` I'd also argue it should be simplified to be case-insensitive so that `TRUE` == `true` (which requires you actually assert the incoming `val` is actuall stringy enough to have `.lower()`, rather than simply doing an equality match against any of the values)
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
FYI, in case you switch to working on whole config, `get_config.config` gives you the whole tree. (`get_config(:foo).config` is generally equivalent to `get_config.config[:foo]`.) Also, VMDB::Config is deprecated. May be better to use `::Settings.to_hash`.
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
This can be single lined: ```python '%s\n%s' % (combined_list[prev], combined_list[k]), ```
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
That does make sense. Thanks.
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
If you're just raising, you can skip the try/except since it's handled in the caller.
the `f.close()` is implied as part of the with block. Also, I don't think this should be in the module since it looks like a debug statement.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Minor but I'd move this control flow block after the `weights` one to match the args order.
Probably want to use `**extra_context` added in 53361589905c7c07692cd77f398ce6cb5ac39779 rather than mutate `self`.
#19359 will take more time to discuss (so far no reaction at all) and it makes no sense to stall this pr on it. We can remove the examples for now and if we find we keep `IndexLookup` anyway add them again.
I opened an issue for better visibility of this discussion: https://github.com/elastic/elasticsearch/issues/19359
Hm. Not sure about the custom similarity part. For you maybe :) The whole purpose of IndexLookup was to have an easy to use api for access to term stats. I found it very convenient. Whether or not we should have IndexLookup is a different discussion because it can be used in other contexts too.
I don't think we should even have IndexLookup. It is no more work to write and use a custom similarity than to write and use a native script, and the point of similarity is for customizing exactly this.
I don't think we should be promoting using this feature of scripting. If someone wants this level of control over scores, they should implement their own Similarity; that is its purpose.
Docstring shouldn't have a trailing space. Docstring sentences should end with a period.
I was able to fix this locally by changing to `if r == self.payload:`. No idea if there's some case this doesn't work for though. At scale ansible might not be the right tool. However I have no desire to deal with adding another tool just for updating a single record.
Guess it's better to use `self.assertGreater(len(para), 0)` instead
```suggestion Set I(version=latest) to get the most recent version of a given image. ```
This shouldn't be automatically adjusted, just a note saying refs style must be 64.
Probably we need something similar for all the `module.run_command()` stuff. As I don't know if they fail gracefully by themselves. Easy to test though.
So we have to probably use try-except blocks for all the stuff that could fail. Nowadays with python3 you'd be doing something like: ```python from ansible.module_utils.pycompat24 import get_exception .... try: <whatever action that could fail> except: e = get_exception() module.fail_json(msg=str(e))
I am a bit scared by this. Because the module indicates it supports check-mode, but if I look at the code it seems to be doing all the stuff and only at the end appear to say there hasn't been a change. That cannot be right (it's dangerous!).
```suggestion - Must end with '.vhd'. - If no name is provided, defaults to the VM name + '.vhd'. ```
(In general, I don't think modules should have such options.)
s/will created/will be created/
This isn't really clear to me. Are the '*'s separating the valid values? Commas might be more clear.
resource group should be parsed from image.id, considering the list_all scenario.
I know you asked me about this but it might be a good idea to explain that PowerShell unpacks arrays if there's only a single entry so this is a defensive check on that.
I don't see any need for this attribute.
Seems like this should either be ```b_missing.append(b_path)``` or ```missing.append(path)```. I don't see missing being compared or combined with any other strings later, so it may be that the latter is fine. Then again, it may be confusing to have a single variable which contains non-byte paths so it might be more readable to use the former.
In the module args, you can set certain arguments as mutually exclusive so users don't specify them together. For this module I think resource_url should be exclusive with the _name, _location, and _type options.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
This fails for me: ``` AssertionError: <MagicMock name='WatchmanReloader()' id='140033332456584'> is not an instance of <class 'django.utils.autoreload.StatReloader'> ``` perhaps that's the failure that Jenkins encountered.
This should be on the top of the file.
Either use `result['changed'] = ` and remove the `changed =` OR use `changed =` and remove the `result['changed']` but do not use both. That applies to all `main()` function.
I know you asked me about this but it might be a good idea to explain that PowerShell unpacks arrays if there's only a single entry so this is a defensive check on that.
Seems like this should either be ```b_missing.append(b_path)``` or ```missing.append(path)```. I don't see missing being compared or combined with any other strings later, so it may be that the latter is fine. Then again, it may be confusing to have a single variable which contains non-byte paths so it might be more readable to use the former.
```suggestion b_opath = os.path.realpath(to_bytes(os.path.join(path, group))) ```
This does not work if you have datacenter nested inside a folder. For example, datacenter `DC0` is nested under folder `F0` and name does not contain `/` then this will always returns `F0` with no vm name in list.
We should probably do more here than just reraise the exception with a different type. Add a message here so it gives context about the failure. The same with the next one too.
In the module args, you can set certain arguments as mutually exclusive so users don't specify them together. For this module I think resource_url should be exclusive with the _name, _location, and _type options.
This fails for me: ``` AssertionError: <MagicMock name='WatchmanReloader()' id='140033332456584'> is not an instance of <class 'django.utils.autoreload.StatReloader'> ``` perhaps that's the failure that Jenkins encountered.
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
Returns -> Return use period
I know you asked me about this but it might be a good idea to explain that PowerShell unpacks arrays if there's only a single entry so this is a defensive check on that.
Seems like this should either be ```b_missing.append(b_path)``` or ```missing.append(path)```. I don't see missing being compared or combined with any other strings later, so it may be that the latter is fine. Then again, it may be confusing to have a single variable which contains non-byte paths so it might be more readable to use the former.
I think you can use `batch_objs` instead of `objs`, because we update only a single batch.
I think we can move `pks` into `self.filter()`, e.g. `self.filter(pk__in=[obj.pk for obj in batch_objs])`. I do not see much value in creating a temporary variable here.
This does not work if you have datacenter nested inside a folder. For example, datacenter `DC0` is nested under folder `F0` and name does not contain `/` then this will always returns `F0` with no vm name in list.
multiple dictionary access: instead of ```python if required_config.get('rotation', None): rotation = required_config['rotation'] ``` use: ```python rotation = required_config('rotation') if rotation is not None: # do your stuff ``` use this rule for all dictionary access below
In the module args, you can set certain arguments as mutually exclusive so users don't specify them together. For this module I think resource_url should be exclusive with the _name, _location, and _type options.
Pre-computing the updates by moving lines 501-513 before line 500 would reduce the length of a time the transaction is open for, which is a good thing for database throughput
No need for get(key, None) as None is the default fix also for following get()
This fails for me: ``` AssertionError: <MagicMock name='WatchmanReloader()' id='140033332456584'> is not an instance of <class 'django.utils.autoreload.StatReloader'> ``` perhaps that's the failure that Jenkins encountered.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
These parens aren't necessary for unpacking the return values.
This isn't needed, since we now only support 2.6+ anyway.
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
the `f.close()` is implied as part of the with block. Also, I don't think this should be in the module since it looks like a debug statement.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
These parens aren't necessary for unpacking the return values.
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
This isn't needed, since we now only support 2.6+ anyway.
the `f.close()` is implied as part of the with block. Also, I don't think this should be in the module since it looks like a debug statement.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
These parens aren't necessary for unpacking the return values.
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
This isn't needed, since we now only support 2.6+ anyway.
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
the `f.close()` is implied as part of the with block. Also, I don't think this should be in the module since it looks like a debug statement.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
Not something that you have to change but I think this could be simplified to ```python for obj_func in self.obj_func_mapping.values(): obj_func() ```
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
I think this might explode if `obj_type` is not one of `functions`, `indexes` or `tables`, maybe it would be safer to do something like? ```suggestion obj_func = self.obj_func_mapping.get(obj_type.strip()) if obj_func is not None: obj_func() else: module.warn("Unknown filter option '{}'".filter(obj_type.strip()) ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
These parens aren't necessary for unpacking the return values.
This isn't needed, since we now only support 2.6+ anyway.
Why not also do this when `module.check_mode` is `True`? Then the check mode behavior is closer to the real thing.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
These parens aren't necessary for unpacking the return values.
This isn't needed, since we now only support 2.6+ anyway.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
Hmm, I thought we'd have `LATEST_0_10_1` and `LATEST_0_10_0` instead of `LATEST_0_10` (which doesn't make sense because we have multiple feature releases in the `0_10` line.
Nit: maybe there should be no default for `should_fail` since we always pass a value.
AFAIK no timestamp type defaults to CreateTime, so I think this test and the test below will test the same thing. But need @becketqin to confirm this.
I'd personally expand these to `kafka1` and `kafka2` as I reserve single character varnames (or variants with numbers appended) to indexes, but that's very much a nit.
In another PR (https://github.com/apache/kafka/pull/563/files), Rajini is claiming that this doesn't work as expected. @granders is verifying that claim and we may want to update this based on the outcome of that investigation.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
netrics => metrics
Calling `super(...)` makes `test_context` a field in the test object. I.e. no need for `self.context = test_context`
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
Flesh out the docstring here - it's important to have good docstrings in these test methods since the docstrings are actually propagated up to the test report as the test description.
Nit: maybe there should be no default for `should_fail` since we always pass a value.
AFAIK no timestamp type defaults to CreateTime, so I think this test and the test below will test the same thing. But need @becketqin to confirm this.
I'd personally expand these to `kafka1` and `kafka2` as I reserve single character varnames (or variants with numbers appended) to indexes, but that's very much a nit.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
In another PR (https://github.com/apache/kafka/pull/563/files), Rajini is claiming that this doesn't work as expected. @granders is verifying that claim and we may want to update this based on the outcome of that investigation.
netrics => metrics
Calling `super(...)` makes `test_context` a field in the test object. I.e. no need for `self.context = test_context`
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
Flesh out the docstring here - it's important to have good docstrings in these test methods since the docstrings are actually propagated up to the test report as the test description.
like above, no need for security protocol here
the `f.close()` is implied as part of the with block. Also, I don't think this should be in the module since it looks like a debug statement.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
I think this might explode if `obj_type` is not one of `functions`, `indexes` or `tables`, maybe it would be safer to do something like? ```suggestion obj_func = self.obj_func_mapping.get(obj_type.strip()) if obj_func is not None: obj_func() else: module.warn("Unknown filter option '{}'".filter(obj_type.strip()) ```
Not something that you have to change but I think this could be simplified to ```python for obj_func in self.obj_func_mapping.values(): obj_func() ```
These parens aren't necessary for unpacking the return values.
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
This isn't needed, since we now only support 2.6+ anyway.
I'd personally expand these to `kafka1` and `kafka2` as I reserve single character varnames (or variants with numbers appended) to indexes, but that's very much a nit.
AFAIK no timestamp type defaults to CreateTime, so I think this test and the test below will test the same thing. But need @becketqin to confirm this.
Shouldn't need this line, it's handled by the superclass's constructor.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
In another PR (https://github.com/apache/kafka/pull/563/files), Rajini is claiming that this doesn't work as expected. @granders is verifying that claim and we may want to update this based on the outcome of that investigation.
netrics => metrics
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
Calling `super(...)` makes `test_context` a field in the test object. I.e. no need for `self.context = test_context`
Flesh out the docstring here - it's important to have good docstrings in these test methods since the docstrings are actually propagated up to the test report as the test description.
Minor style point: I'd probably put as a method inside ZK, but that's a bit subjective really.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
These parens aren't necessary for unpacking the return values.
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
This isn't needed, since we now only support 2.6+ anyway.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
These parens aren't necessary for unpacking the return values.
This isn't needed, since we now only support 2.6+ anyway.
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
These parens aren't necessary for unpacking the return values.
This isn't needed, since we now only support 2.6+ anyway.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
it seems this and other parameters are missing from docs
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
I think it would be better to use `MiqServer.my_server.add_settings_for_resource(Ã¢Â€Â¦)` here. This will queue a settings reload for all workers running on the target server after save.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
This should be wrapped like this: ``` state=dict( required=False, default="present", choices=['present', 'absent']) ```
These parens aren't necessary for unpacking the return values.
I guess it's best to remove the version number here (and in all the other copies of the config). Not sure whether that should happen in this PR though :)
I don't know anything about this file or this part of the docs build but Ansible 2.2 seems really old
This isn't needed, since we now only support 2.6+ anyway.
it seems this and other parameters are missing from docs
I thought we had these in a GCP docs fragment, but now I'm not able to find it.
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
Remove extra whitespaces: ```suggestion class. If provided, tests will be shuffled within each `classes` group, but keeping tests with other tests of their TestCase class. Reversing is ```
These parens aren't necessary for unpacking the return values.
This isn't needed, since we now only support 2.6+ anyway.
This feels a lot like reimplementing argument spec validation methods inside a module. I would advise against this. > We need raw user input so that we can distinguish between cases when (a) a user explicitly assigns a value to an option and (b) a value gets assigned because of default value for the option. I don't fully understand this need. The module should not care if a user provided the value or if it came from the default setting in the argument spec. It should only care about the value. Treating the value differently based on where it was set, particularly if the provided value is the same as the default value, seems overly complicated.
I think the standard way to do this (i.e. what all other modules do) is to not specify a default value for that option, and use `None` as "was not specified by user". I don't know how the core team sees this. If modules should be allowed to know where a value comes from (user, default value, environment, ...), I think there should be a common mechanism which can be used by all modules.
If `name` is an alias of `display_name`, and the user sets `name: xxx`, `params['display_name']` will be `'xxx'`. Ansible does that internally so no module has to do that.
The aliases should be available via the standard name. I'm not sure what this function is for ie ```python display_name=dict(type="str", required=False, aliases=["name"]), foo = module.params["display_name"] # foo would be set to the what's passed in as `display_name` or `name` ```
AFAIK no timestamp type defaults to CreateTime, so I think this test and the test below will test the same thing. But need @becketqin to confirm this.
Good call. I've confirmed that if we leave this as-is, the topic creation command leaves out `min.insync.replicas` We want `"min.insync.replicas": 2` -> `'configs': {'min.insync.replicas': 2}`
In another PR (https://github.com/apache/kafka/pull/563/files), Rajini is claiming that this doesn't work as expected. @granders is verifying that claim and we may want to update this based on the outcome of that investigation.
I think you want: `wait_until(lambda: self.producer.num_acked == MAX_MESSAGES, ...)` since if you specify `max_messages`, the verifiable producer will produce at most `max_messages` In the current form, I think it is guaranteed to time out
netrics => metrics
Calling `super(...)` makes `test_context` a field in the test object. I.e. no need for `self.context = test_context`
Flesh out the docstring here - it's important to have good docstrings in these test methods since the docstrings are actually propagated up to the test report as the test description.
like above, no need for security protocol here
Minor style point: I'd probably put as a method inside ZK, but that's a bit subjective really.
We can ditch this logic block since it is not relevant to your test - it was a very specific check on the correctness of the ConsoleConsumer start command to ensure the log4j settings were correct.
Basically `os.path.isfile(metadata_filename)` is superfluous here since we control the file lifetime on our own and since we don't delete the file it should exist. This check may only fail if someone touched our file that is unexpected scenario that normally should not happen. In such cases we should stop right at failed `os.remove` rather than skipping such unexpected outcome with this check. If someone touches our files then it's definitely wrong and we should not continue.
Exception is raised if the file can't be created or opened. You'll never reach here in this case.
What is this supposed to achieve? Modules are cached on import, they'll be picked up from that cache regardless of the pointers being listed in `sys.modules`.
> They can't be multiline, can they? Yep. According to [ECMA 262 5.1](http://www.ecma-international.org/ecma-262/5.1/), CR (U+000D), LF (U+000A), LS (U+2028) and PS (U+2029) are not allowed in RegExp literals
``` >>> re.match(r'/(?=[^*])[^/\n]*/[gimy]{0,4}', r'''/\/\/\//''') <_sre.SRE_Match object; span=(0, 3), match='/\\/'> ```
Function calls are complex. For example: ``` from youtube_dl.jsinterp import JSInterpreter jsi = JSInterpreter(''' function a(x) { return x; } function b(x) { return x; } function c() { return [a, b][0](0); } ''') print(jsi.call_function('c')) ```
We didn't change anything in building frames so it's enough to check that frames are not empty.
I think you can use `with self.assertRaisesMessage` equivalently here (context manager form is much easier to read IMO)
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
Why don't you just encode suffix like `u'-\u00c5\u00d1\u015a\u00cc\u03b2\u0141\u00c8'.encode('utf-8')` and for the no_temp_unicode override just do `suffix = b'-ansible'`? No need for this check which is arguably more confusing.
These last four lines are duplicated in both conditions, should therefore come after the if block.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
don't use print, display.error or raise an AnsibleError exception, Ansible already does not fail due to an Exception from a callback event.
okay, but would be helpful to say _why_ we need to always return True.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
You are right! Fixed it in https://github.com/mheap/ansible/pull/3.
Here, `self.count_upgrade` is an int, and `outdated` (as above) a `dict` resp. `list`.
Again, this only works on the primary credential cache. If the ticket is in another, this might not work.
I would actually assume most of the users will not be using ID but name. Don't think it matters that much though.
This can be single lined.
This line is related to the `save` method, and should sit beneath it, rather than the new `get_updated_model` method.
This property should probably also be `_` prefixed- the way the aliases are encoded in here right now makes it pretty useless for anything but generating that string in the error case.
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
okay, but would be helpful to say _why_ we need to always return True.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
use /latest/ instead of version specific, since we will eventually archive docs for older versions
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
to_text and u prefix on string.
Display should use text strings, will need a to_text in the format and u prefix the string so we don't have any ASCII encoding errors on Python 2.
Please change this to a guard expression, just like the one above: ```python if group_data is None: return for key, data in group_data.items(): # it's one tab less now ^_^ ``` It helps to keep nesting low and style would be more consistent.
Maybe add "ansible versions below 2.10" or something so it's clear this is a one-time problem, not that they can never upgrade `ansible` again...
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
I think the interfaces should be the same. It's probably the right (more forgiving) behaviour for a response, which isn't a dict. Folks will `s/response[/response.headers[/` not the least. But the docs also say this is the behaviour.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
It doesn't make sense to turn a unicode string into a unicode string. It's just `str(str(''))`, which is probably not what you want. Maybe you wanted to turn bytes into text here? In such case check'd be against `binary_type` and the translation should use `to_text()` shim.
I'm on the fence about holding this up to try and come up with something clever that seems like we know what the future will hold. With this being the only use case now, it's hard to know what another warning could look like in the future, and how we might need to represent that.
Maybe just `_` prefix `warnings`on `ValidationResult` for now? This definitely doesn't seem like it'd be generally useful as-is, and we'd almost certainly have to restructure it later to create actual warning objects if we have more than one (rather than a list of dicts).
I was also confused by this logic for a while and I got an optimization idea from the [Raft dissertation](http://wcl.cs.rpi.edu/pilots/library/papers/consensus/RAFTOngaroPhD.pdf) about this, The sentence below is taken from section "5.4.2 Committing entries from previous terms": ``` There are some situations where a leader could safely conclude that an older log entry is committed (for example, if that entry is stored on every server), but Raft takes a more conservative approach for simplicity. ``` so we can also update commitIndex(highWatermark) if logEndOffset of all followers have passed the highWatermark. I don't think this is a good idea since it makes the logic opaque but will not necessarily really optimize any performance, so I just mention it here and feel free to ignore it Ã°ÂŸÂ™Âˆ.
```suggestion * 5) {@link FetchSnapshotRequestData}: Sent by the follower to the epoch leader in order to fetch a snapshot. * This happens when a FetchResponse includes a snapshot ID due to the follower's log end offset being less * than the leader's log start offset. This API is similar to the Fetch API since the snapshot is stored * as FileRecords, but we use {@link UnalignedRecords} in FetchSnapshotResponse because the records * are not necessarily offset-aligned. ```
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
chop extra space after period
pageParams is missing from the equality check
Use another lookup instead of `epoch` e.g. `second`.
```suggestion Test that the returned value for timezone consists of only uppercase ```
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
Thanks for that note ewen. I learned something!
+1 for consistency
Thanks. This PR is assigned to the 2.12 milestone, which we'll be going through after creating the stable-2.11 branch (which will be done when 2.11 RC1 comes out).
There is no point to use `remove_start` since line is always a string.
I removed it.
Ahh true, sorry for the noise. No changes are required.
I think that this part can be dropped: ```diff diff --git a/tests/db_functions/test_datetime.py b/tests/db_functions/test_datetime.py index 51dbcb6..3560a76 100644 --- a/tests/db_functions/test_datetime.py +++ b/tests/db_functions/test_datetime.py @@ -211,12 +211,6 @@ class DateFunctionTests(TestCase): self.create_model(end_datetime, start_datetime) self.assertQuerysetEqual( - DTModel.objects.annotate(extracted=Extract('duration', 'epoch')).order_by('start_datetime'), - [(start_datetime, int((end_datetime - start_datetime).total_seconds())), - (end_datetime, int((start_datetime - end_datetime).total_seconds()))], - lambda m: (m.start_datetime, m.extracted) - ) - self.assertQuerysetEqual( ```
Single line looks fine here. For consistency with other code, I'd pass`epoch_seconds` as an arg.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Use another lookup instead of `epoch` e.g. `second`.
Please use a single quote.
This doesn't allow the caller to cancel downtime by scope. _get_downtime returns an empty dictionary when id is not specified. It'd be great if this was changed to require id or scope instead of just id.
check here for the regex fix https://github.com/Qalthos/ansible/pull/5/commits/89f2a106db4f7296822e118ad24f66cea84f3be9#diff-e72efaa6b72beb5339ad847e21a31220L87 Good catch on the first_port / last_port ... will add a fix for those. This is a common pattern to be used by eos modules. I have held off on pushing the modules upstream until we get the initial push of the refactored network shared modules
I believe you would need to add a custom `save_form` method to a `ModelAdmin` and somehow incorporate the `change` flag in it -- perhaps modify the form's cleaned_data to assign the field to a model field before save.
I would handle this in `as_sql()`, i.e. ```python def as_sql(self, compiler, connection, template=None, **extra_context): sql, params = super().as_sql(compiler, connection, template, **extra_context) if self.invert: sql = '!!({})'.format(sql) return sql, params ```
Tests for `formset_factory()` and `formset_factory()` are missing.
`copy()` in unnecessary.
`_combine()` is unnecessary since its introduction, I moved it to a separate commit b62c58d5fccaeedc7bc4a064b4678e4ed1e77581.
chop "one of" add comma before "or"
We can keep `invert` directly in the `SearchQuery` instead of `extra`, e.g. ```python ... self.invert = invert super().__init__(*expressions, output_field=output_field, function=function) ```
No need to specify `choices`
I think this should be false (not a string)
Ditto. ```suggestion # Check empty result with non-execution optimization. with self.assertNumQueries(0): ```
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Ditto. ```suggestion # Check empty result with non-execution optimization. with self.assertNumQueries(0): ```
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
Ok I understand that you have to determine if it's either a field reference or an explicit value but I think you should do it the other way around. That is accept both `six.text_type` and `six.integer_types` and consider `six.text_type` as field references (`F()`) and everything else as an explicit value (`Value()`).
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
@coldmind I think @charettes was suggesting change the original `_parse_expressions` to differentiate between strings and numbers, and I think that's a great idea. Rather than define the method above on `StatFunc`, we should change the method directly on `Func` which will then propagate to StatFunc through inheritance. It may be a little magical, but the rules can be defined quite clearly. "string is a field ref", "resolve_expression passed verbatim", everything else is a "Value(value)". I like this enough to write a patch today for inclusion in 1.8beta - and update all the applicable tests. Will land it today hopefully, and then we can rebase this PR here.
```None if 'globals' not in test else test['globals']``` => ```test.get('globals')```
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
Maybe these functions should go into `module_utils/crypto`? Seem useful for CSRs and certificates (maybe even SSH keys?) too.
Ditto. ```suggestion # Check empty result with non-execution optimization. with self.assertNumQueries(0): ```
Ditto. ```suggestion # Check empty result when query must be executed. with self.assertNumQueries(1): ```
is the `any` required here - `if metrics_to_disable:` should suffice, I'd think
The problem with this line is that further down you're referencing `self.x` and `self.y` directly. If another part of the code calls `set_source_expressions`, you're going to be using old values. You should override get/set_source_expressions to read from and assign to self.x, self.y.
This as_sql method can be deleted if you remove the custom template and implement get/set_source_expressions as above.
Ok I get it, `compiler.compile(Value(5))` would return something like `'%d', (5,)`.
I think this would probably cause more confusion than it's worth. I'd be in favour of not supporting default_alias for new aggregates, and requiring users to supply their own alias. I'm biased against default_alias though, so I won't veto. Interested in what others thing.
``` .aggregate(regr=RegrSYY('col_a', 'col_b')) # vs .aggregate(RegrSYY('col_a', 'col_b')) -> col_a_col_b_regrsyy ``` It's a long and unwieldy default alias. You also have to be careful about the column name length in this situation. Oracle only supports names up to 30 chars, which means both column names need to be 10 chars or less to account for the extra bits of the underscores and aggregate name. I don't see the value.
The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
Thanks for that note ewen. I learned something!
This line is related to the `save` method, and should sit beneath it, rather than the new `get_updated_model` method.
`parallell` => `parallel` `get_upodated_models` => `get_updated_models`
`update_fields` can be any iterable, `iter({}) -> dict_keyiterator` ```suggestion obj.save(using=self.db, update_fields=defaults) ```
Arf, this is also not optimal either. `pre_save` can have side-effects, like `django.db.models.fields.files.FileField.pre_save` does ðŸ˜• We probably don't want to trigger those here. I mean, serendipitously it would work for the `FileField` because even if the returned value is still the same (so we don't add the `field.name` to `updated_fields`), we actually triggered the side-effect committing the file ðŸ˜‚ However, that seems pretty brittle ðŸ˜… I'm not sure what the cleanest/Djangoest approach would be here ðŸ¤” We could add an attribute on the Field class, like `Field.has_pre_save: bool`, but that creates a precedent and users/libs must update their code accordingly. But at least, we would know _for sure_ which fields need to be added and which don't. Any other suggestion is very welcome!
Maybe :thinking: ```suggestion if not ( field.primary_key or field.__class__.pre_save is Field.pre_save ): ``` according to de Morgan's laws.
I tried a similar approach while working on acfaec3db5ba39de52f6e607e74343dccf72fba1 and came to the conclusion that this approach can't work (due to something like module caching). As far as I know, you'll have to register the admin to a separate `AdminSite`.
I think there isn't much organization there. Using an existing site should be fine.
Shouldn't `w[0].message` be `w[0].category`? You could also deindent this line.
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
First we should verify this passes before we toggle `is_active` to False.
This line is related to the `save` method, and should sit beneath it, rather than the new `get_updated_model` method.
`parallell` => `parallel` `get_upodated_models` => `get_updated_models`
`update_fields` can be any iterable, `iter({}) -> dict_keyiterator` ```suggestion obj.save(using=self.db, update_fields=defaults) ```
Arf, this is also not optimal either. `pre_save` can have side-effects, like `django.db.models.fields.files.FileField.pre_save` does ðŸ˜• We probably don't want to trigger those here. I mean, serendipitously it would work for the `FileField` because even if the returned value is still the same (so we don't add the `field.name` to `updated_fields`), we actually triggered the side-effect committing the file ðŸ˜‚ However, that seems pretty brittle ðŸ˜… I'm not sure what the cleanest/Djangoest approach would be here ðŸ¤” We could add an attribute on the Field class, like `Field.has_pre_save: bool`, but that creates a precedent and users/libs must update their code accordingly. But at least, we would know _for sure_ which fields need to be added and which don't. Any other suggestion is very welcome!
Maybe :thinking: ```suggestion if not ( field.primary_key or field.__class__.pre_save is Field.pre_save ): ``` according to de Morgan's laws.
I tried a similar approach while working on acfaec3db5ba39de52f6e607e74343dccf72fba1 and came to the conclusion that this approach can't work (due to something like module caching). As far as I know, you'll have to register the admin to a separate `AdminSite`.
I think there isn't much organization there. Using an existing site should be fine.
Shouldn't `w[0].message` be `w[0].category`? You could also deindent this line.
Is there a way to format the stdout? When applying it gets hard to read: ``` "sdtout": "aws_instance.example: Creating...\n ami: \"\" => \"ami-ff4ea59d\"\n associate_public_ip_address: \"\" => \"<computed>\"\n availability_zone: \"\" => \"<computed>\"\n ebs_block_device.#: \"\" => \"<computed>\"\n ephemeral_block_device.#: \"\" => \"<computed>\"\n instance_state: \"\" => \"<computed>\"\n instance_type: \"\" => \"t2.micro\"\n ipv6_address_count: \"\" => \"<computed>\"\n ipv6_addresses.#: \"\" => \"<computed>\"\n key_name: \"\" => \"<computed>\"\n network_interface.#: \"\" => \"<computed>\"\n network_interface_id: \"\" => \"<computed>\"\n placement_group: \"\" => \"<computed>\"\n primary_network_interface_id: \"\" => \"<computed>\"\n private_dns: \"\" => \"<computed>\"\n private_ip: \"\" => \"<computed>\"\n public_dns: \"\" => \"<computed>\"\n public_ip: \"\" => \"<computed>\"\n root_block_device.#: \"\" => \"<computed>\"\n security_groups.#: \"\" => \"<computed>\"\n source_dest_check: \"\" => \"true\"\n subnet_id: \"\" => \"<computed>\"\n tenancy: \"\" => \"<computed>\"\n volume_tags.%: \"\" => \"<computed>\"\n vpc_security_group_ids.#: \"\" => \"<computed>\"\naws_instance.example: Still creating... (10s elapsed)\naws_instance.example: Still creating... (20s elapsed)\naws_instance.example: Still creating... (30s elapsed)\naws_instance.example: Creation complete after 34s (ID: i-04362e7210c113e2c)\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n" ```
First we should verify this passes before we toggle `is_active` to False.
Why double-underscore? I would also rename `func` to `callback`: ```suggestion for _, callback in connections[using].run_on_commit[start_count:]: callbacks.append(callback) if execute: callback() ```
This should work: ```suggestion transaction.on_commit(self.enqueue_callback) ```
Maybe I'm missing sth, but can we use `@contextmanager`? ```python class TestCase(TransactionTestCase): ... @contextmanager def captureOnCommitCallbacks(self, *, using=DEFAULT_DB_ALIAS, execute=False): self.callbacks = [] start_count = len(connections[using].run_on_commit) try: yield self.callbacks finally: run_on_commit = connections[using].run_on_commit[start_count:] self.callbacks[:] = [func for sids, func in run_on_commit] if execute: for callback in self.callbacks: callback() ```
`itertools` looks unnecessary :thinking: I would use `int` variables and alphabetized hooks, e.g. ```python branch_1_call_counter = 0 branch_2_call_counter = 0 leaf_1_call_counter = 0 leaf_2_call_counter = 0 leaf_3_call_counter = 0 def leaf_1(): nonlocal leaf_1_call_counter leaf_1_call_counter += 1 def leaf_2(): nonlocal leaf_2_call_counter leaf_2_call_counter += 1 def leaf_3(): nonlocal leaf_3_call_counter leaf_3_call_counter += 1 def branch_1(): nonlocal branch_1_call_counter branch_1_call_counter += 1 transaction.on_commit(branch_2) transaction.on_commit(leaf_3) def branch_2(): nonlocal branch_2_call_counter branch_2_call_counter += 1 transaction.on_commit(leaf_1) transaction.on_commit(leaf_2) ```
Yes, `nonlocal` is necessary.
Argh, I just realized the PluginLoader attribute stuffing (update_object) has been happening on `class_only` cases, which is broken for "same name, different behavior" cases (since subsequent loads will overwrite the class vars on the existing type object, not instance vars). It works here, since `all` is `yield`ing the plugin class immediately after calling `update_object` on it, but the whole pattern is busted for `class_only`, esp for things that might consult the class var during or after `__init__`. We've needed to rethink that for awhile, and this kinda sets it in a little more concrete. :(
```suggestion A visualization of the callback tree tested. Each node is expected to be visited only once: â””â”€branch_1 â”œâ”€branch_2 â”‚ â”œâ”€leaf_1 â”‚ â””â”€leaf_2 â””â”€leaf_3 ```
```suggestion # try to get collection world name first ```
This implies that constructing some callbacks doesn't result in an instance, but also doesn't raise an Exception? That doesn't sound right...
Are we doing singleton `__new__`/`__call__` magic internally on some of the callback plugin types? Seems like this would never be `False` otherwise...
Why double-underscore? I would also rename `func` to `callback`: ```suggestion for _, callback in connections[using].run_on_commit[start_count:]: callbacks.append(callback) if execute: callback() ```
This should work: ```suggestion transaction.on_commit(self.enqueue_callback) ```
Maybe I'm missing sth, but can we use `@contextmanager`? ```python class TestCase(TransactionTestCase): ... @contextmanager def captureOnCommitCallbacks(self, *, using=DEFAULT_DB_ALIAS, execute=False): self.callbacks = [] start_count = len(connections[using].run_on_commit) try: yield self.callbacks finally: run_on_commit = connections[using].run_on_commit[start_count:] self.callbacks[:] = [func for sids, func in run_on_commit] if execute: for callback in self.callbacks: callback() ```
`itertools` looks unnecessary :thinking: I would use `int` variables and alphabetized hooks, e.g. ```python branch_1_call_counter = 0 branch_2_call_counter = 0 leaf_1_call_counter = 0 leaf_2_call_counter = 0 leaf_3_call_counter = 0 def leaf_1(): nonlocal leaf_1_call_counter leaf_1_call_counter += 1 def leaf_2(): nonlocal leaf_2_call_counter leaf_2_call_counter += 1 def leaf_3(): nonlocal leaf_3_call_counter leaf_3_call_counter += 1 def branch_1(): nonlocal branch_1_call_counter branch_1_call_counter += 1 transaction.on_commit(branch_2) transaction.on_commit(leaf_3) def branch_2(): nonlocal branch_2_call_counter branch_2_call_counter += 1 transaction.on_commit(leaf_1) transaction.on_commit(leaf_2) ```
Yes, `nonlocal` is necessary.
Argh, I just realized the PluginLoader attribute stuffing (update_object) has been happening on `class_only` cases, which is broken for "same name, different behavior" cases (since subsequent loads will overwrite the class vars on the existing type object, not instance vars). It works here, since `all` is `yield`ing the plugin class immediately after calling `update_object` on it, but the whole pattern is busted for `class_only`, esp for things that might consult the class var during or after `__init__`. We've needed to rethink that for awhile, and this kinda sets it in a little more concrete. :(
```suggestion A visualization of the callback tree tested. Each node is expected to be visited only once: â””â”€branch_1 â”œâ”€branch_2 â”‚ â”œâ”€leaf_1 â”‚ â””â”€leaf_2 â””â”€leaf_3 ```
```suggestion # try to get collection world name first ```
This implies that constructing some callbacks doesn't result in an instance, but also doesn't raise an Exception? That doesn't sound right...
Are we doing singleton `__new__`/`__call__` magic internally on some of the callback plugin types? Seems like this would never be `False` otherwise...
should this be super()
it's a separate item, but I wonder if we could patch override_settings to handle DATABASE_ROUTERS like is done below
preferred format is "#15346, #15573 - Issue description"
oh, I like the idea of this being simple enough to not require more changes. I'm a little concerned about whether this swapping here could potentially cause circular loops. I think the reduction operations currently assume that things will always be added to the front of the operation list, so keeping that consistent might prevent weird optimizer loops like `[A, B] -> [B, A] -> [A, B] -> [B, A]`. I don't think that affects the core reasoning here
This function is quite big, and it does a lot of things. Might be better to split to to smaller functions, like we do in our other modules.
chop extra space after period
FYI, in case you switch to working on whole config, `get_config.config` gives you the whole tree. (`get_config(:foo).config` is generally equivalent to `get_config.config[:foo]`.) Also, VMDB::Config is deprecated. May be better to use `::Settings.to_hash`.
It'll look more clean if you name these `returncode`, `out`, and `_` (the underscore denotes you're not going to use that, and it's a common pattern in Python). You can also drop the parenthesis, they're not needed
If we do go with this approach, I feel like `VMDB_PATH` should be user-configurable
I'm worried about Ruby quoting. If any setting contains a single quote, this will break. What I suggest is passing via ARGV. `rails r` leaves in ARGV any args following the ruby fragment, e.g. this works: ``` # The \' escaping below is only to type this via shell; # run_command() takes an array of strings and I think this way could pass arbitrary JSON with no other escaping at all. rails r 'puts MiqServer.my_server.set_config(JSON.parse(ARGV.last))' '{"goodbye": "Qapla\', Worf"}' ```
Since Ansible subscribes to unicode sandwich, this should be the callers' (actually, where the data enters the system) responsibility.
Such an extensive docstring is not necessary, IMO.
Would there be any utility to sorted() on files/sub_folders? AFAICT, os.walk will use the os.listdir() results directly for filenames. os.listdir() is doc'ed as 'arbitrary order' (iirc, more or less up to fs). Wondering if sorting those results would make for more reproducible behavior when trying to replicate errors. It shouldn't matter at all if there are no bugs, but it could make tracking down odd failures easier.
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
2.6 or 2.7? Also you `requirements` listed here and the modules.
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
chop extra space after period
```suggestion description: Action returns an C(ansible_facts) dictionary that will update existing host facts ```
```suggestion - Conditionals will work as if C(run_once) is being used, variables used will be from the first available host ```
You might append this to the `using` sql passed to `super()` to fix the test failure on Jenkins (due to usage of TABLESPACE).
a OrderedSet, like @timgraham suggested
Such an extensive docstring is not necessary, IMO.
2.6 or 2.7? Also you `requirements` listed here and the modules.
Would there be any utility to sorted() on files/sub_folders? AFAICT, os.walk will use the os.listdir() results directly for filenames. os.listdir() is doc'ed as 'arbitrary order' (iirc, more or less up to fs). Wondering if sorting those results would make for more reproducible behavior when trying to replicate errors. It shouldn't matter at all if there are no bugs, but it could make tracking down odd failures easier.
Why this variable is called `unnamed_...` since it also stores named group? :thinking:
We don't need to add `named` branches everywhere. I'd return `m` in the tuple and use it when needed. Moreover, we could use a generator instead of list. IMO, `_find_group()` is unnecessarily complicated and unreadable. I'd move `get_group_start_and_end_indices()` to a module hook `_get_group_start_end()`: ```python def _get_group_start_end(start, end, pattern): unmatched_open_brackets, prev_char = 1, None for idx, val in enumerate(pattern[end:]): if val == '(' and prev_char != '\\': unmatched_open_brackets += 1 elif val == ')' and prev_char != '\\': unmatched_open_brackets -= 1 prev_char = val if unmatched_open_brackets == 0: return start, (end + idx + 1) ``` and simplify `_find_groups()` to a few lines generator: ```python def _find_groups(pattern, group_matcher): prev_end = None for m in group_matcher.finditer(pattern): if indices := _get_group_start_end(m.start(0), m.end(0), pattern): start, end = indices if prev_end and start > prev_end or not prev_end: yield start, end, m prev_end = end ``` now you can use `m` when needed, e.g.: ```python def replace_named_groups(pattern): group_pattern_and_name = [ (pattern[start:end], match[1]) for start, end, match in _find_groups(pattern, named_group_matcher) ] for group_pattern, group_name in group_pattern_and_name: pattern = pattern.replace(group_pattern, group_name) return pattern ... def replace_unnamed_groups(pattern): group_start_end_indices = _find_groups(pattern, unnamed_group_matcher) final_pattern, prev_end = '', None for start, end, _ in group_start_end_indices: if prev_end: final_pattern += pattern[prev_end:start] final_pattern += pattern[:start] + '<var>' prev_end = end return final_pattern + pattern[prev_end:] ... ```
chop extra space after period
Use hanging indentation as described in [Python coding style](https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/coding-style/#python-style).
Python 2.6 needs {0}, {1} - empty brackets in various other places too.
This doesn't need to be quoted.
This should also start on the line above if the other is moved.
The same like above.
What kind of values are valid here? I would expect this to be `float` based on the `0.0` in the description.
A general remark: you should always use complete sentences. So this should end with a period.
Would there be any utility to sorted() on files/sub_folders? AFAICT, os.walk will use the os.listdir() results directly for filenames. os.listdir() is doc'ed as 'arbitrary order' (iirc, more or less up to fs). Wondering if sorting those results would make for more reproducible behavior when trying to replicate errors. It shouldn't matter at all if there are no bugs, but it could make tracking down odd failures easier.
```suggestion Test that the returned value for timezone consists of only uppercase ```
Is there precisely one time you can specify this? It sounds to me like this should be allowed to repeat multiple times. So this should be a `list`. Or maybe even better a `dict`, so people can specify things like ``` blkio_weight_device: /dev/sda: 100 /dev/sdb: 200 ```
You should emphasize that the module can and will not do any idempotence checking for this.
(In general, I don't think modules should have such options.)
This doesn't need to be quoted.
What kind of values are valid here? I would expect this to be `float` based on the `0.0` in the description.
I would remove all aliases if possible.
You should provide an example. Also, I would expect this to be of type `dict`, not `str`.
(Same for the related options.)
A general remark: you should always use complete sentences. So this should end with a period.
Please use formatting like `C(<device-on-host>[:<device-on-container>][:<permissions>])`, and `(e.g. device C(/dev/sdc:/dev/xvdc:rwm))` in the line below.
This should really be: ``` - Set custom DNS search domains. (Use I(dns_search) with C('') if you don't wish to set the search domain.) ```
I would remove this `device` here, and similar the option name in the examples for `device_read_bps`, `device_read_iops` etc.
Please turn this into a `dict`. Strings are a really horrible format for complex data such as this. Also, is there precisely one mount available per container? If podman is really trying to be compatible to docker, I cannot believe that.
